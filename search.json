[{"path":"/2024/04/05/linux-docs/设备驱动/Linux 设备和驱动的相遇/","content":"本文结合设备信息集合的详细讲解来认识一下设备和驱动是如何绑定的。所谓设备信息集合，就是根据不同的外设寻找各自的外设信息，我们知道一个完整的开发板有 CPU 和各种控制器（如 I2C 控制器、SPI 控制器、DMA 控制器等），CPU 和控制器可以统称为 SOC，除此之外还有各种外设 IP，如 LCD、HDMI、SD、CAMERA 等，如下图： 我们看到一个开发板有很多的设备，这些设备是如何一层一层展开的呢？设备和驱动又是如何绑定的呢？我们带着这些疑问进入本节的主题。 各级设备的展开内核启动的时候是一层一层展开地去寻找设备，设备树之所以叫设备树也是因为设备在内核中的结构就像树一样，从根部一层一层的向外展开，为了更形象的理解来看一张图： 大的圆圈中就是我们常说的 soc，里面包括 CPU 和各种控制器 A、B、I2C、SPI，soc 外面接了外设 E 和 F。IP 外设有具体的总线，如 I2C 总线、SPI 总线，对应的 I2C 设备和 SPI 设备就挂在各自的总线上，但是在 soc 内部只有系统总线，是没有具体总线的。 第一节中讲了总线、设备和驱动模型的原理，即任何驱动都是通过对应的总线和设备发生联系的，故虽然 soc 内部没有具体的总线，但是内核通过 platform 这条虚拟总线，把控制器一个一个找到，一样遵循了内核高内聚、低耦合的设计理念。下面我们按照 platform 设备、i2c 设备、spi 设备的顺序探究设备是如何一层一层展开的。 1.展开 platform 设备上图中可以看到红色字体标注的 simple-bus，这些就是连接各类控制器的总线，在内核里即为 platform 总线，挂载的设备为 platform 设备。下面看下 platform 设备是如何展开的。 还记得上一节讲到在内核初始化的时候有一个叫做 init_machine() 的回调函数吗？如果你在板级文件里注册了这个函数，那么在系统启动的时候这个函数会被调用，如果没有定义，则会通过调用 of_platform_populate() 来展开挂在“simple-bus”下的设备，如图（分别位于 kernel&#x2F;arch&#x2F;arm&#x2F;kernel&#x2F;setup.c，kernel&#x2F;drivers&#x2F;of&#x2F;platform.c）： 这样就把 simple-bus 下面的节点一个一个的展开为 platform 设备。 2.展开 i2c 设备有经验的小伙伴知道在写 i2c 控制器的时候肯定会调用 i2c_register_adapter() 函数，该函数的实现如下（kernel&#x2F;drivers&#x2F;i2c&#x2F;i2c-core.c）： 注册函数的最后有一个函数 of_i2c_register_devices(adap)，实现如下： of_i2c_register_devices()函数中会遍历控制器下的节点，然后通过of_i2c_register_device()函数把 i2c 控制器下的设备注册进去。 3.展开 spi 设备spi 设备的注册和 i2c 设备一样，在 spi 控制器下遍历 spi 节点下的设备，然后通过相应的注册函数进行注册，只是和 i2c 注册的 api 接口不一样，下面看一下具体的代码（kernel&#x2F;drivers&#x2F;spi&#x2F;spi.c)： 当通过 spi_register_master 注册 spi 控制器的时候会通过 of_register_spi_devices 来遍历 spi 总线下的设备，从而注册。这样就完成了 spi 设备的注册。 各级设备的展开学到这里相信应该了解设备的硬件信息是从设备树里获取的，如寄存器地址、中断号、时钟等等。接下来我们一起看下这些信息在设备树里是怎么记录的，为下一节动手定制开发板做好准备。 1.reg 寄存器 我们先看设备树里的 soc 描述信息，红色标注的代表着寄存器地址用几个数据量来表述，绿色标注的代表着寄存器空间大小用几个数据量来表述。图中的含义是中断控制器的基地址是 0xfec00000，空间大小是 0x1000。如果 address-cells 的值是 2 的话表示需要两个数量级来表示基地址，比如寄存器是 64 位的话就需要两个数量级来表示，每个代表着 32 位的数。 2.ranges 取值范围 ranges 代表了 local 地址向 parent 地址的转换，如果 ranges 为空的话代表着与 cpu 是 1:1 的映射关系，如果没有 range 的话表示不是内存区域。"},{"path":"/2024/04/05/linux-docs/设备驱动/Linux操作系统学习之字符设备/","content":"一. 前言 上文中我们分析了虚拟文件系统的结构以及常见的文件操作从用户态到虚拟文件系统再到底层实际文件系统的过程。而实际上我们并没有说明实际的文件系统如ext4是如何和磁盘进行交互的，这就是本文和下篇文章的重点：I&#x2F;O之块设备和字符设备。输入输出设备我们大致可以分为两类：块设备（Block Device）和字符设备（Character Device）。 块设备将信息存储在固定大小的块中，每个块都有自己的地址。如硬盘就是常见的块设备。 字符设备发送或接收的是字节流，而不用考虑任何块结构，没有办法寻址。如鼠标就是常见的字符设备。 本文首先介绍虚拟文件系统下层直至硬件输入输出设备的结构关系，然后重点分析字符设备相关的整体逻辑情况。 二. I&#x2F;O架构 由于各种输入输出设备具有不同的硬件结构、驱动程序，因此我们采取了设备控制器这一中间层对上提供统一接口。设备控制器通过缓存来处理CPU和硬件I&#x2F;O之间的交互关系，通过中断进行通知，因此我们需要有中断处理器对各种中断进行统一。由于每种设备的控制器的寄存器、缓冲区等使用模式，指令都不同，所以对于操作系统还需要一层对接各个设备控制器的设备驱动程序。 这里需要注意的是，设备控制器不属于操作系统的一部分，但是设备驱动程序属于操作系统的一部分。操作系统的内核代码可以像调用本地代码一样调用驱动程序的代码，而驱动程序的代码需要发出特殊的面向设备控制器的指令，才能操作设备控制器。设备驱动程序中是一些面向特殊设备控制器的代码，不同的设备不同。但是对于操作系统其它部分的代码而言，设备驱动程序有统一的接口。 设备驱动本身作为一个内核模块，通常以ko文件的形式存在，它有着其独特的代码结构： 头文件部分。设备驱动程序至少需要以下头文件 12#include &lt;linux/module.h&gt;#include &lt;linux/init.h&gt; 调用MODULE_LICENSE声明lisence 初始化函数module_init和退出函数module_exit的定义，用于加载和卸载ko文件 文件系统的接口file_operation结构体定义 定义需要的函数 在下文的分析中，我们就将按照此顺序来剖析字符设备的源码，以弄懂字符设备的一般运行逻辑。关于设备驱动代码编写的详细知识可以参考《Linux设备驱动》一书，本文重点不在于如何编写代码，而是在于操作系统中的字符设备和块设备如何工作。 三. 字符设备基本构成 一个字符设备由3个部分组成： 封装对于外部设备的操作的设备驱动程序，即 ko 文件模块，里面有模块初始化函数、中断处理函数、设备操作函数等。加载设备驱动程序模块的时候，模块初始化函数会被调用。在内核维护所有字符设备驱动的数据结构 cdev_map 里面注册设备号后，我们就可以很容易根据设备号找到相应的设备驱动程序。 特殊的设备驱动文件系统 devtmpfs，在&#x2F;dev 目录下生成一个文件表示这个设备。打开一个字符设备文件和打开一个普通的文件有类似的数据结构，有文件描述符、有 struct file、指向字符设备文件的 dentry 和 inode。其对应的 inode 是一个特殊的 inode，里面有设备号。通过它我们可以在 cdev_map 中找到设备驱动程序，里面还有针对字符设备文件的默认操作 def_chr_fops。 字符设备文件的相关操作 file_operations 。一开始会指向 def_chr_fops，在调用 def_chr_fops 里面的 chrdev_open 函数的时候修改为指向设备操作函数，从而读写一个字符设备文件就会直接变成读写外部设备了。 这里主要涉及到了两个结构体：字符设备信息存储的struct cdev以及管理字符设备的cdev_map。 12345678struct cdev &#123; struct kobject kobj; //内嵌的内核对象. struct module *owner; //该字符设备所在的内核模块的对象指针. const struct file_operations *ops; //该结构描述了字符设备所能实现的方法，是极为关键的一个结构体. struct list_head list; //用来将已经向内核注册的所有字符设备形成链表. dev_t dev; //字符设备的设备号，由主设备号和次设备号构成. unsigned int count; //隶属于同一主设备号的次设备号的个数.&#125; __randomize_layout; cdev结构体还有另一个相关联的结构体char_device_struct。这里首先会定义主设备号和次设备号：主设备号用来标识与设备文件相连的驱动程序，用来反映设备类型。次设备号被驱动程序用来辨别操作的是哪个设备，用来区分同类型的设备。这里minorct指的是分配的区域，用于主设备号和次设备号的分配工作。 12345678static struct char_device_struct &#123; struct char_device_struct *next; unsigned int major; unsigned int baseminor; int minorct; char name[64]; struct cdev *cdev; /* will die */&#125; *chrdevs[CHRDEV_MAJOR_HASH_SIZE]; cdev_map用于维护所有字符设备驱动，实际是结构体kobj_map，主要包括了一个互斥锁lock，一个probes[255]数组，数组元素为struct probe的指针，该结构体包括链表项、设备号、设备号范围等。所以我们将字符设备驱动最后保存为一个probe，并用cdev_map&#x2F;kobj_map进行统一管理。 1234567891011121314static struct kobj_map *cdev_map;struct kobj_map &#123; struct probe &#123; struct probe *next; dev_t dev; unsigned long range; struct module *owner; kobj_probe_t *get; int (*lock)(dev_t, void *); void *data; &#125; *probes[255]; struct mutex *lock;&#125;; 四. 打开字符设备 字符设备有很多种，这里以打印机设备为输出设备的例子，源码位于drivers&#x2F;char&#x2F;lp.c。以鼠标为输入设备的例子，源码位于drivers&#x2F;input&#x2F;mouse&#x2F;logibm.c。下面将根据上述的字符设备的三个组成部分分别剖析如何创建并打开字符设备。 4.1 加载 字符设备的使用从加载开始，通常我们会使用insmod命令或者modprobe命令加载ko文件，ko文件的加载则从module_init调用该设备自定义的初始函数开始。对于打印机来说，其初始化函数定义为lp_init_module()，实际调用lp_init()。lp_init()会初始化打印机结构体，并调用register_chardev()注册该字符设备。 1234567891011121314151617module_init(lp_init_module);static int __init lp_init_module(void)&#123;...... return lp_init();&#125;static int __init lp_init(void)&#123;...... if (register_chrdev(LP_MAJOR, &quot;lp&quot;, &amp;lp_fops)) &#123; printk(KERN_ERR &quot;lp: unable to get major %d &quot;, LP_MAJOR); return -EIO; &#125;......&#125; register_chrdev()实际调用__register_chrdev()，该函数会进行字符设备的注册操作。其主要逻辑如下 调用__register_chrdev_region()注册字符设备的主设备号和名称 调用cdev_alloc()分配结构体struct cdev 将 cdev 的 ops 成员变量指向这个模块声明的 file_operations 调用cdev_add()将这个字符设备添加到结构体 struct kobj_map *cdev_map ，该结构体用于统一管理所有字符设备。 12345678910111213141516171819202122232425262728static inline int register_chrdev(unsigned int major, const char *name, const struct file_operations *fops)&#123; return __register_chrdev(major, 0, 256, name, fops);&#125;int __register_chrdev(unsigned int major, unsigned int baseminor, unsigned int count, const char *name, const struct file_operations *fops)&#123; struct char_device_struct *cd; struct cdev *cdev; int err = -ENOMEM; cd = __register_chrdev_region(major, baseminor, count, name); if (IS_ERR(cd)) return PTR_ERR(cd); cdev = cdev_alloc(); if (!cdev) goto out2; cdev-&gt;owner = fops-&gt;owner; cdev-&gt;ops = fops; kobject_set_name(&amp;cdev-&gt;kobj, &quot;%s&quot;, name); err = cdev_add(cdev, MKDEV(cd-&gt;major, baseminor), count);......&#125;// 拼接ma和mi#define MINORBITS\t20#define MKDEV(ma,mi)\t(((ma) &lt;&lt; MINORBITS) | (mi)) 对于鼠标来说，加载也是类似的：注册为logibm_init()函数。但是这里没有调用register_chrdev()而是使用input_register_device()，原因在于输入设备会统一由input_init()初始化，之后加入的输入设备通过input_register_device()注册到input的管理结构体中进行统一管理。 12345678module_init(logibm_init);static int __init logibm_init(void)&#123;...... err = input_register_device(logibm_dev);......&#125; 4.2 创建文件设备 加载完ko文件后，Linux内核会通过mknod在&#x2F;dev目录下创建一个设备文件，只有有了这个设备文件，我们才能通过文件系统的接口对这个设备文件进行操作。mknod本身是一个系统调用，主要逻辑为调用user_path_create()为该设备文件创建dentry，然后对于S_IFCHAR或者S_IFBLK会调用vfs_mknod()去调用对应文件系统的操作。 1234567891011121314151617181920212223242526272829SYSCALL_DEFINE3(mknod, const char __user *, filename, umode_t, mode, unsigned, dev)&#123; return sys_mknodat(AT_FDCWD, filename, mode, dev);&#125;SYSCALL_DEFINE4(mknodat, int, dfd, const char __user *, filename, umode_t, mode, unsigned, dev)&#123; struct dentry *dentry; struct path path;...... dentry = user_path_create(dfd, filename, &amp;path, lookup_flags);...... switch (mode &amp; S_IFMT) &#123;...... case S_IFCHR: case S_IFBLK: error = vfs_mknod(path.dentry-&gt;d_inode,dentry,mode, new_decode_dev(dev)); break;...... &#125;&#125;int vfs_mknod(struct inode *dir, struct dentry *dentry, umode_t mode, dev_t dev)&#123;...... error = dir-&gt;i_op-&gt;mknod(dir, dentry, mode, dev);......&#125; 对于&#x2F;dev目录下的设备驱动来说，所属的文件系统为devtmpfs文件系统，即设备驱动临时文件系统。devtmpfs对应的文件系统定义如下 123456789101112131415static struct file_system_type dev_fs_type = &#123; .name = &quot;devtmpfs&quot;, .mount = dev_mount, .kill_sb = kill_litter_super,&#125;;static struct dentry *dev_mount(struct file_system_type *fs_type, int flags, const char *dev_name, void *data)&#123;#ifdef CONFIG_TMPFS return mount_single(fs_type, flags, data, shmem_fill_super);#else return mount_single(fs_type, flags, data, ramfs_fill_super);#endif&#125; 从这里可以看出，devtmpfs 在挂载的时候有两种模式：一种是 ramfs，一种是 shmem ，都是基于内存的文件系统。这两个 mknod 虽然实现不同，但是都会调用到同一个函数 init_special_inode()。显然这个文件是个特殊文件，inode 也是特殊的。这里这个 inode 可以关联字符设备、块设备、FIFO 文件、Socket 等。我们这里只看字符设备。这里的 inode 的 file_operations 指向一个 def_chr_fops，这里面只有一个 open，就等着你打开它。另外，inode 的 i_rdev 指向这个设备的 dev_t。通过这个 dev_t，可以找到我们刚刚加载的字符设备 cdev。 123456789101112131415161718192021222324252627282930313233static const struct inode_operations ramfs_dir_inode_operations = &#123;...... .mknod = ramfs_mknod,&#125;;static const struct inode_operations shmem_dir_inode_operations = &#123;#ifdef CONFIG_TMPFS...... .mknod = shmem_mknod,&#125;;void init_special_inode(struct inode *inode, umode_t mode, dev_t rdev)&#123; inode-&gt;i_mode = mode; if (S_ISCHR(mode)) &#123; inode-&gt;i_fop = &amp;def_chr_fops; inode-&gt;i_rdev = rdev; &#125; else if (S_ISBLK(mode)) &#123; inode-&gt;i_fop = &amp;def_blk_fops; inode-&gt;i_rdev = rdev; &#125; else if (S_ISFIFO(mode)) inode-&gt;i_fop = &amp;pipefifo_fops; else if (S_ISSOCK(mode)) ; /* leave it no_open_fops */ else printk(KERN_DEBUG &quot;init_special_inode: bogus i_mode (%o) for&quot; &quot; inode %s:%lu &quot;, mode, inode-&gt;i_sb-&gt;s_id, inode-&gt;i_ino); &#125;const struct file_operations def_chr_fops = &#123; .open = chrdev_open,&#125;; 由此我们完成了&#x2F;dev下文件的创建，并利用rdev和生成的字符设备进行了关联。 4.3 打开字符设备 如打开普通文件一样，打开字符设备也会首先分配对应的文件描述符fd，然后生成struct file结构体与其绑定，并将file关联到对应的dentry从而可以接触inode。在进程里面调用 open() 函数，最终会调用到这个特殊的 inode 的 open() 函数，也就是 chrdev_open()。 chrdev_open()主要逻辑为 调用kobj_lookup()，通过设备号i_cdev关联对应的设备驱动程序 调用fops_get()将设备驱动程序自己定义的文件操作p-&gt;ops赋值给fops 调用设备驱动程序的 file_operations 的 open() 函数真正打开设备。对于打印机，调用的是 lp_open()。对于鼠标调用的是 input_proc_devices_open()，最终会调用到 logibm_open()。 1234567891011121314151617181920/* * Called every time a character special file is opened */static int chrdev_open(struct inode *inode, struct file *filp)&#123; const struct file_operations *fops; struct cdev *p; struct cdev *new = NULL;...... p = inode-&gt;i_cdev;...... kobj = kobj_lookup(cdev_map, inode-&gt;i_rdev, &amp;idx);...... fops = fops_get(p-&gt;ops);...... replace_fops(filp, fops); if (filp-&gt;f_op-&gt;open) &#123; ret = filp-&gt;f_op-&gt;open(inode, filp);......&#125; 上述过程借用极客时间中的图来作为总结。 五. 写入字符设备 写入字符设备和写入普通文件一样，调用write()函数执行。该函数在内核里查询系统调用表最终调用sys_write()，并根据fd描述符获取对应的file结构体，接着调用vfs_write()去调用对应的文件系统自定义的写入函数file-&gt;f_op-&gt;write()。对于打印机来说，最终调用的是自定义的lp_write()函数。 这里写入的重点在于调用 copy_from_user() 将数据从用户态拷贝到内核态的缓存中，然后调用 parport_write() 写入外部设备。这里还有一个 schedule() 函数，也即写入的过程中，给其他线程抢占 CPU 的机会。如果写入字节数多，不能一次写完，就会在循环里一直调用 copy_from_user() 和 parport_write()，直到写完为止。 1234567891011121314151617181920212223242526272829303132333435363738394041424344static ssize_t lp_write(struct file *file, const char __user *buf, size_t count, loff_t *ppos)&#123; unsigned int minor = iminor(file_inode(file)); struct parport *port = lp_table[minor].dev-&gt;port; char *kbuf = lp_table[minor].lp_buffer; ssize_t retv = 0; ssize_t written; size_t copy_size = count;...... /* Need to copy the data from user-space. */ if (copy_size &gt; LP_BUFFER_SIZE) copy_size = LP_BUFFER_SIZE;...... if (copy_from_user(kbuf, buf, copy_size)) &#123; retv = -EFAULT; goto out_unlock; &#125;...... do &#123; /* Write the data. */ written = parport_write(port, kbuf, copy_size); if (written &gt; 0) &#123; copy_size -= written; count -= written; buf += written; retv += written; &#125;...... if (need_resched()) schedule(); if (count) &#123; copy_size = count; if (copy_size &gt; LP_BUFFER_SIZE) copy_size = LP_BUFFER_SIZE; if (copy_from_user(kbuf, buf, copy_size)) &#123; if (retv == 0) retv = -EFAULT; break; &#125; &#125; &#125; while (count &gt; 0);......&#125; 六. 字符设备的控制 在Linux中，我们常用ioctl()来对I&#x2F;O设备进行一些读写之外的特殊操作。其参数主要由文件描述符fd，命令cmd以及命令参数arg构成。其中cmd由几个部分拼接成整型，主要结构为 最低8位为 NR，表示命令号； 次低8位为 TYPE，表示类型； 14位表示参数的大小； 最高2位是 DIR，表示写入、读出，还是读写。 ioctl()也是一个系统调用，其中fd 是这个设备的文件描述符，cmd 是传给这个设备的命令，arg 是命令的参数。主要调用do_vfs_ioctl()完成实际功能。 123456789SYSCALL_DEFINE3(ioctl, unsigned int, fd, unsigned int, cmd, unsigned long, arg)&#123; int error; struct fd f = fdget(fd);...... error = do_vfs_ioctl(f.file, fd, cmd, arg); fdput(f); return error;&#125; do_vfs_ioctl()对于已经定义好的 cmd进行相应的处理。如果不是默认定义好的 cmd，则执行默认操作：对于普通文件，调用 file_ioctl，对于其他文件调用 vfs_ioctl。 123456789101112131415161718192021222324252627282930313233343536/* * When you add any new common ioctls to the switches above and below * please update compat_sys_ioctl() too. * * do_vfs_ioctl() is not for drivers and not intended to be EXPORT_SYMBOL()&#x27;d. * It&#x27;s just a simple helper for sys_ioctl and compat_sys_ioctl. */int do_vfs_ioctl(struct file *filp, unsigned int fd, unsigned int cmd, unsigned long arg)&#123; int error = 0; int __user *argp = (int __user *)arg; struct inode *inode = file_inode(filp); switch (cmd) &#123; case FIOCLEX: set_close_on_exec(fd, 1); break; case FIONCLEX: set_close_on_exec(fd, 0); break; case FIONBIO: error = ioctl_fionbio(filp, argp); break; case FIOASYNC: error = ioctl_fioasync(fd, filp, argp); break;...... default: if (S_ISREG(inode-&gt;i_mode)) error = file_ioctl(filp, cmd, arg); else error = vfs_ioctl(filp, cmd, arg); break; &#125; return error;&#125; 对于字符设备驱动程序，最终会调用vfs_ioctl()。这里面调用的是 struct file 里 file_operations 的 unlocked_ioctl() 函数。我们前面初始化设备驱动的时候，已经将 file_operations 指向设备驱动的 file_operations 了。这里调用的是设备驱动的 unlocked_ioctl。对于打印机程序来讲，调用的是 lp_ioctl()。 1234567891011long vfs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)&#123; int error = -ENOTTY; if (!filp-&gt;f_op-&gt;unlocked_ioctl) goto out; error = filp-&gt;f_op-&gt;unlocked_ioctl(filp, cmd, arg); if (error == -ENOIOCTLCMD) error = -ENOTTY; out: return error;&#125; EXPORT_SYMBOL(vfs_ioctl); 打印机的lp_do_ioctl()主要逻辑也是针对cmd采用switch()语句分情况进行处理。主要包括使用LP_XXX()宏定义赋值标记位和调用copy_to_user()将用户想得到的信息返回给用户态。 1234567891011121314151617181920212223242526272829static int lp_do_ioctl(unsigned int minor, unsigned int cmd, unsigned long arg, void __user *argp)&#123; int status; int retval = 0;...... switch ( cmd ) &#123; case LPTIME: if (arg &gt; UINT_MAX / HZ) return -EINVAL; LP_TIME(minor) = arg * HZ/100; break; case LPCHAR: LP_CHAR(minor) = arg; break; case LPABORT: if (arg) LP_F(minor) |= LP_ABORT; else LP_F(minor) &amp;= ~LP_ABORT; break; ...... case LPGETIRQ: if (copy_to_user(argp, &amp;LP_IRQ(minor), sizeof(int))) return -EFAULT; break;......&#125; 总结 本文简单介绍了设备驱动程序的结构，并在此基础上介绍了字符设备从创建到打开、写入以及控制的整个流程。"},{"path":"/2024/04/05/linux-docs/网络协议栈/Linux内核之epoll模型/","content":"同步IO和异步IO，阻塞IO和非阻塞IO分别是什么，到底有什么区别？不同的人在不同的上下文下给出的答案是不同的。 这里主要讨论Linux环境下的network IO。 一 概念说明在进行解释之前，首先要说明几个概念： 用户空间和内核空间 进程切换 进程的阻塞 文件描述符 缓存 I&#x2F;O 用户空间与内核空间现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。 针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间。 而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。 进程切换为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。 从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化： 1、保存处理机上下文，包括程序计数器和其他寄存器。2、更新PCB信息。3、把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。4、选择另一个进程执行，并更新其PCB。5、更新内存管理的数据结构。6、恢复处理机上下文。7、总而言之就是很耗资源 进程的阻塞正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。 文件描述符fd文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。 缓存 I&#x2F;O缓存 I&#x2F;O 又被称作标准 I&#x2F;O，大多数文件系统的默认 I&#x2F;O 操作都是缓存 I&#x2F;O。在 Linux 的缓存 I&#x2F;O 机制中，操作系统会将 I&#x2F;O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。 缓存 I&#x2F;O 的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。 二 IO模式刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段： 1、等待数据准备 (Waiting for the data to be ready)2、将数据从内核拷贝到进程中 (Copying the data from the kernel to the process) 正是因为这两个阶段，linux系统产生了下面五种网络模式的方案。 阻塞 I&#x2F;O（blocking IO） 非阻塞 I&#x2F;O（nonblocking IO） I&#x2F;O 多路复用（ IO multiplexing） 信号驱动 I&#x2F;O（ signal driven IO） 异步 I&#x2F;O（asynchronous IO） 注：由于signal driven IO在实际中并不常用，所以我这只提及剩下的四种IO Model。 阻塞 I&#x2F;O（blocking IO）在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样： 当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 非阻塞 I&#x2F;O（nonblocking IO）linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子： 当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度io讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。 所以，nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。 I&#x2F;O 多路复用（ IO multiplexing）IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select&#x2F;epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。 所以，I&#x2F;O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。 这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。 所以，如果处理的连接数不是很高的话，使用select&#x2F;epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select&#x2F;epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） 在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 异步 I&#x2F;O（asynchronous IO）Linux下的asynchronous IO其实用得很少。先看一下它的流程： I用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 总结blocking和non-blocking的区别调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。 synchronous IO和asynchronous IO的区别在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的： A synchronous I&#x2F;O operation causes the requesting process to be blocked until that I&#x2F;O operation completes; An asynchronous I&#x2F;O operation does not cause the requesting process to be blocked 两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。 有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。 而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。 各个IO Model的比较如图所示： 通过上面的图片，可以发现non-blocking IO和asynchronous IO的区别还是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而asynchronous IO则完全不同。它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。 三 I&#x2F;O 多路复用之select、poll、epoll详解select，poll，epoll都是IO多路复用的机制。I&#x2F;O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I&#x2F;O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I&#x2F;O则无需自己负责进行读写，异步I&#x2F;O的实现会负责把数据从内核拷贝到用户空间。 select1int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。 select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但 是这样也会造成效率的降低。 poll1int poll (struct pollfd *fds, unsigned int nfds, int timeout); 不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。 12345struct pollfd &#123; int fd; /* file descriptor */ short events; /* requested events to watch */ short revents; /* returned events witnessed */&#125;; pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。 从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。 epollepoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。 epoll操作过程epoll操作过程需要三个接口，分别如下： 123int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); 1. int epoll_create(int size);创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议。当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看&#x2F;proc&#x2F;进程id&#x2F;fd&#x2F;，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。 2. int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；函数是对指定描述符fd执行op操作。 epfd：是epoll_create()的返回值。 op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。 fd：是需要监听的fd（文件描述符） epoll_event：是告诉内核需要监听什么事，struct epoll_event结构如下： 1234struct epoll_event &#123; __uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */&#125;; events可以是以下几个宏的集合：EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；EPOLLOUT：表示对应的文件描述符可以写；EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；EPOLLERR：表示对应的文件描述符发生错误；EPOLLHUP：表示对应的文件描述符被挂断；EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 3. int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);等待epfd上的io事件，最多返回maxevents个事件。参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。 工作模式epoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下： LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。 ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。 1. LT模式LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。 2. ET模式ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once) ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读&#x2F;阻塞写操作把处理多个文件描述符的任务饿死。 3. 总结假如有这样一个例子： 我们已经把一个用来从管道中读取数据的文件句柄(RFD)添加到epoll描述符这个时候从管道的另一端被写入了2KB的数据调用epoll_wait(2)，并且它会返回RFD，说明它已经准备好读取操作然后我们读取了1KB的数据调用epoll_wait(2)…… LT模式：如果是LT模式，那么在第5步调用epoll_wait(2)之后，仍然能受到通知。 ET模式：如果我们在第1步将RFD添加到epoll描述符的时候使用了EPOLLET标志，那么在第5步调用epoll_wait(2)之后将有可能会挂起，因为剩余的数据还存在于文件的输入缓冲区内，而且数据发出端还在等待一个针对已经发出数据的反馈信息。只有在监视的文件句柄上发生了某个事件的时候 ET 工作模式才会汇报事件。因此在第5步的时候，调用者可能会放弃等待仍在存在于文件输入缓冲区内的剩余数据。 当使用epoll的ET模型来工作时，当产生了一个EPOLLIN事件后，读数据的时候需要考虑的是当recv()返回的大小如果等于请求的大小，那么很有可能是缓冲区还有数据未读完，也意味着该次事件还没有处理完，所以还需要再次读取： 1234567891011121314151617181920212223while(rs)&#123; buflen = recv(activeevents[i].data.fd, buf, sizeof(buf), 0); if(buflen &lt; 0)&#123; // 由于是非阻塞的模式,所以当errno为EAGAIN时,表示当前缓冲区已无数据可读 // 在这里就当作是该次事件已处理处. if(errno == EAGAIN)&#123; break; &#125; else&#123; return; &#125; &#125; else if(buflen == 0)&#123; // 这里表示对端的socket已正常关闭. &#125; if(buflen == sizeof(buf)&#123; rs = 1; // 需要再次读取 &#125; else&#123; rs = 0; &#125;&#125; Linux中的EAGAIN含义 Linux环境下开发经常会碰到很多错误(设置errno)，其中EAGAIN是其中比较常见的一个错误(比如用在非阻塞操作中)。从字面上来看，是提示再试一次。这个错误经常出现在当应用程序进行一些非阻塞(non-blocking)操作(对文件或socket)的时候。 例如，以 O_NONBLOCK的标志打开文件&#x2F;socket&#x2F;FIFO，如果你连续做read操作而没有数据可读。此时程序不会阻塞起来等待数据准备就绪返回，read函数会返回一个错误EAGAIN，提示你的应用程序现在没有数据可读请稍后再试。又例如，当一个系统调用(比如fork)因为没有足够的资源(比如虚拟内存)而执行失败，返回EAGAIN提示其再调用一次(也许下次就能成功)。 代码演示下面是一段不完整的代码且格式不对，意在表述上面的过程，去掉了一些模板代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117#define IPADDRESS &quot;127.0.0.1&quot;#define PORT 8787#define MAXSIZE 1024#define LISTENQ 5#define FDSIZE 1000#define EPOLLEVENTS 100listenfd = socket_bind(IPADDRESS,PORT);struct epoll_event events[EPOLLEVENTS];//创建一个描述符epollfd = epoll_create(FDSIZE);//添加监听描述符事件add_event(epollfd,listenfd,EPOLLIN);//循环等待for ( ; ; )&#123; //该函数返回已经准备好的描述符事件数目 ret = epoll_wait(epollfd,events,EPOLLEVENTS,-1); //处理接收到的连接 handle_events(epollfd,events,ret,listenfd,buf);&#125;//事件处理函数static void handle_events(int epollfd,struct epoll_event *events,int num,int listenfd,char *buf)&#123; int i; int fd; //进行遍历;这里只要遍历已经准备好的io事件。num并不是当初epoll_create时的FDSIZE。 for (i = 0;i &lt; num;i++) &#123; fd = events[i].data.fd; //根据描述符的类型和事件类型进行处理 if ((fd == listenfd) &amp;&amp;(events[i].events &amp; EPOLLIN)) handle_accpet(epollfd,listenfd); else if (events[i].events &amp; EPOLLIN) do_read(epollfd,fd,buf); else if (events[i].events &amp; EPOLLOUT) do_write(epollfd,fd,buf); &#125;&#125;//添加事件static void add_event(int epollfd,int fd,int state)&#123; struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_ADD,fd,&amp;ev);&#125;//处理接收到的连接static void handle_accpet(int epollfd,int listenfd)&#123; int clifd; struct sockaddr_in cliaddr; socklen_t cliaddrlen; clifd = accept(listenfd,(struct sockaddr*)&amp;cliaddr,&amp;cliaddrlen); if (clifd == -1) perror(&quot;accpet error:&quot;); else &#123; printf(&quot;accept a new client: %s:%d &quot;,inet_ntoa(cliaddr.sin_addr),cliaddr.sin_port); //添加一个客户描述符和事件 add_event(epollfd,clifd,EPOLLIN); &#125; &#125;//读处理static void do_read(int epollfd,int fd,char *buf)&#123; int nread; nread = read(fd,buf,MAXSIZE); if (nread == -1) &#123; perror(&quot;read error:&quot;); close(fd); //记住close fd delete_event(epollfd,fd,EPOLLIN); //删除监听 &#125; else if (nread == 0) &#123; fprintf(stderr,&quot;client close. &quot;); close(fd); //记住close fd delete_event(epollfd,fd,EPOLLIN); //删除监听 &#125; else &#123; printf(&quot;read message is : %s&quot;,buf); //修改描述符对应的事件，由读改为写 modify_event(epollfd,fd,EPOLLOUT); &#125; &#125;//写处理static void do_write(int epollfd,int fd,char *buf) &#123; int nwrite; nwrite = write(fd,buf,strlen(buf)); if (nwrite == -1)&#123; perror(&quot;write error:&quot;); close(fd); //记住close fd delete_event(epollfd,fd,EPOLLOUT); //删除监听 &#125;else&#123; modify_event(epollfd,fd,EPOLLIN); &#125; memset(buf,0,MAXSIZE); &#125;//删除事件static void delete_event(int epollfd,int fd,int state) &#123; struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_DEL,fd,&amp;ev);&#125;//修改事件static void modify_event(int epollfd,int fd,int state)&#123; struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_MOD,fd,&amp;ev);&#125;//注：另外一端我就省了 epoll总结在 select&#x2F;poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在。) epoll的优点主要是以下几个方面： 1、监视的描述符数量不受限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat &#x2F;proc&#x2F;sys&#x2F;fs&#x2F;file-max查看,一般来说这个数目和系统内存关系很大。select的最大缺点就是进程打开的fd是有数量限制的。这对 于连接数量比较大的服务器来说根本不能满足。虽然也可以选择多进程的解决方案( Apache就是这样实现的)，不过虽然linux上面创建进程的代价比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步的高效，所以也不是一种完美的方案。 2、IO的效率不会随着监视fd的数量的增长而下降。epoll不同于select和poll轮询的方式，而是通过每个fd定义的回调函数来实现的。只有就绪的fd才会执行回调函数。 如果没有大量的idle-connection或者dead-connection，epoll的效率并不会比select&#x2F;poll高很多，但是当遇到大量的idle-connection，就会发现epoll的效率大大高于select&#x2F;poll。"},{"path":"/2024/04/05/linux-docs/网络协议栈/Linux内核网络UDP数据包发送（三）—IP协议层分析/","content":"1. 前言Linux内核网络 UDP 协议层通过调用 ip_send_skb 将 skb 交给 IP 协议层，本文通过分析内核 IP 协议层的关键函数来分享内核数据包发送在 IP 协议层的处理，并分享了监控IP层的方法。 2. ip_send_skbip_send_skb 函数定义在 net&#x2F;ipv4&#x2F;ip_output.c 中，非常简短。它只是调用ip_local_out，如果调用失败，就更新相应的错误计数： 1234567891011121314int ip_send_skb(struct net *net, struct sk_buff *skb)&#123; int err; err = ip_local_out(skb); if (err) &#123; if (err &gt; 0) err = net_xmit_errno(err); if (err) IP_INC_STATS(net, IPSTATS_MIB_OUTDISCARDS); &#125; return err;&#125; net_xmit_errno 函数将低层错误转换为 IP 和 UDP 协议层所能理解的错误。如果发生错误， IP 协议计数器 OutDiscards 会递增。稍后我们将看到读取哪些文件可以获取此统计信息。接下来看 ip_local_out。 3. ip_local_out and __ip_local_outip_local_out 和__ip_local_out 都很简单。ip_local_out 只需调用__ip_local_out，如果返回值为 1，则调用路由层 dst_output 发送数据包： 12345678910int ip_local_out(struct sk_buff *skb)&#123; int err; err = __ip_local_out(skb); if (likely(err == 1)) err = dst_output(skb); return err;&#125; 接下来看__ip_local_out 的代码： 123456789int __ip_local_out(struct sk_buff *skb)&#123; struct iphdr *iph = ip_hdr(skb); iph-&gt;tot_len = htons(skb-&gt;len); ip_send_check(iph); return nf_hook(NFPROTO_IPV4, NF_INET_LOCAL_OUT, skb, NULL, skb_dst(skb)-&gt;dev, dst_output);&#125; 可以看到，该函数首先做了两件重要的事情：设置 IP 数据包的长度调用 ip_send_check 来计算要写入 IP 头的校验和。ip_send_check 函数将进一步调用名为 ip_fast_csum 的函数来计算校验和。在 x86 和 x86_64 体系结构上，此函数用汇编实 现。接下来，IP 协议层将通过调用 nf_hook 进入 netfilter，其返回值将传递回 ip_local_out 。如果 nf_hook 返回 1，则表示允许数据包通过，并且调用者应该自己发送数据包。这正是我们在上面看到的情况：ip_local_out 检查返回值 1 时，自己通过调用 dst_output 发送数据包。 3.1 netfilter and nf_hooknf_hook 只是一个 wrapper，它调用 nf_hook_thresh，首先检查是否有为这个协议族和hook 类型（这里分别为 NFPROTO_IPV4 和 NF_INET_LOCAL_OUT）安装的过滤器，然后将返回到 IP 协议层，避免深入到 netfilter 或更下面，比如 iptables 和 conntrack。 如果有非常多或者非常复杂的 netfilter 或 iptables 规则，那些规则将在触发 sendmsg 系统调的用户进程的上下文中执行。如果对这个用户进程设置了 CPU 亲和性，相应的 CPU 将花费系统时间（system time）处理出站（outbound）iptables 规则。如果做性能回归测试，那可能要考虑根据系统的负载，将相应的用户进程绑到到特定的 CPU，或者是减少 netfilter&#x2F;iptables 规则的复杂度，以减少对性能测试的影响。 出于讨论目的，我们假设 nf_hook 返回 1，表示调用者（在这种情况下是 IP 协议层）应该自己发送数据包。 3.2 目的（路由）缓存dst 代码在 Linux 内核中实现协议无关的目标缓存。为了继续学习发送 UDP 数据报的流程 ，我们需要了解 dst 条目是如何被设置的，首先来看 dst 条目和路由是如何生成的。目标缓存，路由和邻居子系统，任何一个都可以拿来单独详细的介绍。现在不深入细节，只是快速地看一下它们是如何组合到一起的。上面看到的代码调用了 dst_output(skb)。此函数只是查找关联到这个 skb 的 dst 条目 ，然后调用 output 方法。代码如下： 12345/* Output packet to network from transport. */static inline int dst_output(struct sk_buff *skb)&#123; return skb_dst(skb)-&gt;output(skb);&#125; 看起来很简单，但是 output 方法之前是如何关联到 dst 条目的？首先很重要的一点，目标缓存条目是以多种不同方式添加的。到目前为止，我们已经在代码中看到的一种方法是从 udp_sendmsg 调用ip_route_output_flow。ip_route_output_flow 函数调用 __ip_route_output_key ，后者进而调用 __mkroute_output。 __mkroute_output 函数创建路由和目标缓存条目。当它执行创建操作时，它会判断哪个 output 方法适合此 dst。大多数时候，这个函数是 ip_output。 4. ip_output在 UDP IPv4 情况下，上面的 output 方法指向的是 ip_output: 12345678910111213int ip_output(struct sk_buff *skb)&#123; struct net_device *dev = skb_dst(skb)-&gt;dev; IP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUT, skb-&gt;len); skb-&gt;dev = dev; skb-&gt;protocol = htons(ETH_P_IP); return NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL, dev, ip_finish_output, !(IPCB(skb)-&gt;flags &amp; IPSKB_REROUTED));&#125; 首先，更新 IPSTATS_MIB_OUT 统计计数。IP_UPD_PO_STATS 宏将更新字节数和包数统计。接下来，设置要发送此 skb 的设备，以及协议。最后，通过调用 NF_HOOK_COND 将控制权交给 netfilter。查看 NF_HOOK_COND 的函数原型 有助于更清晰地解释它如何工作： 1234static inline intNF_HOOK_COND(uint8_t pf, unsigned int hook, struct sk_buff *skb, struct net_device *in, struct net_device *out, int (*okfn)(struct sk_buff *), bool cond) NF_HOOK_COND 通过检查传入的条件来工作。在这里条件是!(IPCB(skb)-&gt;flags &amp; IPSKB_REROUTED。如果此条件为真，则 skb 将发送给 netfilter。如果 netfilter 允许包通过 ，okfn 回调函数将被调用。在这里，okfn 是 ip_finish_output。 5. ip_finish_output1234567891011121314static int ip_finish_output(struct sk_buff *skb)&#123;#if defined(CONFIG_NETFILTER) &amp;&amp; defined(CONFIG_XFRM) /* Policy lookup after SNAT yielded a new policy */ if (skb_dst(skb)-&gt;xfrm != NULL) &#123; IPCB(skb)-&gt;flags |= IPSKB_REROUTED; return dst_output(skb); &#125;#endif if (skb-&gt;len &gt; ip_skb_dst_mtu(skb) &amp;&amp; !skb_is_gso(skb)) return ip_fragment(skb, ip_finish_output2); else return ip_finish_output2(skb);&#125; 如果内核启用了 netfilter 和数据包转换（XFRM），则更新 skb 的标志并通过 dst_output 将 其发回。 更常见的两种情况是：1、如果数据包的长度大于 MTU 并且分片不会 offload 到设备，则会调用 ip_fragment 在发送之前对数据包进行分片2、否则，数据包将直接发送到 ip_finish_output2 Path MTU Discovery Linux 提供了一个功能：路径 MTU 发现 。此功能允许内核自动确定路由的最大传输单元（ MTU ）。发送小于或等于该路由的 MTU 的包意味着可以避免 IP 分片，这是推荐设置，因为数据包分片会消耗系统资源，而避免分片看起来很容易：只需发送足够小的不需要分片的数据包。 可以在应用程序中通过调用 setsockopt 带 SOL_IP 和 IP_MTU_DISCOVER 选项，在 packet 级别来调整路径 MTU 发现设置，相应的合法值参考 IP 协议的man page。例如，可能想设置的值是 ：IP_PMTUDISC_DO，表示“始终执行路径 MTU 发现”。更高级的网络应用程序或诊断工具可能选择自己实现RFC 4821，以在应用程序启动时针对特定的路由做 PMTU。在这种情况下，可以使用 IP_PMTUDISC_PROBE 选项告诉内核设置“Do not Fragment”位，这就会允许发送大于 PMTU 的数据。 应用程序可以通过调用 getsockopt 带 SOL_IP 和 IP_MTU 选项来查看当前 PMTU。可以使用它指导应用程序在发送之前，构造 UDP 数据报的大小。 如果已启用 PMTU 发现，则发送大于 PMTU 的 UDP 数据将导致应用程序收到 EMSGSIZE 错误。这种情况下，应用程序只能减小 packet 大小重试。 强烈建议启用 PTMU 发现，当查看 IP 协议层统计信息时，将解释所有统计信息，包括与分片相关的统计信息。其中许多计数都在 ip_fragment 中更新的。不管分片与否，代码最后都会调到 ip_finish_output2。 6. ip_finish_output2IP 分片后调用 ip_finish_output2，另外 ip_finish_output 也会直接调用它。这个函数在将包发送到邻居缓存之前处理各种统计计数器： 123456789101112131415161718192021222324static inline int ip_finish_output2(struct sk_buff *skb)&#123; /* variable declarations */ if (rt-&gt;rt_type == RTN_MULTICAST) &#123; IP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUTMCAST, skb-&gt;len); &#125; else if (rt-&gt;rt_type == RTN_BROADCAST) IP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUTBCAST, skb-&gt;len); /* Be paranoid, rather than too clever. */ if (unlikely(skb_headroom(skb) &lt; hh_len &amp;&amp; dev-&gt;header_ops)) &#123; struct sk_buff *skb2; skb2 = skb_realloc_headroom(skb, LL_RESERVED_SPACE(dev)); if (skb2 == NULL) &#123; kfree_skb(skb); return -ENOMEM; &#125; if (skb-&gt;sk) skb_set_owner_w(skb2, skb-&gt;sk); consume_skb(skb); skb = skb2; &#125; 如果与此数据包关联的路由是多播类型，则使用 IP_UPD_PO_STATS 宏来增加 OutMcastPkts 和 OutMcastOctets 计数。如果广播路由，则会增加 OutBcastPkts 和 OutBcastOctets 计数。接下来，确保 skb 结构有足够的空间容纳需要添加的任何链路层头。如果空间不够，则调用 skb_realloc_headroom 分配额外的空间，并且新的 skb 的费用（charge）记在相关的 socket 上。 12345rcu_read_lock_bh(); nexthop = (__force u32) rt_nexthop(rt, ip_hdr(skb)-&gt;daddr); neigh = __ipv4_neigh_lookup_noref(dev, nexthop); if (unlikely(!neigh)) neigh = __neigh_create(&amp;arp_tbl, &amp;nexthop, dev, false); 继续，查询路由层找到下一跳，再根据下一跳信息查找邻居缓存。如果未找到，则调用__neigh_create 创建一个邻居。例如，第一次将数据发送到另一台主机的时候，就是这种情况。创建邻居缓存的时候带了 arp_tbl参数。其他系统（如 IPv6 或 DECnet）维护自己的 ARP 表，并将不同的变量传给__neigh_create。邻居缓存如果创建， 会导致缓存表增大。邻居缓存会导出一组统计信息，以便可以衡量这种增长。 12345678910111213if (!IS_ERR(neigh)) &#123; int res = dst_neigh_output(dst, neigh, skb); rcu_read_unlock_bh(); return res; &#125; rcu_read_unlock_bh(); net_dbg_ratelimited(&quot;%s: No header cache and no neighbour! &quot;, __func__); kfree_skb(skb); return -EINVAL;&#125; 最后，如果创建邻居缓存成功，则调用 dst_neigh_output 继续传递 skb；否则，释放 skb 并返回 EINVAL，这会向上传递，导致 OutDiscards 在 ip_send_skb 中递增。 7. dst_neigh_outputdst_neigh_output 函数做了两件重要的事情。首先，如果用户调用 sendmsg 并通过辅助消息指定 MSG_CONFIRM 参数，则会设置一个标志位以指示目标高速缓存条目仍然有效且不应进行垃圾回收。这个检查就是在这个函数里面做的，并且邻居上的 confirm 字段设置为当前的 jiffies 计数。 12345678910111213static inline int dst_neigh_output(struct dst_entry *dst, struct neighbour *n, struct sk_buff *skb)&#123; const struct hh_cache *hh; if (dst-&gt;pending_confirm) &#123; unsigned long now = jiffies; dst-&gt;pending_confirm = 0; /* avoid dirtying neighbour */ if (n-&gt;confirmed != now) n-&gt;confirmed = now; &#125; 其次，检查邻居的状态并调用适当的 output 函数。 123456hh = &amp;n-&gt;hh; if ((n-&gt;nud_state &amp; NUD_CONNECTED) &amp;&amp; hh-&gt;hh_len) return neigh_hh_output(hh, skb); else return n-&gt;output(n, skb);&#125; 邻居被认为是 NUD_CONNECTED，如果它满足以下一个或多个条件：1、NUD_PERMANENT：静态路由2、NUD_NOARP：不需要 ARP 请求（例如，目标是多播或广播地址，或环回设备）3、NUD_REACHABLE：邻居是“可达的。”只要成功处理了ARP 请求，目标就会被标记为可达 进一步，如果“硬件头”（hh）被缓存（之前已经发送过数据，并生成了缓存），将调用 neigh_hh_output。否则，调用 output 函数。以上两种情况，最后都会到 dev_queue_xmit，它将 skb 发送给 Linux 网络设备子系统，在它 进入设备驱动程序层之前将对其进行更多处理。让我们沿着 neigh_hh_output 和 n-&gt;output 代码继续向下，直到达到 dev_queue_xmit。 7.1 neigh_hh_output如果目标是 NUD_CONNECTED 并且硬件头已被缓存，则将调用 neigh_hh_output，在将 skb 移交 给 dev_queue_xmit 之前执行一小部分处理 ： 123456789101112131415161718192021static inline int neigh_hh_output(const struct hh_cache *hh, struct sk_buff *skb)&#123; unsigned int seq; int hh_len; do &#123; seq = read_seqbegin(&amp;hh-&gt;hh_lock); hh_len = hh-&gt;hh_len; if (likely(hh_len &lt;= HH_DATA_MOD)) &#123; /* this is inlined by gcc */ memcpy(skb-&gt;data - HH_DATA_MOD, hh-&gt;hh_data, HH_DATA_MOD); &#125; else &#123; int hh_alen = HH_DATA_ALIGN(hh_len); memcpy(skb-&gt;data - hh_alen, hh-&gt;hh_data, hh_alen); &#125; &#125; while (read_seqretry(&amp;hh-&gt;hh_lock, seq)); skb_push(skb, hh_len); return dev_queue_xmit(skb);&#125; 这个函数理解有点难，部分原因是seqlock这个东西，它用于在缓存的硬件头上做读&#x2F;写锁。可以将上面的 do {} while ()循环想象成一个简单的重试机制，它将尝试在循环中执行，直到成功。循环里处理硬件头的长度对齐。这是必需的，因为某些硬件头（如IEEE 802.11 头）大于 HH_DATA_MOD（16 字节）。将头数据复制到 skb 后，skb_push 将更新 skb 内指向数据缓冲区的指针。最后调用 dev_queue_xmit 将 skb 传递给 Linux 网络设备子系统。 7.2 n-&gt;output如果目标不是 NUD_CONNECTED 或硬件头尚未缓存，则代码沿 n-&gt;output 路径向下。neigbour 结构上的 output 指针指向哪个函数？这得看情况。要了解这是如何设置的，我们需要更多地了解邻居缓存的工作原理。 struct neighbour 包含几个重要字段：我们在上面看到的 nud_state 字段，output 函数和 ops 结构。回想一下，我们之前看到如果在缓存中找不到现有条目，会从 ip_finish_output2 调用__neigh_create 创建一个。当调用__neigh_creaet 时，将分配邻居，其 output 函数最初设置为 neigh_blackhole。随着__neigh_create 代码的进行，它将根据邻居的状态修改 output 值以指向适当的发送方法。 例如，当代码确定是“已连接的”邻居时，neigh_connect 会将 output 设置为 neigh-&gt;ops-&gt;connected_output。或者，当代码怀疑邻居可能已关闭时，neigh_suspect 会将 output 设置为 neigh-&gt;ops-&gt;output（例如，如果已超过 /proc/sys/net/ipv4/neigh/default/delay_first_probe_time 自发送探测以来的 delay_first_probe_time 秒）。 换句话说：neigh-&gt;output 会被设置为 neigh-&gt;ops_connected_output 或 neigh-&gt;ops-&gt;output，具体取决于邻居的状态。neigh-&gt;ops 来自哪里？ 分配邻居后，调用 arp_constructor（net/ipv4/arp.c ）来设置 struct neighbor 的某些字段。特别是，此函数会检查与此邻居关联的设备是否导出来一个 struct header_ops 实例， 该结构体有一个 cache 方法。 neigh-&gt;ops 设置为 net/ipv4/arp 中定义的以下实例： 1234567static const struct neigh_ops arp_hh_ops = &#123; .family = AF_INET, .solicit = arp_solicit, .error_report = arp_error_report, .output = neigh_resolve_output, .connected_output = neigh_resolve_output,&#125;; 所以，不管 neighbor 是不是“已连接的”，或者邻居缓存代码是否怀疑连接“已关闭”， neigh_resolve_output 最终都会被赋给 neigh-&gt;output。当执行到 n-&gt;output 时就会调用它。 7.3 neigh_resolve_output此函数的目的是解析未连接的邻居，或已连接但没有缓存硬件头的邻居。 12345678910111213141516/* Slow and careful. */int neigh_resolve_output(struct neighbour *neigh, struct sk_buff *skb)&#123; struct dst_entry *dst = skb_dst(skb); int rc = 0; if (!dst) goto discard; if (!neigh_event_send(neigh, skb)) &#123; int err; struct net_device *dev = neigh-&gt;dev; unsigned int seq; &#125;&#125; 代码首先进行一些基本检查，然后调用 neigh_event_send。 neigh_event_send 函数是 __neigh_event_send 的简单封装，后者干大部分脏话累活。可以在 net/core/neighbour.c 中读__neigh_event_send的源代码，从大的层面看，三种情况： NUD_NONE 状态（默认状态）的邻居：假设 /proc/sys/net/ipv4/neigh/default/app_solicit 和 /proc/sys/net/ipv4/neigh/default/mcast_solicit 配置允许发送探测（如果不是， 则将状态标记为 NUD_FAILED），将导致立即发送 ARP 请求。邻居状态将更新为 NUD_INCOMPLETE NUD_STALE 状态的邻居：将更新为 NUD_DELAYED 并且将设置计时器以稍后探测它们（ 稍后是现在的时间+&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;neigh&#x2F;default&#x2F;delay_first_probe_time 秒 ） 检查 NUD_INCOMPLETE 状态的邻居（包括上面第一种情形），以确保未解析邻居的排 队 packet 的数量小于等于&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;neigh&#x2F;default&#x2F;unres_qlen。如果超过 ，则数据包会出列并丢弃，直到小于等于 proc 中的值。统计信息中有个计数器会因此 更新 如果需要 ARP 探测，ARP 将立即被发送。__neigh_event_send 将返回 0，表示邻居被视为“已 连接”或“已延迟”，否则返回 1。返回值 0 允许 eigh_resolve_output 继续： 12if (dev-&gt;header_ops-&gt;cache &amp;&amp; !neigh-&gt;hh.hh_len) neigh_hh_init(neigh, dst); 如果邻居关联的设备的协议实现（在我们的例子中是以太网）支持缓存硬件头，并且当前没有缓存，neigh_hh_init 将缓存它。 123456do &#123; __skb_pull(skb, skb_network_offset(skb)); seq = read_seqbegin(&amp;neigh-&gt;ha_lock); err = dev_hard_header(skb, dev, ntohs(skb-&gt;protocol), neigh-&gt;ha, NULL, skb-&gt;len); &#125; while (read_seqretry(&amp;neigh-&gt;ha_lock, seq)); 接下来，seqlock 锁控制对邻居的硬件地址字段（neigh-&gt;ha）的访问。ev_hard_header 为 skb 创建以太网头时将读取该字段。之后是错误检查： 12345if (err &gt;= 0) rc = dev_queue_xmit(skb); else goto out_kfree_skb; &#125; 如果以太网头写入成功，将调用 dev_queue_xmit 将 skb 传递给 Linux 网络设备子系统进行发 送。如果出现错误，goto 将删除 skb，设置并返回错误码： 12345678910out: return rc;discard: neigh_dbg(1, &quot;%s: dst=%p neigh=%p &quot;, __func__, dst, neigh);out_kfree_skb: rc = -EINVAL; kfree_skb(skb); goto out;&#125;EXPORT_SYMBOL(neigh_resolve_output); 接下来看一些用于监控和转换 IP 协议层的文件。 8. 监控: IP 层8.1 &#x2F;proc&#x2F;net&#x2F;snmp这个文件包扩多种协议的统计，IP 层的在最前面，每一列代表什么有说明。前面我们已经看到 IP 协议层有一些地方会更新计数器。这些计数器的类型是 C 枚举类型，定义在include/uapi/linux/snmp.h: 123456789101112enum&#123; IPSTATS_MIB_NUM = 0,/* frequently written fields in fast path, kept in same cache line */ IPSTATS_MIB_INPKTS, /* InReceives */ IPSTATS_MIB_INOCTETS, /* InOctets */ IPSTATS_MIB_INDELIVERS, /* InDelivers */ IPSTATS_MIB_OUTFORWDATAGRAMS, /* OutForwDatagrams */ IPSTATS_MIB_OUTPKTS, /* OutRequests */ IPSTATS_MIB_OUTOCTETS, /* OutOctets */ /* ... */ 一些有趣的统计： 123456OutRequests: Incremented each time an IP packet is attempted to be sent. It appears that this is incremented for every send, successful or not.OutDiscards: Incremented each time an IP packet is discarded. This can happen if appending data to the skb (for corked sockets) fails, or if the layers below IP return an error.OutNoRoute: Incremented in several places, for example in the UDP protocol layer (udp_sendmsg) if no route can be generated for a given destination. Also incremented when an application calls “connect” on a UDP socket but no route can be found.FragOKs: Incremented once per packet that is fragmented. For example, a packet split into 3 fragments will cause this counter to be incremented once.FragCreates: Incremented once per fragment that is created. For example, a packet split into 3 fragments will cause this counter to be incremented thrice.FragFails: Incremented if fragmentation was attempted, but is not permitted (because the “Don’t Fragment” bit is set). Also incremented if outputting the fragment fails. 8.2 &#x2F;proc&#x2F;net&#x2F;netstat格式与前面的类似，除了每列的名称都有 IpExt 前缀之外。一些有趣的统计： 12345OutMcastPkts: Incremented each time a packet destined for a multicast address is sent.OutBcastPkts: Incremented each time a packet destined for a broadcast address is sent.OutOctects: The number of packet bytes output.OutMcastOctets: The number of multicast packet bytes output.OutBcastOctets: The number of broadcast packet bytes output. 9. 总结Linux内核网络数据包发送时，主要用到 ip_send_skb、 ip_local_out、ip_output、ip_finish_output、ip_finish_output2、 st_neigh_output等函数，本文通过分析这些函数来分享Linux内核数据包发送在 IP 层的处理，并对 IP 层进行了数据监控。"},{"path":"/2024/04/05/linux-docs/网络协议栈/Linux内核网络udp数据包发送(一)/","content":"1.前言本文首先从宏观上概述了数据包发送的流程，然后分析了协议层注册进内核以及被套接字的过程，最后介绍了通过套接字发送网络数据的过程。 2.数据包发送宏观视角从宏观上看，一个数据包从用户程序到达硬件网卡的整个过程如下： 1、使用系统调用（如 sendto，sendmsg 等）写数据2、数据分段socket顶部，进入socket协议族（protocol family）系统3、协议族处理：数据跨越协议层，这一过程（在许多情况下）转变数据（数据）转换成数据包（packet）4、数据传输路由层，这会涉及路由缓存和ARP缓存的更新；如果目的MAC不在ARP缓存表中，将触发一次ARP广播来查找MAC地址5、穿过协议层，packet到达设备无关层（设备不可知层）6、使用XPS（如果启用）或散列函数选择发送坐标7、调用网卡驱动的发送函数8、数据传送到网卡的 qdisc（queue纪律，排队规则）9、qdisc会直接发送数据（如果可以），或者将其放到串行，然后触发NET_TX类型软中断（softirq）的时候再发送10、数据从qdisc传送给驱动程序11、驱动程序创建所需的DMA映射，刹车网卡从RAM读取数据12、驱动向网卡发送信号，通知数据可以发送了13、网卡从RAM中获取数据并发送14、发送完成后，设备触发一个硬中断（IRQ），表示发送完成15、中断硬处理函数被唤醒执行。对许多设备来说，会这触发NET_RX类型的软中断，然后NAPI投票循环开始收包16、poll函数会调用驱动程序的相应函数，解除DMA映射，释放数据 3.协议层注册协议层分析我们将关注IP和UDP层，其他协议层可参考这个过程。当用户程序像下面这样创建UDP套接字时会发生什么？ 1sock = socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP) 简单来说，内核会去查找由UDP协议栈创建的一个单独的函数（其中包括用于发送和接收网络数据的函数），并赋予给套接字相应的分区 AF_INET 。内核初始化的很早阶段就执行了 inet_init 函数，这个函数会注册 AF_INET 协议族，以及该协议族内部的各协议栈（TCP，UDP，ICMP和RAW），并初始化函数使协议栈准备好处理网络数据。inet_init 定义在net &#x2F; ipv4 &#x2F; af_inet.c。AF_INET 协议族源自一个包含 create 方法的 struct net_proto_family 类型实例。当从用户程序创建socket时，内核会调用此方法： 12345static const struct net_proto_family inet_family_ops = &#123; .family = PF_INET, .create = inet_create, .owner = THIS_MODULE,&#125;; inet_create 根据传递的套接字参数，在已注册的协议中查找对应的协议： 12345678910111213141516171819202122/* Look for the requested type/protocol pair. */lookup_protocol: err = -ESOCKTNOSUPPORT; rcu_read_lock(); list_for_each_entry_rcu(answer, &amp;inetsw[sock-&gt;type], list) &#123; err = 0; /* Check the non-wild match. */ if (protocol == answer-&gt;protocol) &#123; if (protocol != IPPROTO_IP) break; &#125; else &#123; /* Check for the two wild cases. */ if (IPPROTO_IP == protocol) &#123; protocol = answer-&gt;protocol; break; &#125; if (IPPROTO_IP == answer-&gt;protocol) break; &#125; err = -EPROTONOSUPPORT; &#125; 然后，引入协议的某些方法（集合）赋给这个新创建的socket： 1sock-&gt;ops = answer-&gt;ops; 可以在 af_inet.c 中看到所有协议的初始化参数。下面是TCP和UDP的初始化参数： 12345678910111213141516171819202122232425/* Upon startup we insert all the elements in inetsw_array[] into * the linked list inetsw. */static struct inet_protosw inetsw_array[] =&#123; &#123; .type = SOCK_STREAM, .protocol = IPPROTO_TCP, .prot = &amp;tcp_prot, .ops = &amp;inet_stream_ops, .no_check = 0, .flags = INET_PROTOSW_PERMANENT | INET_PROTOSW_ICSK, &#125;, &#123; .type = SOCK_DGRAM, .protocol = IPPROTO_UDP, .prot = &amp;udp_prot, .ops = &amp;inet_dgram_ops, .no_check = UDP_CSUM_DEFAULT, .flags = INET_PROTOSW_PERMANENT, &#125;, /* .... more protocols ... */ IPPROTO_UDP 协议类型有一个 ops 变量，包含很多信息，包括用于发送和接收数据的替代函数： 123456789101112const struct proto_ops inet_dgram_ops = &#123;\t.family = PF_INET,\t.owner = THIS_MODULE, /* ... */ .sendmsg = inet_sendmsg,\t.recvmsg = inet_recvmsg, /* ... */&#125;;EXPORT_SYMBOL(inet_dgram_ops); prot UDP协议对应的 prot 变量为 udp_prot，定义在net &#x2F; ipv4 &#x2F; udp.c： 123456789101112struct proto udp_prot = &#123;\t.name = &quot;UDP&quot;,\t.owner = THIS_MODULE, /* ... */ .sendmsg = udp_sendmsg,\t.recvmsg = udp_recvmsg, /* ... */&#125;;EXPORT_SYMBOL(udp_prot); 现在，让我们转向发送UDP数据的用户程序，看看 udp_sendmsg 是如何在内核中被调用的。 4.通过套接字发送网络数据用户程序想发送UDP网络数据，因此它使用 sendto 系统调用： 1ret = sendto(socket, buffer, buflen, 0, &amp;dest, sizeof(dest)); 该系统调用串联Linux系统调用（系统调用）层，最后到达net &#x2F; socket.c中的这个函数： 12345678910111213141516/* * Send a datagram to a given address. We move the address into kernel * space and check the user space data area is readable before invoking * the protocol. */SYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len, unsigned int, flags, struct sockaddr __user *, addr, int, addr_len)&#123; /* ... code ... */ err = sock_sendmsg(sock, &amp;msg, len); /* ... code ... */&#125; SYSCALL_DEFINE6 宏会展开成一堆宏，并通过一波复杂操作创建出一个带。6个参数的系统调用（因此叫 DEFINE6）。作为结果之一，会看到内核中的所有系统调用都带 sys_相连。sendto 代码会先将数据整理成一体的可能可以处理的格式，然后调用 sock_sendmsg。特别地，依次传递给 sendto 的地址放到另一个变量（msg）中： 123456789101112131415iov.iov_base = buff;iov.iov_len = len;msg.msg_name = NULL;msg.msg_iov = &amp;iov;msg.msg_iovlen = 1;msg.msg_control = NULL;msg.msg_controllen = 0;msg.msg_namelen = 0;if (addr) &#123; err = move_addr_to_kernel(addr, addr_len, &amp;address); if (err &lt; 0) goto out_put; msg.msg_name = (struct sockaddr *)&amp;address; msg.msg_namelen = addr_len;&#125; 这段代码将用户程序传入到内核的（存放待发送数据的）地址，作为 msg_name 字段嵌入到 struct msghdr 类型变量中。状语从句：这用户程序直接调用 sendmsg 而不是 sendto 发送数据差不多，这之所以可行，因为的英文 sendto 状语从句： sendmsg 底层都会调用 sock_sendmsg。 4.1 sock_sendmsg， __sock_sendmsg， __sock_sendmsg_nosecsock_sendmsg 做一些错误检查，然后调用__sock_sendmsg;后者做一些自己的错误检查，然后调用__sock_sendmsg_nosec。 __sock_sendmsg_nosec 将数据传递到插座子系统的更深处： 123456789static inline int __sock_sendmsg_nosec(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; struct sock_iocb *si = .... /* other code ... */ return sock-&gt;ops-&gt;sendmsg(iocb, sock, msg, size);&#125; 通过前面介绍的socket创建过程，可以知道注册到这里的 sendmsg 方法就是 inet_sendmsg。 4.2 inet_sendmsg从名字可以猜到，的英文这 AF_INET 协议族提供的通用函数此函数首先调用。 sock_rps_record_flow 来记录最后一个处理该（数据所属的）流的CPU; 接下来，调用套接字的协议类型（本例是UDP）对应的 sendmsg 方法： 1234567891011121314int inet_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; struct sock *sk = sock-&gt;sk; sock_rps_record_flow(sk); /* We may need to bind the socket. */ if (!inet_sk(sk)-&gt;inet_num &amp;&amp; !sk-&gt;sk_prot-&gt;no_autobind &amp;&amp; inet_autobind(sk)) return -EAGAIN; return sk-&gt;sk_prot-&gt;sendmsg(iocb, sk, msg, size);&#125;EXPORT_SYMBOL(inet_sendmsg); 本例是UDP协议，因此上面的 sk-&gt;sk_prot-&gt;sendmsg 指向的是之前udp_prot 看到的（通过 导出的）udp_sendmsg 函数。sendmsg（）函数作为分界点，处理逻辑从AF_INET协议族通用处理转移到特定的UDP协议的处理。"},{"path":"/2024/04/05/linux-docs/网络协议栈/Linux内核网络udp数据包发送（二）-UDP协议层分析/","content":"1.前言本文分享了Linux内核网络数据包发送在UDP协议层的处理，主要分析了udp_sendmsg和udp_send_skb函数，并分享了UDP层的数据统计和监控以及套接字发送大小的调优。 2.udp_sendmsg这个函数定义在net &#x2F; ipv4 &#x2F; udp.c，函数很长，分段来看。 2.1 UDP插入UDP udp_sendmsg corking是一项优化技术，允许内核将多个数据累积成一体的数据报发送。在用户程序中有两种方法可以启用此选项： 使用 setsockopt 系统调用设置socket的 UDP_CORK 选项 程序调用 send，sendto 或 sendmsg 时，带 MSG_MORE 参数 udp_sendmsg 代码检查 up-&gt;pending 套接字socket当前是否已被塞住（corked），如果是，则直接跳到 do_append_data 进行数据追加（append）。 1234567891011121314151617181920212223int udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg, size_t len)&#123; /* variables and error checking ... */ fl4 = &amp;inet-&gt;cork.fl.u.ip4; if (up-&gt;pending) &#123; /* * There are pending frames. * The socket lock must be held while it&#x27;s corked. */ lock_sock(sk); if (likely(up-&gt;pending)) &#123; if (unlikely(up-&gt;pending != AF_INET)) &#123; release_sock(sk); return -EINVAL; &#125; goto do_append_data; &#125; release_sock(sk); &#125; 2.2获取目的IP地址和端口接下来获取目的IP地址和端口，有两个可能的来源： 如果之前socket已经建立连接，那socket本身就存储了目标地址 地址通过辅助结构（struct msghdr）预测，逐步我们在 sendto 的内核代码中看到的那样 具体逻辑： 123456789101112131415161718192021222324252627/* * Get and verify the address. */ if (msg-&gt;msg_name) &#123; struct sockaddr_in *usin = (struct sockaddr_in *)msg-&gt;msg_name; if (msg-&gt;msg_namelen &lt; sizeof(*usin)) return -EINVAL; if (usin-&gt;sin_family != AF_INET) &#123; if (usin-&gt;sin_family != AF_UNSPEC) return -EAFNOSUPPORT; &#125; daddr = usin-&gt;sin_addr.s_addr; dport = usin-&gt;sin_port; if (dport == 0) return -EINVAL; &#125; else &#123; if (sk-&gt;sk_state != TCP_ESTABLISHED) return -EDESTADDRREQ; daddr = inet-&gt;inet_daddr; dport = inet-&gt;inet_dport; /* Open fast path for connected socket. Route will not be used, if at least one option is set. */ connected = 1; &#125; UDP代码中出现了 TCP_ESTABLISHED！UDP套接字的状态使用了TCP状态来描述。的上面显示代码了内核如何解析该变量以便设置 daddr 状语从句： dport。 如果没有 struct msghdr 变量，内核函数到达 udp_sendmsg 函数时，会从socket本身检索目的地址和端口，插入socket标记为“已连接”。 2.3套接字发送：簿记和打合并接下来，获取存储在插座上的源地址，设备索引（设备索引）和时间戳选项（例如SOCK_TIMESTAMPING_TX_HARDWARE， SOCK_TIMESTAMPING_TX_SOFTWARE， SOCK_WIFI_STATUS）： 12345ipc.addr = inet-&gt;inet_saddr;ipc.oif = sk-&gt;sk_bound_dev_if;sock_tx_timestamp(sk, &amp;ipc.tx_flags); 2.4辅助消息（辅助消息）除了发送或接收数据包之外，sendmsg 状语从句： recvmsg 系统-调用还网求允许用户设置或请求辅助数据。程序用户可以通过将请求信息组织分类中翻译 struct msghdr 类型变量来利用此辅助数据。一些辅助数据类型记录在IP手册页中。 辅助数据的一个常见例子是 IP_PKTINFO，对于 sendmsg，IP_PKTINFO 网求允许程序在发送数据时设置一个 in_pktinfo 变量。可以程序通过填写回复时 struct in_pktinfo 变量中的字段来指定要在包上使用的源地址。如果程序是监听多个IP地址的服务端程序，那这是一个很有用的选项。在这种情况下，服务端可能想使用客户端连接服务端的那个IP地址来回复客户端，IP_PKTINFO 非常适合这种场景。 setsockopt 可以在套接字级别设置发送包的IP_TTL和IP_TOS。而辅助消息允许在每个数据包级别设置TTL和TOS值。时从qdisc中发送出去。 可以看到内核如何在UDP套接字上处理 sendmsg 的辅助消息： 123456789if (msg-&gt;msg_controllen) &#123; err = ip_cmsg_send(sock_net(sk), msg, &amp;ipc, sk-&gt;sk_family == AF_INET6); if (err) return err; if (ipc.opt) free = 1; connected = 0;&#125; 解析辅助消息的工作是由 ip_cmsg_send 完成的，定义在net &#x2F; ipv4 &#x2F; ip_sockglue.c。传递一个未初始化的辅助数据，将会把这个套接字标记为“未建立连接的”。 2.5设置自定义IP选项接下来，sendmsg 将检查用户是否通过辅助消息设置了任何自定义IP选项。如果设置了，将使用这些自定义值；如果没有，那就使用socket中（已经在用）的参数： 123456789101112if (!ipc.opt) &#123; struct ip_options_rcu *inet_opt; rcu_read_lock(); inet_opt = rcu_dereference(inet-&gt;inet_opt); if (inet_opt) &#123; memcpy(&amp;opt_copy, inet_opt, sizeof(*inet_opt) + inet_opt-&gt;opt.optlen); ipc.opt = &amp;opt_copy.opt; &#125; rcu_read_unlock();&#125; 接下来，该函数检查是否设置了源记录路由（源记录路由，SRR）IP选项。SRR有两种类型：宽松的源记录路由和严格的源记录路由。地址串联其保存到 faddr，插入套接字标记为“未连接”。这将在后面用到： 12345678ipc.addr = faddr = daddr;if (ipc.opt &amp;&amp; ipc.opt-&gt;opt.srr) &#123; if (!daddr) return -EINVAL; faddr = ipc.opt-&gt;opt.faddr; connected = 0;&#125; 处理完SRR选项后，将处理TOS选项，这可以从辅助消息中获取，或者从socket当前值中获取。然后检查： 1、是否（使用 setsockopt）在插座上设置了 SO_DONTROUTE，或2、是否（调用 sendto 或 sendmsg 时）指定了 MSG_DONTROUTE 标志，或3、是否已设置了 is_strictroute，表示需要严格的SRR任何一个为真，tos 扩展的 RTO_ONLINK 位将置1，并且socket被视为“未连接”： 1234567tos = get_rttos(&amp;ipc, inet);if (sock_flag(sk, SOCK_LOCALROUTE) || (msg-&gt;msg_flags &amp; MSG_DONTROUTE) || (ipc.opt &amp;&amp; ipc.opt-&gt;opt.is_strictroute)) &#123; tos |= RTO_ONLINK; connected = 0;&#125; 2.6多播或单播（多播或单播）这有点复杂，因为用户可以通过 IP_PKTINFO 辅助消息来指定发送包的源地址或设备号，如前所述。 如果目标地址是多播地址：1、将多播设备（设备）的索引（索引）设置为发送（写）这个数据包的设备索引，并且2、数据包的源地址将设置为多播 如果目标地址不是一个收件人地址，则发送数据包的设备准备为 inet-&gt;uc_index（单播），除非用户使用 IP_PKTINFO 辅助消息覆盖了它。 123456789if (ipv4_is_multicast(daddr)) &#123; if (!ipc.oif) ipc.oif = inet-&gt;mc_index; if (!saddr) saddr = inet-&gt;mc_addr; connected = 0;&#125; else if (!ipc.oif) ipc.oif = inet-&gt;uc_index; 2.7路由现在开始路由，UDP层中处理路由的代码以快速路径（fast path）开始。如果套接字已连接，则直接尝试获取路由： 12if (connected) rt = (struct rtable *)sk_dst_check(sk, 0); 如果socket未连接，或者虽然已经连接，但sk_dst_check 路由辅助 功能已确定路由已过期，则代码将进入慢速路径（slow path）以生成一条路由记录。首先调用 flowi4_init_output 构造一个描述此UDP流的变量： 12345678910111213141516if (rt == NULL) &#123; struct net *net = sock_net(sk); fl4 = &amp;fl4_stack; flowi4_init_output(fl4, ipc.oif, sk-&gt;sk_mark, tos, RT_SCOPE_UNIVERSE, sk-&gt;sk_protocol, inet_sk_flowi_flags(sk)|FLOWI_FLAG_CAN_SLEEP, faddr, saddr, dport, inet-&gt;inet_sport);````然后，socket为此流程实例实例传递给安全实例，这样SELinux或SMACK这样的系统就可以在流程实例上设置安全ID。接下来，ip_route_output_flow 将调用IP路由代码，创建一个路由实例：```csecurity_sk_classify_flow(sk, flowi4_to_flowi(fl4));rt = ip_route_output_flow(net, fl4, sk); 如果创建路由实例失败，并且返回码是 ENETUNREACH，则 OUTNOROUTES 计数器将会加1。 1234567if (IS_ERR(rt)) &#123; err = PTR_ERR(rt); rt = NULL; if (err == -ENETUNREACH) IP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES); goto out;&#125; 这些统计计数器所在的源文件，其他可用的计数器及其含义，将在下面的UDP监控部分共享。 接下来，如果是广播路由，但socket的 SOCK_BROADCAST 选项未设置，则处理过程终止。如果socket被视为“已连接”，则路由实例将缓存到socket上： 123456err = -EACCES;if ((rt-&gt;rt_flags &amp; RTCF_BROADCAST) &amp;&amp; !sock_flag(sk, SOCK_BROADCAST)) goto out;if (connected) sk_dst_set(sk, dst_clone(&amp;rt-&gt;dst)); 2.8 MSG_CONFIRM：阻止ARP缓存过期如果调用 send， sendto 或 sendmsg 的时候指定了 MSG_CONFIRM 参数，UDP协议层将会如下处理： 123if (msg-&gt;msg_flags&amp;MSG_CONFIRM) goto do_confirm;back_from_confirm: 该标志提示系统去确认一下ARP缓存缓存是否仍然有效，防止其被垃圾回收。 do_confirm 标签位于此函数末尾处： 123456do_confirm: dst_confirm(&amp;rt-&gt;dst); if (!(msg-&gt;msg_flags&amp;MSG_PROBE) || len) goto back_from_confirm; err = 0; goto out; dst_confirm 函数只是在相应的缓存缓存上设置一个标记位，稍后当查询邻居缓存并找到放置时将检查该标志，我们后面一些会看到。此功能通常用于UDP网络应用程序，以减少不必要的ARP流量。此代码确认缓存并然后跳回 back_from_confirm 标签。一旦 do_confirm 代码跳回到 back_from_confirm（或者之前就没有执行到 do_confirm ），代码接下来将处理UDP软木塞和未塞木塞的情况。 2.9 uncorked UDP套接字快速路径：准备待发送数据如果不需要corking，数据就可以封装到一个 struct sk_buff 实例中并传递给 udp_send_skb，离IP协议层更进了一步。这是通过调用 ip_make_skb 来完成的。 先前通过调用ip_route_output_flow 生成的路由可能也会 一起传进来，并保存到skb里。 12345678910/* Lockless fast path for the non-corking case. */if (!corkreq) &#123; skb = ip_make_skb(sk, fl4, getfrag, msg-&gt;msg_iov, ulen, sizeof(struct udphdr), &amp;ipc, &amp;rt, msg-&gt;msg_flags); err = PTR_ERR(skb); if (!IS_ERR_OR_NULL(skb)) err = udp_send_skb(skb, fl4); goto out;&#125; ip_make_skb 函数将创建一个skb，其中需要考虑到很多的事情，例如： MTU UDP阻塞（如果启用） UDP分片卸载（UFO） Fragmentation（分片）：如果硬件不支持UFO，但是要传输的数据大于MTU，需要软件做分片 大多数网络设备驱动程序不支持UFO，因为网络硬件本身不支持此功能。 2.9.1 ip_make_skb定义在net &#x2F; ipv4 &#x2F; ip_output.c，这个函数有点复杂。 构建SKB的时候，ip_make_skb 依赖的底层代码需要使用一个加塞变量和一个队列变量，SKB将通过队列变量传入。如果插座未被软木，则会传入一个假的加塞变量和一个空队列。 现在来看看假corking变量和空数值是如何初始化的： 1234567891011121314151617struct sk_buff *ip_make_skb(struct sock *sk, /* more args */)&#123; struct inet_cork cork; struct sk_buff_head queue; int err; if (flags &amp; MSG_PROBE) return NULL; __skb_queue_head_init(&amp;queue); cork.flags = 0; cork.addr = 0; cork.opt = NULL; err = ip_setup_cork(sk, &amp;cork, /* more args */); if (err) return ERR_PTR(err); 如上所示，cork和queue都是在栈上分配的，ip_make_skb 根本不需要它。 ip_setup_cork 初始化cork变量。然后，调用__ip_append_data 并调用cork和queue变量： 1234err = __ip_append_data(sk, fl4, &amp;queue, &amp;cork, &amp;current-&gt;task_frag, getfrag, from, length, transhdrlen, flags); 我们将在后面看到这个函数是如何工作的，因为无论socket是否被软木塞，最后都会执行它。 现在，我们只需要知道__ip_append_data 将创建一个skb，向其追加数据，转化为该skb添加到队列的队列变量中。如果追加数据失败，则__ip_flush_pending_frame 调用数据并向上返回错误（指针类型）： 1234if (err) &#123; __ip_flush_pending_frames(sk, &amp;queue, &amp;cork); return ERR_PTR(err);&#125; 最后，如果没有发生错误，__ip_make_skb 将skb出队，添加IP选项，并返回一个准备好传递给更连续发送的skb： 1return __ip_make_skb(sk, fl4, &amp;queue, &amp;cork); 2.9.2发送数据如果没有错误，skb将会交给 udp_send_skb，附属会继续将其传给下一层协议，IP协议： 1234err = PTR_ERR(skb);if (!IS_ERR_OR_NULL(skb)) err = udp_send_skb(skb, fl4);goto out; 如果有错误，错误计数就会有相应增加。后面的“错误计数”部分会详细介绍。 2.10没有被软木的数据时的慢路径如果使用了UDP corking，但之前没有数据被cork，则慢路径开始： 对插座加锁 检查应用程序是否有bug：已经被cork的socket是否再次被cork 设置该UDP流的一些参数，为corking做准备 将要发送的数据追加到现有数据 udp_sendmsg 代码继续向下看，就是这一逻辑： 1234567891011121314151617181920212223242526lock_sock(sk); if (unlikely(up-&gt;pending)) &#123; /* The socket is already corked while preparing it. */ /* ... which is an evident application bug. --ANK */ release_sock(sk); LIMIT_NETDEBUG(KERN_DEBUG pr_fmt(&quot;cork app bug 2 &quot;)); err = -EINVAL; goto out; &#125; /* * Now cork the socket to pend data. */ fl4 = &amp;inet-&gt;cork.fl.u.ip4; fl4-&gt;daddr = daddr; fl4-&gt;saddr = saddr; fl4-&gt;fl4_dport = dport; fl4-&gt;fl4_sport = inet-&gt;inet_sport; up-&gt;pending = AF_INET;do_append_data: up-&gt;len += ulen; err = ip_append_data(sk, fl4, getfrag, msg-&gt;msg_iov, ulen, sizeof(struct udphdr), &amp;ipc, &amp;rt, corkreq ? msg-&gt;msg_flags|MSG_MORE : msg-&gt;msg_flags); 2.10.1 ip_append_data这个函数简单封装了__ip_append_data，在调用之前之前，做了两件重要的事情： 检查是否从用户指定了 MSG_PROBE 标志。该标志表示用户不想真正发送数据，只是做路径探测（例如，确定PMTU） 检查socket的发送物理是否为空。如果为空，则意味着没有cork数据等待处理，因此调用ip_setup_cork 来设置corking 一旦处理了上述条件，就调用__ip_append_data 函数，该函数包含用于将数据处理成数据包的大量逻辑。 2.10.2 __ip_append_data如果插座是塞住，则从 ip_append_data 调用此函数;如果插座未被软木，则从 ip_make_skb 。调用此函数在任何一种情况下，函数都将分配一个新缓冲区来存储传入的数据，或者将数据附加到现有数据中。这种工作的方式围绕socket的发送序列。等待发送的现有数据（例如，如果socket被软木塞）将在一部分中有一个对应的对应，可以被追加数据。 这个函数很复杂，它执行很多计算逐步如何构造传递给下面的网络层的skb。 该函数的重点包括： 如果硬件支持，则处理UDP分段卸载（UFO）。绝大多数网络硬件不支持UFO。如果你的网卡驱动程序支持它，设置它将 NETIF_F_UFO 标记位 处理支持分散&#x2F;收集（分散&#x2F;聚集）IO的网卡许多卡都支持此功能，并使用。 NETIF_F_SG 标志进行通告支持该特性的网卡可以处理数据被分散到多个缓冲器的数据包;内核不需要花时间将避免这种额外的复制会提升性能，大多数网卡都支持此功能 通过调用sock_wmalloc 跟踪发送帧 的大小。当分配新的skb时，skb的大小由创建它的socket计费（charge），并计入socket发送的已分配字节数。 （超过计费限制），则skb并分配失败并返回错误。我们将在下面的调优部分中看到如何设置socket发送大小（txqueuelen） 更新错误统计信息。此函数中的任何错误都会增加“丢弃”计数。 函数执行成功后返回0，以及一个适用于网络设备传输的skb。 在unorked情况下，持有skb的队列被作为参数传递给上面描述的__ip_make_skb，在那里它被出队并通过 udp_send_skb 发送到更轻松。 在软木的情况下，__ip_append_data 的返回值向上传递。数据位于发送队列中，直到 udp_sendmsg 确定的英文时候调用 udp_push_pending_frames 来完成SKB，后者会进一步调用 udp_send_skb。 2.10.3冲洗软木塞现在，udp_sendmsg 会继续，检查__ip_append_skb 的返回值（错误码）： 1234567if (err) udp_flush_pending_frames(sk);else if (!corkreq) err = udp_push_pending_frames(sk);else if (unlikely(skb_queue_empty(&amp;sk-&gt;sk_write_queue))) up-&gt;pending = 0;release_sock(sk); 我们来看看每个情况： 如果出现错误（错误为非零），则调用 udp_flush_pending_frames，这将取消cork并从socket的发送数据中删除所有数据 如果在未指定 MSG_MORE 的情况下发送此数据，则调用 udp_push_pending_frames，则数据传递到更下面的网络层 如果发送轴向为空，替换套接字标记为不再软木 如果追加操作完成并且有更多数据要进入cork，则代码将做一些清理工作，并返回追加数据的长度： 12345ip_rt_put(rt);if (free) kfree(ipc.opt);if (!err) return len; 这就是内核如何处理 2.11错误会计如果： non-corking快速路径创建skb失败，或 udp_send_skb 返回错误，或 ip_append_data 无法将数据附加到corked UDP套接字，或 当 udp_push_pending_frames 调用 udp_send_skb 发送corked skb时间返回错误 仅当返回的错误是 ENOBUFS（内核无可用内存）或套接字已设置 SOCK_NOSPACE（发送串行已满）时，SNDBUFERRORS 统计信息才会增加： 123456789101112/* * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space. Reporting * ENOBUFS might not be good (it&#x27;s not tunable per se), but otherwise * we don&#x27;t have a good statistic (IpOutDiscards but it can be too many * things). We could add another new stat but at least for now that * seems like overkill. */if (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags)) &#123; UDP_INC_STATS_USER(sock_net(sk), UDP_MIB_SNDBUFERRORS, is_udplite);&#125;return err; 我们接下来会在后面的数据监控里看到如何读取这些计数。 3. udp_send_skbudp_sendmsg 通过调用 udp_send_skb 函数将skb转到下一网络层，在这里中是IP协议层。 向skb添加UDP头 处理校验和：软件校验和，硬件校验和或无校​​验和（如果替换） 调用 ip_send_skb 将skb发送到IP协议层 更新发送成功或失败的统计计数器 首先，创建UDP头： 123456789101112static int udp_send_skb(struct sk_buff *skb, struct flowi4 *fl4)&#123; /* useful variables ... */ /* * Create a UDP header */ uh = udp_hdr(skb); uh-&gt;source = inet-&gt;inet_sport; uh-&gt;dest = fl4-&gt;fl4_dport; uh-&gt;len = htons(len); uh-&gt;check = 0; 接下来，处理校验和。有几种情况： 首先处理UDP-Lite验证和 接下来，如果socket校正和选项被关闭（setsockopt 带 SO_NO_CHECK 参数），则被标记为校验和关闭 接下来，如果硬件支持UDP校验和，则将调用 udp4_hwcsum 来设置它。请注意，如果数据包是分段的，内核将在软件中生成校正和，可以在udp4_hwcsum的源代码中看到这一点 最后，通过调用 udp_csum 生成软件校验和 123456789101112131415if (is_udplite) /* UDP-Lite */ csum = udplite_csum(skb);else if (sk-&gt;sk_no_check == UDP_CSUM_NOXMIT) &#123; /* UDP csum disabled */ skb-&gt;ip_summed = CHECKSUM_NONE; goto send;&#125; else if (skb-&gt;ip_summed == CHECKSUM_PARTIAL) &#123; /* UDP hardware csum */ udp4_hwcsum(skb, fl4-&gt;saddr, fl4-&gt;daddr); goto send;&#125; else csum = udp_csum(skb); 接下来，添加了伪头： 1234uh-&gt;check = csum_tcpudp_magic(fl4-&gt;saddr, fl4-&gt;daddr, len, sk-&gt;sk_protocol, csum);if (uh-&gt;check == 0) uh-&gt;check = CSUM_MANGLED_0; 如果校验和为0，则根据RFC 768，校验为全1（全部传输（等效于补码算术））。最后，将skb传递给IP协议层并增加统计计数： 123456789101112send: err = ip_send_skb(sock_net(sk), skb); if (err) &#123; if (err == -ENOBUFS &amp;&amp; !inet-&gt;recverr) &#123; UDP_INC_STATS_USER(sock_net(sk), UDP_MIB_SNDBUFERRORS, is_udplite); err = 0; &#125; &#125; else UDP_INC_STATS_USER(sock_net(sk), UDP_MIB_OUTDATAGRAMS, is_udplite); return err; 如果 ip_send_skb 成功，将更新 OUTDATAGRAMS 统计。如果IP协议层报告错误，并且错误是 ENOBUFS（内核内核内存）而且错误queue（inet-&gt;recverr）没有启用，则更新 SNDBUFERRORS。 接下来看看如何在Linux内核中监视和调优UDP协议层。 4.监控：UDP层统计两个非常有用的获取UDP协议统计文件： 12/proc/net/snmp/proc/net/udp 4.1 &#x2F; proc &#x2F; net &#x2F; snmp监控UDP协议层统计： cat /proc/net/snmp | grep Udp\\: 要准确地理解这些计数，需要仔细地阅读内核代码。一些类型的错误计数并不是只出现在一种计数中，而可能是出现在多个计数中。 InDatagrams：当userland程序使用recvmsg读取数据报时增加。当UDP数据包被封装并发回进行处理时，该值也会增加。 NoPorts：当UDP数据包到达没有程序正在侦听的端口时增加。 InErrors：在以下几种情况下增加：在接收队列中没有内存，看到错误的校验和，以及sk_add_backlog无法添加数据报。 OutDatagrams：当将UDP数据包无误传递到要发送的IP协议层时增加。 RcvbufErrors：当sock_queue_rcv_skb报告没有可用的内存时增加；如果sk-&gt; sk_rmem_alloc大于或等于sk-&gt; sk_rcvbuf，则会发生这种情况。 SndbufErrors：如果IP协议层在尝试发送数据包时报告错误，并且未设置错误队列，则增加。如果没有发送队列空间或内核内存可用，也将增加。 InCsumErrors：当检测到UDP校验和失败时增加。请注意，在所有情况下，我都能发现InCsumErrors与InErrors同时增加。因此，InErrors-InCsumErros应该在接收端产生与内存相关的错误计数。 UDP协议层发现的某些错误会出现在其他协议层的统计信息中。一个例子：路由错误。 udp_sendmsg 发现的路由错误将导致IP协议层的 OutNoRoutes 统计增加。 4.2 &#x2F; proc &#x2F; net &#x2F; udp监控UDP套接字统计： cat /proc/net/udp 每一列的意思： sl：套接字的内核哈希槽 local_address：套接字的十六进制本地地址和端口号，以：分隔。 rem_address：套接字的十六进制远程地址和端口号，以：分隔。 st：套接字的状态。奇怪的是，UDP协议层似乎使用了某些TCP套接字状态。在上面的示例中，7是TCP_CLOSE。 tx_queue：在内核中为传出UDP数据报分配的内存量。 rx_queue：在内核中为传入的UDP数据报分配的内存量。 tr，tm-&gt; when，retrnsmt：UDP协议层未使用这些字段。 uid：创建此套接字的用户的有效用户ID。 timeout：未由UDP协议层使用。 inode：与此套接字相对应的inode编号。您可以使用它来帮助您确定哪个用户进程打开了此套接字。检查&#x2F; proc &#x2F; [pid] &#x2F; fd，其中将包含指向socket [：inode]的符号链接。 ref：套接字的当前引用计数。 pointer：struct sock内核中的内存地址。 drops：与此套接字关联的数据报丢弃数。请注意，这不包括与发送数据报有关的任何丢弃（在已塞好的UDP套接字上或以其他方式）；从本博客文章所检查的内核版本开始，该值仅在接收路径中递增。 打印这些计数的代码在net &#x2F; ipv4 &#x2F; udp.c。 5.调优：socket发送收发内存大小发送副本（也叫“写类别”）的副本可以通过设置 net.core.wmem_max sysctl 进行修改。 $ sudo sysctl -w net.core.wmem_max=8388608 sk-&gt;sk_write_queue 用 net.core.wmem_default 初始化，这个值也可以调整。 调整初始发送缓冲区大小： $ sudo sysctl -w net.core.wmem_default=8388608 也可以通过从应用程序调用 setsockopt 并传递 SO_SNDBUF 来设置 sk-&gt;sk_write_queue 。通过 setsockopt 设置的可选是 net.core.wmem_max。 不过，可以通过 setsockopt 并传递传递 SO_SNDBUFFORCE 覆盖 net.core.wmem_max 限制，这需要 CAP_NET_ADMIN 权限。 每次调用__ip_append_data 分配skb时，sk-&gt;sk_wmem_alloc 都会增长。预先我们所看到的，UDP数据报传输速度很快，通常不会在发送当中花费了太多时间。 6.总结本文重点分析了数据包在传输层（UDP协议）的发送过程，并进行了监控和调优，后面的数据包将到达IP协议层，下次再分享，感谢阅读。"},{"path":"/2024/04/05/linux-docs/网络协议栈/Linux操作系统原理—内核网络协议栈/","content":"前言本文主要记录 Linux 内核网络协议栈的运行原理 数据报文的封装与分用 封装：当应用程序用 TCP 协议传送数据时，数据首先进入内核网络协议栈中，然后逐一通过 TCP&#x2F;IP 协议族的每层直到被当作一串比特流送入网络。对于每一层而言，对收到的数据都会封装相应的协议首部信息（有时还会增加尾部信息）。TCP 协议传给 IP 协议的数据单元称作 TCP 报文段，或简称 TCP 段（TCP segment）。IP 传给数据链路层的数据单元称作 IP 数据报（IP datagram），最后通过以太网传输的比特流称作帧（Frame） 分用：当目的主机收到一个以太网数据帧时，数据就开始从内核网络协议栈中由底向上升，同时去掉各层协议加上的报文首部。每层协议都会检查报文首部中的协议标识，以确定接收数据的上层协议。这个过程称作分用。 Linux 内核网络协议栈协议栈的全景图 协议栈的分层结构 逻辑抽象层级： 物理层：主要提供各种连接的物理设备，如各种网卡，串口卡等。 链路层：主要提供对物理层进行访问的各种接口卡的驱动程序，如网卡驱动等。 网路层：是负责将网络数据包传输到正确的位置，最重要的网络层协议是 IP 协议，此外还有如 ICMP，ARP，RARP 等协议。 传输层：为应用程序之间提供端到端连接，主要为 TCP 和 UDP 协议。 应用层：顾名思义，主要由应用程序提供，用来对传输数据进行语义解释的 “人机交互界面层”，比如 HTTP，SMTP，FTP 等协议。 协议栈实现层级： 硬件层（Physical device hardware）：又称驱动程序层，提供连接硬件设备的接口。 设备无关层（Device agnostic interface）：又称设备接口层，提供与具体设备无关的驱动程序抽象接口。这一层的目的主要是为了统一不同的接口卡的驱动程序与网络协议层的接口，它将各种不同的驱动程序的功能统一抽象为几个特殊的动作，如 open，close，init 等，这一层可以屏蔽底层不同的驱动程序。 网络协议层（Network protocols）：对应 IP layer 和 Transport layer。毫无疑问，这是整个内核网络协议栈的核心。这一层主要实现了各种网络协议，最主要的当然是 IP，ICMP，ARP，RARP，TCP，UDP 等。 协议无关层（Protocol agnostic interface），又称协议接口层，本质就是 SOCKET 层。这一层的目的是屏蔽网络协议层中诸多类型的网络协议（主要是 TCP 与 UDP 协议，当然也包括 RAW IP， SCTP 等等），以便提供简单而同一的接口给上面的系统调用层调用。简单的说，不管我们应用层使用什么协议，都要通过系统调用接口来建立一个 SOCKET，这个 SOCKET 其实是一个巨大的 sock 结构体，它和下面的网络协议层联系起来，屏蔽了不同的网络协议，通过系统调用接口只把数据部分呈献给应用层。 BSD（Berkeley Software Distribution）socket：BSD Socket 层，提供统一的 SOCKET 操作接口，与 socket 结构体关系紧密。 INET（指一切支持 IP 协议的网络） socket：INET socket 层，调用 IP 层协议的统一接口，与 sock 结构体关系紧密。 系统调用接口层（System call interface），实质是一个面向用户空间（User Space）应用程序的接口调用库，向用户空间应用程序提供使用网络服务的接口。 协议栈的数据结构 msghdr：描述了从应用层传递下来的消息格式，包含有用户空间地址，消息标记等重要信息。 iovec：描述了用户空间地址的起始位置。 file：描述文件属性的结构体，与文件描述符一一对应。 file_operations：文件操作相关结构体，包括 read()、write()、open()、ioctl() 等。 socket：向应用层提供的 BSD socket 操作结构体，协议无关，主要作用为应用层提供统一的 Socket 操作。 sock：网络层 sock，定义与协议无关操作，是网络层的统一的结构，传输层在此基础上实现了 inet_sock。 sock_common：最小网络层表示结构体。 inet_sock：表示层结构体，在 sock 上做的扩展，用于在网络层之上表示 inet 协议族的的传输层公共结构体。 udp_sock：传输层 UDP 协议专用 sock 结构，在传输层 inet_sock 上扩展。 proto_ops：BSD socket 层到 inet_sock 层接口，主要用于操作 socket 结构。 proto：inet_sock 层到传输层操作的统一接口，主要用于操作 sock 结构。 net_proto_family：用于标识和注册协议族，常见的协议族有 IPv4、IPv6。 softnet_data：内核为每个 CPU 都分配一个这样的 softnet_data 数据空间。每个 CPU 都有一个这样的队列，用于接收数据包。 sk_buff：描述一个帧结构的属性，包含 socket、到达时间、到达设备、各层首部大小、下一站路由入口、帧长度、校验和等等。 sk_buff_head：数据包队列结构。 net_device：这个巨大的结构体描述一个网络设备的所有属性，数据等信息。 inet_protosw：向 IP 层注册 socket 层的调用操作接口。 inetsw_array：socket 层调用 IP 层操作接口都在这个数组中注册。 sock_type：socket 类型。 IPPROTO：传输层协议类型 ID。 net_protocol：用于传输层协议向 IP 层注册收包的接口。 packet_type：以太网数据帧的结构，包括了以太网帧类型、处理方法等。 rtable：路由表结构，描述一个路由表的完整形态。 rt_hash_bucket：路由表缓存。 dst_entry：包的去向接口，描述了包的去留，下一跳等路由关键信息。 napi_struct：NAPI 调度的结构。NAPI 是 Linux 上采用的一种提高网络处理效率的技术，它的核心概念就是不采用中断的方式读取数据，而代之以首先采用中断唤醒数据接收服务，然后采用 poll 的方法来轮询数据。NAPI 技术适用于高速率的短长度数据包的处理。 网络协议栈初始化流程这需要从内核启动流程说起。当内核完成自解压过程后进入内核启动流程，这一过程先在 arch&#x2F;mips&#x2F;kernel&#x2F;head.S 程序中，这个程序负责数据区（BBS）、中断描述表（IDT）、段描述表（GDT）、页表和寄存器的初始化，程序中定义了内核的入口函数 kernel_entry()、kernel_entry() 函数是体系结构相关的汇编代码，它首先初始化内核堆栈段为创建系统中的第一过程进行准备，接着用一段循环将内核映像的未初始化的数据段清零，最后跳到 start_kernel() 函数中初始化硬件相关的代码，完成 Linux Kernel 环境的建立。 start_kenrel() 定义在 init&#x2F;main.c 中，真正的内核初始化过程就是从这里才开始。函数 start_kerenl() 将会调用一系列的初始化函数，如：平台初始化，内存初始化，陷阱初始化，中断初始化，进程调度初始化，缓冲区初始化，完成内核本身的各方面设置，目的是最终建立起基本完整的 Linux 内核环境。 start_kernel() 中主要函数及调用关系如下： start_kernel() 的过程中会执行 socket_init() 来完成协议栈的初始化，实现如下： 123456789101112131415161718192021222324252627282930313233void sock_init(void)//网络栈初始化&#123;\tint i; printk(&quot;Swansea University Computer Society NET3.019 &quot;); /* *\tInitialize all address (protocol) families. */ for (i = 0; i &lt; NPROTO; ++i) pops[i] = NULL; /* *\tInitialize the protocols module. */ proto_init(); #ifdef CONFIG_NET\t/* *\tInitialize the DEV module. */ dev_init(); /* *\tAnd the bottom half handler */ bh_base[NET_BH].routine= net_bh;\tenable_bh(NET_BH);#endif &#125; sock_init() 包含了内核协议栈的初始化工作： sock_init：Initialize sk_buff SLAB cache，注册 SOCKET 文件系统。 net_inuse_init：为每个 CPU 分配缓存。 proto_init：在 &#x2F;proc&#x2F;net 域下建立 protocols 文件，注册相关文件操作函数。 net_dev_init：建立 netdevice 在 &#x2F;proc&#x2F;sys 相关的数据结构，并且开启网卡收发中断；为每个 CPU 初始化一个数据包接收队列（softnet_data），包接收的回调；注册本地回环操作，注册默认网络设备操作。 inet_init：注册 INET 协议族的 SOCKET 创建方法，注册 TCP、UDP、ICMP、IGMP 接口基本的收包方法。为 IPv4 协议族创建 proc 文件。此函数为协议栈主要的注册函数： rc &#x3D; proto_register(&amp;udp_prot, 1);：注册 INET 层 UDP 协议，为其分配快速缓存。 (void)sock_register(&amp;inet_family_ops);：向 static const struct net_proto_family *net_families[NPROTO] 结构体注册 INET 协议族的操作集合（主要是 INET socket 的创建操作）。 inet_add_protocol(&amp;udp_protocol, IPPROTO_UDP) &lt; 0;：向 externconst struct net_protocol *inet_protos[MAX_INET_PROTOS] 结构体注册传输层 UDP 的操作集合。 static struct list_head inetsw[SOCK_MAX]; for (r &#x3D; &amp;inetsw[0]; r &lt; &amp;inetsw[SOCK_MAX];++r) INIT_LIST_HEAD(r);：初始化 SOCKET 类型数组，其中保存了这是个链表数组，每个元素是一个链表，连接使用同种 SOCKET 类型的协议和操作集合。 for (q &#x3D; inetsw_array; q &lt; &amp;inetsw_array[INETSW_ARRAY_LEN]; ++q)： inet_register_protosw(q);：向 sock 注册协议的的调用操作集合。 arp_init();：启动 ARP 协议支持。 ip_init();：启动 IP 协议支持。 udp_init();：启动 UDP 协议支持。 dev_add_pack(&amp;ip_packet_type);：向 ptype_base[PTYPE_HASH_SIZE]; 注册 IP 协议的操作集合。 socket.c 提供的系统调用接口。 协议栈初始化完成后再执行 dev_init()，继续设备的初始化。 Socket 创建流程 协议栈收包流程概述硬件层与设备无关层：硬件监听物理介质，进行数据的接收，当接收的数据填满了缓冲区，硬件就会产生中断，中断产生后，系统会转向中断服务子程序。在中断服务子程序中，数据会从硬件的缓冲区复制到内核的空间缓冲区，并包装成一个数据结构（sk_buff），然后调用对驱动层的接口函数 netif_rx() 将数据包发送给设备无关层。该函数的实现在 net&#x2F;inet&#x2F;dev.c 中，采用了 bootom half 技术，该技术的原理是将中断处理程序人为的分为两部分，上半部分是实时性要求较高的任务，后半部分可以稍后完成，这样就可以节省中断程序的处理时间，整体提高了系统的性能。 NOTE：在整个协议栈实现中 dev.c 文件的作用重大，它衔接了其下的硬件层和其上的网络协议层，可以称它为链路层模块，或者设备无关层的实现。 网络协议层：就以 IP 数据报为例，从设备无关层向网络协议层传递时会调用 ip_rcv()。该函数会根据 IP 首部中使用的传输层协议来调用相应协议的处理函数。UDP 对应 udp_rcv()、TCP 对应 tcp_rcv()、ICMP 对应 icmp_rcv()、IGMP 对应 igmp_rcv()。以 tcp_rcv() 为例，所有使用 TCP 协议的套接字对应的 sock 结构体都被挂入 tcp_prot 全局变量表示的 proto 结构之 sock_array 数组中，采用以本地端口号为索引的插入方式。所以，当 tcp_rcv() 接收到一个数据包，在完成必要的检查和处理后，其将以 TCP 协议首部中目的端口号为索引，在 tcp_prot 对应的 sock 结构体之 sock_array 数组中得到正确的 sock 结构体队列，再辅之以其他条件遍历该队列进行对应 sock 结构体的查询，在得到匹配的 sock 结构体后，将数据包挂入该 sock 结构体中的缓存队列中（由 sock 结构体中的 receive_queue 字段指向），从而完成数据包的最终接收。 NOTE：虽然这里的 ICMP、IGMP 通常被划分为网络层协议，但是实际上他们都封装在 IP 协议里面，作为传输层对待。 协议无关层和系统调用接口层：当用户需要接收数据时，首先根据文件描述符 inode 得到 socket 结构体和 sock 结构体，然后从 sock 结构体中指向的队列 recieve_queue 中读取数据包，将数据包 copy 到用户空间缓冲区。数据就完整的从硬件中传输到用户空间。这样也完成了一次完整的从下到上的传输。 协议栈发包流程概述1、应用层可以通过系统调用接口层或文件操作来调用内核函数，BSD socket 层的 sock_write() 会调用 INET socket 层的 inet_wirte()。INET socket 层会调用具体传输层协议的 write 函数，该函数是通过调用本层的 inet_send() 来实现的，inet_send() 的 UDP 协议对应的函数为 udp_write()。 2、在传输层 udp_write() 调用本层的 udp_sendto() 完成功能。udp_sendto() 完成 sk_buff 结构体相应的设置和报头的填写后会调用 udp_send() 来发送数据。而在 udp_send() 中，最后会调用 ip_queue_xmit() 将数据包下放的网络层。 3、在网络层，函数 ip_queue_xmit() 的功能是将数据包进行一系列复杂的操作，比如是检查数据包是否需要分片，是否是多播等一系列检查，最后调用 dev_queue_xmit() 发送数据。 4、在链路层中，函数调用会调用具体设备提供的发送函数来发送数据包，e.g. dev-&gt;hard_start_xmit(skb, dev);。具体设备的发送函数在协议栈初始化的时候已经设置了。这里以 8390 网卡为例来说明驱动层的工作原理，在 net&#x2F;drivers&#x2F;8390.c 中函数 ethdev_init() 的设置如下： 12345678910111213141516171819202122232425262728293031/* Initialize the rest of the 8390 device structure. */ int ethdev_init(struct device *dev) &#123; if (ei_debug &gt; 1) printk(version); if (dev-&gt;priv == NULL) &#123; //申请私有空间 struct ei_device *ei_local; //8390 网卡设备的结构体 dev-&gt;priv = kmalloc(sizeof(struct ei_device), GFP_KERNEL); //申请内核内存空间 memset(dev-&gt;priv, 0, sizeof(struct ei_device)); ei_local = (struct ei_device *)dev-&gt;priv; #ifndef NO_PINGPONG ei_local-&gt;pingpong = 1; #endif &#125; /* The open call may be overridden by the card-specific code. */ if (dev-&gt;open == NULL) dev-&gt;open = &amp;ei_open; // 设备的打开函数 /* We should have a dev-&gt;stop entry also. */ dev-&gt;hard_start_xmit = &amp;ei_start_xmit; // 设备的发送函数，定义在 8390.c 中 dev-&gt;get_stats = get_stats; #ifdef HAVE_MULTICAST dev-&gt;set_multicast_list = &amp;set_multicast_list; #endif ether_setup(dev); return 0; &#125; UDP 的收发包流程总览 内核中断收包流程 UDP 收包流程 UDP 发包流程"},{"path":"/2024/04/05/linux-docs/网络协议栈/Linux网络栈解剖/","content":"Linux®操作系统的最大功能之一是其网络栈。它最初是BSD协议栈的衍生物，并且组织良好，具有一组干净的接口。其接口范围从协议无关接口（如通用socket层接口或设备层）到各个网络协议的特定接口。本文从网络栈各层的角度探讨了Linux网络栈的结构，并探讨了其一些主要结构体。 协议介绍虽然网络的正式介绍通常是指七层开放系统互连（OSI）模型，但是Linux中基本网络栈的介绍使用互联网模型的四层模型（参见图1）。 网络栈的底部是链路层。链路层是指提供访问物理层的设备驱动程序（物理层可以是各种各样的介质，诸如串行链路或以太网设备）。链路层上面是网络层，负责将数据包引导到目的地。再上一层是传输层，它负责点对点通信（例如，在主机内，像ssh 127.0.0.1）。网络层管理主机之间的通信，传输层管理这些主机之上的端点(Endpoint)之间的通信。最后是应用层，即可以理解所传输的数据的语义层。 核心网络架构现在讨论Linux网络栈的架构以及它如何实现Internet模型。图2提供了Linux网络栈的高级视图。顶部是定义网络栈用户的用户空间层或应用层。底部是提供与网络（串行或高速网络（如以太网））连接的物理设备。在中间的内核空间，是本文重点要讨论的网络子系统。通过网络栈的内部流量socket缓冲区（sk_buffs），它们在源和目的之间移动数据包数据。你很快会看到sk_buff结构。 首先简要介绍Linux网络子系统的核心元素，当然后面还会有更详细的介绍。在顶部（见图2）是系统调用接口。这只是为用户空间应用程序提供访问内核网络子系统的一种方式。接下来是一个协议无关的层，提供了一种常用的方法来处理底层的传输层协议。接下来是实际的协议，在Linux中包括TCP，UDP和IP的内置协议。接下来是另一个设备无关的层，它允许使用通用接口与单个设备驱动程序交互，之后是各个设备驱动程序本身。 系统调用接口系统调用接口可以从两个角度进行描述。当用户进行网络调用时，通过系统调用接口多路复用到内核中。这最终作为 sys_socketcall(./net/socket.c)中的调用，然后进一步解复用到其预期目标的调用。 12345678SYSCALL_DEFINE3(socket, int, family, int, type, int, protocol)SYSCALL_DEFINE3(bind, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)SYSCALL_DEFINE2(listen, int, fd, int, backlog)SYSCALL_DEFINE3(accept, int, fd, struct sockaddr __user *, upeer_sockaddr, int __user *, upeer_addrlen) SYSCALL_DEFINE3(connect, int, fd, struct sockaddr __user *, uservaddr, int, addrlen)... 系统调用接口的另一个角度是使用正常的文件操作进行网络I&#x2F;O。例如，典型的读写操作可以在网络socket（由文件描述符表示，就像普通文件）一样执行。因此，虽然存在一些特定于网络的操作（调用socket创建socket，调用connect将socket连接到目的地等等），但还是有一些适用于网络对象的标准文件操作，就像常规文件一样。 123456789static const struct file_operations socket_file_ops = &#123;\t.owner =\tTHIS_MODULE,\t.llseek =\tno_llseek,\t.read_iter =\tsock_read_iter,\t.write_iter =\tsock_write_iter,struct file *sock_alloc_file(struct socket *sock, int flags, const char *dname)&#123; file = alloc_file(&amp;path, FMODE_READ | FMODE_WRITE, &amp;socket_file_ops); 最后，系统调用接口提供了在用户空间应用程序和内核之间传输控制的手段。 协议无关接口socket层是协议无关接口，其提供一组通用功能，以支持各种不同的协议。socket层不仅支持典型的TCP和UDP协议，还支持IP，原始以太网和其他传输协议，如流控制传输协议（SCTP）。 网络栈使用socket通信。Linux中的socket结构struct sock是在linux&#x2F;include&#x2F;net&#x2F;sock.h中定义的。该大型结构包含特定socket的所有必需状态，包括socket使用的特定协议以及可能在其上执行的操作。 12345678struct sock &#123;\t/* * Now struct inet_timewait_sock also uses sock_common, so please just * don&#x27;t add nothing before this first member (__sk_common) --acme */\tstruct sock_common\t__sk_common; #define sk_prot __sk_common.skc_prot\tstruct proto *sk_prot_creator; 网络子系统通过定义了其功能的特殊结构（即proto）来了解各个可用协议。每个协议维护一个名为proto（在linux&#x2F;include&#x2F;net&#x2F;sock.h中找到）的结构。该结构定义了可以从socket层到传输层执行的特定socket操作（例如，如何创建socket，如何与socket建立连接，如何关闭socket等）。 12345678struct proto tcp_prot = &#123;\t.name = &quot;TCP&quot;,\t.owner = THIS_MODULE,\t.close = tcp_close,\t.connect = tcp_v4_connect,\t.disconnect = tcp_disconnect,\t.accept = inet_csk_accept,... 网络协议网络协议部分定义了可用的特定网络协议（如TCP，UDP等）。这些是在linux&#x2F;net&#x2F;ipv4&#x2F;af_inet.c中的inet_init函数的开头进行初始化的（因为TCP和UDP是协议inet族的一部分）。inet_init函数调用proto_register注册每个内置协议。proto_register在linux&#x2F;net&#x2F;core&#x2F;sock.c中定义，除了将协议添加到活动协议列表之外，还可以根据需要分配一个或多个slab缓存。 12345678910111213141516171819202122232425262728293031323334353637383940414243int proto_register(struct proto *prot, int alloc_slab)&#123;\tif (alloc_slab) &#123; &#125;\tmutex_lock(&amp;proto_list_mutex);\tlist_add(&amp;prot-&gt;node, &amp;proto_list);\tassign_proto_idx(prot);\tmutex_unlock(&amp;proto_list_mutex);static int __init inet_init(void)&#123;\tstruct inet_protosw *q;\tstruct list_head *r;\tint rc = -EINVAL;\tsock_skb_cb_check_size(sizeof(struct inet_skb_parm));\trc = proto_register(&amp;tcp_prot, 1);\tif (rc) goto out;\trc = proto_register(&amp;udp_prot, 1);\tif (rc) goto out_unregister_tcp_proto;..\tfor (q = inetsw_array; q &lt; &amp;inetsw_array[INETSW_ARRAY_LEN]; ++q) inet_register_protosw(q); arp_init(); ip_init(); tcp_init(); udp_init();...static struct inet_protosw inetsw_array[] =&#123;\t&#123; .type = SOCK_STREAM, .protocol = IPPROTO_TCP, .prot = &amp;tcp_prot, .ops = &amp;inet_stream_ops, .flags = INET_PROTOSW_PERMANENT | INET_PROTOSW_ICSK,\t&#125;, 您可以通过linux&#x2F;net&#x2F;ipv4&#x2F;中的文件tcp_ipv4.c，udp.c和raw.c中的proto结构来了解各自的协议。这些协议的proto结构体都按照类型和协议映射到inetsw_array，将内部协议映射到对应的操作(which maps the built-in protocols to their operations.)。结构体inetsw_array及其关系如图3所示。该数组中的每个协议都在初始化inetsw时，通过在inet_init调用inet_register_protosw来初始化。函数inet_init还初始化各种inet模块，如ARP，ICMP，IP模块，TCP和UDP模块。 Socket协议关联回想下，当创建一个socket时，它定义了类型和协议，如 my_sock &#x3D; socket( AF_INET, SOCK_STREAM, 0 )。其中AF_INET表示基于Internet地址族，SOCK_STREAM表示其为流式socket（如上所示inetsw_array）。 从图3可以看出， proto结构定义了特定传输协议的方法，而proto_ops结构定义了一般的socket方法。其他额外的协议可以通过调用inet_register_protosw将自己加入到inetsw协议开关机(protocol switch) 。例如，SCTP通过在linux&#x2F;net&#x2F;sctp&#x2F;protocol.c中调用sctp_init来添加自己。 补充：prot&#x2F;prot_ops二者有点相似，这里特意说明下：kernel的调用顺序是先inet(即prot_ops)，后protocal（即prot），inet层处于socket和具体protocol之间。下面以connect为例，ops即为prot_ops，它调用的bind是inet_listen，然后才是具体protocol的tcp_v4_connect。这一关系主要记录在inetsw_array中。 1234567891011121314151617181920SYSCALL_DEFINE3(connect, int, fd, struct sockaddr __user *, uservaddr, int, addrlen)&#123;\tstruct socket *sock;...\terr = sock-&gt;ops-&gt;connect(sock, (struct sockaddr *)&amp;address, addrlen, sock-&gt;file-&gt;f_flags); int __inet_stream_connect(struct socket *sock, struct sockaddr *uaddr, int addr_len, int flags, int is_sendmsg) &#123; struct sock *sk = sock-&gt;sk;\tswitch (sock-&gt;state) &#123;\tcase SS_UNCONNECTED: err = -EISCONN; if (sk-&gt;sk_state != TCP_CLOSE) goto out; err = sk-&gt;sk_prot-&gt;connect(sk, uaddr, addr_len); if (err &lt; 0) goto out; socket的数据移动使用核心结构socket缓冲区（sk_buff）来进行。一个sk_buff 包含包数据(package data)，和状态数据（state data, 覆盖协议栈的多个层）。每个发送或接收的数据包都用一个sk_buff来表示。该sk_buff 结构是在linux&#x2F;include&#x2F;linux&#x2F;skbuff.h中定义的，并在图4中示出。 如图所示，一个给定连接的多个sk_buff可以串联在一起。每个sk_buff标识了要发送数据包或从其接收数据包的设备结构（net_device *dev）。由于每个包都表示为一个sk_buff，数据包报头可方便地通过一组指针来寻址（th，iph以及mac（MAC报头）），内核会保证这块内存是连续的。由于sk_buff 是socket数据管理的核心，因此kernel已经创建了许多支撑函数来管理它们，包括sk_buff的创建和销毁，克隆和队列管理等函数。 总的来说，内核socket缓冲器设计思路是，某一的socket的sk_buff串链接在一起，并且sk_buff包括许多信息，包括到协议头的指针，时间戳（发送或接收数据包的时间）以及与数据包相关的网络设备。 设备无关接口协议层下面是另一个无关的接口层，将协议连接到具有不同功能的各种硬件设备的驱动程序。该层提供了一组通用的功能，由较低级别的网络设备驱动程序使用，以允许它们使用较高级协议栈进行操作。 首先，设备驱动程序可以通过调用register_netdevice&#x2F;unregister_netdevice将自己注册&#x2F;去注册到内核。调用者首先填写net_device结构，然后将其传入register_netdevice进行注册。内核调用其init功能（如果有定义），执行许多健全检查，创建一个 sysfs条目，然后将新设备添加到设备列表（在内核中Active设备的链表）。你可以 在linux&#x2F;include&#x2F;linux&#x2F;netdevice.h中找到net_device结构。各个函数在linux&#x2F;net&#x2F;core&#x2F;dev.c中实现。 使用dev_queue_xmit函数将sk_buff从协议层发送到网络设备。dev_queue_xmit函数会将sk_buff添加到底层网络设备驱动程序最终要传输的队列中（网络设备在net_device或者sk_buff-&gt;dev中定义）。dev结构包含函数hard_start_xmit，保存用于启动sk_buff传输的驱动程序功能的方法。 通常使用netif_rx接收报文数据。当下级设备驱动程序接收到一个包（包含在新分配的sk_buff）时，内核通过调用netif_rx将sk_buff传递给网络层。然后，netif_rx通过调用netif_rx_schedule将sk_buff排队到上层协议的队列以进行进一步处理。您可以在linux&#x2F;net&#x2F;core&#x2F;dev.c 中找到dev_queue_xmit和netif_rx函数。 最近，在内核中引入了一个新的应用程序接口（NAPI），以允许驱动程序与设备无关层（dev）进行交互。一些驱动程序使用NAPI，但绝大多数仍然使用较旧的帧接收接口（by a rough factor of six to one）。NAPI可以通过避免每个传入帧的中断，在高负载下得到更好的性能。 设备驱动程序网络栈的底部是管理物理网络设备的设备驱动程序。该层的设备示例包括串行接口上​​的SLIP驱动程序或以太网设备上的以太网驱动程序。 在初始化时，设备驱动程序分配一个net_device结构，然后用其必需的例程进行初始化。dev-&gt;hard_start_xmit就是其中一个例程，它定义了上层如何排队sk_buff用以传输。这个程序需要一个sk_buff。此功能的操作取决于底层硬件，但通常将sk_buff中的数据包移动到硬件环或队列。如设备无关层所述，帧接收使用该netif_rx接口或符合NAPI的网络驱动程序的netif_receive_skb。NAPI驱动程序对底层硬件的功能提出了约束。 在设备驱动程序配置其结构中的dev接口后，调用register_netdevice以后驱动就可以使用了。您可以在linux&#x2F;drivers&#x2F;net中找到网络设备专用的驱动程序。"},{"path":"/2024/04/05/linux-docs/网络协议栈/Linux网络源代码学习——整体介绍/","content":"简介 源码目录 网络分层 网络与文件操作 数据结构 sk_buff结构 网络协议栈实现——数据struct 和 协议struct 面向过程&#x2F;对象&#x2F;ioc 其它 以下来自linux1.2.13源码。linux的网络部分由网卡的驱动程序和kernel的网络协议栈部分组成，它们相互交互，完成数据的接收和发送。 源码目录1234567891011121314151617linux-1.2.13||---net | |---protocols.c |---socket.c |---unix | | | |---proc.c | |---sock.c | |---unix.h |---inet | |---af_inet.c |---arp.h,arp.c |---... |---udp.c,utils.c 其中 unix 子文件夹中三个文件是有关 UNIX 域代码， UNIX 域是模拟网络传输方式在本机范围内用于进程间数据传输的一种机制。 系统调用通过 INT $0x80 进入内核执行函数，该函数根据 AX 寄存器中的系统调用号，进一步调用内核网络栈相应的实现函数。 网络分层 从这个图中，可以看到，到传输层时，横生枝节，代码不再针对任何数据包都通用。从下到上，收到的数据包由哪个传输层协议处理，根据从数据包传输层header中解析的数据确定。从上到下，数据包的发送使用什么传输层协议，由socket初始化时确定。 1、vfs层2、socket 是用于负责对上给用户提供接口，并且和文件系统关联。3、sock，负责向下对接内核网络协议栈4、tcp层 和 ip 层， linux 1.2.13相关方法都在 tcp_prot中。在高版本linux 中，sock 负责tcp 层， ip层另由struct inet_connection_sock 和 icsk_af_ops 负责。分层之后，诸如拥塞控制和滑动窗口的 字段和方法就只体现在struct sock和tcp_prot中，代码实现与tcp规范设计是一致的5、ip层 负责路由等逻辑，并执行nf_hook，也就是netfilter。netfilter是工作于内核空间当中的一系列网络（TCP&#x2F;IP）协议栈的钩子（hook），为内核模块在网络协议栈中的不同位置注册回调函数（callback）。也就是说，在数据包经过网络协议栈的不同位置时做相应的由iptables配置好的处理逻辑。6、link 层，先寻找下一跳（ip &#x3D;&#x3D;&gt; mac），有了 MAC 地址，就可以调用 dev_queue_xmit发送二层网络包了，它会调用 __dev_xmit_skb 会将请求放入块设备的队列。同时还会处理一些vlan 的逻辑7、设备层：网卡是发送和接收网络包的基本设备。在系统启动过程中，网卡通过内核中的网卡驱动程序注册到系统中。而在网络收发过程中，内核通过中断跟网卡进行交互。网络包的发送会触发一个软中断 NET_TX_SOFTIRQ 来处理队列中的数据。这个软中断的处理函数是 net_tx_action。在软中断处理函数中，会将网络包从队列上拿下来，调用网络设备的传输函数 ixgb_xmit_frame，将网络包发的设备的队列上去。 网卡中断处理程序为网络帧分配的，内核数据结构 sk_buff 缓冲区；是一个维护网络帧结构的双向链表，链表中的每一个元素都是一个网络帧（Packet）。虽然 TCP&#x2F;IP 协议栈分了好几层，但上下不同层之间的传递，实际上只需要操作这个数据结构中的指针，而无需进行数据复制。 网络与文件操作 VFS为文件系统抽象了一套API，实现了该系列API就可以把对应的资源当作文件使用，当调用socket函数的时候，我们拿到的不是socket本身，而是一个文件描述符fd。 从linux5.9看网络层的设计整个网络层的实际中，主要分为socket层、af_inet层和具体协议层（TCP、UDP等）。当使用网络编程的时候，首先会创建一个socket结构体（socket层），socket结构体是最上层的抽象，然后通过协议簇类型创建一个对应的sock结构体，sock是协议簇抽象（af_inet层），同一个协议簇下又分为不同的协议类型，比如TCP、UDP（具体协议层），然后根据socket的类型（流式、数据包）找到对应的操作函数集并赋值到socket和sock结构体中，后续的操作就调用对应的函数就行，调用某个网络函数的时候，会从socket层到af_inet层，af_inet做了一些封装，必要的时候调用底层协议（TCP、UDP）对应的函数。而不同的协议只需要实现自己的逻辑就能加入到网络协议中。 file_operations 结构定义了普通文件操作函数集。系统中每个文件对应一个 file 结构， file 结构中有一个 file_operations 变量，当使用 write，read 函数对某个文件描述符进行读写操作时，系统首先根据文件描述符索引到其对应的 file 结构，然后调用其成员变量 file_operations 中对应函数完成请求。 1234567891011121314// 参见socket.cstatic struct file_operations socket_file_ops = &#123; sock_lseek, // Î´ÊµÏÖ sock_read, sock_write, sock_readdir,\t// Î´ÊµÏÖ sock_select, sock_ioctl, NULL, /* mmap */ NULL, /* no special open code... */ sock_close, NULL, /* no fsync */ sock_fasync&#125;; 以上 socket_file_ops 变量中声明的函数即是网络协议对应的普通文件操作函数集合。从而使得read， write， ioctl 等这些常见普通文件操作函数也可以被使用在网络接口的处理上。kernel维护一个struct file list，通过fd &#x3D;&#x3D;&gt; struct file &#x3D;&#x3D;&gt; file-&gt;ops &#x3D;&#x3D;&gt; socket_file_ops,便可以以文件接口的方式进行网络操作。同时，每个 file 结构都需要有一个 inode 结构对应。用于存储struct file的元信息 123456789struct inode&#123; ... union &#123; ... struct ext_inode_info ext_i; struct nfs_inode_info nfs_i; struct socket socket_i; &#125;u&#125; 也就是说，对linux系统，一切皆文件，由struct file描述，通过file-&gt;ops指向具体操作，由file-&gt;inode 存储一些元信息。对于ext文件系统，是载入内存的超级块、磁盘块等数据。对于网络通信，则是待发送和接收的数据块、网络设备等信息。从这个角度看，struct socket和struct ext_inode_info 等是类似的。 数据结构sk_buff结构sk_buff部分字段如下， 123456789101112131415161718192021222324252627282930313233343536373839404142434445struct sk_buff &#123; /* These two members must be first. */ struct sk_buff *next; struct sk_buff *prev; struct sk_buff_head *list; struct sock *sk; struct timeval stamp; struct net_device *dev; struct net_device *real_dev; union &#123; struct tcphdr *th; struct udphdr *uh; struct icmphdr *icmph; struct igmphdr *igmph; struct iphdr *ipiph; unsigned char *raw; &#125; h; // Transport layer header union &#123; struct iphdr *iph; struct ipv6hdr *ipv6h; struct arphdr *arph; unsigned char *raw; &#125; nh; // Network layer header union &#123; struct ethhdr *ethernet; unsigned char *raw; &#125; mac; // Link layer header struct dst_entry *dst; struct sec_path *sp; void (*destructor)(struct sk_buff *skb); /* These elements must be at the end, see alloc_skb() for details. */ unsigned int truesize; atomic_t users; unsigned char *head, *data, *tail, *end; &#125;; head和end字段指向了buf的起始位置和终止位置。然后使用header指针指像各种协议填值。然后data就是实际数据。tail记录了数据的偏移值。 sk_buff 是各层通用的，在应用层数据包叫 data，在 TCP 层我们称为 segment，在 IP 层我们叫 packet，在数据链路层称为 frame。下层协议将上层协议数据作为data部分，并加上自己的header。这也是为什么代码注释中说，哪些字段必须在最前，哪些必须在最后， 这个其中的妙处可以自己体会。 sk_buff由sk_buff_head组织 1234567struct sk_buff_head &#123; struct sk_buff * volatile next; struct sk_buff * volatile prev; #if CONFIG_SKB_CHECK int magic_debug_cookie; #endif&#125;; 网络协议栈实现——数据struct 和 协议structsocket分为多种，除了inet还有unix。反应在代码结构上，就是net包下只有net&#x2F;unix,net&#x2F;inet两个文件夹。之所以叫unix域，可能跟描述其地址时，使用unix:&#x2F;&#x2F;xxx有关 The difference is that an INET socket is bound to an IP address-port tuple, while a UNIX socket is “bound” to a special file on your filesystem. Generally, only processes running on the same machine can communicate through the latter. 本文重点是inet,net&#x2F;inet下有以下几个比较重要的文件，这跟网络书上的知识就对上了。 1234567arp.ceth.cip.croute.ctcp.cudp.cdatalink.h // 应该是数据链路层 怎么理解整个表格呢？协议struct和数据struct有何异同？ 1、struct一般由一个数组或链表组织，数组用index，链表用header(比如packet_type_base、inet_protocol_base)指针查找数据。2、协议struct是怎么回事呢？通常是一个函数操作集，类似于controller-server-dao之间的interface定义，类似于本文开头的file_operations，有open、close、read等方法，但对ext是一回事，对socket操作是另一回事。3、数据struct实例可以有很多，比如一个主机有多少个连接就有多少个struct sock，而协议struct个数由协议类型个数决定，具体的协议struct比如tcp_prot就只有一个。比较特别的是，通过tcp_prot就可以找到所有的struct sock实例。4、socket、sock、device等数据struct经常被作为分析的重点，其实各种协议struct 才是流程的关键，并且契合了网络协议分层的理念。 以ip.c为例，在该文件中定义了ip_rcv（读数据）、ip_queue_xmit(用于写数据)，链路层收到数据后，通过ptype_base找到ip_packet_type,进而执行ip_rcv。tcp发送数据时，通过tcp_proto找到ip_queue_xmit并执行。 tcp_protocol是为了从下到上的数据接收，其函数集主要是handler、frag_handler和err_handler，对应数据接收后的处理。tcp_prot是为了从上到下的数据发送(所以struct proto没有icmp对应的结构)，其函数集connect、read等主要对应上层接口方法。 到bsd socket层，相关的结构在&#x2F;include&#x2F;linux下定义，而不是在net包下。这就对上了，bsd socket是一层接口规范，而net包下的相关struct则是linux自己的抽象了。 主要要搞清楚三个问题，具体可以参见相关的书籍，此处不详述。参见Linux1.0中的Linux1.2.13内核网络栈源码分析的第四章节。 1、这些结构如何初始化。有的结构直接就是定义好的，比如tcp_protocol等2、如何接收数据。由中断程序触发。接收数据的时候，可以得到device，从数据中可以取得协议数据，进而从ptype_base及inet_protocol_base执行相应的rcv3、如何发送数据。通常不直接发送，先发到queue里。可以从socket初始化时拿到protocol类型（比如tcp）、目的ip，通过route等决定device，于是一路向下执行xx_write方法 面向过程&#x2F;对象&#x2F;ioc 重要的不是细节，这个过程让我想到了web编程中的controller,service,dao。都是分层，区别是web请求要立即返回，网络通信则不用。 mac &#x3D;&#x3D;&gt; device &#x3D;&#x3D;&gt; ip_rcv &#x3D;&#x3D;&gt; tcp_rcv &#x3D;&#x3D;&gt; 上层url &#x3D;&#x3D;&gt; controller &#x3D;&#x3D;&gt; service &#x3D;&#x3D;&gt; dao &#x3D;&#x3D;&gt; 数据库想一想，整个网络协议栈，其实就是一群loopbackController、eth0Controller、ipService、TcpDao组成，该是一件多么有意思的事。 其它无论 TCP 还是 UDP，端口号都只占 16 位，也就说其最大值也只有 65535。那是不是说，如果使用 TCP 协议，在单台机器、单个 IP 地址时，并发连接数最大也只有 65535 呢？对于这个问题，首先你要知道，Linux 协议栈，通过五元组来标志一个连接（即协议，源 IP、源端口、目的 IP、目的端口)。对客户端来说，每次发起 TCP 连接请求时，都需要分配一个空闲的本地端口，去连接远端的服务器。由于这个本地端口是独占的，所以客户端最多只能发起 65535 个连接。对服务器端来说，其通常监听在固定端口上（比如 80 端口），等待客户端的连接。根据五元组结构，我们知道，客户端的 IP 和端口都是可变的。如果不考虑 IP 地址分类以及资源限制，服务器端的理论最大连接数，可以达到 2 的 48 次方（IP 为 32 位，端口号为 16 位），远大于 65535。服务器端可支持的连接数是海量的，当然，由于 Linux 协议栈本身的性能，以及各种物理和软件的资源限制等，这么大的连接数，还是远远达不到的（实际上，C10M 就已经很难了）。 软中断有专门的内核线程 ksoftirqd处理。每个 CPU 都会绑定一个 ksoftirqd 内核线程，比如， 2 个 CPU 时，就会有 ksoftirqd&#x2F;0 和 ksoftirqd&#x2F;1 这两个内核线程。"},{"path":"/2024/04/05/linux-docs/网络协议栈/驾驭Linux内部网络实现——关键数据结构 sk_buff/","content":"背景在 Linux 的网络栈实现代码中，引用到了一些数据结构。要理解 Linux 内部的网络实现，需要先理清这些数据结构的作用。关键数据结构主要有两个: sk_buff 和 net_device。 struct sk_buff: 是整个网络数据包存储的地方。这个数据结构会被网络协议栈中的各层用来储存它们的协议头、用户数据和其他它们完成工作需要的数据。 struct net_device: 在 Linux 内核中，这个数据结构将用来代表网络设备。它会包含设备的硬件和软件配置信息。 在 Linux 的网络实现中，核心数据结构还有struct sock, 它被用来储存 socket 的信息。但是 Socket 其实是内核为用户态程序提供的一组 Api, 用来访问内核的网络栈实现，所以它不属于内核内部的网络实现，也就不再这里介绍了。 本文将先着重理解 sk_buff 数据结构。 Socket Buffer: sk_buff sk_buff: 在本文中后面部分也会被称为缓冲区 在 Linux 内核的网络代码中，这或许是最重要的数据结构，用来表示已接收或将要传输的数据。定义在 &lt;include&#x2F;linux&#x2F;skbuff.h&gt; 中，它由许多变量组成，目标就是满足所有网络协议的需要。 sk_buff 的结构随着内核的迭代已经被添加了许多新的选项，已经存在的字段也被重新整理了很多遍。可将内部的字段分为以下几类： Layout 负责内存布局的字段 General 通用的字段 Feature-specific 对应特别功能字段 Management functions 一些用来管理 sk_buff 的函数 sk_buff 在不同的网络层被使用（MAC 或其他在 L2 的协议，在 L3 的 IP 协议，在 L4 的 TCP 或 UDP 等），当它从一层传递到另一层时，各个字段也会发生变化。在被传递到 L3 之前，L4 会追加头信息，然后在被传递到 L2 之前，L3 会追加头信息。从一层传递到另一层时，通过追加头信息的方式比将数据在层之间拷贝会更有效率。由于要在 buff 的开头增加空间（与平时常见的在尾部追加空间相比）是一项复杂的操作，内核便提供了 skb_reserve 函数执行这个操作。因此，随着 buffer 从上层到下层的传递，每层协议做的第一件事就是调用 skb_reserve 去为它们的协议头在 buffer 的头部分配空间。在后面，我们将通过一个例子去了解内核如何在当 buffer 在各个层间传递时，确保为每一层保留了足够的空间让它们添加它们自己的协议头。 在接收数据时，buffer 会被从下层到上层传递，在从下到上的过程中，前一层的协议头对于当前层来说已经没有用了。比如：L2 的协议头只会被处理 L2 协议的设备驱动程序使用，L3 并不关心 L2 的头。那么内核怎么做的呢? 内核的实现是：** sk_buff 中有一个指针会指向当前位于的层次的协议的协议头的内存开始地址，于是从 L2 到 L3 时，只需将指向 L2 头部的指针移动到 L3 的头部即可**（又是一步追求效率的操作）。 网络选项和内核结构内核的网络代码提供了大量有用但不是必须的选项，例如防火墙，多播等功能。这些选项都需要内核数据结构中的其他字段。因此，sk_buff 使用 C 语言的预处理命令 #ifdef 来做条件编译。例如在 sk_buff 定义的后面部分: 12345678910struct sk_buff &#123;//...... #ifdef CONFIG_NET_SCHED __u32 tc_index; #ifdef CONFIG_NET_CLS_ACT __u32 tc_verd; __u32 tc_classid; #endif #endif&#125; 通过在编译 Linux 时配置不同的编译选项，能够让编译出来的内核支持不同的功能。 Layout Fields在 sk_buff 中存在一些字段，它们存在的意义只是为了搜索的方便和数据结构的组织。这类字段称为 Layout Fileds。Linux 内核把系统中所有的 sk_buff 实例维护在一个双向链表中。但是组织这个链表比传统的双向链表要复杂一点。 和任何双向链表类似，sk_buff 链表的每个节点也通过 next 和 prev 分别指向后继和前驱节点。但是 sk_buff 链表还要求：每个节点必须能够很快的找到整个链表的头节点。为了实现这个要求，一个额外的数据结构(sk_buff_head)被添加到链表的头部，作为一个空节点： 12345678struct sk_buff_head &#123;\t/* These two members must be first. */\tstruct sk_buff\t*next;\tstruct sk_buff\t*prev;\t__u32 qlen;\tspinlock_t\tlock;&#125;; qlen: 表示链表中的节点数 lock: 用作多线程同步 sk_buff 和 sk_buff_head 开始的两个节点(next prev)是相同的。即使 sk_buff_head 比 sk_buff 更轻量化，也允许这两种结构在链表中共存。另外，可以使用相同函数来操作 sk_buff 和 sk_buff_head。 为了实现通过每个节点都能快速找到链表头，每个节点都会包含一个指向链表中唯一的 sk_buff_head 的指针（list）。 sk_buff 其他字段struct sock sk一个指向 sock 数据结构的指针，表示 sock 对应 socket 拥有这个 sk_buff。当数据是由本地进程生成或接收时需要这个指针，因为数据和 socket相关的信息会被 L4（TCP或UDP）和用户态的程序使用。当一个 sk_buff 仅仅是被转发时（也就是说，源和目标地址不在本地计算机上），这个指针是不需要的，因此将会是 NULL。 unsigned int len表示在 buffer 中数据区域的大小。该长度既包括主缓冲区的数据长度，也包括片段中的数据。因为协议头在向上传递中会被丢弃，在向下传递中会被添加，所以它的值会随着 buffer 在各层间传递而改变。 unsiged int data_len和 len 不同的是，data_len 只记录分段中的数据大小。 unsigned int mac_lenMAC 头部的长度 atomic_t userssk_buff 的引用计数，或引用了此 sk_buff 缓冲区的对象数。 主要用途是避免在有人还在使用时就释放了 sk_buff。users的值可以通过 atomic_inc 和 atomic_dec直接增加和减少，但更多的时候是通过skb_get和kfree_skb 函数进行。 unsigned int truesize这个字段表示 buffer 的总大小，包括了 sk_buff 自己的占用。在执行 alloc_skb 函数时该字段被初始化。 #define SKB_TRUESIZE(X) ((X) + \\ SKB_DATA_ALIGN(sizeof(struct sk_buff)) +\t\\ SKB_DATA_ALIGN(sizeof(struct skb_shared_info))) unsigned char head unsigned char end unsigned char data unsigned char tail 上面4个指针用来表示 buffer 中数据域的边界。当每一层为了任务而准备 buffer 时，为了协议头或数据，可能会分配更多的空间。 head 和 end 指向了 buffer 被分配的内存区域的开始和结束， data 和 tail 指向其中实际数据域的开始和结束。 每一层能够在 head 和 data 之间的区域填充协议头，或者在 tail 和 end 之间的区域填充新的数据。 void (destructor)(…)这个函数指针能够在运行时被赋值，从而在一个 buffer 被移除时，执行一些操作。当一个 buffer 不属于一个 socket 时，这个函数指针通常为空。当一个 buffer 属于一个 socket 时，这个函数指针通常被设置为 sock_rfree 或 sock_wfree。两个 sock_xxx 函数用于更新 socket 在它的队列中持有的大量内存。 General Fields在 sk_buff 中存在一些通用目的的字段，这些字段没有与特定的内核功能绑定： struct timeval stamp这个字段通常仅对接收到的数据包有意义。这是一个时间戳，表示何时接收到一个数据包，或者何时计划发送一个数据包。它由接收 net_timestamp 参数的函数 netif_rx 设置，在接收每个数据包后由设备驱动程序调用。 struct net_device dev该字段描述了网络设备。 dev代表的设备的作用取决于缓冲区中存储的数据包是要发送还是刚刚被接收。当收到数据包后，设备驱动程序会使用指向接收数据接口的指针来更新此字段。当要发送数据包时，此参数表示将通过其发送出去的设备。 struct net_device input_dev表示数据包到来的设备。当数据包是在本地生成的，其值是 NULL。 struct net_device real_dev该字段仅对虚拟设备有意义，代表与虚拟设备关联的实际设备。 例如，Bonding（将两个或多个网络接口组合或合并为一个接口） 和 VLAN(virtual local area network) 接口使用它来记住从何处接收到真实设备入口流量。 union &#123;…&#125; h union &#123;…&#125; nh union &#123;…&#125; mac 上面的3个指针表示了 TCP&#x2F;IP 协议栈中的协议头: h 代表 L4, nh 代表 L3, mac 代表 L2。每个字段都指向的是各种共用体结构，某个结构只能被内核中对应的层的协议理解。比如：h 共用体就包含了L4上的每个协议能理解的头信息。每个共用体的一个成员称为raw，用于初始化。 所有以后的访问都通过协议特定的成员进行。 当接收到数据包时，负责处理第 n 层协议头的函数从第 n-1 层接收一个 buffer，其中skb-&gt;data 指向第 n 层协议头的开头。处理第n层的函数会为此层初始化适当的指针（例如，L3 的处理函数会为 skb-&gt;nh 赋值）以保留 skb-&gt;data 字段，因为当 skb-&gt;data 被赋值为 buffer 内的其他偏移量时，该指针的内容将在下一层的处理过程中丢失。然后，该函数完成第 n 层的处理，并在将数据包传递到第 n+1 层处理程序之前，更新 skb-&gt;data 使其指向第 n 层协议头的末尾（即第n+1 层协议头的开始位置）。 struct dst_entry dst由路由子系统使用。 因为数据结构非常复杂，并且需要了解其他子系统的工作原理，所以留在以后在详解。 char cb[40]control buffer 的简称，或存储一些私有信息，由各层维护以供内部使用。它是在 sk_buff 结构中静态分配的（当前大小为40个字节），并且足够大以容纳每一层所需的任何私有数据。在每一层的代码中，访问都是通过宏进行的，以使代码更具可读性。例如，TCP使用该空间存储 tcp_skb_cb 数据结构，该数据结构在 include&#x2F;net&#x2F;tcp.h 中定义： 12345678struct tcp_skb_cb &#123; //...\t__u32 seq; /* Starting sequence number\t*/\t__u32 end_seq;\t/* SEQ + FIN + SYN + datalen\t*/\t__u8 tcp_flags;\t/* TCP header flags. (tcp[13])\t*/\t__u32 ack_seq;\t/* Sequence number ACK&#x27;d\t*/ //...&#125;; 这是 TCP 代码访问结构的宏。宏仅由一个指针转换组成： #define TCP_SKB_CB(__skb)\t((struct tcp_skb_cb *)&amp;((__skb)-&gt;cb[0])) 这是一个示例，其中 TCP 模块在收到分段后填写 cb 结构： 123456789static void tcp_v4_fill_cb(struct sk_buff *skb, const struct iphdr *iph, const struct tcphdr *th) &#123; TCP_SKB_CB(skb)-&gt;seq = ntohl(th-&gt;seq); TCP_SKB_CB(skb)-&gt;end_seq = (TCP_SKB_CB(skb)-&gt;seq + th-&gt;syn + th-&gt;fin + skb-&gt;len - th-&gt;doff * 4); TCP_SKB_CB(skb)-&gt;ack_seq = ntohl(th-&gt;ack_seq); TCP_SKB_CB(skb)-&gt;tcp_flags = tcp_flag_byte(th); TCP_SKB_CB(skb)-&gt;tcp_tw_isn = 0; TCP_SKB_CB(skb)-&gt;ip_dsfield = ipv4_get_dsfield(iph); TCP_SKB_CB(skb)-&gt;sacked = 0;&#125; unsigned int csumunsigned char ip_summed上面两个字段代表校验和和状态相关的标志。 unsigned char cloned一个布尔标志，设置后表示此结构是另一个sk_buff缓冲区的克隆。 unsigned char pkt_type该字段根据 L2 目的地址对帧的类型进行分类。可能的值在 include&#x2F;linux&#x2F;if_packet.h 中列出。对于以太网设备，此参数由函数 eth_type_trans 初始化。 主要的一些值： PACKET_HOST：接收到的帧的目的地址就是当前接收接口。也就是说，数据包已到达其目的地 PACKET_MULTICAST：接收到的帧的目标地址是当前接收接口已注册过的的多播地址之一 PACKET_BROADCAST：接收帧的目的地址是接收接口的广播地址 PACKET_OTHERHOST：接收帧的目的地址不属于与接口关联的目的地址（单播，组播和广播）；因此，如果启用了转发，则必须转发该帧，否则将其丢弃 PACKET_OUTGOING：表示数据包正在发送 PACKET_LOOPBACK：数据包被发送到回环设备。多亏了此标志，内核在处理回环设备时，可以跳过某些实际设备所需的操作 PACKET_FASTROUTE：数据包正在使用快速路由功能进行路由 unsigned short protocol从 L2 处的网卡设备驱动程序的角度来看，这是在更高层次上使用的协议。典型协议有 IP，IPv6和ARP。完整列表可在 include&#x2F;linux&#x2F;if_ether.h 中找到。由于每个协议都有其自己的处理传入数据包的处理函数，因此驱动程序使用此字段来通知其上一层使用什么处理函数。每个驱动程序都调用 netif_rx 来调用上层网络层的处理函数，因此必须在调用该函数之前初始化协议字段。 unsigned short security表示数据包的安全级别。 该字段最初是为与 IPsec 一起使用而引入的。 Feature-Specific FieldsLinux内核是模块化的，允许你选择要包括的内容和要忽略的内容。因此，只有在编译内核时开启支持像 Netfilter 或 QoS 之类的特定功能的情况下，某些字段才会包含在 sk_buff 数据结构中： unsigned long nfmark__u32 nfcache__u32 nfctinfostruct nf_conntrack *nfct unsigned int nfdebugstruct nf_bridge_info *nf_bridge 这些参数由Netfilter使用。 union {…} private 高性能并行接口HIPPI使用此共用体。 __u32 tc_index__u32 tc_verd__u32 tc_classid 这些参数由流量控制使用。 Management Functions内核提供了许多很简短的简单函数来操纵 sk_buff 节点或链表。 如果查看文件 include&#x2F;linux&#x2F;skbuff.h 和 net&#x2F;core&#x2F;skbuff.c，你会发现几乎所有功能都有两个版本，名称分别为 do_ something 和 __do_something。通常，第一个是包装函数，它在对第二个调用的调用周围添加了额外的健全性检查或锁定机制。内部 __do_something 函数通常不直接调用。该规则的例外通常是编码不良的函数，这些函数最终将被修复。 下图为分别对 sk_buff 执行 skb_put(a)，skb_push(b)，skb_pull(c)，skb_reserve(d) 的前后对比： skb_put：在数据域尾部追加一段空间 skb_push：在数据域的头部追加一段空间 skb_pull：将 skb-&gt;data 指针在数据域下移指定字节 skb_reserve：在 sk_buff 中 skb-&gt;data 之前的空间追加一段空间（在每层追加自己的协议头时常用到） 内存分配 alloc_skb dev_alloc_skb alloc_skb 是分配缓冲区的主要函数，在 net&#x2F;core&#x2F;skbuff.c 中定义。由于数据缓冲区（由 sk_buff 的 head end data tail 指针维护的内存区域）和链表（sk_buff 自身）是两个不同的结构，所以创建单个缓冲区涉及两个内存分配（一个分配给数据缓冲区，另一个分配给 sk_buff 自身结构）。 alloc_skb 通过调用函数 kmem_cache_alloc 从缓存中获取 sk_buff 数据结构，并通过调用 kmalloc 获取数据缓冲区，而 kmalloc 也会使用缓存的内存（如果可用）： struct sk_buff *skb; u8 *data skb = kmem_cache_alloc_node(cache, gfp_mask &amp; ~__GFP_DMA, node); size = SKB_DATA_ALIGN(size); size += SKB_DATA_ALIGN(sizeof(struct skb_shared_info)); data = kmalloc_reserve(size, gfp_mask, node, &amp;pfmemalloc); 在调用 kmalloc 之前，使用宏 SKB_DATA_ALIGN 调整了大小参数以强制对齐。返回之前，该函数将初始化结构体中的一些参数，从而产生下图所示的最终结果： 在图右侧存储块的底部，可以看到为了强制对齐而引入的 Padding 区域。 skb_shared_info 块主要用于处理 IP 的分片（IP协议根据 MTU 和 MSS 对数据包进行的分片传输）。 dev_alloc_skbdev_alloc_skb 是供设备驱动程序使用的缓冲区分配函数。这类驱动程序预计将在中断模式下被执行。它就是简单的包装了下 alloc_skb, 相比 alloc_skb 多分配了一些字节的空间，并请求了原子操作（GFP_ATOMIC），因为设备驱动程序将在中断处理程序中调用。 内存释放 kfree_skb dev_kfree_skb 这两个函数释放一个缓冲区，会让释放的缓冲区返回到缓冲池（缓存）中。dev_kfree_skb 只是通过宏定义的一个标示，内部还是调用的 kfree_skb。 kfree_skb 仅在 skb-&gt;users 计数器为1时（没有缓冲区的用户时）才释放缓冲区。 否则，该函数只会使该计数器递减。因此，如果一个缓冲区有三个用户，则只有当调用第三次 dev_kfree_skb 或 kfree_skb 时才会真正释放内存。 数据保留和对齐 skb_reserve skb_put skb_push skb_pull skb_reserve 在缓冲区的头部保留一些空间，通常用于允许插入协议头或强制将数据在某个边界上对齐。它通过移动标记数据域开始和结束的 data 和 tail 指针来完成操作。 查看以太网网卡驱动程序的代码（比如: drivers&#x2F;net&#x2F;ethernet&#x2F;3com&#x2F;3c59x.c vortex_rx 函数），你能看到它们在将任何数据存储在他们刚刚分配的缓冲区中之前都会使用以下命令： skb_reserve(skb, 2);\t/* Align IP on 16 byte boundaries */ 因为他们知道他们将要把协议头为 14 个字节的以太网帧复制到缓冲区中，所以参数2将缓冲区的 head 指针下移了 2 个字节。这将让紧跟在以太网头之后的 IP 头，从缓冲区的开头在 16 字节边界上对齐。 下图展示了 skb_reserve 在数据从上到下传递（发送数据）时的作用（为下层协议在数据区的头部分配空间）： 1、当要求 TCP 传输某些数据时，它会按照某些条件（TCP Max Segment Size(mss)，对分散收集 I&#x2F;O 支持等）分配一个缓冲区。2、TCP 在缓冲区的头部保留（通过调用 skb_reserve）足够的空间，以容纳所有层（TCP，IP，Link 层）的所有协议头。参数 MAX_TCP_HEADER 是所有级别的所有协议头的总和，并考虑到最坏的情况：因为 TCP 层不知道将使用哪种类型的接口进行传输，因此它为每个层保留最大的标头。它甚至考虑到多个 IP 协议头的可能性（因为当内核编译为支持 IP in IP 时，你可以拥有多个IP 协议头）。3、TCP 的 payload （应用层传输的数据）被复制到缓冲区中。请注意上图只是个例子。TCP 的 payload 可以被不同地组织；例如，可以将其存储为片段。4、TCP 层添加它的协议头。5、TCP 层将缓冲区移交给 IP 层，IP层也添加协议头。6、IP 层将缓冲区移交给下一层，下一层也添加它的协议头。 请注意，当缓冲区在网络栈中向下移动时，每个协议会将 skb-&gt;data 指针向下移动，在其协议头中复制，并更新 skb-&gt;len。 注意，skb_reserve 函数实际上并没有将任何内容移入或移出数据缓冲区。 它只是更新两个指针： 123456static inline void skb_reserve(struct sk_buff *skb, int len)&#123;\tskb-&gt;data += len;\tskb-&gt;tail += len;&#125; 感觉这里是用空间来换了时间，在一开始就分配需要用到的全部空间，然后就可以通过只移动指针来提高效率了。 skb_push 将一个数据块添加到缓冲区的开头，而 skb_put 将一个数据块添加到末尾。像 skb_reserve 一样，这些函数实际上并不会向缓冲区添加任何数据。他们只是将指针移到它的头或尾。数据填充应该由其他功能显式操作。skb_pull 通过将 head 指针向前移动来从缓冲区的头中删除数据块。 skb_shared_info 结构体 &amp; skb_shinfo 函数在上面网卡驱动拷贝帧到缓冲区的例子中出现过 skb_shared_info。它是用来保留与数据域有关的其他信息。这个数据结构紧跟在标记数据域结束的 end 指针后面。 1234567struct skb_shared_info &#123; atomic_t dataref; __u8 nr_frags; struct sk_buff\t*frag_list; skb_frag_t frags[MAX_SKB_FRAGS]; //...&#125;; dataref：代表数据域的「用户」数（数据域被引用的次数） nr_frags, frag_list, frags：用于处理 IP 片段 skb_is_nonlinear 函数可用于检查缓冲区是否已分段，而 skb_linearize 函数可用于将多个片段合为单个缓冲区。 sk_buff 中没有专门的指针指向 skb_shared_info 区域，skb_shinfo 函数就是方便得到指向 skb_shared_info 区域指针的函数: 123456#define skb_shinfo(SKB)\t((struct skb_shared_info *)(skb_end_pointer(SKB)))static inline unsigned char *skb_end_pointer(const struct sk_buff *skb)&#123;\treturn skb-&gt;end;&#125; 克隆和拷贝当相同的缓冲区需要由不同的消费者处理，并且他们可能更改 sk_buff 结构中的内容（协议头的h和 nh 指针等 Layout 字段）时，为了提高效率，内核并没有克隆缓冲区的结构和数据域，而是仅复制 sk_buff 的结构，并使用引用计数进行操作，以避免过早释放共享数据块。skb_clone 函数负责拷贝一个 buffer。 使用克隆的一种情况是，需要将入口数据包分发给多个接收者，例如协议处理程序和一个或多个网络分接头（Network taps）。 sk_buff 克隆不会链接到任何链表，也没有引用套接字所有者。克隆和原始缓冲区中的 skb-&gt;cloned 字段均设置为1。在克隆中将 skb-&gt;users 设置为1，以便第一次尝试删除它（被克隆的 sk_buff）时会成功，并且数据域的引用数（dataref）递增（因为现在有一个新的 sk_buff 指向了）。 skb_clone 会调用 __skb_clone: 1234567891011121314151617181920212223static struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb) &#123;#define C(x) n-&gt;x = skb-&gt;x // 定义的宏，如果 x 是普通变量则是值赋值// 如果 x 是指针，则是指向同一块区域\tn-&gt;next = n-&gt;prev = NULL;\tn-&gt;sk = NULL;\t__copy_skb_header(n, skb); //...\tn-&gt;destructor = NULL;\tC(tail);\tC(end);\tC(head);\tC(head_frag);\tC(data); // data 是一个指针, 所以没有克隆数据域，只是指向了数据域的内存地址\tC(truesize);\trefcount_set(&amp;n-&gt;users, 1); //设置克隆的 sk_buff 的用户数为1\tatomic_inc(&amp;(skb_shinfo(skb)-&gt;dataref)); //增加数据域的引用次数\tskb-&gt;cloned = 1;\treturn n;#undef C&#125; 下图为一个被分段（一个缓冲区，其中一些数据存储在与frags数组链接的数据片段中）了的缓冲区克隆的例子: 当缓冲区被克隆时，无法修改数据块的内容。这意味着代码无需做同步保证即可访问数据。但是，当一个函数不仅需要修改 sk_buff 结构的内容，还需要修改数据域时，就必须要克隆数据域了。如果真要修改数据域，开发者也有两个选项可用。 1、当开发者知道自己仅仅需要修改的数据在 skb-&gt;start 和 skb-&gt;end 的区域时，开发者可以使用 pskb_copy 方法只克隆那个区域。2、当开发者认为自己或许也需要修改分段数据域时，就必须使用 skb_copy。 pskb_copy 和 skb_copy 的不同如下图中的(a)和(b): 在决定克隆或复制缓冲区时，每个子系统的程序员都无法预料其他内核组件（或其子系统的其他用户）是否需要该缓冲区中的原始信息。内核是非常模块化的，并且以非常动态和不可预测的方式进行更改，因此每个子系统都不知道其他子系统可以使用缓冲区做什么。因此，每个子系统的程序员只需跟踪他们对缓冲区所做的任何修改，并注意在修改任何内容之前先进行复制，以防内核的其他部分需要原始信息。 队列管理函数有一些函数用来维护 sk_buff 双向链表（也可以称为队列 queue）中的节点。下面是一些常用的功能函数： skb_queue_head_init使用空节点初始化 sk_buff_head。 skb_queue_head, skb_queue_tail将一个缓冲区添加到队列的头部或尾部。 skb_dequeue, skb_dequeue_tail从队列的头和尾取出一个节点。 skb_queue_purge清空队列。 skb_queue_walk使用 for 循环遍历队列，其实现如下： 1234#define skb_queue_walk(queue, skb) for (skb = (queue)-&gt;next; skb != (struct sk_buff *)(queue); skb = skb-&gt;next) 可以看到其实现是定义了一个宏，预处理编译之后 skb_queue_walk 就会被替换为上面的代码，因为 sk_buff 的队列是一个双向链表，去过遍历到了头节点，说明遍历完成了。 操作队列的所有函数都必须保证是原子操作。也就是说，它们必须获取 sk_buff_head 结构提供的队列自旋锁。否则，它们可能会被异步事件中断，这些异步事件会使队列中的元素入队或出队，例如到期计时器调用的函数会导致争用条件。"},{"path":"/2024/04/05/linux-docs/网络协议栈/透过现象看本质，从linux源码角度看epoll/","content":"对于较多数量的文件描述符的监听无论是select还是poll系统调用都显得捉襟见肘，poll每次都需要将所有的文件描述符复制到内核，内核本身不会对这些文件描述符加以保存，这样的设计就导致了poll的效率的低下。 而epoll则对此做了相应的改进，不是epoll_wait的时候才传入fd，而是通过epoll_ctl把所有fd传入内核，再一起”wait”，这就省掉了不必要的重复拷贝。 其次，在 epoll_wait时，也不是把current轮流的加入fd对应的设备等待队列，而是在设备等待队列醒来时调用一个回调函数（当然，这就需要“唤醒回调”机制），把产生事件的fd归入一个链表，然后返回这个链表上的fd。另外，epoll机制实现了自己特有的文件系统eventpoll filesystem。 epoll初始化当系统启动时，epoll会进行初始化操作 1234567891011121314151617181920static int __init eventpoll_init(void)&#123; mutex_init(&amp;epmutex); /* Initialize the structure used to perform safe poll wait head wake ups */ ep_poll_safewake_init(&amp;psw); /* Allocates slab cache used to allocate &quot;struct epitem&quot; items */ epi_cache = kmem_cache_create(&quot;eventpoll_epi&quot;, sizeof(struct epitem), 0, SLAB_HWCACHE_ALIGN|EPI_SLAB_DEBUG|SLAB_PANIC, NULL); /* Allocates slab cache used to allocate &quot;struct eppoll_entry&quot; */ pwq_cache = kmem_cache_create(&quot;eventpoll_pwq&quot;, sizeof(struct eppoll_entry), 0, EPI_SLAB_DEBUG|SLAB_PANIC, NULL); return 0;&#125;fs_initcall(eventpoll_init); 上面的代码实现一些数据结构的初始化,通过fs&#x2F;eventpoll.c中的注释可以看出,有三种类型的锁机制使用场景: 1.epmutex(mutex):用户关闭文件描述符，但是没有调用EPOLL_CTL_DEL2.ep-&gt;mtx(mutex):用户态与内核态的转换可能会睡眠3.ep-&gt;lock(spinlock):内核态与具体设备中断过程中的转换,poll回调 接下来就是使用slab分配器动态分配内存，第一个结构为当系统中添加一个fd时，就创建一epitem结构体,内核管理的基本数据结构。 内核数据结构epoll在内核主要维护了两个数据结构eventpoll与epitem，其中eventpoll表示每个epoll实例本身，epitem表示的是每一个IO所对应的的事件。 12345678910111213141516171819202122232425262728293031struct epitem &#123; /* RB tree node used to link this structure to the eventpoll RB tree */ struct rb_node rbn; /*用于挂载到eventpoll管理的红黑树*/ /* List header used to link this structure to the eventpoll ready list */ struct list_head rdllink; /*挂载到eventpoll.rdlist的事件就绪队列*/ /* * Works together &quot;struct eventpoll&quot;-&gt;ovflist in keeping the * single linked chain of items. */ struct epitem *next; /*用于主结构体中的链表*/ /* The file descriptor information this item refers to */ struct epoll_filefd ffd; /*该结构体对应的被监听的文件描述符信息(fd+file， 作为红黑树的key)*/ /* Number of active wait queue attached to poll operations */ int nwait; /*poll(轮询操作)的事件个数 /* List containing poll wait queues */ struct list_head pwqlist; /*双向链表，保存被监视文件的等待队列，功能类似于select/poll中的poll_table；同一个文件上可能会监视多种事件，这些事件可能从属于不同的wait_queue中，所以需要使用链表 /* The &quot;container&quot; of this item */ struct eventpoll *ep; /*当前epitem的所有者（多个epitem从属于一个eventpoll）*/ /* List header used to link this item to the &quot;struct file&quot; items list */ struct list_head fllink; /*双向链表，用来链接被监视的文件描述符对应的struct file。因为file里有f_ep_link用来保存所有监视这个文件的epoll节点 /* The structure that describe the interested events and the source fd */ struct epoll_event event; /*注册感兴趣的事件，也就是用户空间的epoll_event&#125;; 而每个epoll fd对应的主要数据结构为： 12345678910111213141516171819202122232425262728293031struct eventpoll &#123; /* Protect the this structure access */ spinlock_t lock; /*自旋锁，在kernel内部用自旋锁加锁，就可以同时多线(进)程对此结构体进行操作，主要是保护ready_list*/ /* * This mutex is used to ensure that files are not removed * while epoll is using them. This is held during the event * collection loop, the file cleanup path, the epoll file exit * code and the ctl operations. */ struct mutex mtx; /*防止使用时被删除*/ /* Wait queue used by sys_epoll_wait() */ wait_queue_head_t wq; /*sys_epoll_wait()使用的等待队列*/ /* Wait queue used by file-&gt;poll() */ wait_queue_head_t poll_wait; /*file-&gt;epoll()使用的等待队列*/ /* List of ready file descriptors */ struct list_head rdllist; /*事件就绪链表*/ /* RB tree root used to store monitored fd structs */ struct rb_root rbr; /*用于管理当前epoll关注的文件描述符（树根）*/ /* * This is a single linked list that chains all the &quot;struct epitem&quot; that * happened while transfering ready events to userspace w/out * holding -&gt;lock. */ struct epitem *ovflist; /*在向用户空间传输就绪事件的时候，将同时发生事件的文件描述符链入到这个链表里面*/&#125;; 函数调用关系epoll_create每个eventpoll通过epoll_create()创建： 12345678910111213141516171819202122232425262728293031323334asmlinkage long sys_epoll_create(int size)&#123; int error, fd = -1; struct eventpoll *ep; DNPRINTK(3, (KERN_INFO &quot;[%p] eventpoll: sys_epoll_create(%d) &quot;, current, size)); /* * Sanity check on the size parameter, and create the internal data * structure ( &quot;struct eventpoll&quot; ). */ error = -EINVAL; /*为ep分配内存并进行初始化*/ if (size &lt;= 0 || (error = ep_alloc(&amp;ep)) &lt; 0) &#123; fd = error; goto error_return; &#125; /* * Creates all the items needed to setup an eventpoll file. That is, * a file structure and a free file descriptor. */ /*调用anon_inode_getfd新建一个struct file，也就是epoll可以看成一个文件（由* 于没有任何文件系统，为匿名文件）。并且将主结构体struct eventpoll *ep放入* file-&gt;private项中进行保存（sys_epoll_ctl会取用）*/ fd = anon_inode_getfd(&quot;[eventpoll]&quot;, &amp;eventpoll_fops, ep); if (fd &lt; 0) ep_free(ep);error_return: DNPRINTK(3, (KERN_INFO &quot;[%p] eventpoll: sys_epoll_create(%d) = %d &quot;, current, size, fd)); return fd;&#125; epoll_ctl123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166asmlinkage long sys_epoll_ctl(int epfd, int op, int fd, struct epoll_event __user *event)&#123; int error; struct file *file, *tfile; struct eventpoll *ep; struct epitem *epi; struct epoll_event epds; DNPRINTK(3, (KERN_INFO &quot;[%p] eventpoll: sys_epoll_ctl(%d, %d, %d, %p) &quot;, current, epfd, op, fd, event)); error = -EFAULT; /*判断参数合法性，将__user *event 复制给epds*/ if (ep_op_has_event(op) &amp;&amp; copy_from_user(&amp;epds, event, sizeof(struct epoll_event))) goto error_return; /* Get the &quot;struct file *&quot; for the eventpoll file */ error = -EBADF; file = fget(epfd); /*epoll fd对应的文件对象*/ if (!file) goto error_return; /* Get the &quot;struct file *&quot; for the target file */ tfile = fget(fd); /*fd对应的文件对象*/ if (!tfile) goto error_fput; /* The target file descriptor must support poll */ error = -EPERM; if (!tfile-&gt;f_op || !tfile-&gt;f_op-&gt;poll) goto error_tgt_fput; ... /* * At this point it is safe to assume that the &quot;private_data&quot; contains * our own data structure. */ ep = file-&gt;private_data; /*在create时存入进去的（anon_inode_getfd），现在取用。*/ mutex_lock(&amp;ep-&gt;mtx); /* * Try to lookup the file inside our RB tree, Since we grabbed &quot;mtx&quot; * above, we can be sure to be able to use the item looked up by * ep_find() till we release the mutex. */ epi = ep_find(ep, tfile, fd); /*防止重复添加（在ep的红黑树中查找是否已经存在这个fd）*/ switch (op) &#123; case EPOLL_CTL_ADD: /*新增一个监听fd*/ if (!epi) &#123; epds.events |= POLLERR | POLLHUP; /*默认包含POLLERR和POLLHUP事件*/ error = ep_insert(ep, &amp;epds, tfile, fd); /*在ep的红黑树中插入这个fd对应的epitm结构体。*/ &#125; else /*重复添加（在ep的红黑树中查找已经存在这个fd）。*/ error = -EEXIST; break; ... &#125; ... return error;&#125;其中ep_insert的实现如下：```cstatic int ep_insert(struct eventpoll *ep, struct epoll_event *event, struct file *tfile, int fd)&#123; int error, revents, pwake = 0; unsigned long flags; struct epitem *epi; struct ep_pqueue epq; error = -ENOMEM; /*分配一个epitem结构体来保存每个存入的fd*/ if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL))) goto error_return; /* Item initialization follow here ... */ /*初始化该结构体*/ INIT_LIST_HEAD(&amp;epi-&gt;rdllink); INIT_LIST_HEAD(&amp;epi-&gt;fllink); INIT_LIST_HEAD(&amp;epi-&gt;pwqlist); epi-&gt;ep = ep; ep_set_ffd(&amp;epi-&gt;ffd, tfile, fd); epi-&gt;event = *event; epi-&gt;nwait = 0; epi-&gt;next = EP_UNACTIVE_PTR; /* Initialize the poll table using the queue callback */ epq.epi = epi; /*安装poll回调函数*/ init_poll_funcptr(&amp;epq.pt, ep_ptable_queue_proc); /* * Attach the item to the poll hooks and get current event bits. * We can safely use the file* here because its usage count has * been increased by the caller of this function. Note that after * this operation completes, the poll callback can start hitting * the new item. */ /* * 调用poll函数来获取当前事件位，其实是利用它来调用注册函数ep_ptable_queue_proc（poll_wait中调用）。 * 如果fd是套接字，f_op为socket_file_ops，poll函数是sock_poll()。 * 如果是TCP套接字的话，进而会调用到tcp_poll()函数。此处调用poll函数查看当前文件描述符的状态，存储在revents中。 * 在poll的处理函数(tcp_poll())中，会调用sock_poll_wait()， * 在sock_poll_wait()中会调用到epq.pt.qproc指向的函数，也就是ep_ptable_queue_proc()。 */ revents = tfile-&gt;f_op-&gt;poll(tfile, &amp;epq.pt); /* Add the current item to the list of active epoll hook for this file */ spin_lock(&amp;tfile-&gt;f_ep_lock); list_add_tail(&amp;epi-&gt;fllink, &amp;tfile-&gt;f_ep_links); spin_unlock(&amp;tfile-&gt;f_ep_lock); /* * Add the current item to the RB tree. All RB tree operations are * protected by &quot;mtx&quot;, and ep_insert() is called with &quot;mtx&quot; held. */ ep_rbtree_insert(ep, epi); /*将该epi插入到ep的红黑树中*/ /* We have to drop the new item inside our item list to keep track of it */ spin_lock_irqsave(&amp;ep-&gt;lock, flags); /* If the file is already &quot;ready&quot; we drop it inside the ready list */ /* * revents &amp; event-&gt;events：刚才fop-&gt;poll的返回值中标识的事件有用户event关心的事件发生。 * !ep_is_linked(&amp;epi-&gt;rdllink)：epi的ready队列中有数据。ep_is_linked用于判断队列是否为空。 */ /* 如果要监视的文件状态已经就绪并且还没有加入到就绪队列中,则将当前的epitem加入到就绪队列中.如果有进程正在等待该文件的状态就绪,则唤醒一个等待的进程。 */ if ((revents &amp; event-&gt;events) &amp;&amp; !ep_is_linked(&amp;epi-&gt;rdllink)) &#123; /*将当前epi插入到ep-&gt;ready队列中。*/ list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); /* Notify waiting tasks that events are available */ /* 如果有进程正在等待文件的状态就绪，也就是调用epoll_wait睡眠的进程正在等待，则唤醒一个等待进程。waitqueue_active(q) 等待队列q中有等待的进程返回1，否则返回0。*/ if (waitqueue_active(&amp;ep-&gt;wq)) wake_up_locked(&amp;ep-&gt;wq); /* 如果有进程等待eventpoll文件本身（???）的事件就绪，则增加临时变量pwake的值，pwake的值不为0时，在释放lock后，会唤醒等待进程。 */ if (waitqueue_active(&amp;ep-&gt;poll_wait)) pwake++; &#125; spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); /* We have to call this outside the lock */ if (pwake) /*唤醒等待eventpoll文件状态就绪的进程*/ ep_poll_safewake(&amp;psw, &amp;ep-&gt;poll_wait); DNPRINTK(3, (KERN_INFO &quot;[%p] eventpoll: ep_insert(%p, %p, %d) &quot;, current, ep, tfile, fd)); return 0;...&#125; init_poll_funcptr(&amp;epq.pt, ep_ptable_queue_proc);和revents &#x3D; tfile-&gt;f_op-&gt;poll(tfile, &amp;epq.pt);这两个函数将ep_ptable_queue_proc注册到epq.pt中的qproc。 1234567typedef struct poll_table_struct &#123;poll_queue_proc qproc;unsigned long key;&#125;poll_table; 执行f_op-&gt;poll(tfile, &amp;epq.pt)时，XXX_poll(tfile, &amp;epq.pt)函数会执行poll_wait()，poll_wait()会调用epq.pt.qproc函数，即ep_ptable_queue_proc。 ep_ptable_queue_proc函数如下： 123456789101112131415161718192021/*当poll醒来时就回调用该函数,在文件操作中的poll函数中调用，将epoll的回调函数加入到目标文件的唤醒队列中。如果监视的文件是套接字，参数whead则是sock结构的sk_sleep成员的地址*/static void ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead, poll_table *pt)&#123; /*pt获取struct ep_queue的epi字段。*/ struct epitem *epi = ep_item_from_epqueue(pt); struct eppoll_entry *pwq; if (epi-&gt;nwait &gt;= 0 &amp;&amp; (pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL))) &#123; init_waitqueue_func_entry(&amp;pwq-&gt;wait, ep_poll_callback); pwq-&gt;whead = whead; pwq-&gt;base = epi; add_wait_queue(whead, &amp;pwq-&gt;wait); list_add_tail(&amp;pwq-&gt;llink, &amp;epi-&gt;pwqlist); epi-&gt;nwait++; &#125; else &#123; /* We have to signal that an error occurred */ /*如果分配内存失败，则将nwait置为-1，表示发生错误，即内存分配失败，或者已发生错误*/ epi-&gt;nwait = -1; &#125;&#125; 其中struct eppoll_entry定义如下： 1234567891011struct eppoll_entry &#123; struct list_head llink; struct epitem *base; wait_queue_t wait; wait_queue_head_t *whead;&#125;; ep_ptable_queue_proc 函数完成 epitem 加入到特定文件的wait队列任务。ep_ptable_queue_proc有三个参数： 12345struct file *file; 该fd对应的文件对象wait_queue_head_t *whead; 该fd对应的设备等待队列（同select中的mydev-&gt;wait_address）poll_table *pt; f_op-&gt;poll(tfile, &amp;epq.pt)中的epq.pt 在ep_ptable_queue_proc函数中，引入了另外一个非常重要的数据结构eppoll_entry。eppoll_entry主要完成epitem和epitem事件发生时的callback（ep_poll_callback）函数之间的关联。首先将eppoll_entry的whead指向fd的设备等待队列（同select中的wait_address），然后初始化eppoll_entry的base变量指向epitem，最后通过add_wait_queue将epoll_entry挂载到fd的设备等待队列上。完成这个动作后，epoll_entry已经被挂载到fd的设备等待队列。 由于ep_ptable_queue_proc函数设置了等待队列的ep_poll_callback回调函数。所以在设备硬件数据到来时，硬件中断处理函数中会唤醒该等待队列上等待的进程时，会调用唤醒函数ep_poll_callback 12345678910111213141516171819202122232425262728293031323334353637383940414243444546static int ep_poll_callback(wait_queue_t *wait, unsigned mode, int sync, void *key)&#123; int pwake = 0; unsigned long flags; struct epitem *epi = ep_item_from_wait(wait); struct eventpoll *ep = epi-&gt;ep; spin_lock_irqsave(&amp;ep-&gt;lock, flags); /* * If the event mask does not contain any poll(2) event, we consider the * descriptor to be disabled. This condition is likely the effect of the * EPOLLONESHOT bit that disables the descriptor when an event is received, * until the next EPOLL_CTL_MOD will be issued. */ if (!(epi-&gt;event.events &amp; ~EP_PRIVATE_BITS)) goto out_unlock; ... /* If this file is already in the ready list we exit soon */ if (ep_is_linked(&amp;epi-&gt;rdllink)) goto is_linked; /*将该fd加入到epoll监听的就绪链表中*/ list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist);is_linked: /* * Wake up ( if active ) both the eventpoll wait list and the -&gt;poll() * wait list. */ /*唤醒调用epoll_wait()函数时睡眠的进程。用户层epoll_wait(...) 超时前返回。*/ if (waitqueue_active(&amp;ep-&gt;wq)) wake_up_locked(&amp;ep-&gt;wq); if (waitqueue_active(&amp;ep-&gt;poll_wait)) pwake++;out_unlock: spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); /* We have to call this outside the lock */ if (pwake) ep_poll_safewake(&amp;psw, &amp;ep-&gt;poll_wait); return 1;&#125; epoll_waitepoll_wait实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950asmlinkage long sys_epoll_wait(int epfd, struct epoll_event __user *events, int maxevents, int timeout)&#123; int error; struct file *file; struct eventpoll *ep; /* The maximum number of event must be greater than zero */ if (maxevents &lt;= 0 || maxevents &gt; EP_MAX_EVENTS) return -EINVAL; /* Verify that the area passed by the user is writeable */ /* 检查用户空间传入的events指向的内存是否可写。参见__range_not_ok()。*/ if (!access_ok(VERIFY_WRITE, events, maxevents * sizeof(struct epoll_event))) &#123; error = -EFAULT; goto error_return; &#125; /* Get the &quot;struct file *&quot; for the eventpoll file */ /* 获取epfd对应的eventpoll文件的file实例，file结构是在epoll_create中创建。 */ error = -EBADF; file = fget(epfd); if (!file) goto error_return; /* * We have to check that the file structure underneath the fd * the user passed to us _is_ an eventpoll file. */ /* 通过检查epfd对应的文件操作是不是eventpoll_fops 来判断epfd是否是一个eventpoll文件。如果不是则返回EINVAL错误。 */ error = -EINVAL; if (!is_file_epoll(file)) goto error_fput; /* * At this point it is safe to assume that the &quot;private_data&quot; contains * our own data structure. */ ep = file-&gt;private_data; /* Time to fish for events ... */ error = ep_poll(ep, events, maxevents, timeout);error_fput: fput(file);error_return: return error;&#125; ep_pollepoll_wait调用ep_poll，ep_poll实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout)&#123; int res, eavail; unsigned long flags; long jtimeout; wait_queue_t wait; /* * Calculate the timeout by checking for the &quot;infinite&quot; value ( -1 ) * and the overflow condition. The passed timeout is in milliseconds, * that why (t * HZ) / 1000. */ /* timeout是以毫秒为单位，这里是要转换为jiffies时间。这里加上999(即1000-1)，是为了向上取整。 */ jtimeout = (timeout &lt; 0 || timeout &gt;= EP_MAX_MSTIMEO) ? MAX_SCHEDULE_TIMEOUT : (timeout * HZ + 999) / 1000;retry: spin_lock_irqsave(&amp;ep-&gt;lock, flags); res = 0; if (list_empty(&amp;ep-&gt;rdllist)) &#123; /* * We don&#x27;t have any available event to return to the caller. * We need to sleep here, and we will be wake up by * ep_poll_callback() when events will become available. */ /* 没有事件，所以需要睡眠。当有事件到来时，睡眠会被ep_poll_callback函数唤醒。*/ init_waitqueue_entry(&amp;wait, current); /*将current进程放在wait这个等待队列中。*/ wait.flags |= WQ_FLAG_EXCLUSIVE; /* 将当前进程加入到eventpoll的等待队列中，等待文件状态就绪或直到超时，或被信号中断。 */ __add_wait_queue(&amp;ep-&gt;wq, &amp;wait); for (;;) &#123; /* * We don&#x27;t want to sleep if the ep_poll_callback() sends us * a wakeup in between. That&#x27;s why we set the task state * to TASK_INTERRUPTIBLE before doing the checks. */ /* 执行ep_poll_callback()唤醒时应当需要将当前进程唤醒，所以当前进程状态应该为“可唤醒”TASK_INTERRUPTIBLE */ set_current_state(TASK_INTERRUPTIBLE); /* 如果就绪队列不为空，也就是说已经有文件的状态就绪或者超时，则退出循环。*/ if (!list_empty(&amp;ep-&gt;rdllist) || !jtimeout) break; /* 如果当前进程接收到信号，则退出循环，返回EINTR错误 */ if (signal_pending(current)) &#123; res = -EINTR; break; &#125; spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); /* * 主动让出处理器，等待ep_poll_callback()将当前进程唤醒或者超时,返回值是剩余的时间。 * 从这里开始当前进程会进入睡眠状态，直到某些文件的状态就绪或者超时。 * 当文件状态就绪时，eventpoll的回调函数ep_poll_callback()会唤醒在ep-&gt;wq指向的等待队列中的进程。 */ jtimeout = schedule_timeout(jtimeout); spin_lock_irqsave(&amp;ep-&gt;lock, flags); &#125; __remove_wait_queue(&amp;ep-&gt;wq, &amp;wait); set_current_state(TASK_RUNNING); &#125; /* Is it worth to try to dig for events ? */ /* * ep-&gt;ovflist链表存储的向用户传递事件时暂存就绪的文件。 * 所以不管是就绪队列ep-&gt;rdllist不为空，或者ep-&gt;ovflist不等于 * EP_UNACTIVE_PTR，都有可能现在已经有文件的状态就绪。 * ep-&gt;ovflist不等于EP_UNACTIVE_PTR有两种情况，一种是NULL，此时 * 可能正在向用户传递事件，不一定就有文件状态就绪， * 一种情况时不为NULL，此时可以肯定有文件状态就绪， * 参见ep_send_events()。 */ eavail = !list_empty(&amp;ep-&gt;rdllist); spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); /* * Try to transfer events to user space. In case we get 0 events and * there&#x27;s still timeout left over, we go trying again in search of * more luck. */ /* 如果没有被信号中断，并且有事件就绪，但是没有获取到事件(有可能被其他进程获取到了)，并且没有超时，则跳转到retry标签处，重新等待文件状态就绪。 */ if (!res &amp;&amp; eavail &amp;&amp; !(res = ep_send_events(ep, events, maxevents)) &amp;&amp; jtimeout) goto retry; /* 返回获取到的事件的个数或者错误码 */ return res;&#125; ep_send_events()函数向用户空间发送就绪事件。 ep_send_events()函数将用户传入的内存简单封装到ep_send_events_data结构中，然后调用ep_scan_ready_list()将就绪队列中的事件传入用户空间的内存。 用户空间访问这个结果，进行处理。"},{"path":"/2024/04/05/linux-docs/网络协议栈/深入分析Linux操作系统对于TCP IP栈的实现原理与具体过程/","content":"一、Linux内核与网络体系结构在我们了解整个linux系统的网络体系结构之前，我们需要对整个网络体系调用，初始化和交互的位置，同时也是Linux操作系统中最为关键的一部分代码——-内核，有一个初步的认知。 1、Linux内核的结构首先，从功能上，我们将linux内核划分为五个不同的部分，分别是 （1）进程管理：主要负载CPU的访问控制，对CPU进行调度管理；（2）内存管理：主要提供对内存资源的访问控制；（3）文件系统：将硬盘的扇区组织成文件系统，实现文件读写等操作；（4）设备管理：用于控制所有的外部设备及控制器；（5）网洛：主要负责管理各种网络设备，并实现各种网络协议栈，最终实现通过网络连接其它系统的功能； 每个部分分别处理一项明确的功能，又向其它各个部分提供自己所完成的功能，相互协调，共同完成操作系统的任务。 Linux内核架构如下图所示： 2、Linux网络子系统内核的基本架构我们已经了解清楚了，接下来我们重点关注到内核中的网络模块，观察在linux内核中，我们是如何实现及运用TCP&#x2F;IP协议，并完成网络的初始化及各个模块调用调度。我们将内核中的网络部分抽出，通过对比TCP&#x2F;IP分层协议，与Linux网络实现体系相对比，深入的了解学习linux内核是怎样具体的实现TCP&#x2F;IP协议栈的。 Linux网络体系与TCP&#x2F;IP协议栈如下图所示。 可以看到，在图中，linux为了抽象与实现相分离，将内核中的网络部分划分为五层： 系统调用接口：系统调用接口是用户空间的应用程序正常访问内核的唯一途径，系统调用一般以sys开头。 协议无关接口：协议无关接口是由socket来实现的，它提供一组通用函数来支持各种不同的协议。Linux中socket结构是struct sock，这个结构定义了socket所需要的所 有状态信息，包括socke所使用的协议以及可以在socket上执行的操作。 网络协议：Linux支持多种协议，每一个协议都对应net_family[]数组中的一项，net_family[]的元素为一个结构体指针，指向一个包含注册协议信息的结构体 net_proto_family; 设备无关接口：设备无关接口net_device实现的，任何设备与上层通信都是通过net_device设备无关接口。它将设备与具有很多功能的不同硬件连接在一起，这一层提供一组通用函数供底层网络设备驱动程序使用，让它们可以对高层协议栈进行操作。 设备驱动程序：网络体系结构的最底部是负责管理物理网络设备的设备驱动程序层。 Linux网络子系统通过这五层结构的相互交互，共同完成TCP&#x2F;IP协议栈的运行。 3、TCP&#x2F;IP协议栈3.1 网络架构Linux网络协议栈的架构如下图所示。该图展示了如何实现Internet模型,在最上面的是用户空间中实现的应用层，而中间为内核空间中实现的网络子系统，底部为物理设备，提供了对网络的连接能力。在网络协议栈内部流动的是套接口缓冲区(SKB)，用于在协议栈的底层、上层以及应用层之间传递报文数据。 网络协议栈顶部是系统调用接口，为用户空间中的应用程序提供一种访问内核网络子系统的接口。下面是一个协议无关层，它提供了一种通用方法来使用传输层协议。然后是传输层的具体协议，包括TCP、UDP。在传输层下面是网络层，之后是邻居子系统，再下面是网络设备接口，提供了与各个设备驱动通信的通用接口。最底层是设备驱动程序。 3.2 协议无关接口通过网络协议栈通信需要对套接口进行操作，套接口是一个与协议无关的接口，它提供了一组接口来支持各种协议，套接口层不但可以支持典型的TCP和UDP协议，还可以支持RAW套接口、RAW以太网以及其他传输协议。 Linux中使用socket结构描述套接口，代表一条通信链路的一端，用来存储与该链路有关的所有信息，这些信息中包括： 所使用的协议 协议的状态信息(包括源地址和目标地址) 到达的连接队列 数据缓存和可选标志等等 其示意图如下所示： 其中最关键的成员是sk和ops，sk指向与该套接口相关的传输控制块，ops指向特定的传输协议的操作集。 下图详细展示了socket结构体中的sk和ops字段，以TCP为例。 sk字段指向与该套接口相关的传输控制块，传输层使用传输控制块来存放套接口所需的信息，在上图中即为TCP传输控制块，即tcp_sock结构。 ops字段指向特定传输协议的操作集接口，proto_pos结构中定义的接口函数是从套接口系统调用到传输层调用的入口，因此其成员与socket系统调用基本上是一一对应的。整个proto_ops结构就是一张套接口系统调用的跳转表，TCP、UDP、RAW套接口的传输层操作集分别为inet_stream_ops、inet_dgram_ops、inet_sockraw_ops。 3.3 套接口缓存如下图所示，网络子系统中用来存储数据的缓冲区叫做套接口缓存，简称为SKB，该缓存区能够处理可变长数据，即能够很容易地在数据区头尾部添加和移除数据，且尽量避免数据的复制，通常每个报文使用一个SKB表示，各协议栈报文头通过一组指针进行定位，由于SKB是网络子系统中数据管理的核心，因此有很多管理函数是对它进行操作的。 SKB主要用于在网络驱动程序和应用程序之间传递、复制数据包。当应用程序要发送一个数据包时： 数据通过系统调用提交到内核中 系统会分配一个SKB来存储数据 之后向下层传递 再传递给网络驱动后才将其释放 当网络设备接收到数据包也要分配一个SKB来对数据进行存储，之后再向上传递，最终将数据复制到应用程序后进行释放。 3.4 重要的数据结构3.4.1 sk_bufsk_buf是Linux网络协议栈最重要的数据结构之一，该数据结构贯穿于整个数据包处理的流程。由于协议采用分层结构，上层向下层传递数据时需要增加包头，下层向上层数据时又需要去掉包头。sk_buff中保存了L2，L3，L4层的头指针，这样在层传递时只需要对数据缓冲区改变头部信息，并调整sk_buff中的指针，而不需要拷贝数据，这样大大减少了内存拷贝的需要。 sk_buf的示意图如下： 各字段含义如下： head：指向分配给的线性数据内存首地址。 data：指向保存数据内容的首地址。 tail：指向数据的结尾。 end：指向分配的内存块的结尾。 len：数据的长度。 head room: 位于head至data之间的空间，用于存储：protocol header，例如：TCP header, IP header, Ethernet header等。 user data: 位于data至tail之间的空间，用于存储：应用层数据，一般系统调用时会使用到。 tail room: 位于tail至end之间的空间，用于填充用户数据未使用完的空间。 skb_shared_info: 位于end之后，用于存储特殊数据结构skb_shared_info，该结构用于描述分片信息。 sk_buf的常用操作函数如下： alloc_skb：分配sk_buf。 skb_reserve：为sk_buff设置header空间。 skb_put：添加用户层数据。 skb_push：向header空间添加协议头。 skb_pull：复位data至数据区。 操作sk_buf的简单示意图如下： 3.4.2 net_device在网络适配器硬件和软件协议栈之间需要一个接口，共同完成操作系统内核中协议栈数据处理与异步收发的功能。在Linux网络体系结构中，这个接口要满足以下要求: （1）抽象出网络适配器的硬件特性。（2）为协议栈提供统一的调用接口。 以上两个要求在Linux内核的网络体系结构中分别由两个软件（设备独立接口文件dev.c和网络设备驱动程序）和一个主要的数据结构net_device实现。 设备独立接口文件dev.c中实现了对上层协议的统一调用接口，dev.c文件中的函数实现了以下主要功能。 协议调用与驱动程序函数对应：dev.c文件中的函数查看数据包由哪个网络设备(由sk_buff结构中*dev数据域指明该数据包由哪个网络设备net_device实例接收&#x2F;发送)传送，根据系统中注册的设备实例,调用网络设备驱动程序函数，实现硬件的收发。 对net_device数据结构的数据域统一初始化：dev.c提供了一些常规函数，来初始化net_device结构中的这样一些数据域:它们的值对所有类型的设备都一样，驱动程序可以调用这些函数来设置其设备实例的默认值，也可以重写由内核初始化的值。 每一个网络设备都必须有一个驱动程序，并提供一个初始化函数供内核启动时调用，或在装载网络驱动程序模块时调用。不管网络设备内部有什么不同，有一件事是所有网络设备驱动程序必须首先完成的任务:初始化一个net_device数据结构的实例作为网络设备在内核中的实体，并将net_device数据结构实例的各数据域初始化为可工作的状态，然后将设备实例注册到内核中，为协议栈提供传送服务。 net_device数据结构从以下两个方面描述了网络设备的硬件特性在内核中的表示。 描述设备属性 net_device数据结构实例是网络设备在内核中的表示，它是每个网络设备在内核中的基础数据结构，它包含的信息不仅仅是网络设备的硬件属性（中断、端口地址、驱动程序函数等)，还包括网络中与设备有关的上层协议栈的配置信息（如IP地址、子网掩码等)。它跟踪连接到 TCP&#x2F;IP协议栈上的所有设备的状态信息。 实现设备驱动程序接口 net_device数据结构代表了上层的网络协议和硬件之间的一个通用接口，使我们可以将网络协议层的实现从具体的网络硬件部件中抽象出来，独立于硬件设备。为了有效地实现这种抽象，net_device中使用了大量函数指针，这样相对于上层的协议栈，它们在做数据收发操作时调用的函数的名字是相同的，但具体的函数实现细节可以根据不同的网络适配器而不同，由设备驱动程序提供，对网络协议栈透明。 3.4.3 socket内核中的进程可以通过socket结构体来访问linux内核中的网络系统中的传输层、网络层以及数据链路层，也可以说socket是内核中的进程与内核中的网络系统的桥梁。 我们知道在TCP层中使用两个协议：tcp协议和udp协议。而在将TCP层中的数据往下传输时，要使用网络层的协议，而网络层的协议很多，不同的网络使用不同的网络层协议。我们常用的因特网中，网络层使用的是IPV4和IPV6协议。所以在内核中的进程在使用struct socket提取内核网络系统中的数据时，不光要指明struct socket的类型(用于说明是提取TCP层中tcp协议负载的数据，还是udp层负载的数据)，还要指明网络层的协议类型(网络层的协议用于负载TCP层中的数据)。 linux内核中的网络系统中的网络层的协议，在linux中被称为address family(地址簇，通常以AF_XXX表示）或protocol family(协议簇，通常以PF_XXX表示)。 二、网络信息处理流程1、硬中断处理首先当数据帧从网线到达网卡上的时候，第一站是网卡的接收队列。网卡在分配给自己的RingBuffer中寻找可用的内存位置，找到后DMA引擎会把数据DMA到网卡之前关联的内存里，这个时候CPU都是无感的。当DMA操作完成以后，网卡会像CPU发起一个硬中断，通知CPU有数据到达。 注意，当RingBuffer满的时候，新来的数据包将给丢弃。ifconfig查看网卡的时候，可以里面有个overruns，表示因为环形队列满被丢弃的包。如果发现有丢包，可能需要通过ethtool命令来加大环形队列的长度。 网卡的硬中断注册的处理函数是igb_msix_ring。 123456789101112//file: drivers/net/ethernet/intel/igb/igb_main.cstatic irqreturn_t igb_msix_ring(int irq, void *data)&#123; struct igb_q_vector *q_vector = data; /* Write the ITR value calculated from the previous interrupt. */ igb_write_itr(q_vector); napi_schedule(&amp;q_vector-&gt;napi); return IRQ_HANDLED;&#125; igb_write_itr只是记录一下硬件中断频率（据说目的是在减少对CPU的中断频率时用到）。顺着napi_schedule调用一路跟踪下去，__napi_schedule&#x3D;&gt;____napi_schedule。 1234567/* Called with irq disabled */static inline void ____napi_schedule(struct softnet_data *sd, struct napi_struct *napi)&#123; list_add_tail(&amp;napi-&gt;poll_list, &amp;sd-&gt;poll_list); __raise_softirq_irqoff(NET_RX_SOFTIRQ);&#125; 这里我们看到，list_add_tail修改了CPU变量softnet_data里的poll_list，将驱动napi_struct传过来的poll_list添加了进来。 其中softnet_data中的poll_list是一个双向列表，其中的设备都带有输入帧等着被处理。紧接着__raise_softirq_irqoff触发了一个软中断NET_RX_SOFTIRQ， 这个所谓的触发过程只是对一个变量进行了一次或运算而已。 1234567void __raise_softirq_irqoff(unsigned int nr)&#123; trace_softirq_raise(nr); or_softirq_pending(1UL &lt;&lt; nr);&#125;//file: include/linux/irq_cpustat.h#define or_softirq_pending(x) (local_softirq_pending() |= (x)) Linux在硬中断里只完成简单必要的工作，剩下的大部分的处理都是转交给软中断的。通过上面代码可以看到，硬中断处理过程真的是非常短。只是记录了一个寄存器，修改了一下下CPU的poll_list，然后发出个软中断。就这么简单，硬中断工作就算是完成了。 2、ksoftirqd内核线程处理软中断 ksoftirqd_should_run代码如下： 1234567891011121314151617181920static int ksoftirqd_should_run(unsigned int cpu)&#123; return local_softirq_pending();&#125;#define local_softirq_pending() \\ __IRQ_STAT(smp_processor_id(), __softirq_pending)这里看到和硬中断中调用了同一个函数local_softirq_pending。使用方式不同的是硬中断位置是为了写入标记，这里仅仅只是读取。如果硬中断中设置了NET_RX_SOFTIRQ,这里自然能读取的到。接下来会真正进入线程函数中run_ksoftirqd处理：static void run_ksoftirqd(unsigned int cpu)&#123; local_irq_disable(); if (local_softirq_pending()) &#123; __do_softirq(); rcu_note_context_switch(cpu); local_irq_enable(); cond_resched(); return; &#125; local_irq_enable();&#125; 在__do_softirq中，判断根据当前CPU的软中断类型，调用其注册的action方法。 123456789101112131415161718asmlinkage void __do_softirq(void)&#123; do &#123; if (pending &amp; 1) &#123; unsigned int vec_nr = h - softirq_vec; int prev_count = preempt_count(); ... trace_softirq_entry(vec_nr); h-&gt;action(h); trace_softirq_exit(vec_nr); ... &#125; h++; pending &gt;&gt;= 1; &#125; while (pending);&#125; 在网络子系统初始化小节， 我们看到我们为NET_RX_SOFTIRQ注册了处理函数net_rx_action。所以net_rx_action函数就会被执行到了。 这里需要注意一个细节，硬中断中设置软中断标记，和ksoftirq的判断是否有软中断到达，都是基于smp_processor_id()的。这意味着只要硬中断在哪个CPU上被响应，那么软中断也是在这个CPU上处理的。所以说，如果你发现你的Linux软中断CPU消耗都集中在一个核上的话，做法是要把调整硬中断的CPU亲和性，来将硬中断打散到不通的CPU核上去。 我们再来把精力集中到这个核心函数net_rx_action上来。 12345678910111213141516171819202122static void net_rx_action(struct softirq_action *h)&#123; struct softnet_data *sd = &amp;__get_cpu_var(softnet_data); unsigned long time_limit = jiffies + 2; int budget = netdev_budget; void *have; local_irq_disable(); while (!list_empty(&amp;sd-&gt;poll_list)) &#123; ...... n = list_first_entry(&amp;sd-&gt;poll_list, struct napi_struct, poll_list); work = 0; if (test_bit(NAPI_STATE_SCHED, &amp;n-&gt;state)) &#123; work = n-&gt;poll(n, weight); trace_napi_poll(n); &#125; budget -= work; &#125;&#125; 函数开头的time_limit和budget是用来控制net_rx_action函数主动退出的，目的是保证网络包的接收不霸占CPU不放。 等下次网卡再有硬中断过来的时候再处理剩下的接收数据包。其中budget可以通过内核参数调整。 这个函数中剩下的核心逻辑是获取到当前CPU变量softnet_data，对其poll_list进行遍历, 然后执行到网卡驱动注册到的poll函数。对于igb网卡来说，就是igb驱动力的igb_poll函数了。 123456789101112131415/** * igb_poll - NAPI Rx polling callback * @napi: napi polling structure * @budget: count of how many packets we should handle **/static int igb_poll(struct napi_struct *napi, int budget)&#123; ... if (q_vector-&gt;tx.ring) clean_complete = igb_clean_tx_irq(q_vector); if (q_vector-&gt;rx.ring) clean_complete &amp;= igb_clean_rx_irq(q_vector, budget); ...&#125; 在读取操作中，igb_poll的重点工作是对igb_clean_rx_irq的调用。 1234567891011121314151617181920212223242526272829303132333435363738394041424344static bool igb_clean_rx_irq(struct igb_q_vector *q_vector, const int budget)&#123; ... do &#123; /* retrieve a buffer from the ring */ skb = igb_fetch_rx_buffer(rx_ring, rx_desc, skb); /* fetch next buffer in frame if non-eop */ if (igb_is_non_eop(rx_ring, rx_desc)) continue; &#125; /* verify the packet layout is correct */ if (igb_cleanup_headers(rx_ring, rx_desc, skb)) &#123; skb = NULL; continue; &#125; /* populate checksum, timestamp, VLAN, and protocol */ igb_process_skb_fields(rx_ring, rx_desc, skb); napi_gro_receive(&amp;q_vector-&gt;napi, skb);&#125;igb_fetch_rx_buffer和igb_is_non_eop的作用就是把数据帧从RingBuffer上取下来。为什么需要两个函数呢？因为有可能帧要占多多个RingBuffer，所以是在一个循环中获取的，直到帧尾部。获取下来的一个数据帧用一个sk_buff来表示。收取完数据以后，对其进行一些校验，然后开始设置sbk变量的timestamp, VLAN id, protocol等字段。接下来进入到napi_gro_receive中://file: net/core/dev.cgro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)&#123; skb_gro_reset_offset(skb); return napi_skb_finish(dev_gro_receive(napi, skb), skb);&#125;dev_gro_receive这个函数代表的是网卡GRO特性，可以简单理解成把相关的小包合并成一个大包就行，目的是减少传送给网络栈的包数，这有助于减少 CPU 的使用量。我们暂且忽略，直接看napi_skb_finish, 这个函数主要就是调用了netif_receive_skb。//file: net/core/dev.cstatic gro_result_t napi_skb_finish(gro_result_t ret, struct sk_buff *skb)&#123; switch (ret) &#123; case GRO_NORMAL: if (netif_receive_skb(skb)) ret = GRO_DROP; break; ......&#125; 在netif_receive_skb中，数据包将被送到协议栈中。声明，以下的3.3, 3.4, 3.5也都属于软中断的处理过程，只不过由于篇幅太长，单独拿出来成小节。 3、网络协议栈处理netif_receive_skb函数会根据包的协议，假如是udp包，会将包依次送到ip_rcv(),udp_rcv()协议处理函数中进行处理。 1234567891011121314151617181920212223242526272829303132333435363738394041//file: net/core/dev.cint netif_receive_skb(struct sk_buff *skb)&#123; //RPS处理逻辑，先忽略 ...... return __netif_receive_skb(skb);&#125;static int __netif_receive_skb(struct sk_buff *skb)&#123; ...... ret = __netif_receive_skb_core(skb, false);&#125;static int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc)&#123; ...... //pcap逻辑，这里会将数据送入抓包点。tcpdump就是从这个入口获取包的 list_for_each_entry_rcu(ptype, &amp;ptype_all, list) &#123; if (!ptype-&gt;dev || ptype-&gt;dev == skb-&gt;dev) &#123; if (pt_prev) ret = deliver_skb(skb, pt_prev, orig_dev); pt_prev = ptype; &#125; &#125; ...... list_for_each_entry_rcu(ptype, &amp;ptype_base[ntohs(type) &amp; PTYPE_HASH_MASK], list) &#123; if (ptype-&gt;type == type &amp;&amp; (ptype-&gt;dev == null_or_dev || ptype-&gt;dev == skb-&gt;dev || ptype-&gt;dev == orig_dev)) &#123; if (pt_prev) ret = deliver_skb(skb, pt_prev, orig_dev); pt_prev = ptype; &#125; &#125;&#125; 在__netif_receive_skb_core中，我看着原来经常使用的tcpdump的抓包点，很是激动，看来读一遍源代码时间真的没白浪费。接着__netif_receive_skb_core取出protocol，它会从数据包中取出协议信息，然后遍历注册在这个协议上的回调函数列表。ptype_base 是一个 hash table，在协议注册小节我们提到过。ip_rcv 函数地址就是存在这个 hash table中的。 12345678//file: net/core/dev.cstatic inline int deliver_skb(struct sk_buff *skb, struct packet_type *pt_prev, struct net_device *orig_dev)&#123; ...... return pt_prev-&gt;func(skb, skb-&gt;dev, pt_prev, orig_dev);&#125; pt_prev-&gt;func这一行就调用到了协议层注册的处理函数了。对于ip包来讲，就会进入到ip_rcv（如果是arp包的话，会进入到arp_rcv）。 4、IP协议层处理我们再来大致看一下linux在ip协议层都做了什么，包又是怎么样进一步被送到udp或tcp协议处理函数中的。 12345678//file: net/ipv4/ip_input.cint ip_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt, struct net_device *orig_dev)&#123; ...... return NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING, skb, dev, NULL, ip_rcv_finish);&#125; 这里NF_HOOK是一个钩子函数，当执行完注册的钩子后就会执行到最后一个参数指向的函数ip_rcv_finish。 1234567891011121314static int ip_rcv_finish(struct sk_buff *skb)&#123; ...... if (!skb_dst(skb)) &#123; int err = ip_route_input_noref(skb, iph-&gt;daddr, iph-&gt;saddr, iph-&gt;tos, skb-&gt;dev); ... &#125; ...... return dst_input(skb);&#125; 跟踪ip_route_input_noref 后看到它又调用了 ip_route_input_mc。 在ip_route_input_mc中，函数ip_local_deliver被赋值给了dst.input, 如下： 123456789//file: net/ipv4/route.cstatic int ip_route_input_mc(struct sk_buff *skb, __be32 daddr, __be32 saddr, u8 tos, struct net_device *dev, int our)&#123; if (our) &#123; rth-&gt;dst.input= ip_local_deliver; rth-&gt;rt_flags |= RTCF_LOCAL; &#125;&#125; 所以回到ip_rcv_finish中的return dst_input(skb)。 12345/* Input packet from network to transport. */static inline int dst_input(struct sk_buff *skb)&#123; return skb_dst(skb)-&gt;input(skb);&#125; skb_dst(skb)-&gt;input调用的input方法就是路由子系统赋的ip_local_deliver。 123456789101112131415161718192021222324252627//file: net/ipv4/ip_input.cint ip_local_deliver(struct sk_buff *skb)&#123; /* * Reassemble IP fragments. */ if (ip_is_fragment(ip_hdr(skb))) &#123; if (ip_defrag(skb, IP_DEFRAG_LOCAL_DELIVER)) return 0; &#125; return NF_HOOK(NFPROTO_IPV4, NF_INET_LOCAL_IN, skb, skb-&gt;dev, NULL, ip_local_deliver_finish);&#125;static int ip_local_deliver_finish(struct sk_buff *skb)&#123; ...... int protocol = ip_hdr(skb)-&gt;protocol; const struct net_protocol *ipprot; ipprot = rcu_dereference(inet_protos[protocol]); if (ipprot != NULL) &#123; ret = ipprot-&gt;handler(skb); &#125;&#125; 如协议注册小节看到inet_protos中保存着tcp_rcv()和udp_rcv()的函数地址。这里将会根据包中的协议类型选择进行分发,在这里skb包将会进一步被派送到更上层的协议中，udp和tcp。 5、UDP协议层处理udp协议的处理函数是udp_rcv。 123456789101112131415161718//file: net/ipv4/udp.cint udp_rcv(struct sk_buff *skb)&#123; return __udp4_lib_rcv(skb, &amp;udp_table, IPPROTO_UDP);&#125;int __udp4_lib_rcv(struct sk_buff *skb, struct udp_table *udptable, int proto)&#123; sk = __udp4_lib_lookup_skb(skb, uh-&gt;source, uh-&gt;dest, udptable); if (sk != NULL) &#123; int ret = udp_queue_rcv_skb(sk, skb &#125; icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);&#125; __udp4_lib_lookup_skb是根据skb来寻找对应的socket，当找到以后将数据包放到socket的缓存队列里。如果没有找到，则发送一个目标不可达的icmp包。 12345678910111213141516171819202122//file: net/ipv4/udp.cint udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)&#123; ...... if (sk_rcvqueues_full(sk, skb, sk-&gt;sk_rcvbuf)) goto drop; rc = 0; ipv4_pktinfo_prepare(skb); bh_lock_sock(sk); if (!sock_owned_by_user(sk)) rc = __udp_queue_rcv_skb(sk, skb); else if (sk_add_backlog(sk, skb, sk-&gt;sk_rcvbuf)) &#123; bh_unlock_sock(sk); goto drop; &#125; bh_unlock_sock(sk); return rc;&#125; sock_owned_by_user判断的是用户是不是正在这个socker上进行系统调用（socket被占用），如果没有，那就可以直接放到socket的接收队列中。如果有，那就通过sk_add_backlog把数据包添加到backlog队列。 当用户释放的socket的时候，内核会检查backlog队列，如果有数据再移动到接收队列中。 sk_rcvqueues_full接收队列如果满了的话，将直接把包丢弃。接收队列大小受内核参数net.core.rmem_max和net.core.rmem_default影响。 三、send分析1、传输层分析send的定义如下所示： 1ssize_t send(int sockfd, const void *buf, size_t len, int flags) 当在调用send函数的时候，内核封装send()为sendto()，然后发起系统调用。其实也很好理解，send()就是sendto()的一种特殊情况，而sendto()在内核的系统调用服务程序为sys_sendto，sys_sendto的代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637int __sys_sendto(int fd, void __user *buff, size_t len, unsigned int flags, struct sockaddr __user *addr, int addr_len)&#123; struct socket *sock; struct sockaddr_storage address; int err; struct msghdr msg; //用来表示要发送的数据的一些属性 struct iovec iov; int fput_needed; err = import_single_range(WRITE, buff, len, &amp;iov, &amp;msg.msg_iter); if (unlikely(err)) return err; sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed); if (!sock) goto out; msg.msg_name = NULL; msg.msg_control = NULL; msg.msg_controllen = 0; msg.msg_namelen = 0; if (addr) &#123; err = move_addr_to_kernel(addr, addr_len, &amp;address); if (err &lt; 0) goto out_put; msg.msg_name = (struct sockaddr *)&amp;address; msg.msg_namelen = addr_len; &#125; if (sock-&gt;file-&gt;f_flags &amp; O_NONBLOCK) flags |= MSG_DONTWAIT; msg.msg_flags = flags; err = sock_sendmsg(sock, &amp;msg); //实际的发送函数out_put: fput_light(sock-&gt;file, fput_needed);out: return err;&#125; __sys_sendto函数其实做了3件事： 通过fd获取了对应的struct socket 创建了用来描述要发送的数据的结构体struct msghdr 调用了sock_sendmsg来执行实际的发送 继续追踪sock_sendmsg，发现其最终调用的是sock-&gt;ops-&gt;sendmsg(sock, msg, msg_data_left(msg))，即socet在初始化时赋值给结构体struct proto tcp_prot的函数tcp_sendmsg，如下所示： 123456789101112131415161718struct proto tcp_prot = &#123; .name = &quot;TCP&quot;, .owner = THIS_MODULE, .close = tcp_close, .pre_connect = tcp_v4_pre_connect, .connect = tcp_v4_connect, .disconnect = tcp_disconnect, .accept = inet_csk_accept, .ioctl = tcp_ioctl, .init = tcp_v4_init_sock, .destroy = tcp_v4_destroy_sock, .shutdown = tcp_shutdown, .setsockopt = tcp_setsockopt, .getsockopt = tcp_getsockopt, .keepalive = tcp_set_keepalive, .recvmsg = tcp_recvmsg, .sendmsg = tcp_sendmsg, ... 而tcp_send函数实际调用的是tcp_sendmsg_locked函数，该函数的定义如下所示： 1234567891011int tcp_sendmsg_locked(struct sock *sk, struct msghdr *msg, size_t size)&#123; struct tcp_sock *tp = tcp_sk(sk);/*进行了强制类型转换*/ struct sk_buff *skb; flags = msg-&gt;msg_flags; ...... if (copied) tcp_push(sk, flags &amp; ~MSG_MORE, mss_now, TCP_NAGLE_PUSH, size_goal);&#125; 在tcp_sendmsg_locked中，完成的是将所有的数据组织成发送队列，这个发送队列是struct sock结构中的一个域sk_write_queue，这个队列的每一个元素是一个skb，里面存放的就是待发送的数据。在该函数中通过调用tcp_push()函数将数据加入到发送队列中。 sock结构体的部分代码如下所示： 123456struct sock&#123; ... struct sk_buff_head sk_write_queue;/*指向skb队列的第一个元素*/ ... struct sk_buff *sk_send_head;/*指向队列第一个还没有发送的元素*/&#125; tcp_push的代码如下所示： 123456789101112131415161718192021222324252627282930313233static void tcp_push(struct sock *sk, int flags, int mss_now, int nonagle, int size_goal)&#123; struct tcp_sock *tp = tcp_sk(sk); struct sk_buff *skb; skb = tcp_write_queue_tail(sk); if (!skb) return; if (!(flags &amp; MSG_MORE) || forced_push(tp)) tcp_mark_push(tp, skb); tcp_mark_urg(tp, flags); if (tcp_should_autocork(sk, skb, size_goal)) &#123; /* avoid atomic op if TSQ_THROTTLED bit is already set */ if (!test_bit(TSQ_THROTTLED, &amp;sk-&gt;sk_tsq_flags)) &#123; NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPAUTOCORKING); set_bit(TSQ_THROTTLED, &amp;sk-&gt;sk_tsq_flags); &#125; /* It is possible TX completion already happened * before we set TSQ_THROTTLED. */ if (refcount_read(&amp;sk-&gt;sk_wmem_alloc) &gt; skb-&gt;truesize) return; &#125; if (flags &amp; MSG_MORE) nonagle = TCP_NAGLE_CORK; __tcp_push_pending_frames(sk, mss_now, nonagle); //最终通过调用该函数发送数据&#125; 在之后tcp_push调用了__tcp_push_pending_frames(sk, mss_now, nonagle);来发送数据 __tcp_push_pending_frames的代码如下所示： 12345678void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss, int nonagle)&#123; if (tcp_write_xmit(sk, cur_mss, nonagle, 0, sk_gfp_mask(sk, GFP_ATOMIC))) //调用该函数发送数据 tcp_check_probe_timer(sk);&#125; 在__tcp_push_pending_frames又调用了tcp_write_xmit来发送数据，代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465static bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle, int push_one, gfp_t gfp)&#123; struct tcp_sock *tp = tcp_sk(sk); struct sk_buff *skb; unsigned int tso_segs, sent_pkts; int cwnd_quota; int result; bool is_cwnd_limited = false, is_rwnd_limited = false; u32 max_segs; /*统计已发送的报文总数*/ sent_pkts = 0; ...... /*若发送队列未满，则准备发送报文*/ while ((skb = tcp_send_head(sk))) &#123; unsigned int limit; if (unlikely(tp-&gt;repair) &amp;&amp; tp-&gt;repair_queue == TCP_SEND_QUEUE) &#123; /* &quot;skb_mstamp_ns&quot; is used as a start point for the retransmit timer */ skb-&gt;skb_mstamp_ns = tp-&gt;tcp_wstamp_ns = tp-&gt;tcp_clock_cache; list_move_tail(&amp;skb-&gt;tcp_tsorted_anchor, &amp;tp-&gt;tsorted_sent_queue); tcp_init_tso_segs(skb, mss_now); goto repair; /* Skip network transmission */ &#125; if (tcp_pacing_check(sk)) break; tso_segs = tcp_init_tso_segs(skb, mss_now); BUG_ON(!tso_segs); /*检查发送窗口的大小*/ cwnd_quota = tcp_cwnd_test(tp, skb); if (!cwnd_quota) &#123; if (push_one == 2) /* Force out a loss probe pkt. */ cwnd_quota = 1; else break; &#125; if (unlikely(!tcp_snd_wnd_test(tp, skb, mss_now))) &#123; is_rwnd_limited = true; break; ...... limit = mss_now; if (tso_segs &gt; 1 &amp;&amp; !tcp_urg_mode(tp)) limit = tcp_mss_split_point(sk, skb, mss_now, min_t(unsigned int, cwnd_quota, max_segs), nonagle); if (skb-&gt;len &gt; limit &amp;&amp; unlikely(tso_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE, skb, limit, mss_now, gfp))) break; if (tcp_small_queue_check(sk, skb, 0)) break; if (unlikely(tcp_transmit_skb(sk, skb, 1, gfp))) //调用该函数发送数据 break; ...... tcp_write_xmit位于tcpoutput.c中，它实现了tcp的拥塞控制，然后调用了tcp_transmit_skb(sk, skb, 1, gfp)传输数据，实际上调用的是__tcp_transmit_skb。 __tcp_transmit_skb的部分代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455static int __tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it, gfp_t gfp_mask, u32 rcv_nxt)&#123; skb_push(skb, tcp_header_size); skb_reset_transport_header(skb); ...... /* 构建TCP头部和校验和 */ th = (struct tcphdr *)skb-&gt;data; th-&gt;source = inet-&gt;inet_sport; th-&gt;dest = inet-&gt;inet_dport; th-&gt;seq = htonl(tcb-&gt;seq); th-&gt;ack_seq = htonl(rcv_nxt); tcp_options_write((__be32 *)(th + 1), tp, &amp;opts); skb_shinfo(skb)-&gt;gso_type = sk-&gt;sk_gso_type; if (likely(!(tcb-&gt;tcp_flags &amp; TCPHDR_SYN))) &#123; th-&gt;window = htons(tcp_select_window(sk)); tcp_ecn_send(sk, skb, th, tcp_header_size); &#125; else &#123; /* RFC1323: The window in SYN &amp; SYN/ACK segments * is never scaled. */ th-&gt;window = htons(min(tp-&gt;rcv_wnd, 65535U)); &#125; ...... icsk-&gt;icsk_af_ops-&gt;send_check(sk, skb); if (likely(tcb-&gt;tcp_flags &amp; TCPHDR_ACK)) tcp_event_ack_sent(sk, tcp_skb_pcount(skb), rcv_nxt); if (skb-&gt;len != tcp_header_size) &#123; tcp_event_data_sent(tp, sk); tp-&gt;data_segs_out += tcp_skb_pcount(skb); tp-&gt;bytes_sent += skb-&gt;len - tcp_header_size; &#125; if (after(tcb-&gt;end_seq, tp-&gt;snd_nxt) || tcb-&gt;seq == tcb-&gt;end_seq) TCP_ADD_STATS(sock_net(sk), TCP_MIB_OUTSEGS, tcp_skb_pcount(skb)); tp-&gt;segs_out += tcp_skb_pcount(skb); /* OK, its time to fill skb_shinfo(skb)-&gt;gso_&#123;segs|size&#125; */ skb_shinfo(skb)-&gt;gso_segs = tcp_skb_pcount(skb); skb_shinfo(skb)-&gt;gso_size = tcp_skb_mss(skb); /* Leave earliest departure time in skb-&gt;tstamp (skb-&gt;skb_mstamp_ns) */ /* Cleanup our debris for IP stacks */ memset(skb-&gt;cb, 0, max(sizeof(struct inet_skb_parm), sizeof(struct inet6_skb_parm))); err = icsk-&gt;icsk_af_ops-&gt;queue_xmit(sk, skb, &amp;inet-&gt;cork.fl); //调用网络层的发送接口 ......&#125; __tcp_transmit_skb是位于传输层发送tcp数据的最后一步，这里首先对TCP数据段的头部进行了处理，然后调用了网络层提供的发送接口： icsk-&gt;icsk_af_ops-&gt;queue_xmit(sk, skb, &amp;inet-&gt;cork.fl);实现了数据的发送，自此，数据离开了传输层，传输层的任务也就结束了。 传输层时序图如下图所示： GDB调试如下所示。 2、网络层分析将TCP传输过来的数据包打包成IP数据报，将数据打包成IP数据包之后，通过调用ip_local_out函数，在该函数内部调用了__ip_local_out，该函数返回了一个nf_hook函数，在该函数内部调用了dst_output 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116int ip_queue_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl)&#123; struct inet_sock *inet = inet_sk(sk); struct net *net = sock_net(sk); struct ip_options_rcu *inet_opt; struct flowi4 *fl4; struct rtable *rt; struct iphdr *iph; int res; /* Skip all of this if the packet is already routed, * f.e. by something like SCTP. */ rcu_read_lock(); /* * 如果待输出的数据包已准备好路由缓存， * 则无需再查找路由，直接跳转到packet_routed * 处作处理。 */ inet_opt = rcu_dereference(inet-&gt;inet_opt); fl4 = &amp;fl-&gt;u.ip4; rt = skb_rtable(skb); if (rt) goto packet_routed; /* Make sure we can route this packet. */ /* * 如果输出该数据包的传输控制块中 * 缓存了输出路由缓存项，则需检测 * 该路由缓存项是否过期。 * 如果过期，重新通过输出网络设备、 * 目的地址、源地址等信息查找输出 * 路由缓存项。如果查找到对应的路 * 由缓存项，则将其缓存到传输控制 * 块中，否则丢弃该数据包。 * 如果未过期，则直接使用缓存在 * 传输控制块中的路由缓存项。 */ rt = (struct rtable *)__sk_dst_check(sk, 0); if (!rt) &#123; /* 缓存过期 */ __be32 daddr; /* Use correct destination address if we have options. */ daddr = inet-&gt;inet_daddr; /* 目的地址 */ if (inet_opt &amp;&amp; inet_opt-&gt;opt.srr) daddr = inet_opt-&gt;opt.faddr; /* 严格路由选项 */ /* If this fails, retransmit mechanism of transport layer will * keep trying until route appears or the connection times * itself out. */ /* 查找路由缓存 */ rt = ip_route_output_ports(net, fl4, sk, daddr, inet-&gt;inet_saddr, inet-&gt;inet_dport, inet-&gt;inet_sport, sk-&gt;sk_protocol, RT_CONN_FLAGS(sk), sk-&gt;sk_bound_dev_if); if (IS_ERR(rt)) goto no_route; sk_setup_caps(sk, &amp;rt-&gt;dst); /* 设置控制块的路由缓存 */ &#125; skb_dst_set_noref(skb, &amp;rt-&gt;dst);/* 将路由设置到skb中 */packet_routed: if (inet_opt &amp;&amp; inet_opt-&gt;opt.is_strictroute &amp;&amp; rt-&gt;rt_uses_gateway) goto no_route; /* OK, we know where to send it, allocate and build IP header. */ /* * 设置IP首部中各字段的值。如果存在IP选项， * 则在IP数据包首部中构建IP选项。 */ skb_push(skb, sizeof(struct iphdr) + (inet_opt ? inet_opt-&gt;opt.optlen : 0)); skb_reset_network_header(skb); iph = ip_hdr(skb);/* 构造ip头 */ *((__be16 *)iph) = htons((4 &lt;&lt; 12) | (5 &lt;&lt; 8) | (inet-&gt;tos &amp; 0xff)); if (ip_dont_fragment(sk, &amp;rt-&gt;dst) &amp;&amp; !skb-&gt;ignore_df) iph-&gt;frag_off = htons(IP_DF); else iph-&gt;frag_off = 0; iph-&gt;ttl = ip_select_ttl(inet, &amp;rt-&gt;dst); iph-&gt;protocol = sk-&gt;sk_protocol; ip_copy_addrs(iph, fl4); /* Transport layer set skb-&gt;h.foo itself. */ /* 构造ip选项 */ if (inet_opt &amp;&amp; inet_opt-&gt;opt.optlen) &#123; iph-&gt;ihl += inet_opt-&gt;opt.optlen &gt;&gt; 2; ip_options_build(skb, &amp;inet_opt-&gt;opt, inet-&gt;inet_daddr, rt, 0); &#125; ip_select_ident_segs(net, skb, sk, skb_shinfo(skb)-&gt;gso_segs ?: 1); /* TODO : should we use skb-&gt;sk here instead of sk ? */ /* * 设置输出数据包的QoS类型。 */ skb-&gt;priority = sk-&gt;sk_priority; skb-&gt;mark = sk-&gt;sk_mark; res = ip_local_out(net, sk, skb); /* 输出 */ rcu_read_unlock(); return res;no_route: rcu_read_unlock(); /* * 如果查找不到对应的路由缓存项， * 在此处理，将该数据包丢弃。 */ IP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES); kfree_skb(skb); return -EHOSTUNREACH;&#125; dst_output()实际调用skb_dst(skb)-&gt;output(skb)，skb_dst(skb)就是skb所对应的路由项。skb_dst(skb)指向的是路由项dst_entry，它的input在收到报文时赋值ip_local_deliver()，而output在发送报文时赋值ip_output()，该函数的作用是处理单播数据报，设置数据报的输出网络设备以及网络层协议类型参数。随后调用ip_finish_output，观察数据报长度是否大于MTU，若大于，则调用ip_fragment分片，否则调用ip_finish_output2输出。在ip_finish_output2函数中会检测skb的前部空间是否还能存储链路层首部。如果不够，就会申请更大的存储空间，最终会调用邻居子系统的输出函数neigh_output进行输出，输出分为有二层头缓存和没有两种情况，有缓存时调用neigh_hh_output进行快速输出，没有缓存时，则调用邻居子系统的输出回调函数进行慢速输出。 网络层时序图如下图所示。 GDB调试结果如下。 3、数据链路层分析网络层最终会通过调用dev_queue_xmit来发送报文，在该函数中调用的是__dev_queue_xmit(skb, NULL);，如下所示： 1234int dev_queue_xmit(struct sk_buff *skb)&#123; return __dev_queue_xmit(skb, NULL);&#125; 直接调用__dev_queue_xmit传入的参数是一个skb 数据包 __dev_queue_xmit函数会根据不同的情况会调用__dev_xmit_skb或者sch_direct_xmit函数，最终会调用dev_hard_start_xmit函数，该函数最终会调用xmit_one来发送一到多个数据包。 数据链路层时序图如下所示。 GDB调试结果。 四、recv分析1、数据链路层分析在数据链路层接受数据并传递给上层的步骤如下所示： 1、一个 package 到达机器的物理网络适配器，当它接收到数据帧时，就会触发一个中断，并将通过 DMA 传送到位于 linux kernel 内存中的 rx_ring。2、网卡发出中断，通知 CPU 有个 package 需要它处理。中断处理程序主要进行以下一些操作，包括分配 skb_buff 数据结构，并将接收到的数据帧从网络适配器I&#x2F;O端口拷贝到skb_buff 缓冲区中；从数据帧中提取出一些信息，并设置 skb_buff相应的参数，这些参数将被上层的网络协议使用，例如skb-&gt;protocol；3、终端处理程序经过简单处理后，发出一个软中断（NET_RX_SOFTIRQ），通知内核接收到新的数据帧。4、内核 2.5 中引入一组新的 API 来处理接收的数据帧，即 NAPI。所以，驱动有两种方式通知内核：(1) 通过以前的函数netif_rx；(2)通过NAPI机制。该中断处理程序调用 Network device的 netif_rx_schedule函数，进入软中断处理流程，再调用net_rx_action函数。5、该函数关闭中断，获取每个 Network device 的 rx_ring 中的所有 package，最终 pacakage 从 rx_ring 中被删除，进入netif _receive_skb处理流程。6、netif_receive_skb是链路层接收数据报的最后一站。它根据注册在全局数组 ptype_all 和 ptype_base 里的网络层数据报类型，把数据报递交给不同的网络层协议的接收函数(INET域中主要是ip_rcv和arp_rcv)。该函数主要就是调用第三层协议的接收函数处理该skb包，进入第三层网络层处理。 数据链路层的时序图如下所示。 GDB调试如下。 2、网络层分析ip层的入口在ip_rcv函数，该函数首先会做包括 package checksum 在内的各种检查，如果需要的话会做 IP defragment（将多个分片合并），然后 packet 调用已经注册的 Pre-routing netfilter hook ，完成后最终到达ip_rcv_finish函数。 12345678910111213int ip_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt, struct net_device *orig_dev)&#123; struct net *net = dev_net(dev); skb = ip_rcv_core(skb, net); if (skb == NULL) return NET_RX_DROP; return NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING, net, NULL, skb, dev, NULL, ip_rcv_finish);&#125; ip_rcv_finish函数如下所示： 1234567891011121314151617static int ip_rcv_finish(struct net *net, struct sock *sk, struct sk_buff *skb)&#123; struct net_device *dev = skb-&gt;dev; int ret; /* if ingress device is enslaved to an L3 master device pass the * skb to its handler for processing */ skb = l3mdev_ip_rcv(skb); if (!skb) return NET_RX_SUCCESS; ret = ip_rcv_finish_core(net, sk, skb, dev, NULL); if (ret != NET_RX_DROP) ret = dst_input(skb); return ret;&#125; ip_rcv_finish 函数最终会调用ip_route_input函数，进入路由处理环节。它首先会调用 ip_route_input 来更新路由，然后查找 route，决定该 package 将会被发到本机还是会被转发还是丢弃： 1、如果是发到本机的话，调用ip_local_deliver 函数，可能会做 de-fragment（合并多个 IP packet），然后调用ip_local_deliver函数。该函数根据 package 的下一个处理层的 protocal number，调用下一层接口，包括 tcp_v4_rcv （TCP）, udp_rcv （UDP），icmp_rcv (ICMP)，igmp_rcv(IGMP)。对于 TCP 来说，函数 tcp_v4_rcv 函数会被调用，从而处理流程进入 TCP 栈。2、如果需要转发 （forward），则进入转发流程。该流程需要处理 TTL，再调用dst_input函数。该函数会 （1）处理 Netfilter Hook （2）执行 IP fragmentation （3）调用 dev_queue_xmit，进入链路层处理流程。 网络层时序图如下图所示。 GDB调试如下图所示。 3、传输层分析对于recv函数，与send函数类似，调用的系统调用是__sys_recvfrom，其代码如下所示： 1234567891011121314151617181920212223242526int __sys_recvfrom(int fd, void __user *ubuf, size_t size, unsigned int flags, struct sockaddr __user *addr, int __user *addr_len)&#123; ...... err = import_single_range(READ, ubuf, size, &amp;iov, &amp;msg.msg_iter); if (unlikely(err)) return err; sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed); ..... msg.msg_control = NULL; msg.msg_controllen = 0; /* Save some cycles and don&#x27;t copy the address if not needed */ msg.msg_name = addr ? (struct sockaddr *)&amp;address : NULL; /* We assume all kernel code knows the size of sockaddr_storage */ msg.msg_namelen = 0; msg.msg_iocb = NULL; msg.msg_flags = 0; if (sock-&gt;file-&gt;f_flags &amp; O_NONBLOCK) flags |= MSG_DONTWAIT; err = sock_recvmsg(sock, &amp;msg, flags); //调用该函数接受数据 if (err &gt;= 0 &amp;&amp; addr != NULL) &#123; err2 = move_addr_to_user(&amp;address, msg.msg_namelen, addr, addr_len); .....&#125; __sys_recvfrom通过调用sock_recvmsg来对数据进行接收，该函数实际调用的是sock-&gt;ops-&gt;recvmsg(sock, msg, msg_data_left(msg), flags); ，同样类似send函数中，调用的实际上是socket在初始化时赋值给结构体struct proto tcp_prot的函数tcp_rcvmsg，如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960struct proto tcp_prot = &#123; .name = &quot;TCP&quot;, .owner = THIS_MODULE, .close = tcp_close, .pre_connect = tcp_v4_pre_connect, .connect = tcp_v4_connect, .disconnect = tcp_disconnect, .accept = inet_csk_accept, .ioctl = tcp_ioctl, .init = tcp_v4_init_sock, .destroy = tcp_v4_destroy_sock, .shutdown = tcp_shutdown, .setsockopt = tcp_setsockopt, .getsockopt = tcp_getsockopt, .keepalive = tcp_set_keepalive, .recvmsg = tcp_recvmsg, .sendmsg = tcp_sendmsg, ...tcp_rcvmsg的代码如下所示：int tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int nonblock, int flags, int *addr_len)&#123; ...... if (sk_can_busy_loop(sk) &amp;&amp; skb_queue_empty(&amp;sk-&gt;sk_receive_queue) &amp;&amp; (sk-&gt;sk_state == TCP_ESTABLISHED)) sk_busy_loop(sk, nonblock); //如果接收队列为空，则会在该函数内循环等待 lock_sock(sk); ..... if (unlikely(tp-&gt;repair)) &#123; err = -EPERM; if (!(flags &amp; MSG_PEEK)) goto out; if (tp-&gt;repair_queue == TCP_SEND_QUEUE) goto recv_sndq; err = -EINVAL; if (tp-&gt;repair_queue == TCP_NO_QUEUE) goto out; ...... last = skb_peek_tail(&amp;sk-&gt;sk_receive_queue); skb_queue_walk(&amp;sk-&gt;sk_receive_queue, skb) &#123; last = skb; ...... if (!(flags &amp; MSG_TRUNC)) &#123; err = skb_copy_datagram_msg(skb, offset, msg, used); //将接收到的数据拷贝到用户态 if (err) &#123; /* Exception. Bailout! */ if (!copied) copied = -EFAULT; break; &#125; &#125; *seq += used; copied += used; len -= used; tcp_rcv_space_adjust(sk); 在连接建立后，若没有数据到来，接收队列为空，进程会在sk_busy_loop函数内循环等待，知道接收队列不为空，并调用函数数skb_copy_datagram_msg将接收到的数据拷贝到用户态，该函数内部实际调用的是__skb_datagram_iter，其代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546int __skb_datagram_iter(const struct sk_buff *skb, int offset, struct iov_iter *to, int len, bool fault_short, size_t (*cb)(const void *, size_t, void *, struct iov_iter *), void *data)&#123; int start = skb_headlen(skb); int i, copy = start - offset, start_off = offset, n; struct sk_buff *frag_iter; /* 拷贝tcp头部 */ if (copy &gt; 0) &#123; if (copy &gt; len) copy = len; n = cb(skb-&gt;data + offset, copy, data, to); offset += n; if (n != copy) goto short_copy; if ((len -= copy) == 0) return 0; &#125; /* 拷贝数据部分 */ for (i = 0; i &lt; skb_shinfo(skb)-&gt;nr_frags; i++) &#123; int end; const skb_frag_t *frag = &amp;skb_shinfo(skb)-&gt;frags[i]; WARN_ON(start &gt; offset + len); end = start + skb_frag_size(frag); if ((copy = end - offset) &gt; 0) &#123; struct page *page = skb_frag_page(frag); u8 *vaddr = kmap(page); if (copy &gt; len) copy = len; n = cb(vaddr + frag-&gt;page_offset + offset - start, copy, data, to); kunmap(page); offset += n; if (n != copy) goto short_copy; if (!(len -= copy)) return 0; &#125; start = end; &#125; 传输层时序图如下图所示。 GDB调试如下图所示。 五、小结从Linux操作系统实现入手，深入的分析了Linux操作系统对于TCP&#x2F;IP栈的实现原理与具体过程，了解了Linux网络子系统的具体构成及流程，通过这次调研，使我对TCP&#x2F;IP协议的原理及具体实现有了极其深入的理解。"},{"path":"/2024/04/05/linux-docs/设备驱动/Linux 总线、设备、驱动模型的探究/","content":"设备驱动模型的需求总线、设备和驱动模型，如果把它们之间的关系比喻成生活中的例子是比较容易理解的。举个例子，充电墙壁插座安静的嵌入在墙面上，无论设备是电脑还是手机，插座都能依然不动的完成它的使命——充电，没有说为了满足各种设备充电而去更换插座的。其实这就是软件工程强调的高内聚、低耦合概念。 所谓高内聚低耦合是模块内各元素联系越紧密就代表内聚性就越高，模块间联系越不紧密就代表耦合性低。所以高内聚、低耦合强调的就是内部要紧紧抱团。设备和驱动就是基于这种模型去实现彼此隔离不相干的。这里，有的读者就要问了，高内聚、低耦合的软件模型理解，可设备和驱动为什么要采用这种模型呢？没错，好问题。下面进入今天的话题——总线、设备和驱动模型的探究。 设想一个叫 GITCHAT 的网卡，它需要接在 CPU 的内部总线上，需要地址总线、数据总线和控制总线，以及中断 pin 脚等。 那么在 GITCHAT 的驱动里需要定义 GITCHAT 的基地址、中断号等信息。假设 GITCHAT 的地址为0x0001，中断号是 2，那么： 1234567891011121314#define GITCHAT_BASE 0x0001#define GITCHAT_INTERRUPT 2int gitchat_send()&#123; writel(GITCHAT_BASE + REG, 1); ...&#125;int gitchat_init()&#123; request_init(GITCHAT_INTERRUPT, ...); ...&#125; 但是世界上的板子千千万，有三星、华为、飞思卡尔……每个板子的信息也都不一样，站在驱动的角度看，当每次重新换板子的时候，GITCHAT_BASE 和 GITCHAT_INTERRUPT 就不再一样，那驱动代码也要随之改变。这样的话一万个开发板要写一万个驱动了，这就是文章刚开始提到的高内聚、低耦合的应用场景。 驱动想以不变应万变的姿态适配各种设备连接的话就要实现设备驱动模型。基本上我们可以认为驱动不会因为 CPU 的改变而改变，所以它应该是跨平台的。自然像 “#define GITCHAT_BASE 0x0001，#define GITCHAT_INTERRUPT 2” 这样描述和 CPU 相关信息的代码不应该出现在驱动里。 设备驱动模型的实现现在 CPU 板级信息和驱动分开的需求已经刻不容缓。但是基地址、中断号等板级信息始终和驱动是有一定联系的，因为驱动毕竟要取出基地址、中断号等。怎么取？有一种方法是 GITCHAT 驱动满世界去询问各个板子：请问你的基地址是多少？中断号是几？细心的读者会发现这仍然是一个耦合的情况。 对软件工程熟悉的读者肯定立刻想到能不能设计一个类似接口适配器的类（adapter）去适配不同的板级信息，这样板子上的基地址、中断号等信息都在一个 adapter 里去维护，然后驱动通过这个 adapter 不同的 API 去获取对应的硬件信息。没错，Linux 内核里就是运用了这种设计思想去对设备和驱动进行适配隔离的，只不过在内核里我们不叫做适配层，而取名为总线，意为通过这个总线去把驱动和对应的设备绑定一起，如图： 基于这种设计思想，Linux 把设备驱动分为了总线、设备和驱动三个实体，这三个实体在内核里的职责分别如下： 模型设计好后，下面来看一下具体驱动的实践，首先把板子的硬件信息填入设备端，然后让设备向总线注册，这样总线就间接的知道了设备的硬件信息。比如一个板子上有一个 GITCHAT，首先向总线注册： 123456789101112131415161718192021222324static struct resource gitchat_resource[] = &#123; &#123; .start = ..., .end = ..., .flags = IORESOURCE_MEM &#125;...&#125;;static struct platform_device gitchat_device = &#123; .name = &quot;gitchat&quot;; .id = 0; .num_resources = ARRAY_SIZE(gitchat_resource); .resource = gitchat_resource,&#125;;static struct platform_device *ip0x_device __initdata = &#123; &amp;gitchat_device, ...&#125;;static ini __init ip0x_init(void)&#123; platform_add_devices(ip0x_device, ARRAY_SIZE(ip0x_device));&#125; 现在 platform 总线上自然知道了板子上关于 GITCHAT 设备的硬件信息，一旦 GITCHAT 的驱动也被注册的话，总线就会把驱动和设备绑定起来，从而驱动就获得了基地址、中断号等板级信息。总线存在的目的就是把设备和对应的驱动绑定起来，让内核成为该是谁的就是谁的和谐世界，有点像我们生活中红娘的角色，把有缘人通过红线牵在一起。设备注册总线的代码例子看完了，下面看下驱动注册总线的代码示例： 123456789101112131415161718192021222324static struct resource gitchat_resource[] = &#123; &#123; .start = ..., .end = ..., .flags = IORESOURCE_MEM &#125;...&#125;;static struct platform_device gitchat_device = &#123; .name = &quot;gitchat&quot;; .id = 0; .num_resources = ARRAY_SIZE(gitchat_resource); .resource = gitchat_resource,&#125;;static struct platform_device *ip0x_device __initdata = &#123; &amp;gitchat_device, ...&#125;;static ini __init ip0x_init(void)&#123; platform_add_devices(ip0x_device, ARRAY_SIZE(ip0x_device));&#125; 现在 platform 总线上自然知道了板子上关于 GITCHAT 设备的硬件信息，一旦 GITCHAT 的驱动也被注册的话，总线就会把驱动和设备绑定起来，从而驱动就获得了基地址、中断号等板级信息。总线存在的目的就是把设备和对应的驱动绑定起来，让内核成为该是谁的就是谁的和谐世界，有点像我们生活中红娘的角色，把有缘人通过红线牵在一起。设备注册总线的代码例子看完了，下面看下驱动注册总线的代码示例： 12345678static int gitchat_probe(struct platform_device *pdev)&#123; ... db-&gt;addr_res = platform_get_resource(pdev, IORESOURCE_MEM, 0); db-&gt;data_res = platform_get_resource(pdev, IORESOURCE_MEM, 1); db-&gt;irq_res = platform_get_resource(pdev, IORESOURCE_IRQ, 2); ...&#125; 从代码中看到驱动是通过总线 API 接口 platform_get_resource 取得板级信息，这样驱动和设备之间就实现了高内聚、低耦合的设计，无论你设备怎么换，我驱动就可以岿然不动。 看到这里，可能有些喜欢探究本质的读者又要问了，设备向总线注册了板级信息，驱动也向总线注册了驱动模块，但总线是怎么做到驱动和设备匹配的呢？接下来就讲下设备和驱动是怎么通过总线进行“联姻”的。 总线里有很多匹配方式，比如： 123456789101112131415161718192021222324static int platform_match(struct device *dev, struct device_driver *drv)&#123; struct platform_device *pdev = to_platform_device(dev); struct platform_driver *pdrv = to_platform_driver(drv); /* When driver_override is set, only bind to the matching driver */ if (pdev-&gt;driver_override) return !strcmp(pdev-&gt;driver_override, drv-&gt;name); /* Attempt an OF style match first */ if (of_driver_match_device(dev, drv)) return 1; /* Then try ACPI style match */ if (acpi_driver_match_device(dev, drv)) return 1; /* Then try to match against the id table */ if (pdrv-&gt;id_table) return platform_match_id(pdrv-&gt;id_table, pdev) != NULL; /* fall-back to driver name match */ return (strcmp(pdev-&gt;name, drv-&gt;name) == 0);&#125; 从上面可知 platform 总线下的设备和驱动是通过名字进行匹配的，先去匹配 platform_driver 中的 id_table 表中的各个名字与 platform_device-&gt;name 名字是否相同，如果相同则匹配。 设备驱动模型的改善 最底层是不同板子的板级文件代码，中间层是内核的总线，最上层是对应的驱动，现在描述板级的代码已经和驱动解耦了，这也是 Linux 设备驱动模型最早的实现机制，但随着时代的发展，就像是人类的贪婪促进了社会的进步一样，开发人员对这种模型有了更高的要求，虽然驱动和设备解耦了，但是天下设备千千万，每次设备的需求改动都要去修改 board-xxx.c 设备文件的话，这样下去，有太多的板级文件需要维护。完美的 Linux 怎么会允许这样的事情存在，于是乎，设备树（DTS）就登向了历史舞台"},{"path":"/2024/04/05/linux-docs/内存管理/浅谈Linux内存管理机制/","content":"活学活用OOM Killer 在 Linux 系统里如果内存不足时，会杀死一个正在运行的进程来释放一些内存。 Linux 里的程序都是调用 malloc() 来申请内存，如果内存不足，直接 malloc() 返回失败就可以，为什么还要去杀死正在运行的进程呢？Linux允许进程申请超过实际物理内存上限的内存。因为 malloc() 申请的是内存的虚拟地址，系统只是给了程序一个地址范围，由于没有写入数据，所以程序并没有得到真正的物理内存。物理内存只有程序真的往这个地址写入数据的时候，才会分配给程序。 内存管理对于内存的访问，用户态的进程使用虚拟地址，内核的也基本都是使用虚拟地址 物理内存空间布局 虚拟内存与物理内存的映射 进程“独占”虚拟内存及虚拟内存划分为了保证操作系统的稳定性和安全性。用户程序不可以直接访问硬件资源，如果用户程序需要访问硬件资源，必须调用操作系统提供的接口，这个调用接口的过程也就是系统调用。每一次系统调用都会存在两个内存空间之间的相互切换，通常的网络传输也是一次系统调用，通过网络传输的数据先是从内核空间接收到远程主机的数据，然后再从内核空间复制到用户空间，供用户程序使用。这种从内核空间到用户空间的数据复制很费时，虽然保住了程序运行的安全性和稳定性，但是牺牲了一部分的效率。 如何分配用户空间和内核空间的比例也是一个问题，是更多地分配给用户空间供用户程序使用，还是首先保住内核有足够的空间来运行。在当前的Windows 32位操作系统中，默认用户空间：内核空间的比例是1:1，而在32位Linux系统中的默认比例是3:1（3GB用户空间、1GB内核空间）（这里只是地址空间，映射到物理地址，可没有某个物理地址的内存只能存储内核态数据或用户态数据的说法）。 左右两侧均表示虚拟地址空间，左侧以描述内核空间为主，右侧以描述用户空间为主。 在内核里面也会有内核的代码，同样有 Text Segment、Data Segment 和 BSS Segment，别忘了内核代码也是 ELF 格式的。 在代码上的体现 12345678// 持有task_struct 便可以访问进程在内存中的所有数据struct task_struct &#123; ... struct mm_struct *mm; struct mm_struct *active_mm; ... void *stack; // 指向内核栈的指针&#125; 内核使用内存描述符mm_struct来表示进程的地址空间，该描述符表示着进程所有地址空间的信息 在用户态，进程觉着整个空间是它独占的，没有其他进程存在。但是到了内核里面，无论是从哪个进程进来的，看到的都是同一个内核空间，看到的都是同一个进程列表。虽然内核栈是各用个的，但是如果想知道的话，还是能够知道每个进程的内核栈在哪里的。所以，如果要访问一些公共的数据结构，需要进行锁保护。 地址空间内的栈栈是主要用途就是支持函数调用。 大多数的处理器架构，都有实现硬件栈。有专门的栈指针寄存器，以及特定的硬件指令来完成 入栈&#x2F;出栈 的操作。 用户栈和内核栈的切换内核在创建进程的时候，在创建task_struct的同时，会为进程创建相应的堆栈。每个进程会有两个栈，一个用户栈，存在于用户空间，一个内核栈，存在于内核空间。当进程在用户空间运行时，cpu堆栈指针寄存器里面的内容是用户堆栈地址，使用用户栈；当进程在内核空间时，cpu堆栈指针寄存器里面的内容是内核栈空间地址，使用内核栈。 当进程因为中断或者系统调用而陷入内核态之行时，进程所使用的堆栈也要从用户栈转到内核栈。 如何相互切换呢？ 进程陷入内核态后，先把用户态堆栈的地址保存在内核栈之中，然后设置堆栈指针寄存器的内容为内核栈的地址，这样就完成了用户栈向内核栈的转换；当进程从内核态恢复到用户态执行时，在内核态执行的最后，将保存在内核栈里面的用户栈的地址恢复到堆栈指针寄存器即可。这样就实现了内核栈和用户栈的互转。 那么，我们知道从内核转到用户态时用户栈的地址是在陷入内核的时候保存在内核栈里面的，但是在陷入内核的时候，我们是如何知道内核栈的地址的呢？ 关键在进程从用户态转到内核态的时候，进程的内核栈总是空的。这是因为，一旦进程从内核态返回到用户态后，内核栈中保存的信息无效，会全部恢复。因此，每次进程从用户态陷入内核的时候得到的内核栈都是空的，直接把内核栈的栈顶地址给堆栈指针寄存器就可以了。 为什么需要单独的进程内核栈？内核地址空间所有进程空闲，但内核栈却不共享。为什么需要单独的进程内核栈？因为同时可能会有多个进程在内核运行。 所有进程运行的时候，都可能通过系统调用陷入内核态继续执行。假设第一个进程 A 陷入内核态执行的时候，需要等待读取网卡的数据，主动调用 schedule() 让出 CPU；此时调度器唤醒了另一个进程 B，碰巧进程 B 也需要系统调用进入内核态。那问题就来了，如果内核栈只有一个，那进程 B 进入内核态的时候产生的压栈操作，必然会破坏掉进程 A 已有的内核栈数据；一但进程 A 的内核栈数据被破坏，很可能导致进程 A 的内核态无法正确返回到对应的用户态了。 进程内核栈在进程创建的时候，通过 slab 分配器从 thread_info_cache 缓存池中分配出来，其大小为 THREAD_SIZE，一般来说是一个页大小 4K； 进程切换带来的用户栈切换和内核栈切换12345678// 持有task_struct 便可以访问进程在内存中的所有数据struct task_struct &#123; ... struct mm_struct *mm; struct mm_struct *active_mm; ... void *stack; // 指向内核栈的指针&#125; 从进程 A 切换到进程 B，用户栈要不要切换呢？当然要，在切换内存空间的时候就切换了，每个进程的用户栈都是独立的，都在内存空间里面。 那内核栈呢？已经在 __switch_to 里面切换了，也就是将 current_task 指向当前的 task_struct。里面的 void *stack 指针，指向的就是当前的内核栈。 内核栈的栈顶指针呢？在 __switch_to_asm 里面已经切换了栈顶指针，并且将栈顶指针在 __switch_to加载到了 TSS 里面。 用户栈的栈顶指针呢？如果当前在内核里面的话，它当然是在内核栈顶部的 pt_regs 结构里面呀。当从内核返回用户态运行的时候，pt_regs 里面有所有当时在用户态的时候运行的上下文信息，就可以开始运行了。 主线程的用户栈和一般现成的线程栈 对应着jvm 一个线程一个栈 中断栈中断有点类似于我们经常说的事件驱动编程，而这个事件通知机制是怎么实现的呢，硬件中断的实现通过一个导线和 CPU 相连来传输中断信号，软件上会有特定的指令，例如执行系统调用创建线程的指令，而 CPU 每执行完一个指令，就会检查中断寄存器中是否有中断，如果有就取出然后执行该中断对应的处理程序。 当系统收到中断事件后，进行中断处理的时候，也需要中断栈来支持函数调用。由于系统中断的时候，系统当然是处于内核态的，所以中断栈是可以和内核栈共享的。但是具体是否共享，这和具体处理架构密切相关。ARM 架构就没有独立的中断栈。 内存管理的进程和硬件背景页表的位置每个进程都有独立的地址空间，为了这个进程独立完成映射，每个进程都有独立的进程页表，这个页表的最顶级的 pgd 存放在 task_struct 中的 mm_struct 的 pgd 变量里面。 在一个进程新创建的时候，会调用 fork，对于内存的部分会调用 copy_mm，里面调用 dup_mm。 12345678910// Allocate a new mm structure and copy contents from the mm structure of the passed in task structure.static struct mm_struct *dup_mm(struct task_struct *tsk)&#123; struct mm_struct *mm, *oldmm = current-&gt;mm; mm = allocate_mm(); memcpy(mm, oldmm, sizeof(*mm)); if (!mm_init(mm, tsk, mm-&gt;user_ns)) goto fail_nomem; err = dup_mmap(mm, oldmm); return mm;&#125; 除了创建一个新的 mm_struct，并且通过memcpy将它和父进程的弄成一模一样之外，我们还需要调用 mm_init 进行初始化。接下来，mm_init 调用 mm_alloc_pgd，分配全局页目录项，赋值给mm_struct 的 pdg 成员变量。 1234static inline int mm_alloc_pgd(struct mm_struct *mm)&#123; mm-&gt;pgd = pgd_alloc(mm); return 0;&#125; 一个进程的虚拟地址空间包含用户态和内核态两部分。为了从虚拟地址空间映射到物理页面，页表也分为用户地址空间的页表和内核页表。在内核里面，映射靠内核页表，这里内核页表会拷贝一份到进程的页表 如果是用户态进程页表，会有 mm_struct 指向进程顶级目录 pgd，对于内核来讲，也定义了一个 mm_struct，指向 swapper_pg_dir（指向内核最顶级的目录 pgd）。 12345678910111213struct mm_struct init_mm = &#123; .mm_rb = RB_ROOT, // pgd 页表最顶级目录 .pgd = swapper_pg_dir, .mm_users\t= ATOMIC_INIT(2), .mm_count\t= ATOMIC_INIT(1), .mmap_sem\t= __RWSEM_INITIALIZER(init_mm.mmap_sem), .page_table_lock = __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock), .mmlist = LIST_HEAD_INIT(init_mm.mmlist), .user_ns\t= &amp;init_user_ns, INIT_MM_CONTEXT(init_mm)&#125;; 页表的应用一个进程 fork 完毕之后，有了内核页表（内核初始化时即弄好了内核页表， 所有进程共享），有了自己顶级的 pgd，但是对于用户地址空间来讲，还完全没有映射过（用户空间页表一开始是不完整的，只有最顶级目录pgd这个“光杆司令”）。这需要等到这个进程在某个 CPU 上运行，并且对内存访问的那一刻了 当这个进程被调度到某个 CPU 上运行的时候，要调用 context_switch 进行上下文切换。对于内存方面的切换会调用 switch_mm_irqs_off，这里面会调用 load_new_mm_cr3。 cr3 是 CPU 的一个寄存器，它会指向当前进程的顶级 pgd。如果 CPU 的指令要访问进程的虚拟内存，它就会自动从cr3 里面得到 pgd 在物理内存的地址，然后根据里面的页表解析虚拟内存的地址为物理内存，从而访问真正的物理内存上的数据。 这里需要注意两点。第一点，cr3 里面存放当前进程的顶级 pgd，这个是硬件的要求。cr3 里面需要存放 pgd 在物理内存的地址，不能是虚拟地址。第二点，用户进程在运行的过程中，访问虚拟内存中的数据，会被 cr3 里面指向的页表转换为物理地址后，才在物理内存中访问数据，这个过程都是在用户态运行的，地址转换的过程无需进入内核态。 这就可以解释，为什么页表数据在 task_struct 的mm_struct里却又 可以融入硬件地址翻译机制了。 通过缺页中断来“填充”页表内存管理并不直接分配物理内存，只有等你真正用的那一刻才会开始分配。只有访问虚拟内存的时候，发现没有映射多物理内存，页表也没有创建过，才触发缺页异常。进入内核调用 do_page_fault，一直调用到 __handle_mm_fault，__handle_mm_fault 调用 pud_alloc 和 pmd_alloc，来创建相应的页目录项，最后调用 handle_pte_fault 来创建页表项。 123456789101112131415161718192021222324252627282930313233343536373839404142static noinline void__do_page_fault(struct pt_regs *regs, unsigned long error_code, unsigned long address)&#123; struct vm_area_struct *vma; struct task_struct *tsk; struct mm_struct *mm; tsk = current; mm = tsk-&gt;mm; // 判断缺页是否发生在内核 if (unlikely(fault_in_kernel_space(address))) &#123; if (vmalloc_fault(address) &gt;= 0) return; &#125; ...... // 找到待访问地址所在的区域 vm_area_struct vma = find_vma(mm, address); ...... fault = handle_mm_fault(vma, address, flags); ......static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address, unsigned int flags)&#123; struct vm_fault vmf = &#123; .vma = vma, .address = address &amp; PAGE_MASK, .flags = flags, .pgoff = linear_page_index(vma, address), .gfp_mask = __get_fault_gfp_mask(vma), &#125;; struct mm_struct *mm = vma-&gt;vm_mm; pgd_t *pgd; p4d_t *p4d; int ret; pgd = pgd_offset(mm, address); p4d = p4d_alloc(mm, pgd, address); ...... vmf.pud = pud_alloc(mm, p4d, address); ...... vmf.pmd = pmd_alloc(mm, vmf.pud, address); ...... return handle_pte_fault(&amp;vmf);&#125; 以handle_pte_fault 的一种场景 do_anonymous_page为例：先通过 pte_alloc 分配一个页表项，然后通过 alloc_zeroed_user_highpage_movable 分配一个页，接下来要调用 mk_pte，将页表项指向新分配的物理页，set_pte_at 会将页表项塞到页表里面。 12345678910111213141516171819202122static int do_anonymous_page(struct vm_fault *vmf)&#123; struct vm_area_struct *vma = vmf-&gt;vma; struct mem_cgroup *memcg; struct page *page; int ret = 0; pte_t entry; ...... if (pte_alloc(vma-&gt;vm_mm, vmf-&gt;pmd, vmf-&gt;address)) return VM_FAULT_OOM; ...... page = alloc_zeroed_user_highpage_movable(vma, vmf-&gt;address); ...... entry = mk_pte(page, vma-&gt;vm_page_prot); if (vma-&gt;vm_flags &amp; VM_WRITE) entry = pte_mkwrite(pte_mkdirty(entry)); vmf-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, vmf-&gt;pmd, vmf-&gt;address, &amp;vmf-&gt;ptl); ...... set_pte_at(vma-&gt;vm_mm, vmf-&gt;address, vmf-&gt;pte, entry); ......&#125;"},{"path":"/2024/04/05/linux-docs/内存管理/熟读精思，熟读玩味，Linux虚拟内存管理，MMU机制，原来如此也/","content":"MMU现代操作系统普遍采用虚拟内存管理（Virtual Memory Management）机制，这需要处理器中的MMU（Memory Management Unit，内存管理单元）提供支持。 首先引入 PA 和 VA 两个概念。 PA如果处理器没有MMU，或者有MMU但没有启用，CPU执行单元发出的内存地址将直接传到芯片引脚上，被内存芯片（以下称为物理内存，以便与虚拟内存区分）接收，这称为PA（Physical Address，以下简称PA），如下图所示。 VA如果处理器启用了MMU，CPU执行单元发出的内存地址将被MMU截获，从CPU到MMU的地址称为虚拟地址（Virtual Address，以下简称VA），而MMU将这个地址翻译成另一个地址发到CPU芯片的外部地址引脚上，也就是将VA映射成PA，如下图所示。 如果是32位处理器，则内地址总线是32位的，与CPU执行单元相连（图中只是示意性地画了4条地址线），而经过MMU转换之后的外地址总线则不一定是32位的。也就是说，虚拟地址空间和物理地址空间是独立的，32位处理器的虚拟地址空间是4GB，而物理地址空间既可以大于也可以小于4GB。 MMU将VA映射到PA是以页（Page）为单位的，32位处理器的页尺寸通常是4KB。例如，MMU可以通过一个映射项将VA的一页0xb70010000xb7001fff映射到PA的一页0x20000x2fff，如果CPU执行单元要访问虚拟地址0xb7001008，则实际访问到的物理地址是0x2008。物理内存中的页称为物理页面或者页帧（Page Frame）。虚拟内存的哪个页面映射到物理内存的哪个页帧是通过页表（Page Table）来描述的，页表保存在物理内存中，MMU会查找页表来确定一个VA应该映射到什么PA。 进程地址空间 x86平台的虚拟地址空间是0x0000 00000xffff ffff，大致上前3GB（0x0000 00000xbfff ffff）是用户空间，后1GB（0xc000 0000~0xffff ffff）是内核空间。 Text Segmest 和 Data Segment Text Segment，包含.text段、.rodata段、.plt段等。是从&#x2F;bin&#x2F;bash加载到内存的，访问权限为r-x。 Data Segment，包含.data段、.bss段等。也是从&#x2F;bin&#x2F;bash加载到内存的，访问权限为rw-。 堆和栈 堆（heap）：堆说白了就是电脑内存中的剩余空间，malloc函数动态分配内存是在这里分配的。在动态分配内存时堆空间是可以向高地址增长的。堆空间的地址上限称为Break，堆空间要向高地址增长就要抬高Break，映射新的虚拟内存页面到物理内存，这是通过系统调用brk实现的，malloc函数也是调用brk向内核请求分配内存的。 栈（stack）：栈是一个特定的内存区域，其中高地址的部分保存着进程的环境变量和命令行参数，低地址的部分保存函数栈帧，栈空间是向低地址增长的，但显然没有堆空间那么大的可供增长的余地，因为实际的应用程序动态分配大量内存的并不少见，但是有几十层深的函数调用并且每层调用都有很多局部变量的非常少见。 如果写程序的时候没有注意好内存的分配问题，在堆和栈这两个地方可能产生以下几种问题： 1、内存泄露：如果你在一个函数里通过 malloc 在堆里申请了一块空间，并在栈里声明一个指针变量保存它，那么当该函数结束时，该函数的成员变量将会被释放，包括这个指针变量，那么这块空间也就找不回来了，也就无法得到释放。久而久之，可能造成下面的内存泄露问题。2、栈溢出：如果你放太多数据到栈中（例如大型的结构体和数组），那么就可能会造成“栈溢出”（Stack Overflow）问题，程序也将会终止。为了避免这个问题，在声明这类变量时应使用 malloc 申请堆的空间。3、野指针 和 段错误：如果一个指针所指向的空间已经被释放，此时再试图用该指针访问已经被释放了的空间将会造成“段错误”（Segment Fault）问题。此时指针已经变成野指针，应该及时手动将野指针置空。 虚拟内存管理的作用1、虚拟内存管理可以控制物理内存的访问权限。物理内存本身是不限制访问的，任何地址都可以读写，而操作系统要求不同的页面具有不同的访问权限，这是利用CPU模式和MMU的内存保护机制实现的。2、虚拟内存管理最主要的作用是让每个进程有独立的地址空间。所谓独立的地址空间是指，不同进程中的同一个VA被MMU映射到不同的PA，并且在某一个进程中访问任何地址都不可能访问到另外一个进程的数据，这样使得任何一个进程由于执行错误指令或恶意代码导致的非法内存访问都不会意外改写其它进程的数据，不会影响其它进程的运行，从而保证整个系统的稳定性。另一方面，每个进程都认为自己独占整个虚拟地址空间，这样链接器和加载器的实现会比较容易，不必考虑各进程的地址范围是否冲突。 3、VA到PA的映射会给分配和释放内存带来方便，物理地址不连续的几块内存可以映射成虚拟地址连续的一块内存。比如要用malloc分配一块很大的内存空间，虽然有足够多的空闲物理内存，却没有足够大的连续空闲内存，这时就可以分配多个不连续的物理页面而映射到连续的虚拟地址范围。 4、一个系统如果同时运行着很多进程，为各进程分配的内存之和可能会大于实际可用的物理内存，虚拟内存管理使得这种情况下各进程仍然能够正常运行。因为各进程分配的只不过是虚拟内存的页面，这些页面的数据可以映射到物理页面，也可以临时保存到磁盘上而不占用物理页面，在磁盘上临时保存虚拟内存页面的可能是一个磁盘分区，也可能是一个磁盘文件，称为交换设备（Swap Device）。当物理内存不够用时，将一些不常用的物理页面中的数据临时保存到交换设备，然后这个物理页面就认为是空闲的了，可以重新分配给进程使用，这个过程称为换出（Page out）。如果进程要用到被换出的页面，就从交换设备再加载回物理内存，这称为换入（Page in）。换出和换入操作统称为换页（Paging），因此：[\\mbox{系统中可分配的内存总量} &#x3D; \\mbox{物理内存的大小} + \\mbox{交换设备的大小}] 如下图所示。第一张图是换出，将物理页面中的数据保存到磁盘，并解除地址映射，释放物理页面。第二张图是换入，从空闲的物理页面中分配一个，将磁盘暂存的页面加载回内存，并建立地址映射。 malloc 和 freeC标准库函数malloc可以在堆空间动态分配内存，它的底层通过brk系统调用向操作系统申请内存。动态分配的内存用完之后可以用free释放，更准确地说是归还给malloc，这样下次调用malloc时这块内存可以再次被分配。 123456#include &lt;stdlib.h&gt;void *malloc(size_t size);返回值：成功返回所分配内存空间的首地址，出错返回NULLvoid free(void *ptr); malloc的参数size表示要分配的字节数，如果分配失败（可能是由于系统内存耗尽）则返回NULL。由于malloc函数不知道用户拿到这块内存要存放什么类型的数据，所以返回通用指针void *，用户程序可以转换成其它类型的指针再访问这块内存。malloc函数保证它返回的指针所指向的地址满足系统的对齐要求，例如在32位平台上返回的指针一定对齐到4字节边界，以保证用户程序把它转换成任何类型的指针都能用。动态分配的内存用完之后可以用free释放掉，传给free的参数正是先前malloc返回的内存块首地址。 示例 123456789101112131415161718192021222324252627#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;typedef struct &#123; int number; char *msg;&#125; unit_t;int main(void)&#123; unit_t *p = malloc(sizeof(unit_t)); if (p == NULL) &#123; printf(&quot;out of memory &quot;); exit(1); &#125; p-&gt;number = 3; p-&gt;msg = malloc(20); strcpy(p-&gt;msg, &quot;Hello world!&quot;); printf(&quot;number: %d msg: %s &quot;, p-&gt;number, p-&gt;msg); free(p-&gt;msg); free(p); p = NULL; return 0;&#125; 说明 unit_t *p = malloc(sizeof(unit_t));这一句，等号右边是void *类型，等号左边是unit_t *类型，编译器会做隐式类型转换，我们讲过void *类型和任何指针类型之间可以相互隐式转换。 虽然内存耗尽是很不常见的错误，但写程序要规范，malloc之后应该判断是否成功。以后要学习的大部分系统函数都有成功的返回值和失败的返回值，每次调用系统函数都应该判断是否成功。 free(p);之后，p所指的内存空间是归还了，但是p的值并没有变，因为从free的函数接口来看根本就没法改变p的值，p现在指向的内存空间已经不属于用户，换句话说，p成了野指针，为避免出现野指针，我们应该在free(p);之后手动置p = NULL;。 应该先free(p-&gt;msg)，再free(p)。如果先free(p)，p成了野指针，就不能再通过p-&gt;msg访问内存了。 内存泄漏如果一个程序长年累月运行（例如网络服务器程序），并且在循环或递归中调用malloc分配内存，则必须有free与之配对，分配一次就要释放一次，否则每次循环都分配内存，分配完了又不释放，就会慢慢耗尽系统内存，这种错误称为内存泄漏（Memory Leak）。另外，malloc返回的指针一定要保存好，只有把它传给free才能释放这块内存，如果这个指针丢失了，就没有办法free这块内存了，也会造成内存泄漏。例如： 12345void foo(void)&#123; char *p = malloc(10); ...&#125; foo函数返回时要释放局部变量p的内存空间，它所指向的内存地址就丢失了，这10个字节也就没法释放了。内存泄漏的Bug很难找到，因为它不会像访问越界一样导致程序运行错误，少量内存泄漏并不影响程序的正确运行，大量的内存泄漏会使系统内存紧缺，导致频繁换页，不仅影响当前进程，而且把整个系统都拖得很慢。 关于malloc和free还有一些特殊情况。malloc(0)这种调用也是合法的，也会返回一个非NULL的指针，这个指针也可以传给free释放，但是不能通过这个指针访问内存。free(NULL)也是合法的，不做任何事情，但是free一个野指针是不合法的，例如先调用malloc返回一个指针p，然后连着调用两次free§;，则后一次调用会产生运行时错误。"},{"path":"/2024/04/05/linux-docs/内存管理/硬件原理 和 分页管理/","content":"前言内存管理相对复杂，涉及到硬件和软件，从微机原理到应用程序到内核。比如，硬件上的cache，CPU如何去寻址内存，页表， DMA，IOMMU。 软件上，要知道底层怎么分配内存，怎么管理内存，应用程序怎么申请内存。 常见的误解包括： 对free命令 cache和buffer的理解。1、应用程序申请10M内存，申请成功其实并没有分配。内存其实是边写边拿。代码段有10M，并不是真的内存里占了10M。2、内存管理学习难，一是网上的资料不准确，二是学习时执行代码，具有欺骗性。看到的东西不一定真实，要想理解必须陷入Linux本身。 学习时，不要过快陷入太多细节，而要先对整个流程整个框架理解。 先理清楚脉络和主干，从硬件到最底层内存的分配算法，–&gt;到内核的内存分配算法，–&gt;应用程序与内核的交互，–&gt;到内存如何做磁盘的缓存， –&gt; 内存如何和磁盘替换。 再动手实践demo。 硬件原理 和 分页管理本文主要让大家理解内存管理最底层的buddy算法，内存为什么要分成多个Zone? CPU寻址内存，虚拟地址、物理地址 MMU 以及RWX权限、kernel和user模式权限 内存的zone: DMA、Normal和HIGHMEM Linux内存管理Buddy算法 连续内存分配器（CMA） 内存分页 CPU 一旦开启MMU，MMU是个硬件。CPU就只知道虚拟地址了。如果地址是32位，0x12345670 。 假设MMU的管理是把每一页的内存分成4K，那么其中的670是页内偏移，作为d；0x12345 是页号，作为p。通过虚拟地址去查对应的物理地址，用0x12345去查一张页表，页表（Page table）本身在内存。 硬件里有寄存器，记录页表的基地址，每次进程切换时，寄存器就会更新一次，因为每个进程的页表不同。 CPU一旦访问虚拟地址，通过页表查到页表项，页表项记录对应的物理地址。 总结：一旦开启MMU，CPU只能看到虚拟地址，MMU才能看到物理地址。虚拟地址是指针，物理地址是个整数。内存中的一切均通过虚拟地址来访问。 1typedef u64 phys_addr_t; 去内存里读取页表会比较慢，CPU里有个高速单元tlb，它是页表的高速缓存。CPU就不需要在内存里读页表，直接在tlb中读取，从虚拟地址到物理地址的映射。如果tlb中读取不到，才回到内存里读取页表映射，并且在tlb中命中。 虚拟地址：0x12345 670 –&gt; 1M 物理地址：1M+670 MMU去访问这个物理地址。 内存的映射以页为单位。 页表（Page table）记录的页权限cpu虚拟地址，mmu根据cpu请求的虚拟地址，访问页表，查得物理地址。 每个MMU中的页表项，除了有虚拟地址到物理地址的映射之外，还可以标注这个页的 RWX权限和 kernel和user模式权限（用户空间,内核空间读取地址的权限），它们是内存管理两个的非常重要的权限。 一是，这一页地址的RWX权限 ，标记这4k地址的权限。一般用来做保护。 Pagefault，是CPU提供的功能。两种情况会出现Pagefault，一是，CPU通过虚拟地址没有查到对应的物理地址。二是，MMU没有访问物理地址的权限。 MPU，memory protection unit. 二是，MMU的页表项中，还可以标注这一页的地址：可以在内核态访问，还是只能在用户态访问。用户一般映射到0～3G，只有当CPU陷入到内核模式，才可以访问3G以上地址。 程序在用户态运行，处于CPU非特权模式，不能访问特权模式才能访问的内存。内核运行在CPU的特权模式，从用户态陷入到内核态，发送 软中断指令，CPU进行切环，x86从3环切到0环，到一个固定的地址去执行。软件就从非特权模式，跳到特权模式去执行。 MMU，能把某一段地址指定为只有特权模式才能够访问，会把内核空间3G以上的页表项里的每一行，指定为只有CPU 0环才能访问。应用程序没有陷入到内核态，是无法访问内核态的东西。 intel的漏洞meltdown，就是让用户可以在用户态读到内核态的东西。 meltdown 攻击原理： 基于时间的 旁路攻击 side-channel 李小璐买汉堡的故事 –&gt; 安全的基于时间的旁路攻击技巧。 比如试探用户名，密码。比如一个软件比较傻，每次第一个字母就不对，就不对比第2个字母了。那我每次26个字母实验换一次，看哪个字母反弹地最慢，就证明是这个字母的密码。 密码是abc, 我敲了d，那么第一个字母就不对，软件这个时候如果快速的返回出错，我知道首字母不是d，我可以实验出来首字母是a，然后接着一个个字母实，就可以把密码试探出来了。类似地原理。。 下面的这个例子，演示 page table记录的RWX权限的作用 页表的权限，RWX权限，和 用户空间,内核空间读取的权限。 内存分Zone下面解释内存为什么分Zone? DMA zone. 内存的分Zone，全都是物理地址的概念。内存条，被分为三个Zone。 分DMA Zone的原因，是DMA引擎的缺陷。DMA引擎 可以直接访问内存空间的地址，但不一定能够访问到所有的内存，访问内存时会存在一定的限制。 当CPU 和DMA同时访问内存时，硬件上会有仲裁器，选择优先级高的去访问内存。 为什么要切DMA zone?DMA Zone的大小，是由硬件决定的。访问不到更高的内存。 什么叫做 normal zone? highmem zone? highmem和lowmem 都是指的内存条，在虚拟地址空间，只能称为highmem，lowmem映射区。 如上图，内存虚拟地址空间0～4G，3～4G是内核空间的虚拟地址，0-3G 是用户空间的虚拟地址。 内核空间，访问任何一片内存都要虚拟地址。Linux为了简化内存访问，开机就把lowmem的物理地址一一映射到虚拟地址。highmem 地址包括了 normal + DMA。 lowmem是开机就直接映射好的内存，CPU访问这片内存，也是通过3G以上的虚拟地址。这段地址的虚拟地址和物理地址是直接线性映射，通过linux的两个api （phys_to_virt &#x2F; virt_to_phys）在虚拟和物理之间进行映射, highmem 不能直接用。 内核空间一般不使用highmem，内核一般使用kmalloc在lowmem申请内存，使用 kmmap在highmem 申请内存。lowmem 映射了，并不代表被内核使用掉了，只是不需要重建页表。内核使用lowmem内存，同样是要申请。 应用程序一样可以申请 lowmem 和highmem。 总结：内存分highmem zone的原因，地址空间整体不够。DMA zone产生的原因，硬件DMA引擎的访问缺陷。 硬件层的内存管理- buddy算法每个zone都会使用buddy算法，把所有的空闲页面变成2的n次方进行管理。 &#x2F;proc&#x2F;buddyinfo 通过&#x2F;proc&#x2F;buddyinfo，可以看出空闲内存的情况 CPU寻址内存的方法：通过MMU提供的虚拟地址到物理地址的映射访问。 如何处理内存碎片 X86 linux 内核有一个线程 compaction, 会进行内存碎片整理，会尽量移出大内存。 CMA：continuous memory allocation 内核把虚拟地址 指向新的物理地址，让应用程序毫无知觉情况，把64M内存腾出来给DMA。当用DMA的api申请内存，会走到CMA。在dts中指定哪块区域做CMA。 Documentation&#x2F;devicetree&#x2F;bindings reserved-memory&#x2F;reserved-memory.txt dma_alloc_coherent CMA, iommu, CMA主要是给需要连续内存的DMA用的。但是为了避免DMA不用的时候浪费，才在DMA不用的时候给可移动的页面用。不能移动的页面，不能从CMA里面拿。所以主要是APP和文件的page cache的内存，才可以在CMA区域拿。 这样当DMA想拿CMA区域的时候，要么移走，要么抛弃。总之，必须保证DMA需要这片CMA区域的时候，之前占着CMA的统统滚蛋。 不具备滚蛋能力的内存，不能从CMA区域申请。你申请也滚蛋不了，待会DMA上来用的时候，DMA就完蛋了。 要搞清楚CMA的真正房东是那些需要连续内存的DMA，其他的人都只是租客。DMA要住的时候，租客必须走。哪个房东会把房子租给一辈子都不准备走的人？内核绝大多数情况下的内存申请，都是无法走的。应用走起来很容易，改下页面就行了。 CMA和不可移动之间，没有任何交集。CMA唯一的好处是，房东不住的时候，免得房子空置。"},{"path":"/2024/04/05/linux-docs/内存管理/进程的内存消耗和泄漏/","content":"进程的内存消耗和泄漏 进程的VMA 进程内存消耗的4个概念： vss、rss、pss和uss page fault的几种可能性， major 和 minor 应用内存泄漏的界定方法 应用内存泄漏的检测方法：valgrind 和 addresssanitizer 本节重点阐述 Linux的应用程序究竟消耗了多少内存？ 一是，看到的内存消耗，并不是一定是真的消耗。 二是，Linux存在大量的内存共享的情况。动态链接库的特点：代码段共享内存，数据段写时拷贝。把一个应用程序跑两个进程，这两个进程的代码段也是共享的。 当我们评估进程消耗多少内存时，就是指在用户空间消耗的内存，即虚拟地址在0～3G的部分，对应的物理地址内存。内核空间的内存消耗属于内核，系统调用申请了很多内存，这些内存是不属于进程消耗的。 进程的虚拟地址空间VMA task_struct里面有个mm_struct指针， 它代表进程的内存资源。pgd，代表 页表的地址； mmap 指向vm_area_struct 链表。 vm_area_struct 的每一段代表进程的一个虚拟地址空间。vma的每一段，都可能是可执行程序的某个数据段、某个代码段，堆、或栈。一个进程的虚拟地址，是在0～3G之间任意分布的。 上图 提供三种方式，看到进程的VMA空间。 pmap 3474 基地址，size, 权限， 通过以上的方式，可以看到进程的虚拟地址空间，分布在0～3G，任意一小段一小段分布的。 应用程序运行起来，就是一堆各种各样的VMA。VMA对应着 堆、栈、代码段、数据段、等，不在任何段里的虚拟地址空间，被认为是非法的。 当指针访问地址时，落在一个非法的地址，即不在任何一个VMA区域。相当于访问一个非法的地址，这些虚拟地址没有对应的物理地址。应用程序收到page fault，查看原因，访问非法位置，返回segv。 在VMA的东西，不等于在内存。调malloc申请了100M内存，立马会多出一个100M的 VMA，代表这段vma区域有r+w权限。 应用程序访问内存，必须落在一个VMA里。其次，落在一个VMA里也不一定对。把100M的堆申请出来，100M内存页全部映射为0页。页表里每一页写的只读，页表和硬件对应，MMU只查页表。而在页表项中指向物理地址的权限是只读，所以在任何时候，去写其中任何一页，硬件都会发生缺页中断。 Linux 内核在缺页中断的处理程序，通过MMU寄存器读出发生page fault的地址和原因。发现此时page fault的原因是写一个页表里记录只读的物理地址，而vma记录的虚拟地址又是r+w，此时，linux会申请一页内存。同时把页表中的权限改为r+w。 总结：Linux 内核通过VMA管理进程每一段虚拟地址空间和权限。一旦发生page fault，如果没有落在任何一个vma区域，会干掉。 VMA的起始地址+size，用来限定程序访问的地址是否合法。VMA中每一段的权限，是来界定访问这段地址是否使用正确的方式访问。 把所有的vma加起来，构成进程的虚拟地址空间，但这并不代表进程真实耗费的内存。拿到之后才是真实耗费的内存，RSS。耗费的虚拟内存，是VSS。 page fault的几种可能性 1、申请堆内存vma，第一次写，页表里的权限是R ，发生page fault，linux会去申请一页内存，此时把页表权限设置为 R+W。2、内存访问落在空白非法区域，程序收到segv段错误。3、代码段在VMA记录是R+X，此时如果对代码段执行写，程序会收到segv段错误。 minor 和major 缺页缺页，分为两种情况：主缺页 和次缺页。 主缺页 和次缺页，区别就是 申请内存时，是否需要读硬盘。前者需要。 如上图第4种情况，在代码段里执行时，出现缺页。linux申请一页内存，而且要从硬盘中读取代码段的内容，此时产生了IO，称为 major缺页。 无论是代码段还是堆，都是边执行边产生缺页中断，申请实际的内存给代码段，且从硬盘中读取代码段的内容到内存。这个过程时间比较长。 minor： malloc的内存，产生缺页中断。去申请一页内存，没有产生IO的行为。major缺页处理时间，远大于minor。 vss、rss、pss和uss的区别 123456VSS - Virtual Set SizeRSS - Resident Set SizePSS - Proportional Set SizeUSS - Unique Set SizeASAN - AddressSanitizerLSAN - LeakSanitizer 如上图，中间是一根内存条。三个进程分别是1044，1045，1054, 每一个进程对应一个page table，页表项记录虚拟地址如何往物理地址转换。硬件里的寄存器，记录页表的物理地址。当linux做进程上下文切换时，页表也跟着一起切换。 三个进程都需要使用libc的代码段。VSS ＝ 1 +2 +3RSS &#x3D; 4 +5 +6PSS&#x3D; 4&#x2F;3 + 5&#x2F;2 + 6 比例化的USS＝ 6 独占且驻留的 工具：smem ，查看进程使用内存的情况。一般来讲，进程使用的内存量，还是看PSS，强调公平性。看内存泄漏看USS 就好了。 内存泄漏 界定和检测方法界定：连续多点采样法，随着时间越久，进程耗费内存越多。 主要由内存申请和释放不是成对引起。RSS&#x2F;USS曲线， 观察方法：使用smem工具查看多次进程使用内存，USS使用量。 检查工具：1、valgrind ，会跑一个虚拟机，运行时检查进程的内存行为。会放慢程序的速度。不需要重新编译程序。2、addressanitizer，需要重新编译程序。编译时加参数，-fsanitizegcc 4.9才支持，只会放慢程序速度2～3倍。"},{"path":"/2024/04/05/linux-docs/文件系统/Linux IO 之 块IO流程与IO调度器/","content":"块I&#x2F;O流程与 I&#x2F;O调度器 一个块IO的一生：从page cache到bio 到 request 块设备层的数据结构 page和bio的关系，request 和bio的关系 O_DIRECT 和O_SYNC blktrace , ftrace IO调度和CFQ调度算法 CFQ和ionice cgroup与IO io性能测试： iotop, iostat 主要内容：从应用程序发起一次IO行为，最终怎么到磁盘，以及在这个路径上有什么trace的方法和 配置。 每次应用程序写磁盘，都是到pagecache 。三进三出 讲解 bio的一生，都是在pagecache以下。 首先，在一次普通的sys_write过程中，会触发以下的函数调用。 123456789101112131415161718192021sys_write -&gt; vfs_write -&gt; generic_file_write_iterPageCache: Linux通过局部性原理，使用页面缓存提高性能。1) generic_perform_write -&gt; write_begin -&gt; copy_data -&gt;write_end generic_perform_write: 开始pagecache的写入过程write_begin: 在内存空间中准备对应index需要的page。例如： ext4_write_begin 中包含 grab_cache_page_write_begin ： 查获取一个缓存页或者创建一个缓存页。 -&gt; page_cache_get_page: 从mapping的radix tree中查找缓存页，假如不存在，则从伙伴系统中申请一个新页插入，并添加到LRU链表中。ext4_write_end 首先调用__block_commit_write提交写入的数据，通过set_buffer_uptodate ：-&gt; mark_buffer_dirty -&gt; set_buffer_dirty/ set_page_dirty/ set_inode_dirty 将该页设脏， 通过wb_wakeup_delayed把writeback任务提交到bdi_writeback队列中。DirectIO: 2) generic_file_direct_write -&gt; filemap_write_and_write_range -&gt; mapping-&gt; a_ops-&gt; direct_IO 如果是buffer IO , 脏页回写是异步的，并且由块设备层负责。 对于新版本的内核，ext4注册的方法是new_sync_write. 12sys_write -&gt; vfs_write -&gt; __vfs_write -&gt; new_sync_write -&gt; filp-&gt;f_op-&gt;write_iter -&gt; ext4_file_write_iter 块设备层的数据结构struct page -&gt; struct inode -&gt; struct bio -&gt; struct request 真正开始做IO之前，即操作block子系统之前，包括以下步骤： 应用程序读一个文件，首先会去查page cache是否命中，下一次再读page cache是命中的。应用程序去写硬盘时，首先会去写page cache，至于什么时候开始写硬盘，由linux flush线程通过（pdflush-&gt;每一个设备的flush -&gt;工作队列）实现。当然也可以是线程本身，通过direct IO去写硬盘。 如上图三个task_struct进程，同时打开一个文件，在内存中生成file数据结构，记录文件打开的实例。 inode是真实存在硬盘里，当inode结构体中的i_mapping，对应的地址空间address_space。一个4M的文件，被分成很多4K单元存在于内存，通过地址 去radix tree来查page cache是否命中。如果查到了，就从radix tree对应的page返回。 如果没有page cache对应，就会通过 address_space_operations（文件系统实现的数据结构） 去 readpage，从硬盘里的块读到pagecache。 free命令看到的buffers和cache有什么区别？答：在内核层面，全部都是pagecache。前者对应的是裸分区产生的page cache，后者对应的是挂载文件系统之后，文件系统目录产生的page cache。如下图内核代码的截图： 文件系统的管理单元是block，内存管理是以page为单位，扇区（section）是硬件读写的最小单元。 假设你把ext4文件系统格式化成1k&#x2F;block，那么一个page对应4个block，此时读一个内存的page，文件系统要操作4个block。假设你把ext4文件系统格式化成4k&#x2F;block，那么page和block可以一一对应。 &#x2F;proc&#x2F;meminfo中的 buffer ram如何计算出？ 调用 nr_blockdev_pages()，这个函数会遍历所有原始块设备（list_for_each_entry），把所有原始块设备的inode中的i_mapping , 指向 address_space_operation。包括radix tree管理的pagecache，和 pagecache读写硬盘的硬件操作的接口。nr_pages: page cache中已经命中的page。 page和 bio的关系所谓的bio，抽象的读写block device的请求，指文件系统的block与内存page的对应。bio要变成实际的block device access还要通过block device driver再排队，并受到ioscheduler的控制。 有时候文件系统格式化block为1k，一次读写page的请求，最终转换成操作多个block。bio为I&#x2F;O请求提供了一个轻量级的表示方法，内核用一个bio的结构体来描述一次块IO操作。 bio结构体如下： 1234567891011121314151617181920212223struct bio &#123; sector_t bi_sector; /*我们想在块设备的第几个扇区上进行io操作（起始扇区），此处扇区大小是按512计算的*/ struct bio *bi_next; struct block_device *bi_bdev; /*指向块设备描述符的指针，该io操作是针对哪个块设备的*/ unsigned long bi_rw; /*该io操作是读还是写*/ unsigned short bi_vcnt; /* bio的bio_vec数组中段的数目 */ unsigned short bi_idx; /* bio的bio_vec数组中段的当前索引值 */ unsigned short bi_phys_segments; //合并之后bio中（内存）物理段的数目 unsigned int bi_size; /* 需要传送的字节数 */ bio_end_io_t *bi_end_io; /* bio的I/O操作结束时调用的方法 */ void *bi_private; //通用块层和块设备驱动程序的I/O完成方法使用的指针 unsigned int bi_max_vecs; /* bio的bio vec数组中允许的最大段数 */ atomic_t bi_cnt; /* bio的引用计数器 */ struct bio_vec *bi_io_vec; /*指向bio的bio_vec数组中的段的指针 */ struct bio_set *bi_pool; struct bio_vec bi_inline_vecs[0];/*一般一个bio就一个段，bi_inline_vecs就可满足，省去了再为bi_io_vec分配空间*/&#125;struct bio_vec &#123; struct page *bv_page; //指向段的页框对应页描述符的指针 unsigned int bv_len; //段的字节长度，长度可以超过一个页 unsigned int bv_offset; //页框中段数据的偏移量&#125;; 一个bio可能有很多个bio段，这些bio段在内存是可能不连续，位于不同的页，但在磁盘上对应的位置是连续的。一般上层构建bio的时候都是只有一个bio段。(新的DMA支持多个不连续内存的数据传输) 可以看到bio的段可能指向多个page，而bio也可以在没有buffer_head的情况下构造。 request 和bio的关系在IO调度器中，上层提交的bio被构造成request结构，每个物理设备会对应一个request_queue，里面顺序存放着相关的request。每个请求包含一个或多个bio结构，bio之间用有序链表连接起来，按bio起始扇区的位置从小到大，而且这些bio之间在磁盘扇区是相邻的，也就是说一个bio的结尾刚好是下一个bio的开头。 通常，通用块层创建一个仅包含一个bio结构的请求，可能存在新bio与请求中已存在的数据物理相邻的情况，就把bio加入该请求，否则用该bio初始化一个新的请求。 buffer 和cache，在linux内核实现上没有区别，在计数上有区别。最后都是 address_space -&gt;radix tree -&gt; read&#x2F;write pages。 buffer_head: 是内核封装的数据结构。它是内核page与磁盘上物理数据块之间的桥梁。一方面，每个page包含多个buffer_head（一般4个），另外一方面，buffer_head中又记录了底层设备块号信息。这样，通过page-&gt;buffer_head-&gt;block就能完成数据的读写。 O_DIRECT 和 O_SYNC 两者区别，O_DIRECT 把一片地址配置成不带cache的空间 , 直接导硬盘 , 而 O_SYNC 类似CPU cache的write through. 当应用程序使用O_SYNC写page cache时，直接写穿到硬盘。 当应用程序同时使用O_SYNC和 O_DIRECT，可能会出现page cache的不一致问题。 writeback机制 与bdiwrite的过程，把要写的page提交到bdi_writeback 队列中，然后由writeback线程将其真正写到block device上。writeback机制：一方面加快了write()的速度，另一方面便于合并和排序多个write请求。 三进三出 讲解 bio page cache只有通过 address_space_operations里的 write_pages和 read_pages，才能产生对文件系统的block产生IO行为。而不管是directIO还是writeback机制最终都会通过submit_bio方法提交bio到block层。 block io( bio ) : 讲文件系统上哪些block 读到内存哪些 page, 文件系统的 block bitmap 和 inode table，bio是从文件系统解析出来，从block到page之间的关系。 generic_make_request实现中请求构建的关键为 make_request_fn, 该函数的调用链路为：blk_init_queue() -&gt; blk_init_queue_node() -&gt; blk_init_allocated_queue -&gt; blk_queue_make_request(q, lk_queue_io), 最后被调用执行的回调函数blk_queue_bio实现如下： page cache和 硬盘数据 通过linux readpages函数关联时，会把文件系统的四个block，转化成4个bio。bio里的指针指向数据在硬盘的位置，也会有把这些数据读出来之后放置到page cache的哪些page。 当一个page对应多个block的情况，一个page 对应几个bio：size(page) &gt; size(block)， bio –&gt; request 每个进程有自己的plug队列，先把bio先发送到plug队列。在plug队列里，把bio转化成request。第一个bio一定是一个request，之后的bio就会查看多个bio是否可以merge到一个request，如果不能合并就产生一个新的request。 bio边发plug队列，边转成request。而这些request 先放到 elevator电梯队列，IO调度电梯做的类似路由器中QoS的功能-&gt;限制端口的流量。 deadline调度算法：优先考虑读，认为写不重要。应用程序写到pagecache以后，就已经写完了。 I&#x2F;O写入流程图 ftracevfs_read&#x2F;vfs_write 慢，到底层操作磁盘差十万八千里。要用ftrace分析，才能知道具体原因。 对于文件读写这种非常复杂的流程，在工程里面可以使用的调试方式是 ftrace。 除了用ftrace工具进行函数级的分析之外，使用blktrace去跟踪整个block io 生命周期，比如什么时候进入plug队列，unplug，什么时候进入电梯调度，什么时候进到驱动队列。 blktrace12345678910apt-get install sleuthkit blktrace#1:blktrace -d /dev/sda -o - | blkparse -i - &gt; 1.trace#2:root@whale-indextest01-1001:/home/gzzhangyi2015/learningLinuxKernel/io-courses/bio-flow# dd if=read.c of=barry oflag=sync0+1 records in0+1 records out219 bytes (219 B) copied, 0.000854041 s, 256 kB/s 扇区号(294220992)&#x2F;8 ＝ block号(36777624) 再用debugfs -R ‘icheck 块号’ &#x2F;dev&#x2F;sda9 再用debugfs -R ‘ncheck inode’ &#x2F;dev&#x2F;sda9 再用 blkcat &#x2F;dev&#x2F;sda9 块号 总结： blktrace 是在内核里关键函数点上加了一些记录，再把记录抓下来。主要是看操作的流程。ftrace 看 函数的流程。 IO调度 和 CFQ调度算法 主要从 进程优先级 ， 流量控制 方面考虑，在通用块层和 I&#x2F;O调度层 进行限速，限制带宽和 IOPS。 Noop：空操作调度算法，也就是没有任何调度操作，并不对io请求进行排序，仅仅做适当的io合并的一个fifo队列。合并的技术，不太适用于排序。适用于固态硬盘，因为固态硬盘基本上可以随机访问。 CFQ：完全公平队列调度类似进程调度里的CFS，指定进程的nice值。它试图给所有进程提供一个完全公平的IO操作环境。它为每个进程创建一个同步IO调度队列，并默认以时间片和请求数限定的方式分配IO资源，以此保证每个进程的IO资源占用是公平的，cfq还实现了针对进程级别的优先级调度。 CFQ对于IO密集型场景不适用，尤其是IO压力集中在某些进程上的场景。该场景下需要更多满足某个或某几个进程的IO响应速度，而不是让所有的进程公平的使用IO。 此时，deadline调度（最终期限调度）就更适应这样的场景。deadline实现了四个队列，其中两个分别处理正常read和write，按扇区号排序，进行正常io的合并处理以提高吞吐量.因为IO请求可能会集中在某些磁盘位置，这样会导致新来的请求一直被合并，于是可能会有其他磁盘位置的io请求被饿死。于是，实现了另外两个处理超时read和write的队列，按请求创建时间排序，如果有超时的请求出现，就放进这两个队列，调度算法保证超时（达到最终期限时间）的队列中的请求会优先被处理，防止请求被饿死。由于deadline的特点，无疑在这里无法区分进程，也就不能实现针对进程的io资源控制。 从原理上看，cfq是一种比较通用的调度算法，是一种以进程为出发点考虑的调度算法，保证大家尽量公平。 deadline是一种以提高机械硬盘吞吐量为思考出发点的调度算法，只有当有io请求达到最终期限的时候才进行调度，非常适合业务比较单一并且IO压力比较重的业务，比如数据库。 而noop呢？其实如果我们把我们的思考对象拓展到固态硬盘，那么你就会发现，无论cfq还是deadline，都是针对机械硬盘的结构进行的队列算法调整，而这种调整对于固态硬盘来说，完全没有意义。对于固态硬盘来说，IO调度算法越复杂，效率就越低，因为额外要处理的逻辑越多。 所以，固态硬盘这种场景下，使用noop是最好的，deadline次之，而cfq由于复杂度的原因，无疑效率最低。 CFQ 和 ionice demo把linux的IO调度算法改成CFQ，并且运行两个不同IO nice值的进程。 123456789101112131415目前的调度算法是 deadlineroot@whale-indextest01-1001:/sys/block/sda/queue# cat schedulernoop [deadline] cfqroot@whale-indextest01-1001:/sys/block/sda/queue# echo cfq &gt; schedulerroot@whale-indextest01-1001:/sys/block/sda/queue# cat schedulernoop deadline [cfq]root@whale-indextest01-1001:/sys/block/sda/queue# ionice -c 2 -n 0 cat /dev/sda9 &gt; /dev/null&amp;[1] 6755root@whale-indextest01-1001:/sys/block/sda/queue# ionice -c 2 -n 7 cat /dev/sda9 &gt; /dev/null&amp;[2] 6757root@whale-indextest01-1001:/sys/block/sda/queue# iotop cgroup与IO 123456root@whale-indextest01-1001:/sys/fs/cgroup/blkio# cgexec -g blkio:A dd if=/dev/sda9 of=/dev/null iflag=direct &amp;[3] 7550root@whale-indextest01-1001:/sys/fs/cgroup/blkio# cgexec -g blkio:B dd if=/dev/sda9 of=/dev/null iflag=direct &amp;[4] 7552 IO性能测试iostathttp://linuxperf.com/?p=156 1、rrqm&#x2F;s &amp; wrqm&#x2F;s 看io合并: 和&#x2F;sys&#x2F;block&#x2F;sdb&#x2F;queue&#x2F;scheduler 设置的io调度算法有关。2、%util与硬盘设备饱和度: iostat 无法看硬盘设备的饱和度。3、即使%util高达100%，硬盘也仍然有可能还有余力处理更多的I&#x2F;O请求，即没有达到饱和状态。 await 是单个I&#x2F;O所消耗的时间，包括硬盘设备处理I&#x2F;O的时间和I&#x2F;O请求在kernel队列中等待的时间.实际场景根据I&#x2F;O模式 随机&#x2F;顺序与否进行判断。如果磁盘阵列的写操作不在一两个毫秒以内就算慢的了；读操作则未必，不在缓存中的数据仍然需要读取物理硬盘，单个小数据块的读取速度跟单盘差不多。 iowait%iowait 表示在一个采样周期内有百分之几的时间属于以下情况：CPU空闲、并且有仍未完成的I&#x2F;O请求。%iowait 升高并不能证明等待I&#x2F;O的进程数量增多了，也不能证明等待I&#x2F;O的总时间增加了。%iowait升高有可能仅仅是cpu空闲时间增加了。%iowait 的高低与I&#x2F;O的多少没有必然关系，而是与I&#x2F;O的并发度相关。所以，仅凭 %iowait 的上升不能得出I&#x2F;O负载增加 的结论。 它是一个非常模糊的指标，如果看到 %iowait 升高，还需检查I&#x2F;O量有没有明显增加，avserv&#x2F;avwait&#x2F;avque等指标有没有明显增大，应用有没有感觉变慢。 FAQ:什么是Bufferd IO&#x2F; Direct IO? 如何解释cgroup的blkio对buffered IO是没有限速支持的？答：这里面的buffer的含义跟内存中buffer cache有概念上的不同。实际上这里Buffered IO的含义，相当于内存中的buffer cache+page cache，就是IO经过缓存的意思。 cgroup针对IO的资源限制实现在了通用块设备层，对哪些IO操作有影响呢？ 原则上说都有影响，因为绝大多数数据都是要经过通用块设备层写入存储的，但是对于应用程序来说感受可能不一样。在一般IO的情况下，应用程序很可能很快的就写完了数据（在数据量小于缓存空间的情况下），然后去做其他事情了。这时应用程序感受不到自己被限速了，而内核在处理write-back的阶段，由于没有相关page cache中的inode是属于那个cgroup的信息记录，所以所有的page cache的回写只能放到cgroup的root组中进行限制，而不能在其他cgroup中进行限制，因为root组的cgroup一般是不做限制的。 而在Sync IO和Direct IO的情况下，由于应用程序写的数据是不经过缓存层的，所以能直接感受到速度被限制，一定要等到整个数据按限制好的速度写完或者读完，才能返回。这就是当前cgroup的blkio限制所能起作用的环境限制。"},{"path":"/2024/04/05/linux-docs/文件系统/Linux IO 之 文件系统的实现/","content":"Ext2&#x2F;3&#x2F;4 的layout 文件系统的一致性： append一个文件的全流程 掉电与文件系统的一致性 fsck 文件系统的日志 ext4 mount选项 文件系统的debug和dump Copy On Write 文件系统： btrfs 预备知识：数据库里的transaction(事务)有什么特性？ 原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。 一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。 持久性（Durability）：一个事务一旦提交，他对数据库的修改应该永久保存在数据库中。 Ext2&#x2F;3&#x2F;4 的layout 如上图，任何一个文件，在硬盘上有inode、 datablocks，和一些元数据信息(- 描述数据的数据)。其中，inode的信息包括，inode bitmap 和 inode table。通过inode bitmap和block bitmap来描述具体的inode table 和data blocks是否被占用。inode table包括文件的 读写权限和 指针表。 Linux对硬盘上一个文件，是分不同角度描述。创建一个文件，包括修改inode bitmap 和 block bitmap的描述。包括修改datablock和 inode bitmap的信息等等。所以“修改文件”这个操作，并不是原子的。所以存在文件系统的执行一致性的问题。 分Group的好处，在同一个目录下的东西，尽量放在同一个group，用来减少硬盘的来回寻道。 文件系统的一致性： append一个文件的全流程"},{"path":"/2024/04/05/linux-docs/文件系统/Linux IO 之 文件系统的架构/","content":"本文概述：应用程序 -&gt;read -&gt;文件系统的代码 如何实现？当目录里面 A&#x2F;B&#x2F;C ，是如何找到C的全过程？文件系统如何描述文件在磁盘的哪些位置？硬链接和 符号链接的详细区别？userspace的文件系统的实现？ 一切都是文件： VFS 文件系统的设计，类似 抽象基类，面向对象的思想。 虚函数都必须由底层派生出的实例实现，使用成员函数 file_operations。在linux里面的文件操作，底层都要实现file_operations，抽象出owner，write，open，release。所以，无论是字符块，还是文件系统的文件，最终操作就必须是file_operations。 例如，实现一个字符设备驱动，就是去实现file_operations。VFS_read时就会调用字符设备的file_operations。 字符设备文件、块设备文件块设备的两种访问方法，一是访问裸分区，二是访问文件系统。 当直接访问裸分区，是通过fs&#x2F;block_dev.c 中的 file_operations def_blk_fops，也有read,write,open，一切继承到file_operations。如果是访问文件系统，就会通过实现 {ext4}_file_operations 来对接VFS对文件的操作。 块设备驱动就不需要知道file_operations，无论是裸设备，还是文件系统的file。他们实现的file_operations是把linux中的各种设备，hook进 VFS的方法。 文件最终如何转化成对磁盘的访问？file_operation 跟pagecache 以及硬盘的关系？ 整个文件系统里，除了放文件本身的数据，还包括文件的管理数据，包括 super block，保存在全局的 superblock结构中。 inode，是文件的唯一特定标识，文件系统使用bitmap来标识，inode是否使用。 block bitmap，来表示这些block是否占用，它在改变文件大小，创建删除等操作时，都会改变。 inode table&#x2F;diagram ： bitmap 只是表示inode和block是否被占用。 超级块、目录、inode file_system_type 数据结构： 指的是 文件系统的类型，mount&#x2F;umount 的时候会用。 superblock数据结构：包含super_operations，其中包含如何分配&#x2F;销毁一个inode。 inode 数据结构：包含 inode_operations 和 file_operations。 file_operations里面记录这种类型的文件，包含哪些操作。inode_operations里面包含如何生成新的inode，根据文件名字找到inode，如何mkdir,unlink. dentry 数据结构: 对应路径，目录在文件系统里面是一个特殊的文件，文件的内容是一个inode和文件的表格。 file 数据结构: inode表：包含文件的一些基本信息，大小，创建日期，属性。还有一些成员指向硬盘所在的位置。申请slab区域，比如 ext4_inode_cache , ext3_inode_cache. 这些cache会创建单独的slab，这些slab和内存里的page一一对应。 ext2&#x2F;ext4文件系统中存在间接映射表。 硬盘里的inode diagram里的数据结构，在内存中会通过slab分配器，组织成 xxx_inode_cache，出现在meminfo的可回收的内存。 inode表也会记录每一个inode 在硬盘中摆放的位置。 目录的组织 目录在硬盘里是一个特殊的文件，和之前的file结构体不同。目录在硬盘中对应一个inode，记录文件的名字和inode号。查找一个文件时，文件系统的 根inode和目录，根据根目录和根inode，找到根目录所在硬盘的位置。再去做字符串匹配，能够找到 &#x2F;A&#x2F;B&#x2F; 。inode表也会记录每一个inode 在硬盘中摆放的位置。 icache和dcache，slab shrink 文件系统在实现时，在vfs这一层的 inode cache 和 dentry cache，不管硬盘的系统，跨所有文件系统的通用信息。 针对这些cache，这些可以回收的slab，linux提供了专门的slab shrink- 收缩函数。最后所有可回收的内存，都必须通过LRU算法去回收。有些自己申请的 reclaim的内存，由于没有写 shrink函数，所以就无法进行内存的回收。 文件读写如何通过file_operation 和pagecache的关系 发现并读取&#x2F;usr&#x2F;bin&#x2F;xxx的全流程 如上图，当你在硬盘查找 &#x2F;usr&#x2F;bin&#x2F;emacs文件时，从根的inode和dentry，根据&#x2F;的inode表，找到&#x2F; 目录文件所在的硬盘中的位置，读硬盘&#x2F;目录文件的内容，发现 usr 对应inode 2, bin 对应inode 3, share 对应inode4。再去查inode表，inode 2所在硬盘的位置，即&#x2F;usr 目录文件所在硬盘的位置。读出内容包括 var 对应 inode 10, bin 对应inode 11, opt对应inode 12，。 这个过程会查找很多inode和 dentry，这些都会通过 icache 和dcache缓存。 符号链接 与 硬链接 文件名是特殊目录文件的内容，比如 A目录下有b\\c\\d，其实就是 A这个目录文件，里面对应目录b,c,d和对应inode的表。 硬链接：在硬盘中是同一个inode存在，在目录文件中多了一个目录和inode对应。 符号链接：是linux中是真实存在的实体文件，文件内容指向 其他文件。符号链接和文件是不同的inode。 1、硬链接不能跨本地文件系统2、硬链接不能针对目录3、针对目录的软链接，用rm -fr 删除不了目录里的内容4、针对目录的软链接，”cd ..”进的是软链接所在目录的上级目录5、可以对文件执行unlink或rm，但是不能对目录执行unlink 用户空间的文件系统： FUSE用户空间文件系统 是操作系统中的概念，指完全在用户态实现的文件系统。 目前Linux通过内核模块对此进行支持。一些文件系统如ZFS，glusterfs使用FUSE实现。 FUSE的工作原理如上图所示。假设基于FUSE的用户态文件系统hello挂载在&#x2F;tmp&#x2F;fuse目录下。当应用层程序要访问&#x2F;tmp&#x2F;fuse下的文件时，通过glibc中的函数进行系统调用，处理这些系统调用的VFS中的函数会调用FUSE在内核中的文件系统；内核中的FUSE文件系统将用户的请求，发送给用户态文件系统hello；用户态文件系统收到请求后，进行处理，将结果返回给内核中的FUSE文件系统；最后，内核中的FUSE文件系统将数据返回给用户态程序。"},{"path":"/2024/04/05/linux-docs/文件系统/Linux 操作系统原理-文件系统(1)/","content":"简介 虚拟文件系统（Virtual File System，简称VFS）是Linux内核的子系统之一，它为用户程序提供文件和文件系统操作的统一接口，屏蔽不同文件系统的差异和操作细节。借助VFS可以直接使用open()、read()、write()这样的系统调用操作文件，而无须考虑具体的文件系统和实际的存储介质。 通过VFS系统，Linux提供了通用的系统调用，可以跨越不同文件系统和介质之间执行，极大简化了用户访问不同文件系统的过程。另一方面，新的文件系统、新类型的存储介质，可以无须编译的情况下，动态加载到Linux中。 “一切皆文件”是Linux的基本哲学之一，不仅是普通的文件，包括目录、字符设备、块设备、套接字等，都可以以文件的方式被对待。实现这一行为的基础，正是Linux的虚拟文件系统机制。 VFS之所以能够衔接各种各样的文件系统，是因为它抽象了一个通用的文件系统模型，定义了通用文件系统都支持的、概念上的接口。新的文件系统只要支持并实现这些接口，并注册到Linux内核中，即可安装和使用。 举个例子，比如Linux写一个文件： 1int ret = write(fd, buf, len); 调用了write()系统调用，它的过程简要如下： 首先，勾起VFS通用系统调用sys_write()处理。接着，sys_write()根据fd找到所在的文件系统提供的写操作函数，比如op_write()。最后，调用op_write()实际的把数据写入到文件中。操作示意图如下： 文件系统Linux下常用文件系统介绍 swap 文件系统swap文件系统用于Linux的交换分区。在Linux中，使用整个交换分区来提供虚拟内存，其分区大小一般应是系统物理内存的2倍，在安装Linux操作系统时，就应创建交换分区，它是Linux正常运行所必需的，其类型必须是swap,交换分区由操作系统自行管理 Btrfs 文件系统 Ext系列文件系统ext是第一个专门为Linux设计的文件系统类型，称为扩展文件系统。目前已经到了第四版分别是ext2, ext3,ext4 其中 centOS6默认是ext4文件系统 xfs 文件系统 NFS文件系统 FAT系列文件系统 比较 文件系统 最大文件名长度 最大文件大小 最大分区大小 ext2 255 bytes 2 TB 16 TB ext3 255 bytes 2 TB 16 TB ext4 255 bytes 16 TB 1 EB XFS 255 bytes 8 EB 8 EB Btrfs 255 bytes 16 EB 16 EB 选择 文件系统 适用场景 原因 ext2 U盘 U盘一般不会存很多文件，且U盘的文件在电脑上有备份，安全性要求没那么高，由于ext2不写日志（journal），所以写U盘性能比较好。当然由于ext2的兼容性没有fat好，目前大多数U盘格式还是用fat ext3 对稳定性要求高的地方 有了ext4后，好像没什么原因还要用ext3，ext4现在的问题是出来时间不长，还需要一段时间变稳定 ext4 小文件较少 ext系列的文件系统都不支持inode动态分配，所以如果有大量小文件需要存储的话，不建议用ext4 xfs 小文件多或者需要大的xttr空间，如openstack swift将数据文件的元数据放在了xttr里面 xfs支持inode动态分配，所以不存在inode不够的情况，并且xttr的最大长度可以达到64K btrfs 没有频繁的写操作，且需要btrfs的一些特性 btrfs虽然还不稳定，但支持众多的功能，如果你需要这些功能，且不会频繁的写文件，那么选择btrfs 文件系统的结构磁盘的组成原理除了固态硬盘之外，硬盘一般都由磁盘、主轴马达、磁头臂、磁头、永磁铁等部分组成。 盘片盘片的表面涂有磁性物质，这些磁性物质用来记录二进制数据。因为正反两面都可涂上磁性物质，故一个盘片可能会有两个盘面，硬盘的存储介质是磁性材料，磁头通过电流改变磁盘的磁性来存储数据。硬盘在逻辑上被划分为磁道、柱面以及扇区。 扇区，磁道每个盘片被划分为一个个磁道，每个磁道又划分为一个个扇区。如下图： 其中，最内侧磁道上的扇区面积最小，因此数据密度最大。 柱面硬盘通常由重叠的一组盘片构成,每个盘面都被划分为数目相等的磁道,并从外缘的“0”开始编号,具有相同编号的磁道形成一个圆柱,称之为磁盘的柱面。 分区 为什么要对硬盘进行分区呢？ 因为我们必须要告诉操作系统：这块硬盘可以访问的区域是从 A 柱面到 B 柱面。如此一来，操作系统才能控制硬盘磁头去 A-B 范围内的柱面上访问数据。如果没有告诉操作系统这些信息，它就无法在磁盘上存取数据。所以对磁盘分区的要点是：记录每一个分区的起始与结束柱面。实际上，分区时指定的开始和结束位置是柱面上的扇区(sector)： 下面我们以CentOS7 为例来看一下分区情况： 1234567891011121314151617181920212223[root@CentOS7 ~]# fdisk -l磁盘 /dev/sda：21.5 GB, 21474836480 字节，41943040 个扇区Units = 扇区 of 1 * 512 = 512 bytes扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节磁盘标签类型：dos # 使 用dos---MBR 分区方式磁盘标识符：0x000ce1c0 设备 Boot Start End Blocks Id System/dev/sda1 * 2048 2099199 1048576 83 Linux/dev/sda2 2099200 41943039 19921920 8e Linux LVM（逻辑卷）# sda 分区磁盘 /dev/mapper/centos-root：18.2 GB, 18249416704 字节，35643392 个扇区Units = 扇区 of 1 * 512 = 512 bytes扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节# 交换分区磁盘 /dev/mapper/centos-swap：2147 MB, 2147483648 字节，4194304 个扇区Units = 扇区 of 1 * 512 = 512 bytes扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节 分区的组成结构 MBR那么，这些分区的信息(起始柱面与结束柱面)被存放在磁盘的主引导区(Master Boot Recorder, MBR)。MBR 在一块硬盘的第 0 轨上，这也是计算机启动之后要去使用硬盘时必须读取的第一个区域。 这个区域内记录了硬盘里所有分区的信息即磁盘分区表，以及启动时可以写入引导程序的位置。因此 MBR 对于硬盘来说至关重要，如果它坏掉了，这块磁盘也就寿终正寝了。 主引导记录由三个部分组成： 引导程序占用其中的前446字节（偏移0～1BDH） 随后的64字节（偏移1BEH～1FDH）为DPT（Disk Partition Table，硬盘分区表） 最后的两个字节“55 AA”（偏移1FEH～1FFH）是结束标志。 分区表: GDTDisk label type: gpt—GPT 在MBR硬盘中，分区信息直接存储于主引导记录（MBR）中（主引导记录中还存储着系统的引导程序）。但在GPT硬盘中，分区表的位置信息储存在GPT头中。但出于兼容性考虑，硬盘的第一个扇区仍然用作MBR，之后才是GPT头。 跟现代的MBR一样，GPT也使用逻辑区块地址（LBA）取代了早期的CHS寻址方式。传统MBR信息存储于LBA 0，GPT头存储于LBA 1，接下来才是分区表本身。 为什么要提出新的方案呢？那就让我们看看MBR分区方案有什么问题。前面已经提到了 主分区数目不能超过4个的限制 关键的是MBR分区方案无法支持超过2TB容量的磁盘。因为这一方案用4个字节存储分区的总扇区数，最大能表示2的32次方的扇区个数，按每扇区512字节计算，每个分区最大不能超过2TB。磁盘容量超过2TB以后，分区的起始位置也就无法表示了。 有了扇区（sector），有了柱面（cylinder），有了 磁头（head），显然可以定位数据了，这就是数据定位(寻址)方式之一，CHS（也称3D），对早期的磁盘（上图所示）非常有效，知道用哪个 磁头，读取哪个柱面上的第几扇区就OK了。CHS模式支持的硬盘容量有限，用8bit来存储 磁头地址，用10bit来存储柱面地址，用6bit来存储扇区地址，而一个扇区共有512Byte，这样使用CHS寻址一块硬盘最大容量为256 * 1024 * 63 * 512B &#x3D; 8064 MB(1MB &#x3D; 1048576B)（若按1MB&#x3D;1000000B来算就是8.4GB） 但现在很多硬盘采用同密度盘片，意味着内外磁道上的扇区数量不同，扇区数量增加，容量增加，3D很难定位寻址，新的寻址模式：LBA(Logical Block Addressing)。在LBA地址中，地址不再表示实际硬盘的实际 物理地址（柱面、 磁头和扇区）。LBA编址方式将CHS这种三维寻址方式转变为一维的线性寻址，它把硬盘所有的 物理扇区的C&#x2F;H&#x2F;S编号通过一定的规则转变为一线性的编号，系统效率得到大大提高，避免了烦琐的 磁头&#x2F;柱面&#x2F;扇区的寻址方式。在访问硬盘时，由硬盘控制器再将这种 逻辑地址转换为实际硬盘的 物理地址。 LBA下的编号，扇区编号是从0开始。逻辑扇区号LBA的公式： LBA(逻辑扇区号)&#x3D;磁头数 × 每磁道扇区数 × 当前所在柱面号 + 每磁道扇区数 × 当前所在磁头号 + 当前所在扇区号 – 1例如：CHS&#x3D;0&#x2F;0&#x2F;1，则根据公式LBA&#x3D;255 × 63 × 0 + 63 × 0 + 1 – 1&#x3D; 0 也就是说 物理0柱面0 磁头1扇区，是 逻辑0扇区。也就是说 LBA就是扇区的编号， 按照磁道 柱面 和 磁头 从小到大的顺序编号 计算机启动的过程1. BIOS 程序启动上个世纪70年代初，“只读内存”（read-only memory，缩写为ROM）发明，开机程序被刷入ROM芯片，计算机通电后，第一件事就是读取它。这块芯片里的程序叫做”基本輸出輸入系統”（Basic Input&#x2F;Output System），简称为BIOS。 2. 硬件自检BIOS程序首先检查，计算机硬件能否满足运行的基本条件，这叫做”硬件自检”（Power-On Self-Test），缩写为POST。如果硬件出现问题，主板会发出不同含义的蜂鸣，启动中止。如果没有问题，屏幕就会显示出CPU、内存、硬盘等信息。 硬件自检完成后，BIOS把控制权转交给下一阶段的启动程序。这时，BIOS需要知道，“下一阶段的启动程序”具体存放在哪一个设备。也就是说，BIOS需要有一个外部储存设备的排序，排在前面的设备就是优先转交控制权的设备。这种排序叫做”启动顺序”（Boot Sequence）。 打开BIOS的操作界面，里面有一项就是”设定启动顺序“。 3. 主引导记录BIOS按照”启动顺序”，把控制权转交给排在第一位的储存设备。 这时，计算机读取该设备的第一个扇区，也就是读取最前面的512个字节。如果这512个字节的最后两个字节是0x55和0xAA，表明这个设备可以用于启动；如果不是，表明设备不能用于启动，控制权于是被转交给”启动顺序”中的下一个设备。这最前面的512个字节，就叫做”主引导记录”（Master boot record，缩写为MBR）。主引导记录”只有512个字节，放不了太多东西。它的主要作用是，告诉计算机到硬盘的哪一个位置去找操作系统 4. 启动管理器在这种情况下，计算机读取”主引导记录”前面446字节的机器码之后，不再把控制权转交给某一个分区，而是运行事先安装的”启动管理器”（boot loader），由用户选择启动哪一个操作系统。Linux环境中，目前最流行的启动管理器是Grub。 5. 操作系统控制权转交给操作系统后，操作系统的内核首先被载入内存。以Linux系统为例，先载入&#x2F;boot目录下面的kernel。内核加载成功后，第一个运行的程序是&#x2F;sbin&#x2F;init。它根据配置文件（Debian系统是&#x2F;etc&#x2F;initab， CentOS 是systemd）产生init进程。这是Linux启动后的第一个进程，pid进程编号为1，其他进程都是它的后代。 然后，init线程加载系统的各个模块，比如窗口程序和网络程序，直至执行&#x2F;bin&#x2F;login程序，跳出登录界面，等待用户输入用户名和密码。 文件存贮文件系统到底是怎么管理磁盘的被？首先，操作系统会将磁盘分区后，同一个文件系统中，我们以ext系列为例来说明：ext系统将空间（这里的空间是指的一段连续的磁盘空间）划分为不同的功能区，比如元数据区和数据区。元数据去主要存贮文件的一些属性，比如说大小，快信息，这些信息被存贮在inode当中，而数据去以datablock 为存贮单元，主要是存放了文件的数据。 因为磁盘上的数据要和内存交互，而内存通常是以4KB为单位的，所以从逻辑上，把磁盘按照4KB划分比较方便（称为一个block）。现在假设由一个文件系统管理64个blocks的一个磁盘区域： 顺序文件结构顾名思义，文件的存贮数据块是连续的空间。 优点是不需要额外的空间开销，只要在文件目录中指出文件的大小和首块的块号即可，对顺序的访问效率很高。适应于顺序存取且文件不经常修改的情况。缺点是 文件动态地增长和缩小时系统开销很大； 文件创建时要求用户提供文件的大小； 存储空间浪费较大。 链式文件系统 克服了连续文件的不足之处，但文件的随机访问系统开销较大。适应于顺序访问的文件。 索引式文件系统在UNIX时代，就已经实现了索引式的文件系统。它的原理是为一个文件的所有块建立一个索引表，索引表就是块地址数组，每个数组元素就是块的地址，第n个数组元素指向文件中的第n个块，这样访问任意一个块的时候，只需要从索引表中获得块地址就可以了。而且文件中的块依然可以分散到不连续的零散空间中。其结构如下图所示 既适应于顺序存访问，也适应于随机访问，是一种比较好的文件物理结构，但要有用于索引表的空间开销和文件索引的时间开销 Ext 文件分区布局 每个分区，将若干个块儿组成一个块组，每个块组会有以下几个结构 超级块1）超级块(Super Block)描述整个分区的文件系统信息，如inode&#x2F;block的大小、总量、使用量、剩余量，以及文件系统的格式与相关信息。超级块在每个块组的开头都有一份拷贝（第一个块组必须有，后面的块组可以没有）。 为了保证文件系统在磁盘部分扇区出现物理问题的情况下还能正常工作，就必须保证文件系统的super block信息在这种情况下也能正常访问。所以一个文件系统的super block会在多个block group中进行备份，这些super block区域的数据保持一致。超级块记录的信息有： 1、block 与 inode 的总量（分区内所有Block Group的block和inode总量）；2、未使用与已使用的 inode &#x2F; block 数量；3、block 与 inode 的大小 (block 为 1, 2, 4K，inode 为 128 bytes)；4、filesystem 的挂载时间、最近一次写入数据的时间、最近一次检验磁盘 (fsck) 的时间等文件系统的相关信息；5、一个 valid bit 数值，若此文件系统已被挂载，则 valid bit 为 0 ，若未被挂载，则 valid bit 为 1 。 它的结构如图所示 对于ext2&#x2F;3&#x2F;4文件系统，以上介绍的这些inode bitmap, data block bitmap和inode table，都可以通过一个名为”dumpe2fs”的工具来查看其在磁盘上的具体位置 GDT2）块组描述符表(GDT,Group Descriptor Table)由很多块组描述符组成，整个分区分成多个块组就对应有多少个块组描述符。每个块组描述符存储一个块组的描述信息，如在这个块组中从哪里开始是inode Table，从哪里开始是Data Blocks，空闲的inode和数据块还有多少个等等。 Inode 和 Block 位图4）inode位图(inode Bitmap)和块位图类似，本身占一个块，其中每个bit表示一个inode是否空闲可用。 Inode bitmap的作用是记录block group中Inode区域的使用情况，Ext文件系统中一个block group中可以有16384个Inode，代表着这个Ext文件系统中一个block group最多可以描述16384个文件。 inode索引表的索引结构称为inode，是”index node”的简称，用来索引，跟踪一个文件的所有块。inode是文件索引结构组织形式的具体体现，一个文件就必须对应一个inode。 5）inode表(inode Table)由一个块组中的所有inode组成。一个文件除了数据需要存储之外，一些描述信息也需要存储，如文件类型，权限，文件大小，创建、修改、访问时间等，这些信息存在inode中而不是数据块中。 inode表占多少个块在格式化时就要写入块组描述符中。 在Ext2&#x2F;Ext3文件系统中，每个文件在磁盘上的位置都由文件系统block group中的一个Inode指针进行索引，Inode将会把具体的位置指向一些真正记录文件数据的block块，需要注意的是这些block可能和Inode同属于一个block group也可能分属于不同的block group。我们把文件系统上这些真实记录文件数据的block称为Data blocks。 索引表本身要占用存储空间，如果文件很大时，块就比较多，索引表就会很大。UNIX为了解决这个问题，采用间接索引表来处理。 ls -i 命令可以显示inode 号 12345➜ command ls -ilttotal 243003623(inode号) -rwxr-xr-x 1 root root 78 8 10 16:43 jump3003622(inode号) -rwxr-xr-x 1 root root 476 8 10 16:42 jumper.sh3003624(inode号) -rwxr-xr-x 1 root root 3346 3 24 2019 imgcat data block6）数据块(Data Block)是用来放置文件内容数据的地方。根据不同的文件类型有以下几种情况：对于普通文件，文件的数据存储在数据块中。对于目录，该目录下的所有文件名和目录名存储在所在目录的数据块中，除了文件名外，ls -l命令看到的其它信息保存在该文件的inode中。 文件分区实践我们根据实践一下磁盘分区的步骤，实践一下ext4下的文件管理系统的步骤。我们的系统如下。 12Linux CentOS7 3.10.0-1127.el7.x86_64 #1 SMP Tue Mar 31 23:36:51 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux 首先第一步，我们需要一块硬盘，用于我是CentOS7的系统，这里我在虚拟机上新建了一块硬盘,容量是1G，这块硬盘还没有格式，我可以在我的&#x2F;dev 目录下找到这块硬盘。 12[root@CentOS7 ~]# ls /dev/sd*/dev/sda /dev/sda1 /dev/sda2 /dev/sdb 查看分区,我一共有两块硬盘，一块是sda,一块是sdb。我们接下来需要格式化一下sdb. 格式化硬盘首先，我们需要用fdisk 将sdb 1234567891011121314151617181920212223242526272829303132333435363738394041[root@CentOS7 ~]# fdisk /dev/sdb欢迎使用 fdisk (util-linux 2.23.2)。更改将停留在内存中，直到您决定将更改写入磁盘。使用写入命令前请三思。Device does not contain a recognized partition table使用磁盘标识符 0x30c1a40d 创建新的 DOS 磁盘标签。命令(输入 m 获取帮助)：m #获取帮助命令命令操作 a toggle a bootable flag b edit bsd disklabel c toggle the dos compatibility flag d delete a partition g create a new empty GPT partition table G create an IRIX (SGI) partition table l list known partition types m print this menu n add a new partition o create a new empty DOS partition table p print the partition table q quit without saving changes s create a new empty Sun disklabel t change a partition&#x27;s system id u change display/entry units v verify the partition table w write table to disk and exit x extra functionality (experts only)命令(输入 m 获取帮助)：nPartition type: p primary (0 primary, 0 extended, 4 free) （这里表示，我们没有任何分许，允许你创建4个可用分区，0个主分区，和一个逻辑分区） e extendedSelect (default p): p （我们来创建主分区）分区号 (1-4，默认 1)： （选择分区号，默认即可）起始 扇区 (2048-2097151，默认为 2048)： （0-2048） # 前2048是系统预留空间 将使用默认值 2048Last 扇区, +扇区 or +size&#123;K,M,G&#125; (2048-2097151，默认为 2097151)： （我们直接全部将1g划分为这个分区）将使用默认值 2097151分区 1 已设置为 Linux 类型，大小设为 1023 MiB 创建文件系统使用CentOS7 自带的文件系统格式工具，将sdb整个硬盘格式化成ext4文件格式。mke2fs：ext系列文件系统专用管理工具 123456789101112131415161718192021222324[root@CentOS7 ~]# mkfs.ext4 /dev/sdbmke2fs 1.42.9 (28-Dec-2013)/dev/sdb is entire device, not just one partition!无论如何也要继续? (y,n) y文件系统标签=OS type: Linux块大小=4096 (log=2) 分块大小=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks65536 inodes, 262144 blocks # inodes数量和blicks 是数量13107 blocks (5.00%) reserved for the super user第一个数据块=0Maximum filesystem blocks=2684354568 block groups # 8个块组32768 blocks per group, 32768 fragments per group8192 inodes per group # 每组 inode 的数量Superblock backups stored on blocks: # 超级快的存贮位置\t32768, 98304, 163840, 229376Allocating group tables: 完成正在写入inode表: 完成Creating journal (8192 blocks): 完成 # 创建日志区Writing superblocks and filesystem accounting information: 完成 查看分区经过生面的步骤，我们已经将分区创建完毕，我们使用blkid 命令来查看一下分区 1234567[root@CentOS7 ~]# blkid/dev/sda1: UUID=&quot;79678d4f-9276-4d1e-8093-28623d77461e&quot; TYPE=&quot;xfs&quot;/dev/sda2: UUID=&quot;FtqgeH-yiRM-f1Wr-73LT-QkQf-2kgX-BkXYi1&quot; TYPE=&quot;LVM2_member&quot;/dev/sr0: UUID=&quot;2020-04-22-00-54-00-00&quot; LABEL=&quot;CentOS 7 x86_64&quot; TYPE=&quot;iso9660&quot; PTTYPE=&quot;dos&quot;/dev/mapper/centos-root: UUID=&quot;d0412c8e-07f5-4716-8be7-8ed2da9affca&quot; TYPE=&quot;xfs&quot;/dev/mapper/centos-swap: UUID=&quot;ac1ae2c7-d653-4496-97fa-d9315f56993f&quot; TYPE=&quot;swap&quot;/dev/sdb: UUID=&quot;6285b923-0ee4-444d-9c68-d6af94914bc3&quot; TYPE=&quot;ext4&quot; 查看 超级块和块组123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172dumpe2fs 1.42.9 (28-Dec-2013)Filesystem volume name: &lt;none&gt;Last mounted on: &lt;not available&gt;Filesystem UUID: 6285b923-0ee4-444d-9c68-d6af94914bc3Filesystem magic number: 0xEF53Filesystem revision #: 1 (dynamic)Filesystem features: has_journal ext_attr resize_inode dir_index filetype extent 64bit flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isizeFilesystem flags: signed_directory_hashDefault mount options: user_xattr aclFilesystem state: cleanErrors behavior: ContinueFilesystem OS type: LinuxInode count: 65536Block count: 262144Reserved block count: 13107Free blocks: 249189Free inodes: 65525First block: 0Block size: 4096Fragment size: 4096Group descriptor size: 64Reserved GDT blocks: 127Blocks per group: 32768Fragments per group: 32768Inodes per group: 8192Inode blocks per group: 512Flex block group size: 16Filesystem created: Wed Sep 30 12:22:05 2020Last mount time: n/aLast write time: Wed Sep 30 12:22:05 2020Mount count: 0Maximum mount count: -1Last checked: Wed Sep 30 12:22:05 2020Check interval: 0 (&lt;none&gt;)Lifetime writes: 33 MBReserved blocks uid: 0 (user root)Reserved blocks gid: 0 (group root)First inode: 11Inode size: 256Required extra isize: 28Desired extra isize: 28Journal inode: 8Default directory hash: half_md4Directory Hash Seed: 61f49ab4-d494-4be8-95b7-404358b685aaJournal backup: inode blocksJournal features: (none)日志大小: 32MJournal length: 8192Journal sequence: 0x00000001Journal start: 0Group 0: (Blocks 0-32767) Checksum 0x4cc2, unused inodes 8181 主 superblock at 0, Group descriptors at 1-1 保留的GDT块位于 2-128 Block bitmap at 129 (+129), Inode bitmap at 145 (+145) Inode表位于 161-672 (+161) 28521 free blocks, 8181 free inodes, 2 directories, 8181个未使用的inodes 可用块数: 142-144, 153-160, 4258-32767 可用inode数: 12-8192Group 1: (Blocks 32768-65535) [INODE_UNINIT] Checksum 0xabae, unused inodes 8192 备份 superblock at 32768, Group descriptors at 32769-32769 保留的GDT块位于 32770-32896 Block bitmap at 130 (bg #0 + 130), Inode bitmap at 146 (bg #0 + 146) Inode表位于 673-1184 (bg #0 + 673) 32639 free blocks, 8192 free inodes, 0 directories, 8192个未使用的inodes 可用块数: 32897-65535 可用inode数: 8193-16384..."},{"path":"/2024/04/05/linux-docs/文件系统/Linux 操作系统原理-文件系统(2)/","content":"文件类型普通文件类型理解了文件系统的结构之后，我们来看一下文件的类型。 Linux以文件的形式对计算机中的数据和硬件资源进行管理，也就是彻底的一切皆文件，反映在Linux的文件类型上就是：普通文件、目录文件（也就是文件夹）、设备文件、链接文件、管道文件、套接字文件（数据通信的接口）等等。而这些种类繁多的文件被Linux使用目录树进行管理， 所谓的目录树就是以根目录（&#x2F;）为主，向下呈现分支状的一种文件结构。 普通文件从Linux的角度来说，类似mp4、pdf、html这样应用层面上的文件类型都属于普通文件，Linux用户可以根据访问权限对普通文件进行查看、更改和删除。我们知道，文件的属性，权限，大小，占用那些数据块是存在inode当中。所以，这里注意一旦，inode 当中并没有存放文件名，至于为什么，我们接下来看目录文件。 目录文件本质上来书，目录页是文件，目录文件inode除了存放一些目录的权限，等属性之外，目录文件的内容则是该目录文件下文件名和其inode编号的一个映射关系。最简单的保存格式就是列表，就是一项一项地将目录下的文件信息（如文件名、文件 inode、文件类型等）列在表里。 文件目录块： 通常，第一项是「.」，表示当前目录，第二项是「…」，表示上一级目录，接下来就是一项一项的文件名和 inode。如果一个目录有超级多的文件，我们要想在这个目录下找文件，按照列表一项一项的找，效率就不高了。 于是，保存目录的格式改成哈希表，对文件名进行哈希计算，把哈希值保存起来，如果我们要查找一个目录下面的文件名，可以通过名称取哈希。如果哈希能够匹配上，就说明这个文件的信息在相应的块里面。 Linux 系统的 ext 文件系统就是采用了哈希表，来保存目录的内容，这种方法的优点是查找非常迅速，插入和删除也较简单，不过需要一些预备措施来避免哈希冲突。 目录查询是通过在磁盘上反复搜索完成，需要不断地进行 I&#x2F;O 操作，开销较大。所以，为了减少 I&#x2F;O 操作，把当前使用的文件目录缓存在内存，以后要使用该文件时只要在内存中操作，从而降低了磁盘操作次数，提高了文件系统的访问速度。 文件inode 文件操作文件链接 硬链接 一般情况下，文件名和inode号码是”一一对应”关系，每个inode号码对应一个文件名。但是，Unix&#x2F;Linux系统允许，多个文件名指向同一个inode号码。这意味着，可以用不同的文件名访问同样的内容；对文件内容进行修改，会影响到所有文件名；但是，删除一个文件名，不影响另一个文件名的访问。这种情况就被称为”硬链接”（hard link）。 其实原理很简单，我们会在某个目录下创建一个文件名，这个文件名和硬链接的文件inode 相同，并且会在这个inode的记录中增加链接数量。我们看到的就是两个链接到同一个inode 的文件其实是一个文件和inode的映射。 ln命令可以创建硬链接： 123456789101112131415161718192021222324252627root@CentOS7 lnDemo]# ln source source_ln[root@CentOS7 lnDemo]# ls -ali总用量 833575032 drwxr-xr-x. 2 root root 37 9月 30 17:35 .33574977 dr-xr-x---. 4 root root 161 9月 30 17:34 ..33575033 -rw-r--r--. 2 root root 7 9月 30 17:31 source33575033 -rw-r--r--. 2 root root 7 9月 30 17:31 source_ln[root@CentOS7 lnDemo]# stat source 文件：&quot;source&quot; 大小：7 块：8 IO 块：4096 普通文件设备：fd00h/64768d\tInode：33575033 硬链接：2权限：(0644/-rw-r--r--) Uid：( 0/ root) Gid：( 0/ root)环境：unconfined_u:object_r:admin_home_t:s0最近访问：2020-09-30 17:34:54.975711266 +0800最近更改：2020-09-30 17:31:49.124074625 +0800最近改动：2020-09-30 17:35:05.665345071 +0800创建时间：-[root@CentOS7 lnDemo]# stat source_ln 文件：&quot;source_ln&quot; 大小：7 块：8 IO 块：4096 普通文件设备：fd00h/64768d\tInode：33575033 硬链接：2权限：(0644/-rw-r--r--) Uid：( 0/ root) Gid：( 0/ root)环境：unconfined_u:object_r:admin_home_t:s0最近访问：2020-09-30 17:34:54.975711266 +0800最近更改：2020-09-30 17:31:49.124074625 +0800最近改动：2020-09-30 17:35:05.665345071 +0800创建时间：- 根据我们上面的研究，我们发现，硬链接有一下几个问题： 目录不允许硬链接 如果目录允许硬链接，那么我们完全可以将两个目录链接起来，那么操作系统则在找文件的时候，就会在两个目录跳来跳去，形成死循环。 [root@CentOS7 ~]# ln lnDemo&#x2F; .&#x2F;lnDemo2 ln: “lnDemo&#x2F;“: 不允许将硬链接指向目录 [root@CentOS7 ~]# 不同分区不允许硬链接 由于硬链接是在本分区指向相同的inode，那么就意味着inode的命名空间需要一直，但是不同的分区，inode的编号将会重置，所有不能通过inode映射同一个文件。 软连接 而软连接则不同，当创建软连接的时候，linux确实已经创建了一个inode 和起对应来的data block，只不过，在data block存放的是字符串，字符串的内容则是 链接文件的地址。 [root@CentOS7 lnDemo]# ln -s source source_sln [root@CentOS7 lnDemo]# ls source source_sln [root@CentOS7 lnDemo]# ls -alt 总用量 4 drwxr-xr-x. 2 root root 38 9月 30 18:08 . lrwxrwxrwx. 1 root root 6 9月 30 18:08 source_sln -&gt; source dr-xr-x---. 5 root root 172 9月 30 17:55 .. -rw-r--r--. 1 root root 7 9月 30 17:31 source [root@CentOS7 lnDemo]# ls -ailt 总用量 4 33575032 drwxr-xr-x. 2 root root 38 9月 30 18:08 . 33575034 lrwxrwxrwx. 1 root root 6 9月 30 18:08 source_sln -&gt; source 33574977 dr-xr-x---. 5 root root 172 9月 30 17:55 .. 33575033 -rw-r--r--. 1 root root 7 9月 30 17:31 source 我们发现 source_sln 的文件类型为 l ，且inode和source 不同。大小很小，原因就是我们存放的是地址字符。 文件新建(复制) (1).读取GDT，找到各个(或部分)块组imap中未使用的inode号，并为待存储文件分配inode号； ((2).在inode table中完善该inode号所在行的记录； ((3).在目录的data block中添加一条该文件的相关记录； ((4).将数据填充到data block中。 注意，填充到data block中的时候会调用block分配器：一次分配4KB大小的block数量，当填充完4KB的data block后会继续调用block分配器分配4KB的block，然后循环直到填充完所有数据。也就是说，如果存储一个100M的文件需要调用block分配器100*1024&#x2F;4&#x3D;25600次。另一方面，在block分配器分配block时，block分配器并不知道真正有多少block要分配，只是每次需要分配时就分配，在每存储一个data block前，就去bmap中标记一次该block已使用，它无法实现一次标记多个bmap位。这一点在ext4中进行了优化。 (5)填充完之后，去inode table中更新该文件inode记录中指向data block的寻址指针。 文件删除删除文件分为普通文件和目录文件，知道了这两种类型的文件的删除原理，就知道了其他类型特殊文件的删除方法。 对于删除普通文件： (1)找到文件的inode和data block(根据前一个小节中的方法寻找)； (1.5) 如果inode的硬链接是数量不是1 ，则将硬链接的数量-1， 否则执行真正的删除。 (2)将inode table中该inode记录中的data block指针删除； (3)在imap中将该文件的inode号标记为未使用； (4)在其所在目录的data block中将该文件名所在的记录行删除，删除了记录就丢失了指向inode的指针（实际上不是真的删除，直接删除的话会在目录data block的数据结构中产生空洞，所以实际的操作是将待删除文件的inode号设置为特殊的值0，这样下次新建文件时就可以重用该行记录）； (5)将bmap中data block对应的block号标记为未使用。 对于删除目录文件： 找到目录和目录下所有文件、子目录、子文件的inode和data block； 在imap中将这些inode号标记为未使用；将bmap中将这些文件占用的 block号标记为未使用； 在该目录的父目录的data block中将该目录名所在的记录行删除。需要注意的是，删除父目录data block中的记录是最后一步，如果该步骤提前，将报目录非空的错误，因为在该目录中还有文件占用。 文件搜索当执行”cat &#x2F;var&#x2F;log&#x2F;messages”命令在系统内部进行了什么样的步骤呢？ 找到根文件系统的块组描述符表所在的blocks，读取GDT(已在内存中)找到inode table的block号。 根文件系统是不需被引用的，因为在操作系统加载到内存当中的时候，跟文件系统已经存在，其中第inode编号也已经注册到了操作系统内核当中。根文件系统的GDT早已经在内存中了，在系统开机的时候会挂载根文件系统，挂载的时候就已经将所有的GDT放进内存中。 在inode table的block中定位到根”&#x2F;“的inode，找出”&#x2F;“指向的data block。 在”&#x2F;“的datablock中记录了var目录名和var的inode号，找到该inode记录，inode记录中存储了指向var的block指针，所以也就找到了var目录文件的data block。 通过var目录的inode号，可以寻找到var目录的inode记录，但是在寻找的过程中，还需要知道该inode记录所在的块组以及所在的inode table，所以需要读取GDT，同样，GDT已经缓存到了内存中。 在var的data block中记录了log目录名和其inode号，通过该inode号定位到该inode所在的块组及所在的inode table，并根据该inode记录找到log的data block。 在log目录文件的data block中记录了messages文件名和对应的inode号，通过该inode号定位到该inode所在的块组及所在的inode table，并根据该inode记录找到messages的data block。 最后读取messages对应的datablock。 当然，在每次定位到inode记录后，都会先将inode记录加载到内存中，然后查看权限，如果权限允许，将根据block指针找到对应的data block。 文件移动同文件系统下移动文件实际上是修改目标文件所在目录的data block，向其中添加一行指向inode table中待移动文件的inode指针，如果目标路径下有同名文件，则会提示是否覆盖，实际上是覆盖目录data block中冲突文件的记录，由于同名文件的inode记录指针被覆盖，所以无法再找到该文件的data block，也就是说该文件被标记为删除 所以在同文件系统内移动文件相当快，仅仅在所在目录data block中添加或覆盖了一条记录而已。也因此，移动文件时，文件的inode号是不会改变的。 对于不同文件系统内的移动，相当于先复制再删除的动作。 文件挂载Linux 系统下，文件是虚拟文件系统，当我们ls &#x2F; 的时候，linux 会吧所有磁盘，所有分区下的且挂载在根目录下的所有目录列出来。挂载文件系统到某个目录下，例如”mount &#x2F;dev&#x2F;cdrom &#x2F;mnt”，挂载成功后&#x2F;mnt目录中的文件全都暂时不可见了，且挂载后权限和所有者(如果指定允许普通用户挂载)等的都改变了，知道为什么吗？ 下面就以通过”mount &#x2F;dev&#x2F;cdrom &#x2F;mnt”为例，详细说明挂载过程中涉及的细节。 在将文件系统&#x2F;dev&#x2F;cdrom(此处暂且认为它是文件系统)挂载到挂载点&#x2F;mnt之前，挂载点&#x2F;mnt是根文件系统中的一个目录，”&#x2F;“的data block中记录了&#x2F;mnt的一些信息，其中包括inode号inode_n，而在inode table中，&#x2F;mnt对应的inode记录中又存储了block指针block_n，此时这两个指针还是普通的指针。 当文件系统&#x2F;dev&#x2F;cdrom挂载到&#x2F;mnt上后，&#x2F;mnt此时就已经成为另一个文件系统的入口了，因此它需要连接两边文件系统的inode和data block。 在根文件系统的inode table中，为&#x2F;mnt重新分配一个inode记录m，该记录的block指针block_m指向文件系统&#x2F;dev&#x2F;cdrom中的data block。 &#x2F;mnt分配了新的inode记录m，那么在”&#x2F;“目录的data block中，也需要修改其inode指针为inode_m以指向m记录。 同时，原来inode table中的inode记录n就被标记为暂时不可用。 block_m指向的是文件系统&#x2F;dev&#x2F;cdrom的data block，所以严格说起来，除了&#x2F;mnt的元数据信息即inode记录m还在根文件系统上，&#x2F;mnt的data block已经是在&#x2F;dev&#x2F;cdrom中的了。这就是挂载新文件系统后实现的跨文件系统，它将挂载点的元数据信息和数据信息分别存储在不同的文件系统上。 挂载完成后，将在&#x2F;proc&#x2F;self&#x2F;{mounts,mountstats,mountinfo}这三个文件中写入挂载记录和相关的挂载信息，并会将&#x2F;proc&#x2F;self&#x2F;mounts中的信息同步到&#x2F;etc&#x2F;mtab文件中，当然，如果挂载时加了-n参数，将不会同步到&#x2F;etc&#x2F;mtab。 而卸载文件系统，其实质是移除临时新建的inode记录(当然，在移除前会检查是否正在使用)及其指针，并将指针指回原来的inode记录，这样inode记录中的block指针也就同时生效而找回对应的data block了。由于卸载只是移除inode记录，所以使用挂载点和文件系统都可以实现卸载，因为它们是联系在一起的。 下面是分析或结论。 (1).挂载点挂载时的inode记录是新分配的。 挂载前挂载点&#x2F;mnt的inode号 [root@server2 tmp]# ll -id /mnt 100663447 drwxr-xr-x. 2 root root 6 Aug 12 2015 /mnt [root@server2 tmp]# mount /dev/cdrom /mnt # 挂载后挂载点的inode号 [root@server2 tmp]# ll -id /mnt 1856 dr-xr-xr-x 8 root root 2048 Dec 10 2015 mnt 由此可以验证，inode号确实是重新分配的。 (2).挂载后，挂载点的内容将暂时不可见、不可用，卸载后文件又再次可见、可用。 在挂载前，向挂载点中创建几个文件 [root@server2 tmp]# touch /mnt/a.txt [root@server2 tmp]# mkdir /mnt/abcdir # 挂载 [root@server2 tmp]# mount /dev/cdrom /mnt 挂载后，挂载点中将找不到刚创建的文件 [root@server2 tmp]# ll /mnt total 636 -r--r--r-- 1 root root 14 Dec 10 2015 CentOS_BuildTag dr-xr-xr-x 3 root root 2048 Dec 10 2015 EFI -r--r--r-- 1 root root 215 Dec 10 2015 EULA -r--r--r-- 1 root root 18009 Dec 10 2015 GPL dr-xr-xr-x 3 root root 2048 Dec 10 2015 images dr-xr-xr-x 2 root root 2048 Dec 10 2015 isolinux dr-xr-xr-x 2 root root 2048 Dec 10 2015 LiveOS dr-xr-xr-x 2 root root 612352 Dec 10 2015 Packages dr-xr-xr-x 2 root root 4096 Dec 10 2015 repodata -r--r--r-- 1 root root 1690 Dec 10 2015 RPM-GPG-KEY-CentOS-7 -r--r--r-- 1 root root 1690 Dec 10 2015 RPM-GPG-KEY-CentOS-Testing-7 -r--r--r-- 1 root root 2883 Dec 10 2015 TRANS.TBL 卸载后，挂载点&#x2F;mnt中的文件将再次可见 [root@server2 tmp]# umount /mnt [root@server2 tmp]# ll /mnt total 0 drwxr-xr-x 2 root root 6 Jun 9 08:18 abcdir -rw-r--r-- 1 root root 0 Jun 9 08:18 a.txt 之所以会这样，是因为挂载文件系统后，挂载点原来的inode记录暂时被标记为不可用，关键是没有指向该inode记录的inode指针了。在卸载文件系统后，又重新启用挂载点原来的inode记录，”&#x2F;“目录下的mnt的inode指针又重新指向该inode记录。 (3).挂载后，挂载点的元数据和data block是分别存放在不同文件系统上的。(4).挂载点即使在挂载后，也还是属于源文件系统的文件。 文件描述符先看一段最文件描述符的官方说明 维基百科:文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。 作用 Linux 系统中，把一切都看做是文件，当进程打开现有文件或创建新文件时，内核向进程返回一个文件描述符，文件描述符就是内核为了高效管理已被打开的文件所创建的索引，用来指向被打开的文件，所有执行I&#x2F;O操作的系统调用都会通过文件描述符。 概念定义 文件描述符 是 用来访问资源(文件，输入输出设备等)的一种抽象指示符。 文件描述符 是POSIX(Portable Operating System Interface)规范的组成部分 文件描述符 通常是非负整数，C 语言中使用int类型。 FD 具体可以指向什么 文件&#x2F;目录 files&#x2F;directories 输入输出源 input&#x2F;output 管道 pipes 套接字 sockets 其他 Unix 文件类型 other Unix files 默认的fds 每一个 Unix 进程中，通常会有三个预制的 FD。它们分别是 标准输入 Standard input 标准输入 用于程序接受数据 标准输出 Standard output 标准输出 用于程序输出数据 标准错误(输出) Standard error 标准错误 用于程序输出错误或者诊断信息 文件描述符的意义一个 Linux 进程启动后，会在内核空间中创建一个 PCB 控制块，PCB 内部有一个文件描述符表（File descriptor table），记录着当前进程所有可用的文件描述符，也即当前进程所有打开的文件。 除了文件描述符表，系统还需要维护另外两张表：打开文件表（Open file table）i-node 表（i-node table） 文件描述符表每个进程都有一个，打开文件表和 i-node 表整个系统只有一个，它们三者之间的关系如下图所示。 文件描述符的意义 首先，为什么不把文件位置干脆存放在索引节点中，而要多此一举，设一个新的数据结构呢？我们知道，Linux中的文件是能够共享的，假如把文件位置存放在索引节点中，则如果有两个或更多个进程同时打开同一个文件时，它们将去访问同一个索引节点，于是一个进程的LSEEK操作将影响到另一个进程的读操作，这显然是不允许也是不可想象的。 另一个想法是既然进程是通过文件描述符访问文件的，为什么不用一个与文件描述符数组相平行的数组来保存每个打开文件的文件位置？这个想法也是不能实现的，原因就在于在生成一个新进程时，子进程要共享父进程的所有信息，包括文件描述符数组。 我们知道，一个文件不仅可以被不同的进程分别打开，而且也可以被同一个进程先后多次打开。一个进程如果先后多次打开同一个文件，则每一次打开都要分配一个新的文件描述符，并且指向一个新的file结构，尽管它们都指向同一个索引节点，但是，如果一个子进程不和父进程共享同一个file结构，而是也如上面一样，分配一个新的file结构，会出现什么情况了？让我们来看一个例子： 假设有一个输出重定位到某文件A的shell script（shell脚本），我们知道，shell是作为一个进程运行的，当它生成第一个子进程时，将以0作为A的文件位置开始输出，假设输出了2K的数据，则现在文件位置为2K。然后，shell继续读取脚本，生成另一个子进程，它要共享shell的file结构，也就是共享文件位置，所以第二个进程的文件位置是2K，将接着第一个进程输出内容的后面输出。如果shell不和子进程共享文件位置，则第二个进程就有可能重写第一个进程的输出了，这显然不是希望得到的结果。 查看文件描述符lsof（list open files）是一个查看当前系统文件的工具。在linux环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。如传输控制协议 (TCP) 和用户数据报协议 (UDP) 套接字等，系统在后台都为该应用程序分配了一个文件描述符，该文件描述符提供了大量关于这个应用程序本身的信息。 lsof打开的文件可以是： 普通文件 目录 网络文件系统的文件 字符或设备文件 (函数)共享库 管道，命名管道 符号链接 网络文件（例如：NFS file、网络socket，unix域名socket） 还有其它类型的文件，等等 我们用java 新写一段代码 12345public static void main(String[] args) throws Exception &#123; String s =&quot;/tmp/file.test&quot;; FileOutputStream fileOutputStream = new FileOutputStream(new File(s)); System.in.read();&#125; 运行上述代码，并用jps找到其对应的pid 利用lsof -i 命令来查看 ... java 36562 lizhipeng mem REG 253,0 142144 50547 /usr/lib64/libpthread-2.17.so java 36562 lizhipeng mem REG 253,0 163312 42066 /usr/lib64/ld-2.17.so java 36562 lizhipeng mem REG 253,0 32768 51151094 /tmp/hsperfdata_lizhipeng/36562 java 36562 lizhipeng 0u CHR 136,4 0t0 7 /dev/pts/4 java 36562 lizhipeng 1u CHR 136,4 0t0 7 /dev/pts/4 java 36562 lizhipeng 2u CHR 136,4 0t0 7 /dev/pts/4 java 36562 lizhipeng 3r REG 253,0 73861866 33613070 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.262.b10-0.el7_8.x86_64/jre/lib/rt.jar java 36562 lizhipeng 4r REG 253,0 1027597 33613060 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.262.b10-0.el7_8.x86_64/jre/lib/jfr.jar java 36562 lizhipeng 5w REG 253,0 0 33671497 /tmp/file.test PCB 进程控制块为了描述控制进程的运行，系统中存放进程的管理和控制信息的数据结构称为进程控制块（PCB Process Control Block），它是进程实体的一部分，是操作系统中最重要的记录性数据结构。它是进程管理和控制的最重要的数据结构，每一个进程均有一个PCB，在创建进程时，建立PCB，伴随进程运行的全过程，直到进程撤消而撤消。 在linux中 PCB 用task_struct 数据结构来表示 PCB fs_struct1、与进程相关的文件首先，文件必须由进程打开，每个进程都有它自己当前的工作目录和它自己的根目录。task_struct的fs字段指向进程的fs_struct结构，files字段指向进程的files_struct结构。 1234567struct fs_struct &#123; atomic_t count; rwlock_t lock; int umask; struct dentry * root, * pwd, * altroot; struct vfsmount * rootmnt, * pwdmnt, * altrootmnt;&#125;; count：共享这个表的进程个数 lock：用于表中字段的读&#x2F;写自旋锁 umask：当打开文件设置文件权限时所使用的位掩码 root：根目录的目录项 pwd：当前工作目录的目录项 files_struct每个进程用一个 files_struct 结构来记录文件描述符的使用情况， 这个 files_struct结构称为用户打开文件表， 它是进程的私有数据。 files_struct 结构在include&#x2F;linux&#x2F;sched.h 中定义如下： 12345678910struct files_struct &#123; atomic_t count; struct fdtable *fdt; struct fdtable fdtab; int next_fd; struct embedded_fd_set close_on_exec_init; struct embedded_fd_set open_fds_init; struct file * fd_array[NR_OPEN_DEFAULT];&#125;; ulimitulimit命令可以查看当前shell下的文件描述符的数量。 ulimit 用于限制 shell 启动进程所占用的资源，支持以下各种类型的限制：所创建的内核文件的大小、进程数据块的大小、Shell 进程创建文件的大小、内存锁住的大小、常驻内存集的大小、打开文件描述符的数量、分配堆栈的最大大小、CPU 时间、单个用户的最大线程数、Shell 进程所能使用的最大虚拟内存。同时，它支持硬资源和软资源的限制。 作为临时限制，ulimit 可以作用于通过使用其命令登录的 shell 会话，在会话终止时便结束限制，并不影响于其他 shell 会话。而对于长期的固定限制，ulimit 命令语句又可以被添加到由登录 shell 读取的文件中，作用于特定的 shell 用户。 语法： 1234567891011121314151617ulimit (选项)选项：-a：显示目前资源限制的设定；-c ：设定core文件的最大值，单位为区块； -d &lt;数据节区大小&gt;：程序数据节区的最大值，单位为KB； -f &lt;文件大小&gt;：shell所能建立的最大文件，单位为区块； -H：设定资源的硬性限制，也就是管理员所设下的限制； -m &lt;内存大小&gt;：指定可使用内存的上限，单位为KB； -n &lt;文件数目&gt;：指定同一时间最多可开启的文件数； -p &lt;缓冲区大小&gt;：指定管道缓冲区的大小，单位512字节； -s &lt;堆叠大小&gt;：指定堆叠的上限，单位为KB； -S：设定资源的弹性限制； -t ：指定CPU使用时间的上限，单位为秒； -u &lt;程序数目&gt;：用户最多可开启的程序数目； -v &lt;虚拟内存大小&gt;：指定可使用的虚拟内存上限，单位为KB。实例： 来看一下具体的用法 123456789101112131415161718192021222324[root@Centos ~]# ulimit -acore file size (blocks, -c) 0 #core文件的最大值为100 blocks。data seg size (kbytes, -d) unlimited #进程的数据段可以任意大。scheduling priority (-e) 0file size (blocks, -f) unlimited #文件可以任意大。pending signals (-i) 3794 #最多有98304个待处理的信号。max locked memory (kbytes, -l) 64 #一个任务锁住的物理内存的最大值为32KB。max memory size (kbytes, -m) unlimited #一个任务的常驻物理内存的最大值。open files (-n) 1024 #一个任务最多可以同时打开1024的文件。pipe size (512 bytes, -p) 8 #管道的最大空间为4096字节。POSIX message queues (bytes, -q) 819200 #POSIX的消息队列的最大值为819200字节。real-time priority (-r) 0stack size (kbytes, -s) 10240 #进程的栈的最大值为10240字节。cpu time (seconds, -t) unlimited #进程使用的CPU时间。max user processes (-u) 1024 #当前用户同时打开的进程（包括线程）的最大个数为98304。virtual memory (kbytes, -v) unlimited #没有限制进程的最大地址空间。file locks (-x) unlimited #所能锁住的文件的最大个数没有限制。Linux默认的文件打开数是1024,现在设置打开数为2048.[root@Centos ~]# ulimit -n --查看打开数为10241024[root@Centos ~]# ulimit -n 2048 --设置打开数为2048[root@Centos ~]# ulimit -n --再次查看2048 特殊文件类型 Linux设备驱动程序工作原理 系统调用是操作系统内核和应用程序之间的接口，设备驱动程序是操作系统内核和机器硬件之间的接口。设备驱动程序为应用程序屏蔽了硬件的细节，这样在应用程序看来，硬件设备只是一个设备文件， 应用程序可以象操作普通文件一样对硬件设备进行操作。设备驱动程序是内核的一部分，运行在核心态，它完成以下的功能： 1.对设备初始化和释放.2.把数据从内核传送到硬件和从硬件读取数据.3.读取应用程序传送给设备文件的数据和回送应用程序请求的数据.4.检测和处理设备出现的错误. 在Linux操作系统下有三类主要的设备文件类型：字符设备、块设备和网络接口。字符设备和块设备的主要区别是：在对字符设备发出读&#x2F;写请求时，实际的硬件I&#x2F;O一般就紧接着发生了块设备则不然，它利用一块系统内存作缓冲区，当用户进程对设备请求能满足用户的要求，就返回请求的数据，如果不能，就调用请求函数来进行实际的I&#x2F;O操作。这也就是进程管理的Page cache的作用，块设备是主要针对磁盘等慢速设备设计的，以免耗费过多的CPU时间来等待。 换句话说， 当发生块设备的IO的时候， 操作系统实际是先写到Page cache上，而 pageCashe 会有一个映射规则，映射到某个块设备的具体地址，在发生操作以系统的IO的时候，比如说 写某个文件 当我们点击保存的时候，实际是写到了Page cache上， 此时操作系统将当前pageCash标记为脏页，之后如何将脏页刷新会磁盘就要看各个操作系统策略了。 字符设备、块设备每个设备文件都有其文件属性(c&#x2F;b)，表示是字符设备还是块设备, 另外每个文件都有两个设备号，第一个是主设备号，标识驱动程序，第二个是从设备号，标识使用同一个设备驱动程序的不同的硬件设备，比如有两个软盘，就可以用 从设备号来区分他们。设备文件的的主设备号必须与设备驱动程序在登记时申请的主设备号一致，否则用户进程将无法访问到驱动程序。 我可以通过 ls 来看一下 1234567891011121314151617181920212223命令：ls -alti输出详解：# =================== 10605 (inode 编号)brw-rw----. b 块设备， c 字符设备 1 root （所属用户）cdrom （用户组） 11,（主设备号） 0（次设备号） 10月 12 17:41 sr0 设备名称# =================== ex:10445 brw-rw----. 1 root disk 8, 1 10月 12 17:41 sda110446 brw-rw----. 1 root disk 8, 2 10月 12 17:41 sda210444 brw-rw----. 1 root disk 8, 0 10月 12 17:41 sda10449 brw-rw----. 1 root disk 8, 16 10月 12 17:41 sdb 8535 crw-------. 1 root root 247, 1 10月 12 17:41 usbmon110104 crw-------. 1 root root 246, 0 10月 12 17:41 hidraw0 8550 crw-------. 1 root root 247, 2 10月 12 17:41 usbmon2 文件操作的关键结构 由于用户进程是通过设备文件同硬件打交道，对设备文件的操作方式不外乎就是一些系统调用，如 open，read，write，close…， 注意，不是fopen， fread，但是如何把系统调用和驱动程序关联起来呢?这需要了解一个非常关键的数据结构 file_operations: 123456789101112131415struct file_operations &#123; int (*seek) (struct inode * ，struct file *， off_t ，int); int (*read) (struct inode * ，struct file *， char ，int); int (*write) (struct inode * ，struct file *， off_t ，int); int (*readdir) (struct inode * ，struct file *， struct dirent * ，int); int (*select) (struct inode * ，struct file *， int ，select_table *); int (*ioctl) (struct inode * ，struct file *， unsined int ，unsigned long); int (*mmap) (struct inode * ，struct file *， struct vm_area_struct *); int (*open) (struct inode * ，struct file *); int (*release) (struct inode * ，struct file *); int (*fsync) (struct inode * ，struct file *); int (*fasync) (struct inode * ，struct file *，int); int (*check_media_change) (struct inode * ，struct file *); int (*revalidate) (dev_t dev);&#125; 这个结构的每一个成员的名字都对应着一个系统调用。用户进程利用系统调用在对设备文件进行诸如read&#x2F;write操作时，系统调用通过设备文件的主设备号找到相应的设备驱动程序，然后读取这个数据结构相应的函数指针，接着把控制权交给该函数。这是linux的设备驱动程序工作的基本原理这里不再详细阐述。 链接文件软连接文件，详情请看上文 文件链接 管道文件 什么是管道？ 管道，英文为pipe。这是一个我们在学习Linux命令行的时候就会引入的一个很重要的概念。它的发明人是道格拉斯.麦克罗伊，这位也是UNIX上早期shell的发明人。他在发明了shell之后，发现系统操作执行命令的时候，经常有需求要将一个程序的输出交给另一个程序进行处理，这种操作可以使用输入输出重定向加文件搞定，比如： [lizhipeng@CentOS7 ~]$ ls /etc/ &gt; etc.txt [lizhipeng@CentOS7 ~]$ wc -l etc.txt 但是这样未免显得太麻烦了。所以，管道的概念应运而生。目前在任何一个shell中，都可以使用“|”连接两个命令，shell会将前后两个进程的输入输出用一个管道相连，以便达到进程间通信的目的： [lizhipeng@CentOS7 ~]$ ls -l /etc/ | wc -l 对比以上两种方法，我们也可以理解为，管道本质上就是一个文件，前面的进程以写方式打开文件，后面的进程以读方式打开。这样前面写完后面读，于是就实现了通信。实际上管道的设计也是遵循UNIX的“一切皆文件”设计原则的，它本质上就是一个文件。Linux系统直接把管道实现成了一种文件系统，借助VFS给应用程序提供操作接口。 虽然实现形态上是文件，但是管道本身并不占用磁盘或者其他外部存储的空间。在Linux的实现上，它占用的是内存空间。所以，Linux上的管道就是一个操作方式为文件的内存缓冲区。 Linux上的管道分两种类型： 匿名管道 命名管道 这两种管道也叫做有名或无名管道。匿名管道最常见的形态就是我们在shell操作中最常用的”|”。它的特点是只能在父子进程中使用，父进程在产生子进程前必须打开一个管道文件，然后fork产生子进程，这样子进程通过拷贝父进程的进程地址空间获得同一个管道文件的描述符，以达到使用同一个管道通信的目的。此时除了父子进程外，没人知道这个管道文件的描述符，所以通过这个管道中的信息无法传递给其他进程。这保证了传输数据的安全性，当然也降低了管道了通用性，于是系统还提供了命名管道。 我们可以使用mkfifo或mknod命令来创建一个命名管道，这跟创建一个文件没有什么区别： [lizhipeng@CentOS7 ~]$ mkfifo pip [lizhipeng@CentOS7 ~]$ ls prw-rw-r--. 1 lizhipeng lizhipeng 0 11月 17 13:24 pip 可以看到创建出来的文件类型比较特殊，是p类型。表示这是一个管道文件。有了这个管道文件，系统中就有了对一个管道的全局名称，于是任何两个不相关的进程都可以通过这个管道文件进行通信了。比如我们现在让一个进程写这个管道文件： [lizhipeng@CentOS7 ~]$ echo xxxxxxxxxxxxxx &gt; pip 此时这个写操作会阻塞，因为管道另一端没有人读。这是内核对管道文件定义的默认行为。此时如果有进程读这个管道，那么这个写操作的阻塞才会解除： [lizhipeng@CentOS7 ~]$ cat pip xxxxxxxxxxxxxx 大家可以观察到，当我们cat完这个文件之后，另一端的echo命令也返回了。这就是命名管道 接下来我们来看一下匿名管道，我们需要用到 shell 的代码块 命令如下 [lizhipeng@CentOS7 ~]$ &#123; echo $BASHPID; read x ; &#125; | &#123; cat ; echo $BASHPID; read y; &#125; 37057 {} 花括号的代码会先执行，遇到管道后，会开启另外一个进程，两个进程实现通讯。此时父进程输出了父进程的pid且阻塞在了read x 这个代码块中，此时我们可以通过结果拿到父进程的 pid 37057 我们通过pstree来验证一下我们的关系 [lizhipeng@CentOS7 ~]$ pstree -p ... ─sshd(36387)───bash(36388)─┬─bash(37057+ │ │ └─bash(37058+ │ └─sshd(36713)───sshd(36717)───bash(36718)───pstree(370+ ... 我们看到了 37057 进程生出了 37058的子进程。我们来看一下管道的文件描述符 [lizhipeng@CentOS7 fd]$ ls -alt /proc/37057/fd 总用量 0 lrwx------. 1 lizhipeng lizhipeng 64 11月 17 13:33 0 -&gt; /dev/pts/4 l-wx------. 1 lizhipeng lizhipeng 64 11月 17 13:33 1 -&gt; pipe:[253711] lrwx------. 1 lizhipeng lizhipeng 64 11月 17 13:33 2 -&gt; /dev/pts/4 lrwx------. 1 lizhipeng lizhipeng 64 11月 17 13:33 255 -&gt; /dev/pts/4 dr-x------. 2 lizhipeng lizhipeng 0 11月 17 13:33 . dr-xr-xr-x. 9 lizhipeng lizhipeng 0 11月 17 13:30 .. [lizhipeng@CentOS7 fd]$ ls -alt /proc/37058/fd 总用量 0 lr-x------. 1 lizhipeng lizhipeng 64 11月 17 13:34 0 -&gt; pipe:[253711] lrwx------. 1 lizhipeng lizhipeng 64 11月 17 13:34 1 -&gt; /dev/pts/4 lrwx------. 1 lizhipeng lizhipeng 64 11月 17 13:34 2 -&gt; /dev/pts/4 lrwx------. 1 lizhipeng lizhipeng 64 11月 17 13:34 255 -&gt; /dev/pts/4 dr-x------. 2 lizhipeng lizhipeng 0 11月 17 13:34 . dr-xr-xr-x. 9 lizhipeng lizhipeng 0 11月 17 13:30 .. 由此我们可以看到，37057 通过重定向 1 号文件描述符来讲管道 重定向到了 37058 的0号描述符。这就是匿名管道。"},{"path":"/2024/04/05/linux-docs/文件系统/Linux操作系统学习之文件系统/","content":"一. 前言 本节开始将分析Linux的文件系统。Linux一切皆文件的思想可谓众所周知，而其文件系统又是字符设备、块设备、管道、进程间通信、网络等等的必备知识，因此其重要性可想而知。本文将先介绍文件系统基础知识，然后介绍最重要的结构体inode以及构建于其上的一层层的文件系统。 二. 文件系统基础知识 一切设计均是为了实现需求，因此我们从文件系统需要的基本功能来看看其该如何设计。首先，一个文件系统需要有以下基本要求 文件需要让人易于读写，并避免名字冲突等 文件需要易于查找、整理归类 操作系统需要有文档记录功能以便管理 由此，文件系统设计了如下特性 采取树形结构、文件夹设计 对热点文件进行缓存，便于读写 采用索引结构，便于查找分类 维护一套数据结构用于记录哪些文档正在被哪些任务使用 依此基本设计，我们可以开始慢慢展开看看Linux博大而精神的文件系统。 三. inode结构体和文件系统3.1 块存储的表示 硬盘中我们以块为存储单元，而在文件系统中，我们需要有一个存储块信息的基本结构体，这就是文件系统的基石inode，其源码如下。inode意为index node，即索引节点。从这个数据结构中我们可以看出，inode 里面有文件的读写权限 i_mode，属于哪个用户 i_uid，哪个组 i_gid，大小是多少 i_size_lo，占用多少个块 i_blocks_lo。另外，这里面还有几个与文件相关的时间。i_atime 即 access time，是最近一次访问文件的时间；i_ctime 即 change time，是最近一次更改 inode 的时间；i_mtime 即 modify time，是最近一次更改文件的时间。 12345678910111213141516171819202122232425/* * Structure of an inode on the disk */struct ext4_inode &#123; __le16\ti_mode; /* File mode */ __le16\ti_uid; /* Low 16 bits of Owner Uid */ __le32\ti_size_lo;\t/* Size in bytes */ __le32\ti_atime;\t/* Access time */ __le32\ti_ctime;\t/* Inode Change time */ __le32\ti_mtime;\t/* Modification time */ __le32\ti_dtime;\t/* Deletion Time */ __le16\ti_gid; /* Low 16 bits of Group Id */ __le16\ti_links_count;\t/* Links count */ __le32\ti_blocks_lo;\t/* Blocks count */ __le32\ti_flags;\t/* File flags */...... __le32\ti_block[EXT4_N_BLOCKS];/* Pointers to blocks */......&#125;;#define EXT4_NDIR_BLOCKS 12#define EXT4_IND_BLOCK EXT4_NDIR_BLOCKS#define EXT4_DIND_BLOCK (EXT4_IND_BLOCK + 1)#define EXT4_TIND_BLOCK (EXT4_DIND_BLOCK + 1)#define EXT4_N_BLOCKS (EXT4_TIND_BLOCK + 1) 这里我们需要重点关注以下i_block，该成员变量实际存储了文件内容的每一个块。在ext2和ext3格式的文件系统中，我们用前12个块存放对应的文件数据，每个块4KB，如果文件较大放不下，则需要使用后面几个间接存储块来保存数据，下图很形象的表示了其存储原理。 该存储结构带来的问题是对于大型文件，我们需要多次调用才可以访问对应块的内容，因此访问速度较慢。为此，ext4提出了新的解决方案：Extents。简单的说，Extents以一个树形结构来连续存储文件块，从而提高访问速度，大致结构如下图所示。 主要结构体为节点ext4_extent_header，eh_entries 表示这个节点里面有多少项。这里的项分两种： 如果是叶子节点，这一项会直接指向硬盘上的连续块的地址，我们称为数据节点 ext4_extent； 如果是分支节点，这一项会指向下一层的分支节点或者叶子节点，我们称为索引节点 ext4_extent_idx。这两种类型的项的大小都是 12 个 byte。 如果文件不大，inode 里面的 i_block 中，可以放得下一个 ext4_extent_header 和 4 项 ext4_extent。所以这个时候，eh_depth 为 0，也即 inode 里面的就是叶子节点，树高度为 0。如果文件比较大，4 个 extent 放不下，就要分裂成为一棵树，eh_depth&gt;0 的节点就是索引节点，其中根节点深度最大，在 inode 中。最底层 eh_depth&#x3D;0 的是叶子节点。除了根节点，其他的节点都保存在一个块 4k 里面，4k 扣除 ext4_extent_header 的 12 个 byte，剩下的能够放 340 项，每个 extent 最大能表示 128MB 的数据，340 个 extent 会使你表示的文件达到 42.5GB。这已经非常大了，如果再大，我们可以增加树的深度。 123456789101112131415161718192021222324252627282930313233/* * Each block (leaves and indexes), even inode-stored has header. */struct ext4_extent_header &#123; __le16\teh_magic;\t/* probably will support different formats */ __le16\teh_entries;\t/* number of valid entries */ __le16\teh_max; /* capacity of store in entries */ __le16\teh_depth;\t/* has tree real underlying blocks? */ __le32\teh_generation;\t/* generation of the tree */&#125;;/* * This is the extent on-disk structure. * It&#x27;s used at the bottom of the tree. */struct ext4_extent &#123; __le32 ee_block; /* first logical block extent covers */ __le16 ee_len; /* number of blocks covered by extent */ __le16 ee_start_hi; /* high 16 bits of physical block */ __le32 ee_start_lo; /* low 32 bits of physical block */&#125;;/* * This is index on-disk structure. * It&#x27;s used at all the levels except the bottom. */struct ext4_extent_idx &#123; __le32 ei_block; /* index covers logical blocks from &#x27;block&#x27; */ __le32 ei_leaf_lo; /* pointer to the physical block of the next * * level. leaf or next index could be there */ __le16 ei_leaf_hi; /* high 16 bits of physical block */ __u16 ei_unused;&#125;; 由此，我们可以通过inode来表示一系列的块，从而构成了一个文件。在硬盘上，通过一系列的inode，我们可以存储大量的文件。但是我们尚需要一种方式去存储和管理inode，这就是位图。同样的，我们会用块位图去管理块的信息。如下所示为创建inode的过程中对位图的访问，我们需要找出下一个0位所在，即空闲inode的位置。 1234567891011121314struct inode *__ext4_new_inode(handle_t *handle, struct inode *dir, umode_t mode, const struct qstr *qstr, __u32 goal, uid_t *owner, __u32 i_flags, int handle_type, unsigned int line_no, int nblocks)&#123;...... inode_bitmap_bh = ext4_read_inode_bitmap(sb, group);...... ino = ext4_find_next_zero_bit((unsigned long *) inode_bitmap_bh-&gt;b_data, EXT4_INODES_PER_GROUP(sb), ino);......&#125; 3.2 文件系统的格式 inode和块是文件系统的最小组成单元，在此之上还有多级系统，大致有如下这些： 块组：存储一块数据的组成单元，数据结构为ext4_group_desc。这里面对于一个块组里的 inode 位图 bg_inode_bitmap_lo、块位图 bg_block_bitmap_lo、inode 列表 bg_inode_table_lo均有相应的定义。一个个块组，就基本构成了我们整个文件系统的结构。 块组描述符表：多个块组的描述符构成的表 超级块：对整个文件系统的情况进行描述，即ext4_super_block，存储全局信息，如整个文件系统一共有多少 inode：s_inodes_count；一共有多少块：s_blocks_count_lo，每个块组有多少 inode：s_inodes_per_group，每个块组有多少块：s_blocks_per_group 等。 引导块：对于整个文件系统，我们需要预留一块区域作为引导区用于操作系统的启动，所以第一个块组的前面要留 1K，用于启动引导区。 超级块和块组描述符表都是全局信息，而且这些数据很重要。如果这些数据丢失了，整个文件系统都打不开了，这比一个文件的一个块损坏更严重。所以，这两部分我们都需要备份，但是采取不同的策略。 默认策略：在每个块中均保存一份超级块和块组描述表的备份 sparse_super策略：采取稀疏存储的方式，仅在块组索引为 0、3、5、7 的整数幂里存储。 Meta Block Groups策略：我们将块组分为多个元块组（Meta Block Groups)，每个元块组里面的块组描述符表仅仅包括自己的内容，一个元块组包含 64 个块组，这样一个元块组中的块组描述符表最多 64 项。这种做法类似于merkle tree，可以在很大程度上优化空间。 3.3 目录的存储格式 为了便于文件的查找，我们必须要有索引，即文件目录。其实目录本身也是个文件，也有 inode。inode 里面也是指向一些块。和普通文件不同的是，普通文件的块里面保存的是文件数据，而目录文件的块里面保存的是目录里面一项一项的文件信息。这些信息我们称为 ext4_dir_entry。这里有两个版本，第二个版本 ext4_dir_entry_2 是将一个 16 位的 name_len，变成了一个 8 位的 name_len 和 8 位的 file_type。 12345678910111213struct ext4_dir_entry &#123; __le32 inode; /* Inode number */ __le16 rec_len; /* Directory entry length */ __le16 name_len; /* Name length */ char name[EXT4_NAME_LEN]; /* File name */&#125;;struct ext4_dir_entry_2 &#123; __le32 inode; /* Inode number */ __le16 rec_len; /* Directory entry length */ __u8 name_len; /* Name length */ __u8 file_type; char name[EXT4_NAME_LEN]; /* File name */&#125;; 在目录文件的块中，最简单的保存格式是列表，就是一项一项地将 ext4_dir_entry_2 列在哪里。每一项都会保存这个目录的下一级的文件的文件名和对应的 inode，通过这个 inode，就能找到真正的文件。第一项是“.”，表示当前目录，第二项是“…”，表示上一级目录，接下来就是一项一项的文件名和 inode。有时候，如果一个目录下面的文件太多的时候，我们想在这个目录下找一个文件，按照列表一个个去找太慢了，于是我们就添加了索引的模式。如果在 inode 中设置 EXT4_INDEX_FL 标志，则目录文件的块的组织形式将发生变化，变成了下面定义的这个样子： 1234567891011121314151617struct dx_root&#123; struct fake_dirent dot; char dot_name[4]; struct fake_dirent dotdot; char dotdot_name[4]; struct dx_root_info &#123; __le32 reserved_zero; u8 hash_version; u8 info_length; /* 8 */ u8 indirect_levels; u8 unused_flags; &#125; info; struct dx_entry entries[0];&#125;; 当前目录和上级目录不变，文件列表改用dx_root_info结构体，其中最重要的成员变量是 indirect_levels，表示间接索引的层数。索引项由结构体 dx_entry表示，本质上是文件名的哈希值和数据块的一个映射关系。 12345struct dx_entry&#123; __le32 hash; __le32 block;&#125;; 如果我们要查找一个目录下面的文件名，可以通过名称取哈希。如果哈希能够匹配上，就说明这个文件的信息在相应的块里面。然后打开这个块，如果里面不再是索引，而是索引树的叶子节点的话，那里面还是 ext4_dir_entry_2 的列表，我们只要一项一项找文件名就行。通过索引树，我们可以将一个目录下面的 N 多的文件分散到很多的块里面，可以很快地进行查找。 3.4 软链接和硬链接的存储格式 软链接和硬链接也是文件的一种，可以通过如下命令创建。ln -s 创建的是软链接，不带 -s 创建的是硬链接。 ln [参数][源文件或目录][目标文件或目录] 硬链接与原始文件共用一个 inode ，但是 inode 是不跨文件系统的，每个文件系统都有自己的 inode 列表，因而硬链接是没有办法跨文件系统的。而软链接不同，软链接相当于重新创建了一个文件。这个文件也有独立的 inode，只不过打开这个文件看里面内容的时候，内容指向另外的一个文件。这就很灵活了。我们可以跨文件系统，甚至目标文件被删除了链接文件也依然存在，只不过指向的文件找不到了而已。 四. 总结 本文主要从文件系统的设计角度出发，逐步分析了inode和基于inode的ext4文件系统结构和主要组成部分，下面引用极客时间中的一张图作为总结。"},{"path":"/2024/04/05/linux-docs/文件系统/Linux文件系统详解/","content":"Linux的一切皆文件Linux 中的各种事物比如像文档、目录（Mac OS X 和 Windows 系统下称之为文件夹）、键盘、监视器、硬盘、可移动媒体设备、打印机、调制解调器、虚拟终端，还有进程间通信（IPC）和网络通信等输入&#x2F;输出资源都是定义在文件系统空间下的字节流。一切都可看作是文件，其最显著的好处是对于上面所列出的输入&#x2F;输出资源，只需要相同的一套 Linux 工具、实用程序和 API。你可以使用同一套api(read, write)和工具(cat , 重定向, 管道)来处理unix中大多数的资源.设计一个系统的终极目标往往就是要找到原子操作，一旦锁定了原子操作，设计工作就会变得简单而有序。“文件”作为一个抽象概念，其原子操作非常简单，只有读和写，这无疑是一个非常好的模型。通过这个模型，API的设计可以化繁为简，用户可以使用通用的方式去访问任何资源，自有相应的中间件做好对底层的适配。现代操作系统为解决信息能独立于进程之外被长期存储引入了文件，文件作为进程创建信息的逻辑单元可被多个进程并发使用。在 UNIX 系统中，操作系统为磁盘上的文本与图像、鼠标与键盘等输入设备及网络交互等 I&#x2F;O 操作设计了一组通用 API，使他们被处理时均可统一使用字节流方式。换言之，UNIX 系统中除进程之外的一切皆是文件，而 Linux 保持了这一特性。为了便于文件的管理，Linux 还引入了目录（有时亦被称为文件夹）这一概念。目录使文件可被分类管理，且目录的引入使 Linux 的文件系统形成一个层级结构的目录树 在Linux系统中，一切都是文件，理解文件系统，对于学习Linux来说，是一个非常有必要的前提 Linux上的文件系统一般来说就是EXT2或EXT3，但这篇文章并不准备一上来就直接讲它们，而希望结合Linux操作系统并从文件系统建立的基础——硬盘开始，一步步认识Linux的文件系统。 1. 机械硬盘的物理存储机制 现代计算机大部分文件存储功能都是由机械硬盘这种设备提供的。（现在的SSD和闪存从概念和逻辑上都部分继承自机械硬盘，所以使用机械硬盘来进行理解也是没有问题的） 机械硬盘能实现信息存储的功能基于：磁性存储介质能够被磁化，且磁化后会长久保留被磁化的状态，这种被磁化状态能够被读取出来，同时这种磁化状态还能够不断被修改，磁化正好有两个方向，所以可以表示0和1。于是硬盘就是把这种磁性存储介质做成一个个盘片，每一个盘片上都分布着数量巨大的磁性存储单位，使用磁性读写头对盘片进行写入和读取（从原理上类似黑胶唱片的播放）。 一个硬盘中的磁性存储单位数以亿计（1T硬盘就有约80亿个），所以需要一套规则来规划信息如何存取（比如一本存储信息的书我们还会分为页，每一页从上到下从左到右读取，同时还有章节目录）于是就有了这些物理、逻辑概念：1、一个硬盘有多张盘片叠成，不同盘片有编号2、每张盘片上的存储颗粒成环形一圈圈地排布，每一圈称为磁道，有编号3、每条磁道上都有一圈存储颗粒，每512*8（512字节，0.5KB）个存储颗粒作为一个扇区，扇区是硬盘上存储的最小物理单位4、N个扇区可以组成簇，N取决于不同的文件系统或是文件系统的配置，簇是此文件系统中的最小存储单位5、所有盘面上的同一磁道构成一个圆柱，称为柱面，柱面是系统分区的最小单位磁头读写文件的时候，首先是分区读写的，由inode编号（区内唯一的编号后面介绍）找到对应的磁道和扇区，然后一个柱面一个柱面地进行读写。机械硬盘的读写控制系统是一个令人叹为观止的精密工程（一个盘面上有几亿个存储单位，每个磁道宽度不到几十纳米，磁盘每分钟上万转），同时关于读写的逻辑也是有诸多细节（比如扇区的编号并不是连续的），非常有意思，可以自行搜索文章拓展阅读。有了硬盘并不意味着LInux可以立刻把它用来存储，还需要组合进Linux的文件体系才能被Linux使用。 2.Linux文件体系Linux以文件的形式对计算机中的数据和硬件资源进行管理，也就是彻底的一切皆文件，反映在Linux的文件类型上就是：普通文件、目录文件（也就是文件夹）、设备文件、链接文件、管道文件、套接字文件（数据通信的接口）等等。而这些种类繁多的文件被Linux使用目录树进行管理， 所谓的目录树就是以根目录（&#x2F;）为主，向下呈现分支状的一种文件结构。不同于纯粹的ext2之类的文件系统，我把它称为文件体系，一切皆文件和文件目录树的资源管理方式一起构成了Linux的文件体系，让Linux操作系统可以方便使用系统资源。所以文件系统比文件体系涵盖的内容少很多，Linux文件体系主要在于把操作系统相关的东西用文件这个载体实现：文件系统挂载在操作系统上，操作系统整个系统又放在文件系统里。但本文中文件体系的相关内容不是很多，大部分地方都可以用文件系统代替文件体系。 1. Linux中的文件类型:1.1. 普通文件（-）从Linux的角度来说，类似mp4、pdf、html这样应用层面上的文件类型都属于普通文件Linux用户可以根据访问权限对普通文件进行查看、更改和删除 1.2. 目录文件（d，directory file）目录文件对于用惯Windows的用户来说不太容易理解，目录也是文件的一种目录文件包含了各自目录下的文件名和指向这些文件的指针，打开目录事实上就是打开目录文件，只要有访问权限，你就可以随意访问这些目录下的文件（普通文件的执行权限就是目录文件的访问权限），但是只有内核的进程能够修改它们虽然不能修改，但是我们能够通过vim去查看目录文件的内容 1.3. 符号链接（l，symbolic link）这种类型的文件类似Windows中的快捷方式，是指向另一个文件的间接指针，也就是我们常说的软链接 1.4. 块设备文件（b，block）和字符设备文件（c，char）这些文件一般隐藏在&#x2F;dev目录下，在进行设备读取和外设交互时会被使用到比如磁盘光驱就是块设备文件，串口设备则属于字符设备文件系统中的所有设备要么是块设备文件，要么是字符设备文件，无一例外 1.5. FIFO（p，pipe）管道文件主要用于进程间通讯。比如使用mkfifo命令可以创建一个FIFO文件，启用一个进程A从FIFO文件里读数据，启动进程B往FIFO里写数据，先进先出，随写随读。 1.6. 套接字（s，socket）用于进程间的网络通信，也可以用于本机之间的非网络通信这些文件一般隐藏在&#x2F;var&#x2F;run目录下，证明着相关进程的存在Linux 的文件是没有所谓的扩展名的，一个 Linux文件能不能被执行与它是否可执行的属性有关，只要你的权限中有 x ，比如[ -rwx-r-xr-x ] 就代表这个文件可以被执行，与文件名没有关系。跟在 Windows下能被执行的文件扩展名通常是 .com .exe .bat 等不同。不过，可以被执行跟可以执行成功不一样。比如在 root 主目彔下的 install.log 是一个文本文件，修改权限成为 -rwxrwxrwx 后这个文件能够真的执行成功吗？ 当然不行，因为它的内容根本就没有可以执行的数据。所以说，这个 x 代表这个文件具有可执行的能力， 但是能不能执行成功，当然就得要看该文件的内容了。虽然如此，不过我们仍然希望能从扩展名来了解该文件是什么东西，所以一般我们还是会以适当的扩展名来表示该文件是什么种类的。所以Linux 系统上的文件名真的只是让你了解该文件可能的用途而已， 真正的执行与否仍然需要权限的规范才行。比如常见的&#x2F;bin&#x2F;ls 这个显示文件属性的指令要是权限被修改为无法执行，那么ls 就变成不能执行了。这种问题最常发生在文件传送的过程中。例如你在网络上下载一个可执行文件，但是偏偏在你的 Linux 系统中就是无法执行，那就可能是档案的属性被改变了。而且从网络上传送到你 的 Linux 系统中，文件的属性权限确实是会被改变的 2. Linux目录树对Linux系统和用户来说，所有可操作的计算机资源都存在于目录树这个逻辑结构中，对计算机资源的访问都可以认为是目录树的访问。就硬盘来说，所有对硬盘的访问都变成了对目录树中某个节点也就是文件夹的访问，访问时不需要知道它是硬盘还是硬盘中的文件夹。目录树的逻辑结构也非常简单，就是从根目录（&#x2F;）开始，不断向下展开各级子目录。 3.硬盘分区硬盘分区是硬盘结合到文件体系的第一步，本质是「硬盘」这个物理概念转换成「区」这个逻辑概念，为下一步格式化做准备。所以分本身并不是必须的，你完全可以把一整块硬盘作为一个区。但从数据的安全性以及系统性能角度来看，分区还是有很多用处的，所以一般都会对硬盘进行分区。 讲分区就不得不先提每块硬盘上最重要的第一扇区，这个扇区中有硬盘主引导记录(Master boot record, MBR) 及分区表(partition table)， 其中 MBR 占有 446 bytes，而分区表占有 64 bytes。硬盘主引导记录放有最基本的引导加载程序，是系统开机启动的关键环节，在附录中有更详细的说明。而分区表则跟分区有关，它记录了硬盘分区的相关信息，但因分区表仅有 64bytes ， 所以最多只能记彔四块分区（分区本身其实就是对分区表进行设置）。 只能分四个区实在太少了，于是就有了扩展分区的概念，既然第一个扇区所在的分区表只能记录四条数据， 那我可否利用额外的扇区来记录更多的分区信息。把普通可以访问的分区称为主分区，扩展分区不同于主分区，它本身并没有内容，它是为进一步逻辑分区提供空间的。在某块分区指定为扩展分区后，就可以对这块扩展分区进一步分成多个逻辑分区。操作系统规定： 1、四块分区每块都可以是主分区或扩展分区2、扩展分区最多只能有一个（也没必要有多个）3、扩展分区可以进一步分割为多个逻辑分区4、扩展分区只是逻辑概念，本身不能被访问，也就是不能被格式化后作为数据访问的分区，能够作为数据访问的分区只有主分区和逻辑分区5、逻辑分区的数量依操作系统而不同，在 Linux 系统中，IDE 硬盘最多有 59 个逻辑分区(5 号到 63 号)， SATA 硬盘则有 11 个逻辑分区(5 号到 15 号) 一般给硬盘进行分区时，一个主分区一个扩展分区，然后把扩展分区划分为N个逻辑分区是最好的 是否可以不要主分区呢？不知道，但好像不用管，你创建分区的时候会自动给你配置类型特殊的，你最好单独分一个swap区（内存置换空间），它独为一类，功能是：当有数据被存放在物理内存里面，但是这些数据又不是常被 CPU 所取用时，那么这些不常被使用的程序将会被丢到硬盘的 swap 置换空间当中， 而将速度较快的物理内存空间释放出来给真正需要的程序使用 4.格式化我们知道Linux操作系统支持很多不同的文件系统，比如ext2、ext3、XFS、FAT等等，而Linux把对不同文件系统的访问交给了VFS（虚拟文件系统），VFS能访问和管理各种不同的文件系统。所以有了区之后就需要把它格式化成具体的文件系统以便VFS访问。 标准的Linux文件系统Ext2是使用「基于inode的文件系统」 1、我们知道一般操作系统的文件数据除了文件实际内容外， 还带有很多属性，例如 Linux 操作系统的文件权限(rwx)与文件属性(拥有者、群组、 时间参数等)，文件系统通常会将属性和实际内容这两部分数据分别存放在不同的区块2、在基于inode的文件系统中，权限与属性放置到 inode 中，实际数据放到 data block 区块中，而且inode和data block都有编号 Ext2 文件系统在此基础上 1、文件系统最前面有一个启动扇区(boot sector) 这个启动扇区可以安装开机管理程序， 这个设计让我们能将不同的引导装载程序安装到个别的文件系统前端，而不用覆盖整个硬盘唯一的MBR， 也就是这样才能实现多重引导的功能 2、把每个区进一步分为多个块组 (block group)，每个块组有独立的inode&#x2F;block体系 如果文件系统高达数百 GB 时，把所有的 inode 和block 通通放在一起会因为 inode 和 block的数量太庞大，不容易管理 这其实很好理解，因为分区是用户的分区，实际计算机管理时还有个最适合的大小，于是计算机会进一步的在分区中分块 （但这样岂不是可能出现大文件放不了的问题？有什么机制善后吗？） 3、每个块组实际还会分为分为6个部分，除了inode table 和 data block外还有4个附属模块，起到优化和完善系统性能的作用 所以整个分区大概会这样划分： inode table1.主要记录文件的属性以及该文件实际数据是放置在哪些block中，它记录的信息至少有这些： 大小、真正内容的block号码（一个或多个） 访问模式(read&#x2F;write&#x2F;excute) 拥有者与群组(owner&#x2F;group) 各种时间：建立或状态改变的时间、最近一次的读取时间、最近修改的时间 没有文件名！文件名在目录的block中！ 2、一个文件占用一个 inode，每个inode有编号3、Linux 系统存在 inode 号被用完但磁盘空间还有剩余的情况4、注意，这里的文件不单单是普通文件，目录文件也就是文件夹其实也是一个文件，还有其他的也是5、inode 的数量与大小在格式化时就已经固定了，每个inode 大小均固定为128 bytes (新的ext4 与xfs 可设定到256 bytes)6、文件系统能够建立的文件数量与inode 的数量有关，存在空间还够但inode不够的情况7、系统读取文件时需要先找到inode，并分析inode 所记录的权限与使用者是否符合，若符合才能够开始实际读取 block 的内容8、inode 要记录的资料非常多，但偏偏又只有128bytes ， 而inode 记录一个block 号码要花掉4byte ，假设我一个文件有400MB 且每个block 为4K 时， 那么至少也要十万条block 号码的记录！inode 哪有这么多空间来存储？为此我们的系统很聪明的将inode 记录block 号码的区域定义为12个直接，一个间接, 一个双间接与一个三间接记录区（详细见附录） data block1、放置文件内容数据的地方2、在格式化时block的大小就固定了，且每个block都有编号，以方便inode的记录 原则上，block 的大小与数量在格式化完就不能够再改变了(除非重新格式化) 3、在Ext2文件系统中所支持的block大小有1K, 2K及4K三种，由于block大小的区别，会导致该文件系统能够支持的最大磁盘容量与最大单一文件容量各不相同： Block 大小 1KB 2KB 4KB 最大单一档案限制 16GB 256GB 2TB 最大档案系统总容量 2TB 8TB 16TB 4、每个block 内最多只能够放置一个文件的资料，但一个文件可以放在多个block中（大的话）5、若文件小于block ，则该block 的剩余容量就不能够再被使用了(磁盘空间会浪费) 所以如果你的档案都非常小，但是你的block 在格式化时却选用最大的4K 时，可能会产生容量的浪费 既然大的block 可能会产生较严重的磁碟容量浪费，那么我们是否就将block 大小定为1K ？这也不妥，因为如果block 较小的话，那么大型档案将会占用数量更多的block ，而inode 也要记录更多的block 号码，此时将可能导致档案系统不良的读写效能 事实上现在的磁盘容量都太大了，所以一般都会选择4K 的block 大小 superblock1、记录整个文件系统相关信息的地方，一般大小为1024bytes，记录的信息主要有： block 与inode 的总量 未使用与已使用的inode &#x2F; block 数量 一个valid bit 数值，若此文件系统已被挂载，则valid bit 为0 ，若未被挂载，则valid bit 为1 block 与inode 的大小 (block 为1, 2, 4K，inode 为128bytes 或256bytes)； 其他各种文件系统相关信息：filesystem 的挂载时间、最近一次写入资料的时间、最近一次检验磁碟(fsck) 的时间 2、Superblock是非常重要的， 没有Superblock ，就没有这个文件系统了，因此如果superblock死掉了，你的文件系统可能就需要花费很多时间去挽救3、每个块都可能含有superblock，但是我们也说一个文件系统应该仅有一个superblock 而已，那是怎么回事？事实上除了第一个块内会含有superblock 之外，后续的块不一定含有superblock，而若含有superblock则该superblock主要是做为第一个块内superblock的备份，这样可以进行superblock的救援 Filesystem Description1、文件系统描述2、这个区段可以描述每个block group的开始与结束的block号码，以及说明每个区段(superblock, bitmap, inodemap, data block)分别介于哪一个block号码之间 block bitmap1、块对照表2、如果你想要新增文件时要使用哪个block 来记录呢？当然是选择「空的block」来记录。那你怎么知道哪个block 是空的？这就得要通过block bitmap了，它会记录哪些block是空的，因此我们的系统就能够很快速的找到可使用的空间来记录3、同样在你删除某些文件时，那些文件原本占用的block号码就得要释放出来， 此时在block bitmap 中对应该block号码的标志位就得要修改成为「未使用中」 inode bitmap1、与block bitmap 是类似的功能，只是block bitmap 记录的是使用与未使用的block 号码， 至于inode bitmap 则是记录使用与未使用的inode 号码 5.挂载在一个区被格式化为一个文件系统之后，它就可以被Linux操作系统使用了，只是这个时候Linux操作系统还找不到它，所以我们还需要把这个文件系统「注册」进Linux操作系统的文件体系里，这个操作就叫「挂载」 (mount)。挂载是利用一个目录当成进入点（类似选一个现成的目录作为代理），将文件系统放置在该目录下，也就是说，进入该目录就可以读取该文件系统的内容，类似整个文件系统只是目录树的一个文件夹（目录）。这个进入点的目录我们称为「挂载点」。 由于整个 Linux 系统最重要的是根目录，因此根目录一定需要挂载到某个分区。 而其他的目录则可依用户自己的需求来给予挂载到不同的分去。 到这里Linux的文件体系的构建过程其实已经大体讲完了，总结一下就是：硬盘经过分区和格式化，每个区都成为了一个文件系统，挂载这个文件系统后就可以让Linux操作系统通过VFS访问硬盘时跟访问一个普通文件夹一样。这里通过一个在目录树中读取文件的实际例子来细讲一下目录文件和普通文件。 6.目录树的读取过程首先我们要知道 1、每个文件（不管是一般文件还是目录文件）都会占用一个inode2、依据文件内容的大小来分配一个或多个block给该文件使用3、创建一个文件后，文件完整信息分布在3处地方，生成2个新文件： 文件名记录在该文件所在目录的目录文件的block中，没有新文件生成 文件属性、权限信息、记录具体内容的block编号记录在inode中，inode是新生成文件 文件具体内存记录在block中，block是新生成文件 4、因为文件名的记录是在目录的block当中，「新增&#x2F;删除&#x2F;更名文件名」与目录的w权限有关所以在Linux&#x2F;Unix中，文件名称只是文件的一个属性，叫别名也好，叫绰号也罢，仅为了方便用户记忆和使用，但系统内部并不需要用文件名来定为文件位置，这样处理最直观的好处就是，你可以对正在使用的文件改名，换目录，甚至放到废纸篓，都不会影响当前文件的使用，这在Windows里是无法想象的。比如你打开个Word文件，然后对其进行重命名操作，Windows会告诉你门儿都没有，关闭文件先！但在Mac里就毫无压力，因为Mac的操作系统同样采用了inode的设计。 创建文件过程当在ext2下建立一个一般文件时， ext2 会分配一个inode 与相对于该文件大小的block 数量给该文件 例如：假设我的一个block 为4 Kbytes ，而我要建立一个100 KBytes 的文件，那么linux 将分配一个inode 与25 个block 来储存该文件 但同时请注意，由于inode 仅有12 个直接指向，因此还要多一个block 来作为区块号码的记录 创建目录过程当在ext2文件系统建立一个目录时（就是新建了一个目录文件），文件系统会分配一个inode与至少一块block给该目录 inode记录该目录的相关权限与属性，并记录分配到的那块block号码 而block则是记录在这个目录下的文件名与该文件对应的inode号 block中还会自动生成两条记录，一条是.文件夹记录，inode指向自身，另一条是..文件夹记录，inode指向父文件夹 从目录树中读取某个文件过程 因为文件名是记录在目录的block当中，因此当我们要读取某个文件时，就一定会经过目录的inode与block ，然后才能够找到那个待读取文件的inode号码，最终才会读到正确的文件的block内的资料。 由于目录树是由根目录开始，因此操作系统先通过挂载信息找到挂载点的inode号，由此得到根目录的inode内容，并依据该inode读取根目录的block信息，再一层一层的往下读到正确的文件。举例来说，如果我想要读取&#x2F;etc&#x2F;passwd 这个文件时，系统是如何读取的呢？先看一下这个文件以及有关路径文件夹的信息： 12341$ ll -di / /etc /etc/passwd2 128 dr-xr-x r-x . 17 root root 4096 May 4 17:56 /333595521 drwxr-x r-x . 131 root root 8192 Jun 17 00:20 /etc436628004 -rw-r-- r-- . 1 root root 2092 Jun 17 00:20 /etc/passwd 于是该文件的读取流程为： 1、&#x2F;的inode：通过挂载点的信息找到inode号码为128的根目录inode，且inode规定的权限让我们可以读取该block的内容(有r与x)2、&#x2F;的block：经过上个步骤取得block的号码，并找到该内容有etc&#x2F;目录的inode号码(33595521)3、etc&#x2F;的inode：读取33595521号inode得知具有r与x的权限，因此可以读取etc&#x2F;的block内容4、etc&#x2F;的block：经过上个步骤取得block号码，并找到该内容有passwd文件的inode号码(36628004)5、passwd的inode：读取36628004号inode得知具有r的权限，因此可以读取passwd的block内容6、passwd的block：最后将该block内容的资料读出来"},{"path":"/2024/04/05/linux-docs/文件系统/磁盘IO那些事/","content":"背景计算机硬件性能在过去十年间的发展普遍遵循摩尔定律，通用计算机的CPU主频早已超过3GHz，内存也进入了普及DDR4的时代。然而传统硬盘虽然在存储容量上增长迅速，但是在读写性能上并无明显提升，同时SSD硬盘价格高昂，不能在短时间内完全替代传统硬盘。传统磁盘的I&#x2F;O读写速度成为了计算机系统性能提高的瓶颈，制约了计算机整体性能的发展。 硬盘性能的制约因素是什么？如何根据磁盘I&#x2F;O特性来进行系统设计？针对这些问题，本文将介绍硬盘的物理结构和性能指标，以及操作系统针对磁盘性能所做的优化，最后讨论下基于磁盘I&#x2F;O特性设计的技巧。 硬盘的物理结构硬盘内部主要部件为磁盘盘片、传动手臂、读写磁头和主轴马达。实际数据都是写在盘片上，读写主要是通过传动手臂上的读写磁头来完成。实际运行时，主轴让磁盘盘片转动，然后传动手臂可伸展让读取头在盘片上进行读写操作。磁盘物理结构如下图所示： 由于单一盘片容量有限，一般硬盘都有两张以上的盘片，每个盘片有两面，都可记录信息，所以一张盘片对应着两个磁头。盘片被分为许多扇形的区域，每个区域叫一个扇区，硬盘中每个扇区的大小固定为512字节。盘片表面上以盘片中心为圆心，不同半径的同心圆称为磁道，不同盘片相同半径的磁道所组成的圆柱称为柱面。磁道与柱面都是表示不同半径的圆，在许多场合，磁道和柱面可以互换使用。磁盘盘片垂直视角如下图所示： 早期的硬盘每磁道扇区数相同，此时由磁盘基本参数可以计算出硬盘的容量：存储容量&#x3D;磁头数磁道（柱面）数每道扇区数*每扇区字节数。由于每磁道扇区数相同，外圈磁道半径大，里圈磁道半径小，外圈和里圈扇区面积自然会不一样。同时，为了更好的读取数据，即使外圈扇区面积再大也只能和内圈扇区一样存放相同的字节数（512字节）。这样一来，外圈的记录密度就要比内圈小，会浪费大量的存储空间。 如今的硬盘都使用ZBR（Zoned Bit Recording，区位记录）技术，盘片表面由里向外划分为数个区域，不同区域的磁道扇区数目不同，同一区域内各磁道扇区数相同，盘片外圈区域磁道长扇区数目较多，内圈区域磁道短扇区数目较少，大体实现了等密度，从而获得了更多的存储空间。此时，由于每磁道扇区数各不相同，所以传统的容量计算公式就不再适用。实际上如今的硬盘大多使用LBA（Logical Block Addressing）逻辑块寻址模式，知道LBA后即可计算出硬盘容量。 影响硬盘性能的因素影响磁盘的关键因素是磁盘服务时间，即磁盘完成一个I&#x2F;O请求所花费的时间，它由寻道时间、旋转延迟和数据传输时间三部分构成。 1. 寻道时间Tseek是指将读写磁头移动至正确的磁道上所需要的时间。寻道时间越短，I&#x2F;O操作越快，目前磁盘的平均寻道时间一般在3-15ms。 2. 旋转延迟Trotation是指盘片旋转将请求数据所在的扇区移动到读写磁盘下方所需要的时间。旋转延迟取决于磁盘转速，通常用磁盘旋转一周所需时间的1&#x2F;2表示。比如：7200rpm的磁盘平均旋转延迟大约为60*1000&#x2F;7200&#x2F;2 &#x3D; 4.17ms，而转速为15000rpm的磁盘其平均旋转延迟为2ms。 3. 数据传输时间Ttransfer是指完成传输所请求的数据所需要的时间，它取决于数据传输率，其值等于数据大小除以数据传输率。目前IDE&#x2F;ATA能达到133MB&#x2F;s，SATA II可达到300MB&#x2F;s的接口数据传输率，数据传输时间通常远小于前两部分消耗时间。简单计算时可忽略。 衡量性能的指标机械硬盘的连续读写性能很好，但随机读写性能很差，这主要是因为磁头移动到正确的磁道上需要时间，随机读写时，磁头需要不停的移动，时间都浪费在了磁头寻址上，所以性能不高。衡量磁盘的重要主要指标是IOPS和吞吐量。 1. IOPSIOPS（Input&#x2F;Output Per Second）即每秒的输入输出量（或读写次数），即指每秒内系统能处理的I&#x2F;O请求数量。随机读写频繁的应用，如小文件存储等，关注随机读写性能，IOPS是关键衡量指标。可以推算出磁盘的IOPS &#x3D; 1000ms &#x2F; (Tseek + Trotation + Transfer)，如果忽略数据传输时间，理论上可以计算出随机读写最大的IOPS。常见磁盘的随机读写最大IOPS为： 7200rpm的磁盘 IOPS &#x3D; 76 IOPS 10000rpm的磁盘IOPS &#x3D; 111 IOPS 15000rpm的磁盘IOPS &#x3D; 166 IOPS 2. 吞吐量吞吐量（Throughput），指单位时间内可以成功传输的数据数量。顺序读写频繁的应用，如视频点播，关注连续读写性能、数据吞吐量是关键衡量指标。它主要取决于磁盘阵列的架构，通道的大小以及磁盘的个数。不同的磁盘阵列存在不同的架构，但他们都有自己的内部带宽，一般情况下，内部带宽都设计足够充足，不会存在瓶颈。磁盘阵列与服务器之间的数据通道对吞吐量影响很大，比如一个2Gbps的光纤通道，其所能支撑的最大流量仅为250MB&#x2F;s。最后，当前面的瓶颈都不再存在时，硬盘越多的情况下吞吐量越大。 操作系统层的优化虽然15000rpm的磁盘计算出的理论最大IOPS仅为166，但在实际运行环境中，实际磁盘的IOPS往往能够突破200甚至更高。这其实就是在系统调用过程中，操作系统进行了一系列的优化。 那么操作系统是如何操作硬盘的呢？类似于网络的分层结构，下图显示了Linux系统中对于磁盘的一次读请求在核心空间中所要经历的层次模型。从图中看出：对于磁盘的一次读请求，首先经过虚拟文件系统层（VFS Layer），其次是具体的文件系统层（例如Ext2），接下来是Cache层（Page Cache Layer）、通用块层（Generic Block Layer）、I&#x2F;O调度层（I&#x2F;O Scheduler Layer）、块设备驱动层（Block Device Driver Layer），最后是物理块设备层（Block Device Layer）。 虚拟文件系统层（VFS Layer）VFS（Virtual File System）虚拟文件系统是一种软件机制，更确切的说扮演着文件系统管理者的角色，与它相关的数据结构只存在于物理内存当中。它的作用是：屏蔽下层具体文件系统操作的差异，为上层的操作提供一个统一的接口。正是因为有了这个层次，Linux中允许众多不同的文件系统共存并且对文件的操作可以跨文件系统而执行。 VFS中包含着向物理文件系统转换的一系列数据结构，如VFS超级块、VFS的Inode、各种操作函数的转换入口等。Linux中VFS依靠四个主要的数据结构来描述其结构信息，分别为超级块、索引结点、目录项和文件对象。 1、超级块（Super Block）：超级块对象表示一个文件系统。它存储一个已安装的文件系统的控制信息，包括文件系统名称（比如Ext2）、文件系统的大小和状态、块设备的引用和元数据信息（比如空闲列表等等）。VFS超级块存在于内存中，它在文件系统安装时建立，并且在文件系统卸载时自动删除。同时需要注意的是对于每个具体的文件系统来说，也有各自的超级块，它们存放于磁盘。 2、索引结点（Inode）：索引结点对象存储了文件的相关元数据信息，例如：文件大小、设备标识符、用户标识符、用户组标识符等等。Inode分为两种：一种是VFS的Inode，一种是具体文件系统的Inode。前者在内存中，后者在磁盘中。所以每次其实是将磁盘中的Inode调进填充内存中的Inode，这样才是算使用了磁盘文件Inode。当创建一个文件的时候，就给文件分配了一个Inode。一个Inode只对应一个实际文件，一个文件也会只有一个Inode。 3、目录项（Dentry）：引入目录项对象的概念主要是出于方便查找文件的目的。不同于前面的两个对象，目录项对象没有对应的磁盘数据结构，只存在于内存中。一个路径的各个组成部分，不管是目录还是普通的文件，都是一个目录项对象。如，在路径&#x2F;home&#x2F;source&#x2F;test.java中，目录 &#x2F;, home, source和文件 test.java都对应一个目录项对象。VFS在查找的时候，根据一层一层的目录项找到对应的每个目录项的Inode，那么沿着目录项进行操作就可以找到最终的文件。 4、文件对象（File）：文件对象描述的是进程已经打开的文件。因为一个文件可以被多个进程打开，所以一个文件可以存在多个文件对象。一个文件对应的文件对象可能不是惟一的，但是其对应的索引节点和目录项对象肯定是惟一的。 Ext2文件系统VFS的下一层即是具体的文件系统，本节简要介绍下Linux的Ext2文件系统。 一个文件系统一般使用块设备上一个独立的逻辑分区。对于Ext2文件系统来说，硬盘分区首先被划分为一个个的Block，一个Ext2文件系统上的每个Block都是一样大小的。但是不同Ext2文件系统，Block大小可能不同，这是在创建Ext2系统决定的，一般为1k或者4k。由于Block数量很多，为了方便管理，Ext2将这些Block聚集在一起分为几个大的块组（Block Group），每个块组包含的等量的物理块，在块组的数据块中存储文件或目录。Ext2文件系统存储结构如下图所示： Ext2中的Super Block和Inode Table分别对应VFS中的超级块和索引结点，存放在磁盘。每个块组都有一个块组描述符GDT（Group Descriptor Table），存储一个块组的描述信息，例如在这个块组中从哪里开始是Inode表，从哪里开始是数据块等等。Block Bitmap和Inode Bitmap分别表示Block和Inode是否空闲可用。Data Block数据块是用来真正存储文件内容数据的地方，下面我们看一下具体的存储规则。 在Ext2文件系统中所支持的Block大小有1K、2K、4K三种。在格式化时Block的大小就固定了，且每个Block都有编号，方便Inode的记录。每个Block内最多只能够放置一个文件的数据，如果文件大于Block的大小，则一个文件会占用多个Block；如果文件小于Block，则该Block的剩余容量就不能够再被使用了，即磁盘空间会浪费。下面看看Inode和Block的对应关系。 Inode要记录的数据非常多，但大小仅为固定的128字节，同时记录一个Block号码就需要4字节，假设一个文件有400MB且每个Block为4K时，那么至少也要十万笔Block号码的记录。Inode不可能有这么多的记录信息，因此Ext2将Inode记录Block号码的区域定义为12个直接、一个间接、一个双间接与一个三间接记录区。Inode存储结构如下图所示： 最左边为Inode本身（128 bytes），里面有12个直接指向Block号码的对照，这12笔记录能够直接取得Block号码。至于所谓的间接就是再拿一个Block来当作记录Block号码的记录区，如果文件太大时，就会使用间接的Block来记录编号。如上图当中间接只是拿一个Block来记录额外的号码而已。 同理，如果文件持续长大，那么就会利用所谓的双间接，第一个Block仅再指出下一个记录编号的Block在哪里，实际记录的在第二个Block当中。依此类推，三间接就是利用第三层Block来记录编号。 Page Cache层引入Cache层的目的是为了提高Linux操作系统对磁盘访问的性能。Cache层在内存中缓存了磁盘上的部分数据。当数据的请求到达时，如果在Cache中存在该数据且是最新的，则直接将数据传递给用户程序，免除了对底层磁盘的操作，提高了性能。Cache层也正是磁盘IOPS为什么能突破200的主要原因之一。 在Linux的实现中，文件Cache分为两个层面，一是Page Cache，另一个Buffer Cache，每一个Page Cache包含若干Buffer Cache。Page Cache主要用来作为文件系统上的文件数据的缓存来用，尤其是针对当进程对文件有read&#x2F;write操作的时候。Buffer Cache则主要是设计用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。 磁盘Cache有两大功能：预读和回写。预读其实就是利用了局部性原理，具体过程是：对于每个文件的第一个读请求，系统读入所请求的页面并读入紧随其后的少数几个页面（通常是三个页面），这时的预读称为同步预读。对于第二次读请求，如果所读页面不在Cache中，即不在前次预读的页中，则表明文件访问不是顺序访问，系统继续采用同步预读；如果所读页面在Cache中，则表明前次预读命中，操作系统把预读页的大小扩大一倍，此时预读过程是异步的，应用程序可以不等预读完成即可返回，只要后台慢慢读页面即可，这时的预读称为异步预读。任何接下来的读请求都会处于两种情况之一：第一种情况是所请求的页面处于预读的页面中，这时继续进行异步预读；第二种情况是所请求的页面处于预读页面之外，这时系统就要进行同步预读。 回写是通过暂时将数据存在Cache里，然后统一异步写到磁盘中。通过这种异步的数据I&#x2F;O模式解决了程序中的计算速度和数据存储速度不匹配的鸿沟，减少了访问底层存储介质的次数，使存储系统的性能大大提高。Linux 2.6.32内核之前，采用pdflush机制来将脏页真正写到磁盘中，什么时候开始回写呢？下面两种情况下，脏页会被写回到磁盘： 1、在空闲内存低于一个特定的阈值时，内核必须将脏页写回磁盘，以便释放内存。2、当脏页在内存中驻留超过一定的阈值时，内核必须将超时的脏页写会磁盘，以确保脏页不会无限期地驻留在内存中。 回写开始后，pdflush会持续写数据，直到满足以下两个条件： 1、已经有指定的最小数目的页被写回到磁盘。2、空闲内存页已经回升，超过了阈值。 Linux 2.6.32内核之后，放弃了原有的pdflush机制，改成了bdi_writeback机制。bdi_writeback机制主要解决了原有fdflush机制存在的一个问题：在多磁盘的系统中，pdflush管理了所有磁盘的Cache，从而导致一定程度的I&#x2F;O瓶颈。bdi_writeback机制为每个磁盘都创建了一个线程，专门负责这个磁盘的Page Cache的刷新工作，从而实现了每个磁盘的数据刷新在线程级的分离，提高了I&#x2F;O性能。 回写机制存在的问题是回写不及时引发数据丢失（可由sync|fsync解决），回写期间读I&#x2F;O性能很差。 通用块层通用块层的主要工作是：接收上层发出的磁盘请求，并最终发出I&#x2F;O请求。该层隐藏了底层硬件块设备的特性，为块设备提供了一个通用的抽象视图。 对于VFS和具体的文件系统来说，块（Block）是基本的数据传输单元，当内核访问文件的数据时，它首先从磁盘上读取一个块。但是对于磁盘来说，扇区是最小的可寻址单元，块设备无法对比它还小的单元进行寻址和操作。由于扇区是磁盘的最小可寻址单元，所以块不能比扇区还小，只能整数倍于扇区大小，即一个块对应磁盘上的一个或多个扇区。一般来说，块大小是2的整数倍，而且由于Page Cache层的最小单元是页（Page），所以块大小不能超过一页的长度。 大多情况下，数据的传输通过DMA方式。旧的磁盘控制器，仅仅支持简单的DMA操作：每次数据传输，只能传输磁盘上相邻的扇区，即数据在内存中也是连续的。这是因为如果传输非连续的扇区，会导致磁盘花费更多的时间在寻址操作上。而现在的磁盘控制器支持“分散&#x2F;聚合”DMA操作，这种模式下，数据传输可以在多个非连续的内存区域中进行。为了利用“分散&#x2F;聚合”DMA操作，块设备驱动必须能处理被称为段（segments）的数据单元。一个段就是一个内存页面或一个页面的部分，它包含磁盘上相邻扇区的数据。 通用块层是粘合所有上层和底层的部分，一个页的磁盘数据布局如下图所示： I&#x2F;O调度层I&#x2F;O调度层的功能是管理块设备的请求队列。即接收通用块层发出的I&#x2F;O请求，缓存请求并试图合并相邻的请求。并根据设置好的调度算法，回调驱动层提供的请求处理函数，以处理具体的I&#x2F;O请求。 如果简单地以内核产生请求的次序直接将请求发给块设备的话，那么块设备性能肯定让人难以接受，因为磁盘寻址是整个计算机中最慢的操作之一。为了优化寻址操作，内核不会一旦接收到I&#x2F;O请求后，就按照请求的次序发起块I&#x2F;O请求。为此Linux实现了几种I&#x2F;O调度算法，算法基本思想就是通过合并和排序I&#x2F;O请求队列中的请求，以此大大降低所需的磁盘寻道时间，从而提高整体I&#x2F;O性能。 常见的I&#x2F;O调度算法包括Noop调度算法（No Operation）、CFQ（完全公正排队I&#x2F;O调度算法）、DeadLine（截止时间调度算法）、AS预测调度算法等。 Noop算法：最简单的I&#x2F;O调度算法。该算法仅适当合并用户请求，并不排序请求。新的请求通常被插在调度队列的开头或末尾，下一个要处理的请求总是队列中的第一个请求。这种算法是为不需要寻道的块设备设计的，如SSD。因为其他三个算法的优化是基于缩短寻道时间的，而SSD硬盘没有所谓的寻道时间且I&#x2F;O响应时间非常短。 CFQ算法：算法的主要目标是在触发I&#x2F;O请求的所有进程中确保磁盘I&#x2F;O带宽的公平分配。算法使用许多个排序队列，存放了不同进程发出的请求。通过散列将同一个进程发出的请求插入同一个队列中。采用轮询方式扫描队列，从第一个非空队列开始，依次调度不同队列中特定个数（公平）的请求，然后将这些请求移动到调度队列的末尾。 Deadline算法：算法引入了两个排队队列分别包含读请求和写请求，两个最后期限队列包含相同的读和写请求。本质就是一个超时定时器，当请求被传给电梯算法时开始计时。一旦最后期限队列中的超时时间已到，就想请求移至调度队列末尾。Deadline算法避免了电梯调度策略（为了减少寻道时间，会优先处理与上一个请求相近的请求）带来的对某个请求忽略很长一段时间的可能。 AS算法：AS算法本质上依据局部性原理，预测进程发出的读请求与刚被调度的请求在磁盘上可能是“近邻”。算法统计每个进程I&#x2F;O操作信息，当刚刚调度了由某个进程的一个读请求之后，算法马上检查排序队列中的下一个请求是否来自同一个进程。如果是，立即调度下一个请求。否则，查看关于该进程的统计信息，如果确定进程p可能很快发出另一个读请求，那么就延迟一小段时间。 前文中计算出的IOPS是理论上的随机读写的最大IOPS，在随机读写中，每次I&#x2F;O操作的寻址和旋转延时都不能忽略不计，有了这两个时间的存在也就限制了IOPS的大小。现在如果我们考虑在读取一个很大的存储连续分布在磁盘的文件，因为文件的存储的分布是连续的，磁头在完成一个读I&#x2F;O操作之后，不需要重新寻址，也不需要旋转延时，在这种情况下我们能到一个很大的IOPS值。这时由于不再考虑寻址和旋转延时，则性能瓶颈仅是数据传输时延，假设数据传输时延为0.4ms，那么IOPS&#x3D;1000 &#x2F; 0.4 &#x3D; 2500 IOPS。 在许多的开源框架如Kafka、HBase中，都通过追加写的方式来尽可能的将随机I&#x2F;O转换为顺序I&#x2F;O，以此来降低寻址时间和旋转延时，从而最大限度的提高IOPS。 块设备驱动层驱动层中的驱动程序对应具体的物理块设备。它从上层中取出I&#x2F;O请求，并根据该I&#x2F;O请求中指定的信息，通过向具体块设备的设备控制器发送命令的方式，来操纵设备传输数据。这里不再赘述。 基于磁盘I&#x2F;O特性设计的技巧在上一节中我们了解了Linux系统中请求到达磁盘的一次完整过程，期间Linux通过Cache以及排序合并I&#x2F;O请求来提高系统的性能。其本质就是由于磁盘随机读写慢、顺序读写快。本节针对常见开源系统阐述一些基于磁盘I&#x2F;O特性的设计技巧。 采用追加写在进行系统设计时，良好的读性能和写性能往往不可兼得。在许多常见的开源系统中都是优先在保证写性能的前提下来优化读性能。那么如何设计能让一个系统拥有良好的写性能呢？一个好的办法就是采用追加写，每次将数据添加到文件。由于完全是顺序的，所以可以具有非常好的写操作性能。但是这种方式也存在一些缺点：从文件中读一些数据时将会需要更多的时间：需要倒序扫描，直到找到所需要的内容。当然在一些简单的场景下也能够保证读操作的性能： 数据是被整体访问，比如HDFS HDFS建立在一次写多次读的模型之上。在HDFS中就是采用了追加写并且设计为高数据吞吐量；高吞吐量必然以高延迟为代价，所以HDFS并不适用于对数据访问要求低延迟的场景；由于采用是的追加写，也并不适用于任意修改文件的场景。HDFS设计为流式访问大文件，使用大数据块并且采用流式数据访问来保证数据被整体访问，同时最小化硬盘的寻址开销，只需要一次寻址即可，这时寻址时间相比于传输时延可忽略，从而也拥有良好的读性能。HDFS不适合存储小文件，原因之一是由于NameNode内存不足问题，还有就是因为访问大量小文件需要执行大量的寻址操作，并且需要不断的从一个datanode跳到另一个datanode，这样会大大降低数据访问性能。 知道文件明确的偏移量，比如Kafka 在Kafka中，采用消息追加的方式来写入每个消息，每个消息读写时都会利用Page Cache的预读和后写特性，同时partition中都使用顺序读写，以此来提高I&#x2F;O性能。虽然Kafka能够根据偏移量查找到具体的某个消息，但是查找过程是顺序查找，因此如果数据很大的话，查找效率就很低。所以Kafka中采用了分段和索引的方式来解决查找效率问题。Kafka把一个patition大文件又分成了多个小文件段，每个小文件段以偏移量命名，通过多个小文件段，不仅可以使用二分搜索法很快定位消息，同时也容易定期清除或删除已经消费完的文件，减少磁盘占用。为了进一步提高查找效率，Kafka为每个分段后的数据建立了索引文件，并通过索引文件稀疏存储来降低元数据占用大小。一个段中数据对应结构如下图所示： 在面对更复杂的读场景（比如按key）时，如何来保证读操作的性能呢？简单的方式是像Kafka那样，将文件数据有序保存，使用二分查找来优化效率；或者通过建索引的方式来进行优化；也可以采用hash的方式将数据分割为不同的桶。以上的方法都能增加读操作的性能，但是由于在数据上强加了数据结构，又会降低写操作的性能。比如如果采用索引的方式来优化读操作，那么在更新索引时就需要更新B-tree中的特定部分，这时候的写操作就是随机写。那么有没有一种办法在保证写性能不损失的同时也提供较好的读性能呢？一个好的选择就是使用LSM-tree。LSM-tree与B-tree相比，LSM-tree牺牲了部分读操作，以此大幅提高写性能。 日志结构的合并树LSM（The Log-Structured Merge-Tree）是HBase，LevelDB等NoSQL数据库的存储引擎。Log-Structured的思想是将整个磁盘看做一个日志，在日志中存放永久性数据及其索引，每次都添加到日志的末尾。并且通过将很多小文件的存取转换为连续的大批量传输，使得对于文件系统的大多数存取都是顺序的，从而提高磁盘I&#x2F;O。LSM-tree就是这样一种采用追加写、数据有序以及将随机I&#x2F;O转换为顺序I&#x2F;O的延迟更新，批量写入硬盘的数据结构。LSM-tree将数据的修改增量先保存在内存中，达到指定的大小限制后再将这些修改操作批量写入磁盘。因此比较旧的文件不会被更新，重复的纪录只会通过创建新的纪录来覆盖，这也就产生了一些冗余的数据。所以系统会周期性的合并一些数据，移除重复的更新或者删除纪录，同时也会删除上述的冗余。在进行读操作时，如果内存中没有找到相应的key，那么就是倒序从一个个磁盘文件中查找。如果文件越来越多那么读性能就会越来越低，目前的解决方案是采用页缓存来减少查询次数，周期合并文件也有助于提高读性能。在文件越来越多时，可通过布隆过滤器来避免大量的读文件操作。LSM-tree牺牲了部分读性能，以此来换取写入的最大化性能，特别适用于读需求低，会产生大量插入操作的应用环境。 文件合并和元数据优化目前的大多数文件系统，如XFS&#x2F;Ext4、GFS、HDFS，在元数据管理、缓存管理等实现策略上都侧重大文件。上述基于磁盘I&#x2F;O特性设计的系统都有一个共性特点就是都运行在这些文件系统之上。这些文件系统在面临海量时在性能和存储效率方面都大幅降低，本节来探讨下海量小文件下的系统设计。 常见文件系统在海量小文件应用下性能表现不佳的根本原因是磁盘最适合顺序的大文件I&#x2F;O读写模式，而非常不适合随机的小文件I&#x2F;O读写模式。主要原因体现在元数据管理低效和数据布局低效： 元数据管理低效：由于小文件数据内容较少，因此元数据的访问性能对小文件访问性能影响巨大。Ext2文件系统中Inode和Data Block分别保存在不同的物理位置上，一次读操作需要至少经过两次的独立访问。在海量小文件应用下，Inode的频繁访问，使得原本的并发访问转变为了海量的随机访问，大大降低了性能。另外，大量的小文件会快速耗尽Inode资源，导致磁盘尽管有大量Data Block剩余也无法存储文件，会浪费磁盘空间。 数据布局低效：Ext2在Inode中使用多级指针来索引数据块。对于大文件，数据块的分配会尽量连续，这样会具有比较好的空间局部性。但是对于小文件，数据块可能零散分布在磁盘上的不同位置，并且会造成大量的磁盘碎片，不仅造成访问性能下降，还大量浪费了磁盘空间。数据块一般为1KB、2KB或4KB，对于小于4KB的小文件，Inode与数据的分开存储破坏了空间局部性，同时也造成了大量的随机I&#x2F;O。 对于海量小文件应用，常见的I&#x2F;O流程复杂也是造成磁盘性能不佳的原因。对于小文件，磁盘的读写所占用的时间较少，而用于文件的open()操作占用了绝大部分系统时间，导致磁盘有效服务时间非常低，磁盘性能低下。针对于问题的根源，优化的思路大体上分为： 1、针对数据布局低效，采用小文件合并策略，将小文件合并为大文件。2、针对元数据管理低效，优化元数据的存储和管理。针对这两种优化方式，业内也出现了许多优秀的开源软件。 小文件合并 小文件合并为大文件后，首先减少了大量元数据，提高了元数据的检索和查询效率，降低了文件读写的I&#x2F;O操作延时。其次将可能连续访问的小文件一同合并存储，增加了文件之间的局部性，将原本小文件间的随机访问变为了顺序访问，大大提高了性能。同时，合并存储能够有效的减少小文件存储时所产生的磁盘碎片问题，提高了磁盘的利用率。最后，合并之后小文件的访问流程也有了很大的变化，由原来许多的open操作转变为了seek操作，定位到大文件具体的位置即可。如何寻址这个大文件中的小文件呢？其实就是利用一个旁路数据库来记录每个小文件在这个大文件中的偏移量和长度等信息。其实小文件合并的策略本质上就是通过分层的思想来存储元数据。中控节点存储一级元数据，也就是大文件与底层块的对应关系；数据节点存放二级元数据，也就是最终的用户文件在这些一级大块中的存储位置对应关系，经过两级寻址来读写数据。 淘宝的TFS就采用了小文件合并存储的策略。TFS中默认Block大小为64M，每个块中会存储许多不同的小文件，但是这个块只占用一个Inode。假设一个Block为64M，数量级为1PB。那么NameServer上会有 1 * 1024 * 1024 * 1024 &#x2F; 64 &#x3D; 16.7M个Block。假设每个Block的元数据大小为0.1K，则占用内存不到2G。在TFS中，文件名中包含了Block ID和File ID，通过Block ID定位到具体的DataServer上，然后DataServer会根据本地记录的信息来得到File ID所在Block的偏移量，从而读取到正确的文件内容。TFS一次读过程如下图所示： 元数据管理优化 一般来说元数据信息包括名称、文件大小、设备标识符、用户标识符、用户组标识符等等，在小文件系统中可以对元数据信息进行精简，仅保存足够的信息即可。元数据精简可以减少元数据通信延时，同时相同容量的Cache能存储更多的元数据，从而提高元数据使用效率。另外可以在文件名中就包含元数据信息，从而减少一个元数据的查询操作。最后针对特别小的一些文件，可以采取元数据和数据并存的策略，将数据直接存储在元数据之中，通过减少一次寻址操作从而大大提高性能。 TFS中文件命名就隐含了位置信息等部分元数据，从而减少了一个元数据的查询操作。在Rerserfs中，对于小于1KB的小文件，Rerserfs可以将数据直接存储在Inode中。 总结本文从磁盘性能指标出发，探究了操作系统与磁盘的交互以及对磁盘读写的优化，最后列举了一些常用开源系统中基于磁盘I&#x2F;O特性的设计特点。期望通过展现磁盘I&#x2F;O的特性，为存储系统设计和解决一些系统性能问题提供一种新思路。 作者简介喻枭，2016年加入美团，就职于美团酒店旅游事业群境内度假研发组。"},{"path":"/2024/04/05/linux-docs/网络协议栈/Linux 中的五种IO模型/","content":"IO模型的选择在Linux网络编程中十分重要，在Unix&#x2F;Linux环境中主要提供了五种不同的IO模型，分别是阻塞式IO、非阻塞式IO、IO多路复用、信号驱动式IO和异步IO。 通常一个输入操作包含两个不同阶段： 等待数据准备好 从内核向进程复制数据 例如，对于一个网络套接字上的输入操作，第一步通常涉及到发生系统调用，用户态切换到内核态并等待数据从网络中到达，当所有等待分组到达时，数据被复制到内核中的某个缓冲区。第二步则是将数据从内核缓冲区复制到应用进程缓冲区。 磁盘文件的IO比较特殊，内核采用缓冲区cache加速磁盘IO请求。因而一旦请求的数据到达内核缓冲区cache，对磁盘的write()操作立即返回，而不用等待将数据写入磁盘后再返回（除非在打开文件时指定了O_SYNC标志）。与之相对应的read()操作将数据从内核缓冲区cache移动到用户的缓冲区中，如果请求的数据不在内核缓冲区cache中，内核会让进程休眠，同时执行对磁盘的读操作。所以实际上在磁盘IO中，等待阶段是不存在的，因为磁盘文件并不像网络IO那样，需要等待远程传输数据。 阻塞式I&#x2F;O模型 Linux中，默认情况下所有的socket都是阻塞的。这里有必要辨析以下阻塞和非阻塞这两个概念，这两个概念描述的是用户线程调用内核I&#x2F;O操作的方式，其中阻塞是指I&#x2F;O操作需要彻底完成后才返回到用户空间；而非阻塞则是指I&#x2F;O操作被调用后立即返回给用户一个状态值，不需要等到I&#x2F;O操作彻底完成。 除非特别指定，几乎所有的I&#x2F;O接口都是阻塞型的，即系统调用时不返回调用结果，只有当该系统调用获得结果或者超时出错才返回。这样的机制给网络编程带来了较大的影响，当线程因处理数据而处于阻塞状态时，线程将无法执行任何运算或者相应任何网络请求。 改进方案 在服务器端使用阻塞I&#x2F;O模型时结合多进程&#x2F;多线程技术。让每一个连接都拥有独立的进程&#x2F;线程，任何一个连接的阻塞都不会影响到其他连接。（选择多进程还是多线程并无统一标准，因为进程的开销远大于线程，所以在连接数较大的情况下推荐使用多线程。而进程相较于线程具有更高的安全性，所以如果单个服务执行体需要消耗较多的CPU资源，如需要进行大规模或长时间的数据运算或文件访问推荐使用多进程）。 当连接数规模继续增大，无论使用多线程还是多进程都会严重占据系统资源，降低系统对外界的响应效率，线程或者进程本身也更容易陷入假死。此时可以采用“线程池”或“连接池”来降低创建和销毁进程&#x2F;线程的频率，减少系统开销。 非阻塞式I&#x2F;O模型 进程把一个套接字设置成非阻塞是在通知内核：当请求的I&#x2F;O操作非得把本进程投入睡眠才能完成时，不要把本进程投入睡眠，而是返回一个错误。 因此如果在打开文件时设定了O_NONBLOCK标志，则会以非阻塞方式打开文件。如果I&#x2F;O系统调用不能立即完成，则会返回错误而不是阻塞进程。非阻塞式I&#x2F;O可以实现周期性检查（轮询）某个文件描述符是否可执行I&#x2F;O操作。比如，设定一个输入文件描述符为非阻塞式的，然后周期性的执行非阻塞式读操作。如果需要同时检测多个文件描述符，则将其都设为非阻塞，然后一次轮询。但是这种轮询的效率不高，在轮询频率不高的情况下，程序响应I&#x2F;O事件的延迟将难以接受。换句话说，在一个紧凑的循环中做轮询就是在浪费CPU时间，因为多数时间调用会立即出错并返回。 对于不能满足非阻塞式I&#x2F;O操作，System V会返回EAGAIN错误而源于Berkeley的4.3BSD返回EWOULDBLOCK。如今大多数系统都把这两个错误码定义为相同的值。（可查看&lt;sys&#x2F;errno.h&gt;） 如果不希望进程在对文件描述符执行I&#x2F;O操作时被阻塞，则可以结合使用多进程&#x2F;多线程技术，创建一个新的进程来执行I&#x2F;O操作，而父进程则可以去做其他工作，子进程将阻塞到I&#x2F;O操作完成。当有多个文件描述符进行I&#x2F;O操作时，就需要创建多个子进程。当子进程收到文件结束符，那么该子进程将会终止，父进程收到SIGCHLD信号，但是当父进程终止，那么父进程应通知子进程停止，为此可以使用一个信号如SUGUSR1，这使得这种方法的开销将十分昂贵且复杂。使用多线程而不是多进程的方式将减少资源的占用，并有效简化程序设计。但线程之间仍然需要处理同步的问题，当面对大量并发客户线程时，但是这也使得程序编写十分复杂。 I&#x2F;O多路复用模型 I&#x2F;O多路复用（也叫做事件驱动I&#x2F;O）通过系统调用select()、poll、或者epoll()实现进程同时检查多个文件描述符，以找出其中任何一个是否可执行I&#x2F;O操作。通过上图可以看出I&#x2F;O多路复用与阻塞I&#x2F;O模型差别并不大，事实上还要差一些，因为这里使用了两个系统调用而阻塞I&#x2F;O只是用了一个系统调用。但是I&#x2F;O多路复用的优势是其可以同时处理多个连接。因此如果处理的连接数不是特别多的情况下使用I&#x2F;O多路复用模型的web server不一定比使用多线程技术的阻塞I&#x2F;O模型好。 select()和poll()的原理基本相同： 注册待侦听的fd(这里的fd创建时最好使用非阻塞) 每次调用都去检查这些fd的状态，当有一个或者多个fd就绪的时候返回 返回结果中包括已就绪和未就绪的fd 1int select (int maxfdp, fd_set *readfds, fd_set *writefds, fd_set *errorfds, struct timeval *timeout); select maxfdp 指集合中所有文件描述符的范围，即所有文件描述符的最大值+1 readfds、writefds、errorfds 指向文件描述符集合的指针，分别检测输入、输出是否就绪和异常情况是否发生 timeout 时select()的超时时间，控制着select()的阻塞行为 readfds、writefds、errorfds所指结构体都是保存结果的地方，在调用select()之前，这些参数指向的结构体必须初始化以包含我们所感兴趣的文件描述符集合。之后select()会修改这些结构体，当其返回时他们包含的就是处于就绪态的文件描述符集合。 当timeout设为NULL或者其指向的结构体字段非零时，select()将阻塞到有下列事件发生 readfds、writefds、errorfds 中指定的文件描述符中至少有一个成为就绪态（NULL） 该调用被信号处理程序中断 timeout中指定的时间上限已超时 select()的返回值当select()函数返回-1表示出错，错误码包括EBADF表示存在非法文件描述符，EINTR表示该调用被信号处理程序中断了（select不会自动恢复）。返回0表示超时，此时每个文件描述符集合都会被清空。返回一个正整数表示准备就绪的文件描述符个数，如果同一个文件描述符在返回的描述符集中出现多次，select会将其统计多次。 一个文件描述符是否阻塞并不影响select()是否阻塞，也就是说如果希望读一个非阻塞文件描述符，并且以5s为超时值调用select()，则select()最多阻塞5s。同理若是指定超时值为NULL，则在该描述符就绪或者捕捉到一个信号之前select()会一直阻塞。 所有关于文件描述符集合的操作都是通过以下四个宏完成，除此之外，常量FD_SETSIZE规定了文件描述符的最大容量。 123456#include &lt;sys/select.h&gt;void FD_ZERO(fd_set *fdset); //将fdset所指集合初始化为空void FD_SET(int fd, fd_set *fdset); //将文件描述符fd添加到由fdset指向的集合中void FD_CLR(int fd, fd_set *fdset); //将文件描述符fd从fdset所指集合中移出void FD_ISSET(int fd, fd_set *fdset); //检测fd是否是fdset所指集合成员 poll12#include &lt;poll.h&gt;int poll (struct pollfd fds[], nfds_t nfds, int timeout); poll和select的任务很相似，主要区别在于我们如何指定待检查的文件描述符（程序接口不同）。poll不为每个条件构造一个描述符集合，而是构造了一个pollfd结构的数组，每个数组元素指定一个描述符编号以及我们对该描述符感兴趣的条件。 12345struct pollfd &#123; int fd; //文件描述符 short events; //等待的事件 short revents; //实际发生了的事件&#125; 每个pollfd结构体指定了一个被监视的文件描述符，可以传递多个结构体，指示poll()监视多个文件描述符。每个结构体的events域是监视该文件描述符的事件掩码，由用户来设置这个域的属性。revents域是文件描述符的操作结果事件掩码，内核在调用返回时设置这个域，并且events中请求的任何事件都可能在revents中返回。 参数timeout的设置与select()中有所不同（poll的timeout参数是一个整型而select是一个结构体）。 1、当timeout等于-1时，表示无限超时。poll会一直阻塞到fds数组中列出的文件描述符有一个达到就绪态（定义在对应的events字段中）或者捕捉到一个信号2、当timeout等于0时，poll不会阻塞——只执行一次检查看看哪个文件描述符已经就绪3、当timeout大于0时，poll至多阻塞timeout毫秒数，无论IO是否准备好，poll都会返回 poll的返回值当poll()函数返回-1表示出错，错误码包括EBADF表示存在非法文件描述符，EINTR表示该调用被信号处理程序中断了（poll不会自动恢复）。返回0表示超时。返回一个正整数表示准备就绪的文件描述符个数，与select不同，poll返回的就是就绪文件描述符的个数每个文件描述符只统计一次。 select()和poll()的区别Linux实现层面 select()和poll()都使用了相同的内核轮询（poll）程序集合，与系统调用poll()本身不同，内核的每个poll例程都返回有关单个文件描述符就绪的信息，这个信息以位掩码的形式返回，其值同poll()系统调用返回的revent字段中的比特值相关。poll()系统调用的实现包括为每个文件描述符调用内核poll例程，并将结果信息填入到对应的revents字段中。对于系统调用select()则可以使用一组宏将内核poll例程返回的信息转化为由select()返回的与之对应的事件集合。 123#define POLLIN_SET (POLLIN | POLLRDNORM | POLLRDBAND | POLLHUP | POLLERR) /*读就绪*/#define POLLOUT_SET (POLLOUT | POLLWRNORM | POLLWRBAND | POLLERR) /*写就绪*/#define POLLEX_SET (POLLPRI) /*异常*/ 以上宏定义展现了select()和poll()返回信息间的语义关系，唯一一点不同是如果被检查的文件描述符中有一个关闭了，poll()在revent字段中返回POLLNVAL，而select()返回-1并把错误码置为EBADF。 API设计层面 1、select()使用的数据类型fd_set对于被检查的文件描述数量有一个上限（FD_SETSIZE）。相对也较小（1024&#x2F;2048），如果要修改这个默认值需要重新编译内核。与之相反，poll()没有对于被检查文件描述符的数量限制。2、由于select()的参数fd_set同时也是保存结果的地方，在select()返回之后会发生变化，所以每当在下一次进入select()之前需要重新初始化fd_set。poll()通过两个独立的字段events和revents将监控的输入输出分开，允许被监控的文件数组被复用而不需要重新初始化。3、select()提供的超时精度（微妙）比poll()提供的超时精度（毫秒）高。4、select()的超时参数在返回时也是未定义的，考虑到可移植性，每次在超时之后在下一次进入到select()之前都需要重新设置超时参数。5、poll()不要求开发者计算最大文件描述符时进行+1操作 性能层面 在待检查文件描述符范围较小（最大文件描述符较低），或者有大量文件描述符待检查，但是其分布比较密集时poll()和select()性能相似。在被检查文件描述符集合很稀疏的情况，poll()要优于select()。 select()和poll()的不足1、IO效率随着文件描述符的数量增加而线性下降。每次调用select()或poll()内核都要检查所有的被指定的文件描述符的状态（但是实际上只有部分的文件描述符会是活跃的），当有文件描述符集合增大时，IO的效率也随之下降。2、当检查大量文件描述符时，用户空间和内核空间消息传递速度较慢。每次调用select()或poll()时，程序都必须传递一个表示所有需要被检查的文件描述符的数据结构到内核，在内核完成检查之后，修个这个数据结构并返回给程序。（此外select()每次调用之前还需要初始化该数据结构）对于poll()调用需要将用户传入的pollfd数组拷贝到内核空间，这是一个O(n)的操作。当事件发生后，poll()将获得的数据传送到用户空间，并执行释放内存和剥离等待队列等工作同样是O(n)的操作。因此随着文件描述符的增加消息传递速度会逐步下降。对于select()来说，传递的数据结构大小固定为FD_SETSIZE，与待检查的文件描述符数量无关。3、select()或poll()调用完成之后，程序必须检查返回的数据结构中每个元素，已确定那个文件描述符处于就绪态4、select()对一个进程打开的文件描述符数目有上限值，而且较少（1024&#x2F;2048）。 epollepoll API是Linux专有的特性，相较于select和poll，epoll更加灵活且没有描述符限制。epoll设计也与select和poll不同，主要包含以下三个接口： 123#include &lt;sys/epoll.h&gt;int epoll_create(int size); //创建一个epoll句柄 参数size指定内核需要监听的文件描述符个数，但该参数与select中的maxfdp不同，并非一个上限（Linux 2.6.8以后该参数被忽略不用）。此外函数返回代表新创建的epoll句柄的文件描述符（在Linux下查看&#x2F;proc&#x2F;进程的id&#x2F;fd&#x2F;可看到该fd的值），因此当不再使用该文件描述符时应该通过close()关闭，当所用与epoll句柄相关的文件描述符都关闭时，该句柄被销毁并被系统回收其资源。 123#include &lt;sys/epoll.h&gt;int epoll_ctl(int epfd, int op, int fd, struct epoll_event *ev); //修改兴趣列表（事件注册函数） 与select()的在监听事件时告诉内核需要监听的事件类型不同，epoll()需要先注册要监听的事件类型。参数op表示要执行的动作通过三个宏表示：1.EPOLL_CTL_ADD注册新的fd到epfd中；1.EPOLL_CTL_MOD修改已经注册的fd的监听事件；3.EPOLL_CTL_DEL从epfd中删除一个fd。参数fd表示需要监听的fd。最后一个参数ev指向结构体epoll_event则是告诉内核需要监听的事件类型，定义如下： 1234struct epoll_event &#123; uint32_t events; //epoll events (bit mask) epoll_data_t data; //user data variable&#125; 其中data的类型为： 123456typedef union epoll_data &#123; void *ptr; //pointer to user defined data int fd; //file descriptor uint_32 u32; //32-bit integer uint_64 u64; //64-bit integer&#125; epoll_data_t; 其中字段event表示事件掩码指定待监听的文件描述符fd上所感兴趣的事件集合，除了增加了一个前缀E外，这些掩码的名称与poll中对应名称相同（两个例外EPOLLET表示设置为边缘触发、EPOLLONESHOT表示只监听一次）。data字段是一个联合体，当描述符fd就绪后，联合体成员可以用来指定传回给调用进程的信息。data字段是唯一可以获知同这个事件相关的文件描述符的途径，因此调用epoll_ctl()将文件描述符添加到兴趣列表中时，应该要么将ev.data.fd设为文件描述符，要么将ev.data.ptr设为指向包含该文件描述的结构体。 123#include &lt;sys/epoll.h&gt;int epoll_wait(int epfd, struct epoll_event *evlist, int maxevents, int timeout); 等待事件的产生，参数evlist所指向的结构体数组中返回就需文件描述的信息，数组evlist的空间由调用者负责申请，所包含的元素个数由参数maxevents指定。 epoll的实现原理（以网络socket监听为例）在linux，一切皆文件．所以当调用epoll_create时，内核给这个epoll分配一个文件描述符，但是这个不是普通的文件，而是只服务于epoll． 所以当内核初始化epoll时，会开辟一块内核高速cache区，用于安置我们监听的socket，这些socket会以红黑树的形式保存在内核的cache里，以支持快速的查找，插入，删除．同时，建立了一个list链表，用于存储准备就绪的事件．所以调用epoll_wait时，在timeout时间内，只是简单的观察这个list链表是否有数据，如果没有，则睡眠至超时时间到返回；如果有数据，则在超时时间到，拷贝至用户态events数组中． 那么，这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl()时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。 epoll支持两种模式LT(水平触发)和ET(边缘触发)，LT模式下，主要缓冲区数据一次没有处理完，那么下次epoll_wait返回时，还会返回这个句柄；而ET模式下，缓冲区数据一次没处理结束，那么下次是不会再通知了，只在第一次返回．所以在ET模式下，一般是通过while循环，一次性读完全部数据．epoll默认使用的是LT． 这件事怎么做到的呢？当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表，最后，epoll_wait干了件事，就是检查这些socket，如果不是ET模式（就是LT模式的句柄了），并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了。所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回。而ET模式的句柄，除非有新中断到，即使socket上的事件没有处理完，也是不会次次从epoll_wait返回的． 经常看到比较ET和LT模式到底哪个效率高的问题．有一个回答是说ET模式下减少epoll系统调用．这话没错，也可以理解，但是在ET模式下，为了避免数据饿死问题，用户态必须用一个循环，将所有的数据一次性处理结束．所以在ET模式下下，虽然epoll系统调用减少了，但是用户态的逻辑复杂了，write&#x2F;read调用增多了．所以这不好判断，要看用户的性能瓶颈在哪． epoll设计的特点1、功能分离socket低效的原因之一便是将“维护等待队列”和“阻塞进程”两个功能不加分离，每次调用 select 都需要这两步操作，然而大多数应用场景中，需要监视的 socket 相对固定，并不需要每次都修改。epoll 将这两个操作分开，先用epoll_ctl()维护等待队列，再调用 epoll_wait 阻塞进程。显而易见地，效率就能得到提升。 而epoll则是实现了功能分离，通过epoll_create()创建一个 epoll 对象 epfd，再通过epoll_ctl()将需要监视的 socket 添加到 epfd 中，最后调用 epoll_wait() 等待数据使得epoll有了优化的可能。 2、就绪列表select 低效的另一个原因在于程序不知道哪些 socket 收到数据，只能一个个遍历。如果内核维护一个“就绪列表”，引用收到数据的 socket，就能避免遍历。如下图所示，计算机共有三个 socket，收到数据的 sock2 和 sock3 被就绪列表 rdlist 所引用。当进程被唤醒后，只要获取 rdlist 的内容，就能够知道哪些 socket 收到数据。 epoll的实现细节epoll主要由两个结构体：eventpoll与epitem。epitem是每一个IO所对应的的事件。比如 epoll_ctl()的EPOLL_CTL_ADD操作的时候，就需要创建一个epitem。eventpoll是每一个epoll所对应的。比如epoll_create就是创建一个eventpoll。如下图所示，eventpoll 包含了 lock、mtx、wq（等待队列）与 rdlist 等成员，其中 rdlist 和 rbr 是我们所关心的。 就绪列表的数据结构（rdlist）就绪列表引用着就绪的 socket，所以它应能够快速的插入数据。 程序可能随时调用 epoll_ctl() 添加监视 socket，也可能随时删除。当删除时，若该 socket 已经存放在就绪列表中，它也应该被移除。所以就绪列表应是一种能够快速插入和删除的数据结构。 双向链表就是这样一种数据结构，epoll 使用双向链表来实现就绪队列。 索引结构（rbr)既然 epoll 将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的 socket，至少要方便地添加和移除，还要便于搜索，以避免重复添加。红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O(log(N))，效率较好，epoll 使用了红黑树作为索引结构（对应上图的 rbr）。 注：因为操作系统要兼顾多种功能，以及由更多需要保存的数据，rdlist 并非直接引用 socket，而是通过 epitem 间接引用，红黑树的节点也是 epitem 对象。同样，文件系统也并非直接引用着 socket。为方便理解，本文中省略了一些间接结构。 epoll的优点1、没有最大打开文件描述符限制epoll支持的最大打开文件数与系统内存相关，可通过cat &#x2F;proc&#x2F;sys&#x2F;fs&#x2F;file-max查看具体数目2、IO效率不随文件描述符数目增加而线性下降传统的select&#x2F;poll在拥有较大的一个socket集合时，不过由于网络延迟，任意时间只有部分socket是活跃的，但是select&#x2F;poll每次调用都会线性扫描全部的集合，导致效率呈线性下降。而epoll通过在内核中实现的根据每个文件描述符上的回调函数callback函数实现了每次只对“活跃的”的socket进行操作，从而使epoll实现了一个伪AIO，使其效率不会随文件描述符的增加而先行下降。3、使用mmap加速内核与用户空间的消息传递select、poll和epoll都需要内核把fd消息通知给用户空间，但是epoll采用了内核与用户空间mmap处于同一块内存来实现，具有较高的效率。 信号驱动式I&#x2F;O模型 信号驱动I&#x2F;O中，当文件描述符上可执行I&#x2F;O操作时，进程请求内核为自己发送一个信号，之后进程可以执行其他任务直到I&#x2F;O就绪为止，此时内核会发送信号给进程。建立一个针对套接字的信号驱动式I&#x2F;O需要进程执行以下三个步骤： 建立SIGIO信号处理函数 2. 设置该套接字的属主，通常使用fcntl的F_SETOWN命令设置 3. 开启该套接字的信号驱动式I&#x2F;O，通常通过使用fcnt的F_SETFL命令打开O_ASYNC标志完成 使用信号驱动式I&#x2F;O模型的主要优点是在等待数据到达期间，进程不会被阻塞。 信号驱动式I&#x2F;O的应用 对于UDP上的使用比较简单，SIGIO信号只有在数据报到达套接字或者套接字发生异步错误时产生。因此当捕获对于某个UDP套接字的SIGIO信号时，我们调用recvfrom或者读入到达的数据报或者获取发生的异步错误。 信号驱动式I&#x2F;O对于TCP套接字几乎无用，主要原因是SIGIO信号产生会过于频繁，并且其出现并没有告知我们发生了什么事件。比如，当一个进程既读又写一个TCP套接字时，当有数据到达或者当前写出的数据得到确认时，SIGIO信号都会产生，而信号处理函数无法区分这两种情况。 &gt; 应该只考虑对监听TCP套接字使用SIGIO，因为对于监听TCP套接字产生SIGIO的唯一条件是某个新连接的完成。 实际上I&#x2F;O多路复用、信号驱动I&#x2F;O以及epoll都是用来实现同一个目标的技术——同时检查多个文件描述符是否准备好执行I&#x2F;O操作（准确的说是看I&#x2F;O系统调用是否可以非阻塞地执行）。文件描述符就绪状态的转化是通过一些I&#x2F;O事件来触发的，如输入数据到达、套接字连接建立完成或者是之前满载的套接字发送缓冲区在TCP将队列中的数据传送到对端之后有了剩余空间。但是以上这三种技术都不会实际执行I&#x2F;O操作，只会告诉我们某个文件描述符已经处于就绪状态，此时我们还需要调用其他系统调用来实际完成I&#x2F;O操作。AIO技术是POSIX异步I&#x2F;O，其允许进程将I&#x2F;O操作排列到一个文件中，当操作完成后得到通知，其优点是最初的I&#x2F;O调用会立刻返回，进程不会一直等待数据传达到内核或者等待操作完成。这使得进程可以同I&#x2F;O操作一起并行处理其他任务。 I&#x2F;O模型的技术选择 select()和poll()可移植性高但是当同时检查大量的文件描述符时性能延展性不佳。 epoll()关键优势是可以高效的检查大量文件描述符，但是可移植性差属于只能用于Linux的API。 信号驱动I&#x2F;O和epoll一样可以高效的检查大量文件描述符，但是epool具有许多信号驱动i&#x2F;O不具备的优势。 避免了处理信号的复杂性 可以指定想要检查的事件类型（读就绪或者写就绪） 可以选择水平触发或者边缘触发的形式来通知进程。 如要用到信号驱动I&#x2F;O的优点需要用到不可移植的Linux的专有特性，如此其可移植性也不会优于epoll。 通过以上总结可知，select和poll具有良好的可移植性而epoll和信号驱动I&#x2F;O具有更好的性能，因此通过一个软件抽象层来检查文件描述符事件，从而可移植程序就能在提供epoll API系统上使用epoll而在其他系统使用select或poll了。如Libevent的底层机制能够使用以上四种机制中的任意一种。 文件描述符准备就绪的通知方式 水平触发通知：如果文件描述符上可以非阻塞地执行I&#x2F;O系统调用，此时认为其已经就绪。 边缘触发通知：如果文件描述符自上次状态检查以来有了新的I&#x2F;O活动（如新的输入），此时需要触发通知。 通过上表可知，epoll与其他模式的区别在于其同时支持水平触发（默认）与边缘触发。 触发方式对程序设计的影响 水平触发 当采用水平触发时，我们可以在任意时刻检查文件描述符的就绪状态。当确定其就绪状态后就可以对其进行I&#x2F;O操作，然后重复检查文件描述符，已确定是否仍然处于就绪状态，此时就可以执行更多的I&#x2F;O。也就是说，因为水平触发允许我们在任意时刻重复检查I&#x2F;O状态，也就没有必要每次文件描述符就绪后就尽可能多地执行I&#x2F;O（尽可能多地读取字节，亦或是不去执行I&#x2F;O）。 边缘触发 当采用边缘触发时，只有当I&#x2F;O事件发生时才会得到通知。在另一个I&#x2F;O时间到来之前我们不会收到任何新的通知。此外当文件描述符收到I&#x2F;O事件通知时，我们并不知道要处理多少I&#x2F;O（有多少数据可读）。因此采用边缘触发通知的程序应该在接收到一个I&#x2F;O事件通知后，程序在某个时刻（在有些时候我们确定文件描述符是就绪态时，此时不适合大量的I&#x2F;O操作。因为如果我们仅对一个文件描述符执行大量I&#x2F;O操作，可能会让其他文件描述符处于饥饿状态）应该在相应的文件描述符上尽可能多地执行I&#x2F;O（尽可能多地读取字节，与水平触发相反）。但是若是程序如此设计，就可能失去执行I&#x2F;O的机会。因为知道产生另一个I&#x2F;O事件为止，在此之前程序都不会在接收到通知了，也就不知道何时执行I&#x2F;O了。这也将导致数据丢失或者程序中出现阻塞。 如果程序采用循环来对文件描述符执行尽可能多的I&#x2F;O，而文件描述符又被置为可阻塞的，那么最终当没有更多文件可执行时，I&#x2F;O系统调用就会阻塞。因此，每个被检查的文件描述符通常都应该被设为非阻塞模式。在得到I&#x2F;O事件通知后会重复执行I&#x2F;O操作，直到相应的系统调用 (如read()、write()) 以错误码EAGAIN或EWOULDBLOCK的形式失败。 异步I&#x2F;O模型 对于I&#x2F;O操作主要有两种分别是异步I&#x2F;O和同步I&#x2F;O，对于同步I&#x2F;O会导致请求进程阻塞，直到I&#x2F;O操作完成，即必须等待I&#x2F;O操作完成以后控制权才返回给用户进程；而异步I&#x2F;O不会导致请求进程阻塞，即无需等待I&#x2F;O操作完成就将控制权返回给用户进程。 异步I&#x2F;O模型的工作机制 告知内核启动某个操作，并让内核在整个操作（包括将数据从内核复制到进程缓冲区）完成后通知我们。主要方式是调用aio_read函数向内核传递描述符、缓冲区指针、缓冲区大小（与read相同的三个参数）和文件偏移，并告知内核当整个操作完成时如何通知用户进程。该系统调用立即返回，在等待I&#x2F;O完成期间进程不被阻塞。 与信号驱动式I&#x2F;O模型的区别信号驱动式I&#x2F;O是由内核告诉我们何时可以启动一个I&#x2F;O操作，而异步I&#x2F;O模型则是由内核通知我们I&#x2F;O操作何时完成。"},{"path":"/2024/04/05/linux-docs/文件系统/虚拟文件系统/","content":"一. 前言 基于上文介绍的文件系统的基本结构，本文将继续深入Linux文件系统的精髓所在：虚拟文件系统。操作文件的本质是将磁盘文件数据映射到进程中，上文的文件系统是如何存储文件数据，而从进程如何映射到该文件系统，中间还有一系列的过程，主要包括 进程发出文件操作命令，通过系统调用如sys_open、sys_read、sys_write调用相应内核函数 在内核中为进程打开的文件和系统文件创建数据结构进行维护 通过虚拟文件系统对各种不同的文件系统操作，如I&#x2F;O设备、管道、进程间通信、网络等进行抽象并统一接口 实现虚拟文件系统和实际文件系统如ext4的挂载 提供文件系统和I&#x2F;O设备层的设备驱动接口及加快读写效率的缓存 整体层次如上图所示，其实可以概括为文件系统层、通用快层和设备层，如下图所示。 文件系统层，包括虚拟文件系统和其他各种文件系统的具体实现。它为上层的应用程序，提供标准的文件访问接口；对下会通过通用块层，来存储和管理磁盘数据。 通用块层，包括块设备 I&#x2F;O 队列和 I&#x2F;O 调度器。它会对文件系统的 I&#x2F;O 请求进行排队，再通过重新排序和请求合并，然后才要发送给下一级的设备层。 设备层，包括存储设备和相应的驱动程序，负责最终物理设备的 I&#x2F;O 操作。 下面就此展开详细叙述虚拟文件系统打开文件的整个过程。 二. dentry介绍 在介绍从用户态调用至打开文件执行操作的流程前，我们先分析一下虚拟文件系统中的重要结构体，然后再分析映射关系的建立，以便于后文的理解。通过上文我们知道磁盘中实际的文件系统有ext4, NTFS等类型，通过超级块、块描述符、块位图、块列表等结构构成，其中一系列的文件块由文件系统对应的结构体如ext4_inode构成。虚拟文件系统是对文件系统的抽象，保存在内存之中，由dentry结构体和inode构成。 索引节点，即 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等。索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。所以记住，索引节点同样占用磁盘空间。 目录项，即 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。不过，不同于索引节点，目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存。 inode对应于实际的索引节点如ext4_inode，而dentry则表示了不同层级之间的关系，也是链接所使用的结构体。dentry通过d_parent来和上级目录构成链接关系，通过d_op来存储对应的实际文件系统的文件操作，如创建、删除、打开、读写等。d_sb指向实际文件系统的超级块，该结构在上文已详细介绍。d_inode指向对应的inode，d_name表示该文件的文件名。 123456789101112131415struct dentry &#123;...... struct hlist_bl_node d_hash;\t/* lookup hash list */ struct dentry *d_parent;\t/* parent directory */ struct qstr d_name; struct inode *d_inode; /* Where the name belongs to - NULL is negative */ const struct dentry_operations *d_op; struct super_block *d_sb;\t/* The root of the dentry tree */ ...... union &#123; struct list_head d_lru; /* LRU list */ wait_queue_head_t *d_wait;\t/* in-lookup ones only */ &#125;;......&#125; __randomize_layout; 除此之外，dentry有两个特殊的成员变量d_lru和d_hash。这其实关联于两张dentry cache表，顾名思义，该表保存的是一系列已分配过的dentry的缓存池，用于文件操作中快速的查找和使用。 哈希表 dentry_hashtable：dcache 中的所有 dentry 对象都通过 d_hash 指针链到相应的 dentry 哈希链表中； 未使用的 dentry 对象链表 s_dentry_lru：dentry 对象通过其 d_lru 指针链入 LRU 链表中。 这两个列表之间会产生复杂的关系： 引用为 0：一个在散列表中的 dentry 变成没有人引用了，就会被加到 LRU 表中去； 再次被引用：一个在 LRU 表中的 dentry 再次被引用了，则从 LRU 表中移除； 分配：当 dentry 在散列表中没有找到，则从 Slub 分配器中分配一个； 过期归还：当 LRU 表中最长时间没有使用的 dentry 应该释放回 Slub 分配器； 文件删除：文件被删除了，相应的 dentry 应该释放回 Slub 分配器； 结构复用：当需要分配一个 dentry，但是无法分配新的，就从 LRU 表中取出一个来复用。 三. 文件系统挂载 我们通过虚拟文件系统映射到对应的实际文件系统，该操作称之为文件系统的挂载。以ext4文件系统为例，我们需要通过 register_filesystem() 进行注册，传入的参数是 ext4_fs_type，表示注册的是 ext4 类型的文件系统，这里面最重要的一个成员变量就是 ext4_mount。 123456789register_filesystem(&amp;ext4_fs_type);static struct file_system_type ext4_fs_type = &#123; .owner = THIS_MODULE, .name = &quot;ext4&quot;, .mount = ext4_mount, .kill_sb = kill_block_super, .fs_flags = FS_REQUIRES_DEV,&#125;; mount() 系统调用的定义如下，主要调用链为do_mount()-&gt;do_new_mount()-&gt;do_new_mount_fc()。 12345678910111213141516171819202122SYSCALL_DEFINE5(mount, char __user *, dev_name, char __user *, dir_name, char __user *, type, unsigned long, flags, void __user *, data)&#123;...... ret = do_mount(kernel_dev, dir_name, kernel_type, flags, options);......&#125; do_new_mount_fc()先是调用vfs_create_mount()创建 struct mount 结构，每个挂载的文件系统都对应于这样一个结构，接着调用 do_add_mount()完成挂载操作。/* * Create a new mount using a superblock configuration and request it * be added to the namespace tree. */static int do_new_mount_fc(struct fs_context *fc, struct path *mountpoint, unsigned int mnt_flags)&#123; struct vfsmount *mnt;...... mnt = vfs_create_mount(fc);...... error = do_add_mount(real_mount(mnt), mountpoint, mnt_flags);......&#125; 首先看看fs_context结构体，该结构体用于保存超级块的信息的root，而超级块本身包含了该实际文件系统的信息，因此通过该结构体，我们可以使该文件系统和mount结构体建立联系。在旧版的Linux源码中，我们需要使用mount_fs()函数去获取对应的root，而新版则可以通过fs_context直接获取。 12345struct fs_context &#123;...... struct dentry *root; /* The root and superblock */......&#125;; 接着我们来看看mount结构体和vfsmount结构体。每个文件系统会创建一个mount结构体表示其挂载的信息，其中mnt_parent指向该文件系统挂载的父文件系统，mnt_mountpoint 是装载点在父文件系统中的 dentry。vfsmount结构体中mnt_root 是当前文件系统根目录的 dentry，mnt_sb 是指向超级块的指针。这里之所以会有mnt_mountpoint和mnt_root，是因为在挂载时该文件系统的根目录同时成为了其父文件系统的一个子目录（挂载点）。 123456789101112131415161718192021222324struct vfsmount &#123; struct dentry *mnt_root;\t/* root of the mounted tree */ struct super_block *mnt_sb;\t/* pointer to superblock */ int mnt_flags;&#125; __randomize_layout;struct mount &#123; struct hlist_node mnt_hash; struct mount *mnt_parent; struct dentry *mnt_mountpoint; struct vfsmount mnt; union &#123; struct rcu_head mnt_rcu; struct llist_node mnt_llist; &#125;;...... struct list_head mnt_mounts;\t/* list of children, anchored here */ struct list_head mnt_child;\t/* and going through their mnt_child */ struct list_head mnt_instance;\t/* mount instance on sb-&gt;s_mounts */ const char *mnt_devname;\t/* Name of device e.g. /dev/dsk/hda1 */ struct list_head mnt_list;......&#125; __randomize_layout; 由此，我们完成了挂载，并和目录系统dentry构成了对应关系。下面举个简单的例子。在Linux内核启动时，首先会挂载根目录文件系统，因此创建了一个mount结构体和根目录dentry。假设我们挂载一个文件系统A，该文件系统A包含一个文件夹home(由dentry表示)，则会创建一个新的mount结构体指向根目录的mount，并创建一个dentry表示该处为此文件系统A的挂载点。假设还有新的文件系统B包括了一个目录hello&#x2F;world&#x2F;data，则会采用同样的方式进行挂载和映射关系的建立，如下图所示。 在图中，我们还标出了一个结构体file，该结构体保存了指针分别指向dentry和mount，由此可以方便的进行虚拟文件系统的管理。而实际上，我们在用户态调用GLIBC函数获取的文件描述符也是直接和file结构体映射，并由此对虚拟文件系统中的文件进行方便的操作。 四. 打开&#x2F;创建文件操作 从用户态发起的文件操作主要包括创建、删除、打开、读写、权限管理等，这里以open()为例来解读。在系统调用一文中，我们同样以open()为例说明了其从GLIBC调用到到DO_CALL()，通过80软中断陷入内核，并最终通过查找系统调用表sys_call_talbe调用对应的系统调用do_sys_open()。本文就从do_sys_open()开始深入介绍其实现。 do_sys_open()函数首先调用build_open_flags()将传递进来的flags进行解析并存在op中，接着调用get_unused_fd_flags()获取一个可用的文件描述符fd，接着调用do_file_open()创建文件结构f，并通过fd_install()将f其和文件描述符fd关联起来。这里的文件结构f即上文所述的结构体file。 12345678910111213141516171819long do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode)&#123; struct open_flags op; int fd = build_open_flags(flags, mode, &amp;op);...... fd = get_unused_fd_flags(flags); if (fd &gt;= 0) &#123; struct file *f = do_filp_open(dfd, tmp, &amp;op); if (IS_ERR(f)) &#123; put_unused_fd(fd); fd = PTR_ERR(f); &#125; else &#123; fsnotify_open(f); fd_install(fd, f); &#125; &#125; putname(tmp); return fd;&#125; get_unused_fd_flags()函数实际调用__alloc_fd()函数，源码如下所示。这里传参files_struct来源于当前运行的task_struct中的files指针，该结构体最关键的是携带了文件描述符表struct file __rcu * fd_array[NR_OPEN_DEFAULT]。对于任何一个任务，默认情况下文件描述符 0 表示 stdin 标准输入，文件描述符 1 表示 stdout 标准输出，文件描述符 2 表示 stderr 标准错误输出，除此之外打开的文件都会从这个列表中找一个空闲位置分配给它。文件描述符列表的每一项都是一个指向 struct file 的指针，也就是说每打开一个文件都会有一个 struct file 对应。 传入文件描述符表后，首先将fd赋值为files-&gt;next_fd，然后通过find_next_fd()去检查是否可用，如果不可用则会继续自增直至找到可用的文件描述符。找到之后，会将files-&gt;next_fd赋值为fd + 1以备下次使用，最后调用__set_open_fd()将fd位表的修改赋给fdt并保存。 123456789101112131415161718192021/* * allocate a file descriptor, mark it busy. */int __alloc_fd(struct files_struct *files, unsigned start, unsigned end, unsigned flags)&#123; unsigned int fd; struct fdtable *fdt;...... fdt = files_fdtable(files); fd = start; if (fd &lt; files-&gt;next_fd) fd = files-&gt;next_fd; if (fd &lt; fdt-&gt;max_fds) fd = find_next_fd(fdt, fd);...... if (start &lt;= files-&gt;next_fd) files-&gt;next_fd = fd + 1; __set_open_fd(fd, fdt);......&#125; do_file_open()首先初始化了 struct nameidata 结构，该结构用于解析和查找文件路径。接着调用path_openat()，该函数会创建文件结构file，对文件路径进行解析和处理，并最终获取文件对应的索引节点inode并初始化file文件对象。 123456789101112131415161718192021struct file *do_filp_open(int dfd, struct filename *pathname, const struct open_flags *op)&#123; struct nameidata nd; struct file *filp; set_nameidata(&amp;nd, dfd, pathname); filp = path_openat(&amp;nd, op, flags | LOOKUP_RCU);...... restore_nameidata(); return filp;&#125;```c 首先展开看看nameidata结构体，这里主要包括了如路径相关的path，last和root，虚拟文件系统的索引节点inode，标记位flags，序列号seq, m_seq，类型last_type，文件层级深度depth，链接相关的total_link_count和结构体stack，文件名filename *name等。```cstruct nameidata &#123; struct path\tpath; struct qstr\tlast; struct path\troot; struct inode\t*inode; /* path.dentry.d_inode */......&#125; __randomize_layout; 这里展开看一下结构体path，其中struct vfsmount 即上节所述的挂载变量，而dentry则为对应的目录结构体。 1234struct path &#123; struct vfsmount *mnt; struct dentry *dentry;&#125; __randomize_layout; path_openat()函数主要逻辑如下 调用alloc_empty_filp() 生成一个 struct file 结构体，实际最终调用kmem_cache_alloc()分配，即采用前文所述的slab分配器分配； 调用path_init() 初始化 nameidata，准备开始节点路径查找； 调用link_path_walk()对于路径名逐层进行节点路径查找，这里面有一个大的循环，用“&#x2F;”分隔逐层处理。例如，文件“&#x2F;root&#x2F;hello&#x2F;world&#x2F;data”，link_path_walk 会解析前面的路径部分“&#x2F;root&#x2F;hello&#x2F;world”，解析完毕的时候 nameidata 的 dentry 为路径名的最后一部分的父目录“&#x2F;root&#x2F;hello&#x2F;world”，而 nameidata 的 filename 为路径名的最后一部分“data”。 调用do_last() 获取文件对应的 inode 对象，并且初始化 file 对象。 调用terminate_walk()返回路径保存在nd中 12345678910111213141516static struct file *path_openat(struct nameidata *nd, const struct open_flags *op, unsigned flags)&#123; struct file *file; int error; file = alloc_empty_file(op-&gt;open_flag, current_cred());...... const char *s = path_init(nd, flags); while (!(error = link_path_walk(s, nd)) &amp;&amp; (error = do_last(nd, file, op)) &gt; 0) &#123; nd-&gt;flags &amp;= ~(LOOKUP_OPEN|LOOKUP_CREATE|LOOKUP_EXCL); s = trailing_symlink(nd); &#125; terminate_walk(nd);......&#125; do_last()函数如其名字一样，完成了最后一部分的解析和处理工作。首先调用lookup_fast()查找文件路径最后一部分对应的dentry，接着使用lookup_open()判断是否需要创建新的dentry，最终将dentry赋值给path。最后调用vfs_open()真正的打开文件。 1234567891011121314/* * Handle the last step of open() */static int do_last(struct nameidata *nd, struct file *file, const struct open_flags *op)&#123;...... error = lookup_fast(nd, &amp;path, &amp;inode, &amp;seq);...... error = lookup_open(nd, &amp;path, file, op, got_write);...... error = vfs_open(&amp;nd-&gt;path, file);......&#125; 首先来看看lookup_fast()部分，Linux 为了提高目录项对象的处理效率，设计与实现了目录项高速缓存 dentry cache，简称 dcache，在上节中已有详细描述。lookup_fast()会在dcache中去试图找到该对应的dentry，实际最终会调用hlist_bl_for_each_entry_rcu轮询列表进行查找。 123456789101112static int lookup_fast(struct nameidata *nd, struct path *path, struct inode **inode, unsigned *seqp)&#123; struct vfsmount *mnt = nd-&gt;path.mnt; struct dentry *dentry, *parent = nd-&gt;path.dentry;...... dentry = __d_lookup_rcu(parent, &amp;nd-&gt;last, &amp;seq);...... dentry = __d_lookup(parent, &amp;nd-&gt;last);......&#125; lookup_open()会针对没有找到的情况创建一个新的 dentry，并且调用上一级目录的 Inode 的 inode_operations 的 lookup 函数，对于 ext4 来讲，调用的是 ext4_lookup，会到文件系统里面去找对应的ext4_inode。最终找到后将新生成的 dentry 赋给 path 变量。 12345678910111213141516171819static int lookup_open(struct nameidata *nd, struct path *path, struct file *file, const struct open_flags *op, bool got_write, int *opened)&#123;...... dentry = d_alloc_parallel(dir, &amp;nd-&gt;last, &amp;wq);...... struct dentry *res = dir_inode-&gt;i_op-&gt;lookup(dir_inode, dentry, nd-&gt;flags);...... path-&gt;dentry = dentry; path-&gt;mnt = nd-&gt;path.mnt;&#125;const struct inode_operations ext4_dir_inode_operations = &#123; .create = ext4_create, .lookup = ext4_lookup,... do_last() 的最后一步是调用 vfs_open() 真正打开文件，实际调用 f_op-&gt;open，也就是调用 ext4_file_open()。另外一件重要的事情是将打开文件的所有信息填写到 struct file 这个结构里面，从而完成了整个打开的过程。 1234567891011121314151617181920212223242526272829303132333435363738int vfs_open(const struct path *path, struct file *file, const struct cred *cred)&#123; struct dentry *dentry = d_real(path-&gt;dentry, NULL, file-&gt;f_flags, 0);...... file-&gt;f_path = *path; return do_dentry_open(file, d_backing_inode(dentry), NULL, cred);&#125;static int do_dentry_open(struct file *f, struct inode *inode, int (*open)(struct inode *, struct file *), const struct cred *cred)&#123;...... f-&gt;f_mode = OPEN_FMODE(f-&gt;f_flags) | FMODE_LSEEK | FMODE_PREAD | FMODE_PWRITE; path_get(&amp;f-&gt;f_path); f-&gt;f_inode = inode; f-&gt;f_mapping = inode-&gt;i_mapping;...... f-&gt;f_op = fops_get(inode-&gt;i_fop);...... open = f-&gt;f_op-&gt;open;...... error = open(inode, f);...... f-&gt;f_flags &amp;= ~(O_CREAT | O_EXCL | O_NOCTTY | O_TRUNC); file_ra_state_init(&amp;f-&gt;f_ra, f-&gt;f_mapping-&gt;host-&gt;i_mapping); return 0;......&#125;const struct file_operations ext4_file_operations = &#123;...... .open = ext4_file_open,......&#125;; 我们总结一下open()函数的执行流程：其核心在于创建文件描述符和分配对应的file结构体并将二者结合起来，最后返回文件描述符以供后续使用。从这里面我们能看出虚拟文件系统所起到的作用： 对每个task_struct建立一个文件描述符表管理该任务对应的文件 使用mount, vfsmount和dentry建立虚拟文件系统和实际文件系统的关联 创建file结构体来表述dentry和mount，并建立和文件描述符的关系以便使用 通过dentry查找对应inode，分配file结构体并完成初始化工作 通过虚拟文件系统和实际文件系统的映射，调用实际文件系统如ext4对应的函数完成打开操作 由此可见，虚拟文件实际是一层中间的抽象层，将具体的文件系统以及磁盘文件抽象为dentry和对应的inode，对上层的用户态封装为统一的文件描述符（磁盘文件，网络文件等），通过文件描述符和文件的绑定关系进行操作、处理。下图是对上文的一个总结。 五. 读写文件操作 有了上文的基础，文件的读写就容易理解了。首先给出读写对应的系统调用。 123456789101112131415161718SYSCALL_DEFINE3(read, unsigned int, fd, char __user *, buf, size_t, count)&#123; struct fd f = fdget_pos(fd);...... loff_t pos = file_pos_read(f.file); ret = vfs_read(f.file, buf, count, &amp;pos);......&#125;SYSCALL_DEFINE3(write, unsigned int, fd, const char __user *, buf, size_t, count)&#123; struct fd f = fdget_pos(fd);...... loff_t pos = file_pos_read(f.file); ret = vfs_write(f.file, buf, count, &amp;pos);......&#125; 对于 read() 来讲，里面调用 vfs_read()-&gt;__vfs_read()。对于 write() 来讲，里面调用 vfs_write()-&gt;__vfs_write()。每一个打开的文件都有一个 struct file 结构。这里面有一个 struct file_operations f_op，用于定义对这个文件做的操作。__vfs_read() 会调用相应文件系统的 file_operations 里面的 read() 操作，__vfs_write() 会调用相应文件系统 file_operations 里的 write() 操作。 123456789101112131415161718192021ssize_t __vfs_read(struct file *file, char __user *buf, size_t count, loff_t *pos)&#123; if (file-&gt;f_op-&gt;read) return file-&gt;f_op-&gt;read(file, buf, count, pos); else if (file-&gt;f_op-&gt;read_iter) return new_sync_read(file, buf, count, pos); else return -EINVAL;&#125;static ssize_t __vfs_write(struct file *file, const char __user *p, size_t count, loff_t *pos)&#123; if (file-&gt;f_op-&gt;write) return file-&gt;f_op-&gt;write(file, p, count, pos); else if (file-&gt;f_op-&gt;write_iter) return new_sync_write(file, p, count, pos); else return -EINVAL;&#125; 对于 ext4 文件系统来讲，内核定义了一个 ext4_file_operations。由于 ext4 没有定义 read() 和 write() 函数，于是会调用 ext4_file_read_iter() 和 ext4_file_write_iter()。ext4_file_read_iter() 会调用 generic_file_read_iter()，ext4_file_write_iter() 会调用 __generic_file_write_iter()。 12345678910111213141516171819202122232425262728293031323334const struct file_operations ext4_file_operations = &#123;...... .read_iter = ext4_file_read_iter, .write_iter = ext4_file_write_iter,......&#125;ssize_tgeneric_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)&#123;...... if (iocb-&gt;ki_flags &amp; IOCB_DIRECT) &#123;...... struct address_space *mapping = file-&gt;f_mapping;...... retval = mapping-&gt;a_ops-&gt;direct_IO(iocb, iter); &#125;...... retval = generic_file_buffered_read(iocb, iter, retval);&#125;ssize_t __generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)&#123;...... if (iocb-&gt;ki_flags &amp; IOCB_DIRECT) &#123;...... written = generic_file_direct_write(iocb, from);...... &#125; else &#123;...... written = generic_perform_write(file, from, iocb-&gt;ki_pos);...... &#125;&#125; generic_file_read_iter() 和 __generic_file_write_iter() 有相似的逻辑，就是要区分是否用缓存。缓存其实就是内存中的一块空间。因为内存比硬盘快得多，Linux 为了改进性能，有时候会选择不直接操作硬盘，而是读写都在内存中，然后批量读取或者写入硬盘。一旦能够命中内存，读写效率就会大幅度提高。因此，根据是否使用内存做缓存，我们可以把文件的 I&#x2F;O 操作分为两种类型。 第一种类型是缓存 I&#x2F;O。大多数文件系统的默认 I&#x2F;O 操作都是缓存 I&#x2F;O，缓存需要文件和内存页进行关联，这就要用到 address_space。address_space 的相关操作定义在 struct address_space_operations 结构中。对于 ext4 文件系统来讲， address_space 的操作定义在 ext4_aops，direct_IO 对应的函数是 ext4_direct_IO。 对于读操作来讲，操作系统会先检查内核的缓冲区有没有需要的数据。如果已经缓存了，那就直接从缓存中返回；否则从磁盘中读取，然后缓存在操作系统的缓存中。 对于写操作来讲，操作系统会先将数据从用户空间复制到内核空间的缓存中。这时对用户程序来说，写操作就已经完成。至于什么时候再写到磁盘中由操作系统决定，除非显式地调用了 sync 同步命令。 第二种类型是直接 IO，就是应用程序直接访问磁盘数据，而不经过内核缓冲区，从而减少了在内核缓存和用户程序之间数据复制。在读写逻辑中如果设置了IOCB_DIRECT，则会调用address_space的direct_IO函数实现。 ext4_direct_IO 最终会调用到 __blockdev_direct_IO-&gt;do_blockdev_direct_IO，这就跨过了缓存层，到了通用块层，最终到了文件系统的设备驱动层。由于文件系统是块设备，所以这个调用的是 blockdev 相关的函数，有关块设备驱动程序的原理我们会在设备驱动一节详细分析，这一节我们就讲到文件系统到块设备的分界线部分。 下面看看带缓冲的读写generic_perform_write()和generic_file_buffered_read()。 5.1 带缓冲的写操作 generic_perform_write()需要找出这次写入影响的所有的页，然后依次写入。对于每一个循环，主要做四件事情： 对于每一页先调用 address_space 的 write_begin() 做一些准备； 调用 iov_iter_copy_from_user_atomic()将写入的内容从用户态拷贝到内核态的页中； 调用 address_space 的 write_end() 完成写操作； 调用 balance_dirty_pages_ratelimited()看脏页是否太多需要写回硬盘。所谓脏页就是写入到缓存但是还没有写入到硬盘的页面。 123456789101112131415161718192021222324ssize_t generic_perform_write(struct file *file, struct iov_iter *i, loff_t pos)&#123; struct address_space *mapping = file-&gt;f_mapping; const struct address_space_operations *a_ops = mapping-&gt;a_ops;...... do &#123; struct page *page; unsigned long offset;\t/* Offset into pagecache page */ unsigned long bytes;\t/* Bytes to write to page */...... status = a_ops-&gt;write_begin(file, mapping, pos, bytes, flags, &amp;page, &amp;fsdata);...... copied = iov_iter_copy_from_user_atomic(page, i, offset, bytes);...... status = a_ops-&gt;write_end(file, mapping, pos, bytes, copied, page, fsdata);...... balance_dirty_pages_ratelimited(mapping); &#125; while (iov_iter_count(i)); return written ? written : status;&#125;EXPORT_SYMBOL(generic_perform_write); 第一步，对于ext4 来讲调用的是 ext4_write_begin()函数。ext4有着以下几种模式 日志（Journal）模式。日志文件系统比非日志文件系统多了一个 Journal 区域。文件在 ext4 中分两部分存储，一部分是文件的元数据，另一部分是数据。元数据和数据的操作日志 Journal 也是分开管理的。你可以在挂载 ext4 的时候，选择 Journal 模式。这种模式在将数据写入文件系统前，必须等待元数据和数据的日志已经落盘才能发挥作用。这样性能比较差，但是最安全。 order 模式。这个模式不记录数据的日志，只记录元数据的日志，但是在写元数据的日志前必须先确保数据已经落盘。这个折中是默认模式。 writeback，不记录数据的日志，仅记录元数据的日志，并且不保证数据比元数据先落盘。这个性能最好，但是最不安全。 在 ext4_write_begin()，我们能看到对于 ext4_journal_start() 的调用就是在做日志相关的工作。在 ext4_write_begin() 中，还做了另外一件重要的事情，就是调用 grab_cache_page_write_begin()来得到应该写入的缓存页。 12345678910111213141516171819202122static int ext4_write_begin(struct file *file, struct address_space *mapping, loff_t pos, unsigned len, unsigned flags, struct page **pagep, void **fsdata)&#123;...... page = grab_cache_page_write_begin(mapping, index, flags);...... handle = ext4_journal_start(inode, EXT4_HT_WRITE_PAGE, needed_blocks);......&#125;struct page *grab_cache_page_write_begin(struct address_space *mapping, pgoff_t index, unsigned flags)&#123; struct page *page; int fgp_flags = FGP_LOCK|FGP_WRITE|FGP_CREAT; page = pagecache_get_page(mapping, index, fgp_flags, mapping_gfp_mask(mapping)); if (page) wait_for_stable_page(page); return page;&#125; 在内核中，缓存以页为单位放在内存里面，为了知道文件的哪些数据已经存放在内存中，我们需要使用address space。每一个打开的文件都有一个 struct file 结构，每个 struct file 结构都有一个 struct address_space 用于关联文件和内存，就是在这个结构里面，有一棵树用于保存所有与这个文件相关的的缓存页，这就是radix tree。我们查找的时候，往往需要根据文件中的偏移量找出相应的页面，而基数树 radix tree 这种数据结构能够快速根据一个长整型查找到其相应的对象，因而这里缓存页就放在 radix 基数树里面。pagecache_get_page() 就是根据 pgoff_t index 这个长整型，在这棵树里面查找缓存页，如果找不到就会创建一个缓存页。 123456789101112131415161718192021struct address_space &#123; struct inode *host; /* owner: inode, block_device */ struct radix_tree_root page_tree; /* radix tree of all pages */ spinlock_t tree_lock; /* and lock protecting it */......&#125; 第二步，调用 iov_iter_copy_from_user_atomic()。先将分配好的页面调用 kmap_atomic() 映射到内核里面的一个虚拟地址，然后将用户态的数据拷贝到内核态的页面的虚拟地址中，调用 kunmap_atomic() 把内核里面的映射删除。size_t iov_iter_copy_from_user_atomic(struct page *page, struct iov_iter *i, unsigned long offset, size_t bytes)&#123; char *kaddr = kmap_atomic(page), *p = kaddr + offset; iterate_all_kinds(i, bytes, v, copyin((p += v.iov_len) - v.iov_len, v.iov_base, v.iov_len), memcpy_from_page((p += v.bv_len) - v.bv_len, v.bv_page, v.bv_offset, v.bv_len), memcpy((p += v.iov_len) - v.iov_len, v.iov_base, v.iov_len) ) kunmap_atomic(kaddr); return bytes;&#125; 第三步，调用 ext4_write_end() 完成写入。这里面会调用 ext4_journal_stop() 完成日志的写入，会调用 block_write_end-&gt;__block_commit_write-&gt;mark_buffer_dirty，将修改过的缓存标记为脏页。可以看出，其实所谓的完成写入，并没有真正写入硬盘，仅仅是写入缓存后，标记为脏页。 第四步，调用 balance_dirty_pages_ratelimited()，回写脏页。 12345678910111213141516171819/** * balance_dirty_pages_ratelimited - balance dirty memory state * @mapping: address_space which was dirtied * * Processes which are dirtying memory should call in here once for each page * which was newly dirtied. The function will periodically check the system&#x27;s * dirty state and will initiate writeback if needed. */void balance_dirty_pages_ratelimited(struct address_space *mapping)&#123; struct inode *inode = mapping-&gt;host; struct backing_dev_info *bdi = inode_to_bdi(inode); struct bdi_writeback *wb = NULL; int ratelimit;...... if (unlikely(current-&gt;nr_dirtied &gt;= ratelimit)) balance_dirty_pages(mapping, wb, current-&gt;nr_dirtied);......&#125; 在 balance_dirty_pages_ratelimited() 里面，发现脏页的数目超过了规定的数目，就调用 balance_dirty_pages()-&gt;wb_start_background_writeback()，启动一个背后线程开始回写。 1234567891011121314151617181920212223242526272829303132void wb_start_background_writeback(struct bdi_writeback *wb)&#123; /* * We just wake up the flusher thread. It will perform background * writeback as soon as there is no other work to do. */ wb_wakeup(wb);&#125;static void wb_wakeup(struct bdi_writeback *wb)&#123; spin_lock_bh(&amp;wb-&gt;work_lock); if (test_bit(WB_registered, &amp;wb-&gt;state)) mod_delayed_work(bdi_wq, &amp;wb-&gt;dwork, 0); spin_unlock_bh(&amp;wb-&gt;work_lock);&#125;/* bdi_wq serves all asynchronous writeback tasks */struct workqueue_struct *bdi_wq;/** * mod_delayed_work - modify delay of or queue a delayed work * @wq: workqueue to use * @dwork: work to queue * @delay: number of jiffies to wait before queueing * * mod_delayed_work_on() on local CPU. */static inline bool mod_delayed_work(struct workqueue_struct *wq, struct delayed_work *dwork, unsigned long delay)&#123;.... 通过上面的代码我们可以看出，bdi_wq 是一个全局变量，所有回写的任务都挂在这个队列上。mod_delayed_work() 函数负责将一个回写任务 bdi_writeback 挂在这个队列上。bdi_writeback 有个成员变量 struct delayed_work dwork，bdi_writeback 就是以 delayed_work 的身份挂到队列上的，并且把 delay 设置为 0，意思就是一刻不等，马上执行。这里的 bdi 的意思是 backing device info，用于描述后端存储相关的信息。每个块设备都会有这样一个结构，并且在初始化块设备的时候，调用 bdi_init() 初始化这个结构，在初始化 bdi 的时候，也会调用 wb_init() 初始化 bdi_writeback。 1234567891011121314151617181920212223242526static int wb_init(struct bdi_writeback *wb, struct backing_dev_info *bdi, int blkcg_id, gfp_t gfp)&#123; wb-&gt;bdi = bdi; wb-&gt;last_old_flush = jiffies; INIT_LIST_HEAD(&amp;wb-&gt;b_dirty); INIT_LIST_HEAD(&amp;wb-&gt;b_io); INIT_LIST_HEAD(&amp;wb-&gt;b_more_io); INIT_LIST_HEAD(&amp;wb-&gt;b_dirty_time); wb-&gt;bw_time_stamp = jiffies; wb-&gt;balanced_dirty_ratelimit = INIT_BW; wb-&gt;dirty_ratelimit = INIT_BW; wb-&gt;write_bandwidth = INIT_BW; wb-&gt;avg_write_bandwidth = INIT_BW; spin_lock_init(&amp;wb-&gt;work_lock); INIT_LIST_HEAD(&amp;wb-&gt;work_list); INIT_DELAYED_WORK(&amp;wb-&gt;dwork, wb_workfn); wb-&gt;dirty_sleep = jiffies;......&#125;#define __INIT_DELAYED_WORK(_work, _func, _tflags) \\ do &#123; \\ INIT_WORK(&amp;(_work)-&gt;work, (_func)); \\ __setup_timer(&amp;(_work)-&gt;timer, delayed_work_timer_fn, \\ (unsigned long)(_work), \\ 这里面最重要的是 INIT_DELAYED_WORK。其实就是初始化一个 timer，也即定时器，到时候我们就执行 wb_workfn() 这个函数。接下来的调用链为：wb_workfn-&gt;wb_do_writeback-&gt;wb_writeback-&gt;writeback_sb_inodes-&gt;__writeback_single_inode-&gt;do_writepages，写入页面到硬盘。在调用 write 的最后，当发现缓存的数据太多的时候会触发回写，这仅仅是回写的一种场景。另外还有几种场景也会触发回写： 用户主动调用 sync，将缓存刷到硬盘上去，最终会调用 wakeup_flusher_threads，同步脏页； 当内存十分紧张，以至于无法分配页面的时候，会调用 free_more_memory，最终会调用 wakeup_flusher_threads，释放脏页； 脏页已经更新了较长时间，时间上超过了 timer，需要及时回写，保持内存和磁盘上数据一致性。 5.2 带缓冲的读操作 带缓存的写分析完了，接下来，我们看带缓存的读，对应的是函数 generic_file_buffered_read()。 123456789101112131415161718192021222324252627282930313233static ssize_t generic_file_buffered_read(struct kiocb *iocb, struct iov_iter *iter, ssize_t written)&#123; struct file *filp = iocb-&gt;ki_filp; struct address_space *mapping = filp-&gt;f_mapping; struct inode *inode = mapping-&gt;host; for (;;) &#123; struct page *page; pgoff_t end_index; loff_t isize; page = find_get_page(mapping, index); if (!page) &#123; if (iocb-&gt;ki_flags &amp; IOCB_NOWAIT) goto would_block; page_cache_sync_readahead(mapping, ra, filp, index, last_index - index); page = find_get_page(mapping, index); if (unlikely(page == NULL)) goto no_cached_page; &#125; if (PageReadahead(page)) &#123; page_cache_async_readahead(mapping, ra, filp, page, index, last_index - index); &#125; /* * Ok, we have the page, and it&#x27;s up-to-date, so * now we can copy it to user space... */ ret = copy_page_to_iter(page, offset, nr, iter); &#125;&#125; 读取比写入总体而言简单一些，主要涉及预读的问题。在 generic_file_buffered_read() 函数中，我们需要先找到 page cache 里面是否有缓存页。如果没有找到，不但读取这一页，还要进行预读，这需要在 page_cache_sync_readahead() 函数中实现。预读完了以后，再试一把查找缓存页，应该能找到了。如果第一次找缓存页就找到了，我们还是要判断，是不是应该继续预读；如果需要，就调用 page_cache_async_readahead() 发起一个异步预读。最后，copy_page_to_iter() 会将内容从内核缓存页拷贝到用户内存空间。 六. 总结 本文详细的总结了虚拟文件系统，并分析了文件操作中的打开和读写操作，由此我们完成了文件系统的框架的全部解析工作。"},{"path":"/2024/04/05/linux-docs/内存管理/Linux中的内存管理机制/","content":"程序在运行时所有的数据结构的分配都是在堆和栈上进行的，而堆和栈都是建立在内存之上。内存作为现代计算机运行的核心，CPU可以直接访问的通用存储只有内存和处理器内置的寄存器，所有的代码都需要装载到内存之后才能让CPU通过指令寄存器找到相应的地址进行访问。 地址空间和MMU内存管理单元(MMU)是硬件提供的最底层的内存管理机制，是CPU的一部分，用来管理内存的控制线路，提供把虚拟地址映射为物理地址的能力。 在x86体系结构下，CPU对内存的寻址都是通过分段方式进行的。其工作流程为：CPU生成逻辑地址并交给分段单元。分段单元为每个逻辑地址生成一个线性地址。然后线性地址交给分页单元，以生成内存的物理地址。因此也就是分段和分页单元组成了内存管理单元(MMU)。 其中： + 虚拟地址：在段中的偏移地址 + 线性地址：在某个段中“基地址+偏移地址”得出的地址 + 物理地址：在x86中，MMU还提供了分页机制，假如没有开启分页机制，那么线性地址就等于物理地址；否则还需要经过分页机制换算后线性地址才能转换成物理地址。 一个段是由“基地址+段界限（该段长度）+类型”组成，主要确定了段的起始地址，段的界限长度和确定段的属性如是否可读、可写、段的基本粒度单位、表述该段是数据段还是代码段等。 分段允许进程的物理地址空间是非连续的，分页则是提供这一优势的另外一种内存管理方案，并且分页避免了外部碎片和紧缩，分段却不可以。在x86体系中MMU支持多级的分页模型，主要分为以下三种情况： 1. 32为系统分为2级分页模型 2. 32位系统开启了物理地址扩展模式（PAE），则分为3级分页模型 3. 64位系统分为4级分页模型 80x86的分页机制由CR0中的PG位开启，若PG&#x3D;0则禁用分页机制，也就是直接将线性地址作为物理地址。32位的线性地址主要分为三个部分： 22-31位指向页目录表中的某一项，页目录表中的每一项存有4子节地址指向页表。所以页表目录大小为4 * 210 &#x3D; 4K 12-21位指向页表中的某一项，页表大小与页目录表相同为4K 一个物理页为4K，刚好0-11位指向页表中的偏移，一个页表刚好4K(212) 页表和页目录表可以存放在内存的任何地方，当分页机制开启后，需要让CR3寄存器指向页目录表的起始地址。 CR0-CR4这五个寄存器为系统内的控制寄存器，与分页机制密切相关。CR0控制寄存器是一些特殊的寄存器，可以控制CPU的一些重要特性；CR1是未定义的控制寄存器，供将来使用；CR2是页故障线性地址寄存器，保存最后一次出现页故障的全32位线性地址；CR3是页目录基址寄存器，保存页目录表的物理地址（页目录表总是放在4k为单位的存储器边界上，因此其低12位总为0不起作用，即使写上内容也不会被理会）CR4在Pentium系列（包括486后期版本）处理器中才出现，处理事务包括何时启用虚拟8086模式等。 Linux中的分段与分页MMU在保护模式下分段数据主要定义在GDT中。 1234567891011//arch/x86/kernel/cpu/common.cDEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = &#123; .gdt = &#123;... [GDT_ENTRY_KERNEL_CS] = GDT_ENTRY_INIT(0xc09a, 0, 0xfffff), //代码段 [GDT_ENTRY_KERNEL_DS] = GDT_ENTRY_INIT(0xc092, 0, 0xfffff), //数据段 [GDT_ENTRY_DEFAULT_USER_CS] = GDT_ENTRY_INIT(0xc0fa, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER_DS] = GDT_ENTRY_INIT(0xc0f2, 0, 0xfffff),...&#125; &#125;;EXPORT_PER_CPU_SYMBOL_GPL(gdt_page); 通过代码可知道这些段的基地址都是0，界限为4G。说明Linux只定义了一个段，并没有真正利用分段机制。 Linux中只用了一个段，而且基地址从0开始，那么在程序中使用的虚地址就是线性地址了。Linux为了兼容64位、32位及其PAE扩展情况，在代码中通过4级分页机制来做兼容。 Linux的内存分配与管理在32位的x86设备中，Linux为每个进程分配的虚拟地址空间都是0-4GB，其中 0-3GB用于用户态使用 3GB-3GB+896MB映射到物理地址的0-896MB处，作为内核态地址空间 3GB+896MB-4GB之间的128MB空间用于vmalloc保留区域，该区域用于kmalloc、kmap固定地址映射等功能，可以让内核访问高端物理地址空间 Linux中进程的地址空间由mm_struct来描述，一个进程只会有一个mm_struct。系统中的内核态是共享的，不会发生缺页中断或者访问用户进程空间，所以内核线程的task_struct-&gt;mm为NULL。 页表的分配分为两个部分： 1、内核页表，也就是在系统启动中，最后会在paging_init函数中，把ZONE_DMA和ZONE_NORMAL区域的物理页面与虚拟地址空间的3GB-3GB+896MB进行直接映射2、内核高端地址和用户态地址，都是通过MMU机制修改线性地址（虚拟地址）和物理地址的映射关系，然后刷新页表缓存来达到的 物理内存中ZONE_DMA的范围是0-16MB，该区域的物理页面专门供IO设备的DMA使用，之所以要单独管理DMA的物理页面，是因为DMA使用物理地址访问内存不经过MMU，并且需要连续的缓冲区。为了能够提供物理上的连续缓冲区，必须从物理地址专门划分出一段区域用于DMA。 ZONE_NORMAL的范围是16MB-896MB，该区域的物理页面是内核能够直接使用的。 ZONE_HIGHMEM的范围是896MB-结束，该区域即高端内存，内核不能直接使用。 伙伴系统对于物理内存经过频繁地申请和释放后会产生外部碎片，Linux通过伙伴系统来解决外部碎片的问题。 满足:1.具有相同的大小；2.物理地址连续条件的两个块为伙伴。主要实现思路位伙伴系统在申请内存的时候让最小的块满足申请的需求，在归还的时候，尽量让连续的小块内存伙伴合并成大块，降低外部碎片出现的可能性。 在Linux系统中伙伴系统维护了11个块链表，每个块链表分别包含了大小为20-211个连续的物理页。对1024个页的最大请求对应着4MB大小的连续RAM块。每个快的第一个页框的物理地址就是该块大小的整数倍。如大小为16个页框的块，其起始地址为16×212(212&#x3D;4KB这是一个页的大小)的倍数。 系统在初始化的时候把内各节点各区域都释放到伙伴系统中，每个区域还维护了per-cpu高速缓存来处理单页的分配，各个区域都通过伙伴算法进行物理内存的分配。 slab分配器Linux系统通过伙伴算法解决了外部碎片的问题，此外还提供了slab分配器来处理内部碎片的问题。slab分配器也是一种内存预分配机制，是一种空间换时间的做法，并且其假定从slab分配器中获得的内存都是比页还小的小内存块。 slab的设计思想就是把若干的页框合在一起形成一大存储块——slab，并在这个slab中只存储同一类数据，这样就可以在这个slab内部打破页的界限，以该类型数据的大小来定义分配粒度，存放多个数据，这样就可以尽可能地减少页内碎片了。在Linux中，多个存储同类数据的slab的集合叫做一类对象的缓冲区——cache。注意，这不是硬件的那个cache，只是借用这个名词而已。 Linux中slab的可分为以下三种状态： 1、slabs_full：该链表中slab已经完全分配出去2、slabs_free：该链表中的slab都是空闲可分配状态3、labs_partial：该链表中的slab部分已经被分配出去了 其中slab代表物理地址连续的内存块，由1-N个物理页面组成，在一个slab中可以分配多个object对象。 slab的优点： 内核通常依赖于对小对象的分配，它们会在系统生命周期内进行无数次分配。slab 缓存分配器通过对类似大小的对象进行缓存而提供这种功能，从而避免了常见的碎片问题； slab 分配器还支持通用对象的初始化，从而避免了为同一目的而对一个对象重复进行初始化； slab 分配器还可以支持硬件缓存对齐和着色，这允许不同缓存中的对象占用相同的缓存行，从而提高缓存的利用率并获得更好的性能。 slab的缺点： 较多复杂的队列管理。在slab分配器中存在众多的队列，例如针对处理器的本地缓存队列，slab中空闲队列，每个slab处于一个特定状态的队列之中。 slab管理数据和队列的存储开销比较大。每个slab需要一个struct slab数据结构和一个管理者kmem_bufctl_t型的数组。当对象体积较小时，该数组将造成较大的开销（比如对象大小为32字节时，将浪费1&#x2F;8空间）。同时，缓冲区针对节点和处理器的队列也会浪费不少内存。 缓冲区回收、性能调试调优比较复杂。 内核态内存管理根据之前的的Linux的内存管理机制，即伙伴系统和slab分配器。对于内核态的内存分配主要通过函数kmalloc和vmalloc完成。 其中kmalloc函数可以为内核申请连续物理地址的内存空间，由于kmalloc是基于slab分配器实现的，所以比较适合较小块的内存申请。kmalloc函数的调用过程为：kmalloc-&gt;__kmalloc-&gt;__do_kmalloc，其中__do_kmalloc的实现主要分为两步： 1、通过kmalloc_slab找到一个合适的kmem_cache缓存2、通过slab_alloc向slab分配器申请对象内存空间 Linux提供的vmalloc函数可以获得连续的虚拟空间，但是其物理内存不一定连续。vmalloc函数的调用过程为：vmalloc-&gt;__vmalloc_node_flags-&gt;__vmalloc_node-&gt;__vmalloc_node_range。其中__vmalloc_node_range函数也分为两步： 1、通过__get_vm_area_node分配一个可用的虚拟地址空间2、__vmalloc_node_range通过alloc_pages一页一页申请物理内存，再为刚才申请的虚拟地址空间分配物理页表映射"},{"path":"/2024/04/05/linux-docs/内存管理/一文了解，Linux内存管理，malloc、free 实现原理/","content":"malloc &#x2F; free 简介12void *malloc(size_t size)void free(void *ptr) malloc 分配指定大小的内存空间，返回一个指向该空间的指针。大小以字节为单位。返回 void* 指针，需要强制类型转换后才能引用其中的值。free 释放一个由 malloc 所分配的内存空间。ptr 指向一个要释放内存的内存块，该指针应当是之前调用 malloc 的返回值。 使用示例： 123int* ptr; ptr = (int*)malloc(10 * sizeof(int)); /* 进行强制类型转换 */ free(ptr); 动态内存分配的系统调用：brk / sbrk动态分配的内存都在堆中，堆从低地址向高地址增长： Linux 提供了两个系统调用 brk 和 sbrk： 12int brk(void *addr);void *sbrk(intptr_t increment); brk 用于返回堆的顶部地址；sbrk 用于扩展堆，通过参数 increment 指定要增加的大小，如果扩展成功，返回 brk 的旧值。如果 increment 为零，返回 brk 的当前值。 我们不会直接通过 brk 或 sbrk 来分配堆内存，而是先通过 sbrk 扩展堆，将这部分空闲内存空间作为缓冲池，然后通过 malloc / free 管理缓冲池中的内存。这是一种池化思想，能够避免频繁的系统调用，提高程序性能。 malloc &#x2F; free 实现思路malloc 使用空闲链表组织堆中的空闲区块，空闲链表有时也用双向链表实现。每个空闲区块都有一个相同的首部，称为“内存控制块” mem_control_block，其中记录了空闲区块的元信息，比如指向下一个分配块的指针、当前分配块的长度、或者当前区块是否已经被分配出去。这个首部对于程序是不可见的，malloc 返回的是紧跟在首部后面的地址，即可用空间的起始地址。 malloc 分配时会搜索空闲链表，根据匹配原则，找到一个大于等于所需空间的空闲区块，然后将其分配出去，返回这部分空间的指针。如果没有这样的内存块，则向操作系统申请扩展堆内存。注意，返回的指针是从可用空间开始的，而不是从首部开始的： malloc 所实际使用的内存匹配算法有很多，执行时间和内存消耗各有不同。到底使用哪个匹配算法，取决于实现。常见的内存匹配算法有： 最佳适应法 最差适应法 首次适应法 下一个适应法 free 会将区块重新插入到空闲链表中。free 只接受一个指针，却可以释放恰当大小的内存，这是因为在分配的区域的首部保存了该区域的大小。 malloc 的实现方式一：显式空闲链表 + 整块分配malloc 的实现方式有很多种。最简单的方法是使用一个链表来管理所有已分配和未分配的内存块，在每个内存块的首部记录当前块的大小、当前区块是否已经被分配出去。首部对应这样的结构体： 1234struct mem_control_block &#123; int is_available; // 是否可用（如果还没被分配出去，就是 1） int size; // 实际空间的大小&#125;; 使用首次适应法进行分配：遍历整个链表，找到第一个未被分配、大小合适的内存块；如果没有这样的内存块，则向操作系统申请扩展堆内存。 下面是这种实现方式的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263int has_initialized = 0; // 初始化标志void *managed_memory_start; // 指向堆底（内存块起始位置）void *last_valid_address; // 指向堆顶void malloc_init() &#123; // 这里不向操作系统申请堆空间，只是为了获取堆的起始地址 last_valid_address = sbrk(0); managed_memory_start = last_valid_address; has_initialized = 1;&#125;void *malloc(long numbytes) &#123; void *current_location; // 当前访问的内存位置 struct mem_control_block *current_location_mcb; // 只是作了一个强制类型转换 void *memory_location; // 这是要返回的内存位置。初始时设为 // 0，表示没有找到合适的位置 if (!has_initialized) &#123; malloc_init(); &#125; // 要查找的内存必须包含内存控制块，所以需要调整 numbytes 的大小 numbytes = numbytes + sizeof(struct mem_control_block); // 初始时设为 0，表示没有找到合适的位置 memory_location = 0; /* Begin searching at the start of managed memory */ // 从被管理内存的起始位置开始搜索 // managed_memory_start 是在 malloc_init 中通过 sbrk() 函数设置的 current_location = managed_memory_start; while (current_location != last_valid_address) &#123; // current_location 是一个 void 指针，用来计算地址； // current_location_mcb 是一个具体的结构体类型 // 这两个实际上是一个含义 current_location_mcb = (struct mem_control_block *)current_location; if (current_location_mcb-&gt;is_available) &#123; if (current_location_mcb-&gt;size &gt;= numbytes) &#123; // 找到一个可用、大小适合的内存块 current_location_mcb-&gt;is_available = 0; // 设为不可用 memory_location = current_location; // 设置内存地址 break; &#125; &#125; // 否则，当前内存块不可用或过小，移动到下一个内存块 current_location = current_location + current_location_mcb-&gt;size; &#125; // 循环结束，没有找到合适的位置，需要向操作系统申请更多内存 if (!memory_location) &#123; // 扩展堆 sbrk(numbytes); // 新的内存的起始位置就是 last_valid_address 的旧值 memory_location = last_valid_address; // 将 last_valid_address 后移 numbytes，移动到整个内存的最右边界 last_valid_address = last_valid_address + numbytes; // 初始化内存控制块 mem_control_block current_location_mcb = memory_location; current_location_mcb-&gt;is_available = 0; current_location_mcb-&gt;size = numbytes; &#125; // 最终，memory_location 保存了大小为 numbyte的内存空间， // 并且在空间的开始处包含了一个内存控制块，记录了元信息 // 内存控制块对于用户而言应该是透明的，因此返回指针前，跳过内存分配块 memory_location = memory_location + sizeof(struct mem_control_block); // 返回内存块的指针 return memory_location;&#125; 对应的free实现： 123456void free(void *ptr) &#123; // ptr 是要回收的空间 struct mem_control_block *free; free = ptr - sizeof(struct mem_control_block); // 找到该内存块的控制信息的地址 free-&gt;is_available = 1; // 该空间置为可用 return;&#125; 这种方法的缺点是： 1、已分配和未分配的内存块位于同一个链表中，每次分配都需要从头到尾遍历2、采用首次适应法，内存块会被整体分配，容易产生较多内部碎片 malloc 的实现方式二：显式空闲链表 + 按需分配这种实现方式维护一个空闲块链表，只包含未分配的内存块。malloc 分配时会搜索空闲链表，找到第一个大于等于所需空间的空闲区块，然后从该区块的尾部取出所需要的空间，剩余空间还是存在空闲链表中；如果该区块的剩余部分不足以放下首部信息，则直接将其从空闲链表摘除。最后返回这部分空间的指针。下面是这种实现方式的几个示例： 通过 free 释放内存时，会将内存块加入到空闲链表中，并将前后相邻的空闲内存合并，这时使用双向链表管理空闲链表就很有用了。 和第一种方式相比，这种方式的优点主要是： 空闲链表中只包含未被分配的内存块，节省遍历开销 只分配必须大小的空间，避免内存浪费 这种方式的缺点是：多次调用 malloc 后，空闲内存被切成很多的小内存片段，产生较多外部碎片，会导致用户在申请内存使用时，找不到足够大的内存空间。这时需要进行内存整理，将连续的空闲内存合并，但是这会降低函数性能。 注意：内存紧凑在这里一般是不可用的，因为这会改变之前 malloc 返回的空间的地址。 malloc 的实现方式三：分离的空闲链表上面的两种分配方法，分配时间都和空闲块的数量成线性关系。 另一种实现方式是分离存储，即维护多个空闲链表，其中每个链表中的块有大致相等或者相同的大小。一般常见的是根据 2 的幂来划分块大小。分配时，可以直接在某个空闲链表里搜索合适的块。如果没有找到合适的块与之匹配，就搜索下一个链表，以此类推。 简单分离存储每个大小类的空闲链表包含大小相等的块。分配时，从某个空闲链表取下一块，或者向操作系统请求内存片并分割成大小相等的块，形成新的链表。释放时，只需要简单的将块插入到相应空闲链表的前面。 优点一是分配和释放只需要在链表头进行操作，都是常数时间，二是因为每个块大小都是固定的，所以只需要一个 next 指针，不需要额外的控制信息，节省空间。缺点是容易造成内部碎片和外部碎片。内部碎片显而易见，因为每个块都是整体分配的，不会被分割。外部碎片在这样的模式下很容易产生：应用频繁地申请和释放较小大小的内存块，由于这些内存块不会合并，所以系统维护了大量小内存块形成的空闲链表，而没有多余空间来分配大内存块，导致产生外部碎片。 分离适配这种方法同样维护了多个空闲链表，只不过每个链表中的块是大致相等的大小，比如每个链表中的块大小范围可能是： 1 2 3~4 5~8 … 1025~2048 2049~4096 4097~∞ 在分配的时候，需要先根据申请内存的大小选择适当的空闲链表，然后遍历该链表，根据匹配算法（如首次适应）寻找合适的块。如果找到一个块，将其分割（可选），并将剩余部分插入到适当的空闲链表中。如果找不到合适的块，则查找下一个更大的大小类的空闲链表，以此类推，直到找到或者向操作系统申请额外的堆内存。在释放一个块时，合并前后相邻的空闲块，并将结果放到相应的空闲链表中。 分离适配方法是一种常见的选择，C 标准库中提供的 GNU malloc 包就是采用的这种方法。这种方法既快速，对内存的使用也很有效率。由于搜索被限制在堆的某个部分而不是整个堆，所以搜索时间减少了。内存利用率也得到了改善，避免大量内部碎片和外部碎片。 伙伴系统伙伴系统是分离适配的一种特例。它的每个大小类的空闲链表包含大小相等的块，并且大小都是 2 的幂。最开始时，全局只有一个大小为 2m2m 字的空闲块，2m2m 是堆的大小。 假设分配的块的大小都是 2 的幂，为了分配一个大小为 2k2k 的块，需要找到大小恰好是 2k2k 的空闲块。如果找到，则整体分配。如果没有找到，则将刚好比它大的块分割成两块，每个剩下的半块（也叫做伙伴）被放置在相应的空闲链表中，以此类推，直到得到大小恰好是 2k2k 的空闲块。释放一个大小为 2k2k 的块时，将其与空闲的伙伴合并，得到新的更大的块，以此类推，直到伙伴已分配时停止合并。 伙伴系统分配器的主要优点是它的快速搜索和快速合并。主要缺点是要求块大小为 2 的幂可能导致显著的内部碎片。因此，伙伴系统分配器不适合通用目的的工作负载。然而，对于某些特定应用的工作负载，其中块大小预先知道是 2 的幂，伙伴系统分配器就很有吸引力了。 tcmalloctcmalloc 是 Google 开发的内存分配器，全称 Thread-Caching Malloc，即线程缓存的 malloc，实现了高效的多线程内存管理。 tcmalloc 主要利用了池化思想来管理内存分配。对于每个线程，都有自己的私有缓存池，内部包含若干个不同大小的内存块。对于一些小容量的内存申请，可以使用线程的私有缓存；私有缓存不足或大容量内存申请时再从全局缓存中进行申请。在线程内分配时不需要加锁，因此在多线程的情况下可以大大提高分配效率。 总结malloc 使用链表管理内存块。malloc 有多种实现方式，在不同场景下可能会使用不同的匹配算法。 malloc 分配的空间中包含一个首部来记录控制信息，因此它分配的空间要比实际需要的空间大一些。这个首部对用户而言是透明的，malloc 返回的是紧跟在首部后面的地址，即可用空间的起始地址。 malloc 分配的函数应该是字对齐的。在 32 位模式中，malloc 返回的块总是 8 的倍数。在 64 位模式中，该地址总是 16 的倍数。最简单的方式是先让堆的起始位置字对齐，然后始终分配字大小倍数的内存。 malloc 只分配几种固定大小的内存块，可以减少外部碎片，简化对齐实现，降低管理成本。 free 只需要传递一个指针就可以释放内存，空间大小可以从首部读取。"},{"path":"/2024/04/05/linux-docs/内存管理/其他工程问题以及调优/","content":"其他工程问题以及调优 DMA和cache一致性 内存的cgroup memcg子系统分析 性能方面的调优： page in&#x2F;out, swap in&#x2F;out Dirty ratio的一些设置 swappiness DMA和cache一致性 工程中，DMA可以直接在内存和外设进行数据搬移，而CPU访问内存时要经过MMU。DMA访问不到CPU内部的cache，所以会出现cache不一致的问题。因为CPU读写内存时，如果在cache中命中，就不会再访问内存。 当CPU 写memory时，cache有两种算法：write_back ，write_through。一般都采用write_back。cache的硬件，使用LRU算法，把cache中的数据替换到磁盘。 cache一致性问题，主要靠以上两类api来解决这个问题。一致性DMA缓冲区api，和流式DMA映射api。CPU通过MMU访问DMA区域，在页表项中可以配置这片区域是否带cache。 现代的SoC，DMA引擎可以自动维护cache的同步。 内存的cgroup进程分group，内存也分group。 进程调度时，把一组进程加到一个cgroup，控制这一组进程的CPU权重和最大CPU占用率。在&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory创建一个目录，把进程放到这个group。可以限制某个group下的进程不用swap，每个group的swapiness都可以配置。 比如，当你把某个group下的swapiness设置为0，那么这个group下进程的匿名页就不允许交换了。&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;swapiness是控制全局的swap特性，不影响加到group中的进程。 也可以控制每个group的最大内存消耗为200M，当这个group下进程使用的内存达到200M，就oom。 demo: 演示用memory cgroup来限制进程group内存资源消耗的方法 123456789101112swapoff -aecho 1 &gt; /proc/sys/vm/overcommit_memory # 进程申请多少资源，内核都允许root@whale:/sys/fs/cgroup/memory# mkdir Aroot@whale:/sys/fs/cgroup/memory# cd Aroot@whale:/sys/fs/cgroup/memory/A# echo $((200*1024*1024)) &gt; memory.limit_in_bytescgexec -g memory:A ./a.out[ 560.618666] Memory cgroup out of memory: Kill process 5062 (a.out) score 977 or sacrifice child[ 560.618817] Killed process 5062 (a.out) total-vm:2052084kB, anon-rss:204636kB, file-rss:1240kB memory cgroup子系统分析memcg v1的参数有25个, 通过数据结构 res_counter 来计算。 123456789101112131415~~~/* * The core object. the cgroup that wishes to account for some * resource may include this counter into its structures and use * the helpers described beyond */struct res_counter &#123; unsigned long long usage; /* * 目前资源消费的级别 */ unsigned long long max_usage; /* *从counter创建的最大使用值 */ unsigned long long limit; /* * 不能超过的使用限制 */ unsigned long long soft_limit; /* * 可以超过使用的限制 */ unsigned long long failcnt; /* * 尝试消费资源的失败数 */ spinlock_t lock; /* * the lock to protect all of the above. * the routines below consider this to be IRQ-safe */ struct res_counter *parent; /* * Parent counter, used for hierarchial resource accounting */ &#125;; 内存的使用量 mem_cgroup_usage 通过递归RSS和page cache之和来计算。 struct mem_cgroup是负责内存 cgroup 的结构 123456struct mem_cgroup &#123; struct cgroup_subsys_state css; // 通过css关联cgroup. struct res_counter res; // mem统计变量 res_counter memsw; // mem+sw的和 struct res_counter kmem; // 内核内存统计量\t...&#125; 这些参数的入口都在mm&#x2F;memcontrol.c下，比如说memory.usage_in_bytes的读取调用的是mem_cgroup_read函数, 统计的入口是mem_cgroup_charge_common()，如果统计值超过限制就会在cgroup内进行回收。调用者分别是缺页时调用的mem_cgroup_newpage_charge和 page cache相关的mem_cgroup_cache_charge。 当进程进入缺页异常的时候就会分配具体的物理内存，当物理内存使用超过高水平线以后，换页daemon(kswapd)就会被唤醒用于把内存交换到交换空间以腾出内存，当内存恢复至高水平线以后换页daemon进入睡眠。 缺页异常的入口是 __do_fault， RSS在page_fault的时候记录，page cache是插入到inode的radix-tree中才记录的。RSS在完全unmap的时候减少计数，page cache的page在离开inode的radix-tree才减少计数。即使RSS完全unmap，也就是被kswapd给换出，可能作为SwapCache存留在系统中，除非不作为SwapCache，不然还是会被计数。一个换入的page不会马上计数，只有被map的时候才会，当进行换页的时候，会预读一些不属于当前进程的page，而不是通过page fault，所以不在换入的时候计数。 脏页写回的“时空”控制 “脏页”：当进程修改了高速缓存里的数据时，该页就被内核标记为脏页，内核将会在合适的时间把脏页的数据写到磁盘中去，以保持高速缓存中的数据和磁盘中的数据是一致的。 通过时间(dirty_expire_centisecs)和比例，控制Linux脏页返回。 dirty_expire_centisecs：当Linux中脏页的时间到达dirty_expire_centisecs，无论脏页的数量多少，必须立即写回。通过在后台启动进程，进行脏页写回。默认时间设置为30s。 dirty_ratio，dirty_background_ratio 基于空间的脏页写回控制。不能让内存中存在太多空间的脏页。如果一个进程在循环调用write，当达到dirty_background_ratio后，后台进程就开始写回脏页。默认值5%。当达到第2个阈值dirty_ratio时，应用进程被阻塞。当内存中的脏页在两个阈值之间时，应用程序是不会阻塞。 内存何时回收：水位控制脏页写回不是 内存回收。 脏页写回：是保证在内存不在磁盘的数据不要太多。水位控制：是指内存何时开始回收。 由&#x2F;pro&#x2F;sys&#x2F;vm&#x2F;min_free_kbytes 控制，根据内存大小算出来的平方根。pf_mem_alloc，允许内存达到低水位以下，还可以继续申请。内存的回收，在最低水位以上就开始回收。 每个Zone都有自己的三个水位，最小的水位是根据min_free_kbytes控制。5&#x2F;4min_free_kbytes ＝low 3&#x2F;2min_free_kbytes ＝high ,Zone的最小内存达到5&#x2F;4的low 水位，Linux开始后台回收内存。直到达到6&#x2F;4的high水位，开始不回收。当Zone的最小内存达到min水位，应用程序的写会直接阻塞。 实时操作系统， 当你要开始回收内存时，回收比例通过swappiness越大，越倾向于回收匿名页；swappiness越小，越倾向于回收file-backed的页面。当把cgroup中的swapiness设置为0，就不回收匿名页了。当你的应用会经常去访问数据malloc的内存，需要把swapiness设置小。dirty的设置，水位的设置都没有一个标准，要看应用使用内存的情况而定。 getdelays工具：用来评估应用等待CPU，内存，IO，的时间。linux&#x2F;Documents&#x2F;accounting CONFIG_TASK_DELAY_ACCT&#x3D;yCONFIG_TASKSTATS&#x3D;y vmstat 可以展现给定时间间隔的服务器的状态值，包括Linux的CPU使用率，内存使用，虚拟内存交换情况，IO读写情况。 1vmstat 1 Documents&#x2F;sysctl&#x2F;vm.txt 中有所有参数最细节的描述。"},{"path":"/2024/04/05/linux-docs/内存管理/内存与IO的交换/","content":"内存与I&#x2F;O的交换堆、栈、代码段是否常驻内存？本文主要介绍两类不同的页面，以及这两类页面如何在内存和磁盘间进行交换？以及内存和磁盘的颠簸行为- swaping，和硬盘的swap分区。 page cachefile-backed的页面：（有文件背景的页面，比如代码段、比如read&#x2F;write方法读写的文件、比如mmap读写的文件；他们有对应的硬盘文件，因此如果要交换，可以直接和硬盘对应的文件进行交换），此部分页面进page cache。 匿名页：匿名页，如stack，heap，CoW后的数据段等；他们没有对应的硬盘文件，因此如果要交换，只能交换到虚拟内存-swapfile或者Linux的swap硬盘分区），此部分页面，如果系统内存不充分，可以被swap到swapfile或者硬盘的swap分区。 内核通过两种方式打开硬盘的文件，任何时候打开文件，Linux会申请一个page cache，然后把文件读到page cache里。page cache 是内存针对硬盘的缓存。 Linux读写文件有两种方式：read&#x2F;write 和 mmap 1）read&#x2F;write: read会把内核空间的page cache，往用户空间的buffer拷贝。参数 fd, buffer, size ，write只是把用户空间的buffer拷贝到内核空间的page cache。 2）mmap：可以避免内核空间到用户空间拷贝的过程，直接把文件映射成一个虚拟地址指针，指向linux内核申请的page cache。也就知道page cache和硬盘里文件的对应关系。 参数 fd, 文件对于应用程序，只是一部分内存。Linux使用write写文件，只是把文件写进内存，并没有sync。而内存的数据和硬盘交换的功能去完成。 ELF可执行程序的头部会记录，从xxx到xxx是代码段。把代码段映射到虚拟地址，0～3 G, 权限是RX。这段地址映射到内核空间的page cache, 这段page cache又映射到可执行程序。 page cache，会根据LRU算法（最近最少使用）进行替换。 demo演示 page cache会多大程度影响程序执行时间。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455echo 3 &gt; /proc/sys/vm/drop_cachestime python hello.py\\time -v python hello.pyroot@whale:/home/gzzhangyi2015# \\time -v python hello.pyHello World! Love, Python\tCommand being timed: &quot;python hello.py&quot;\tUser time (seconds): 0.01\tSystem time (seconds): 0.00\tPercent of CPU this job got: 40%\tElapsed (wall clock) time (h:mm:ss or m:ss): 0:00.03\tAverage shared text size (kbytes): 0\tAverage unshared data size (kbytes): 0\tAverage stack size (kbytes): 0\tAverage total size (kbytes): 0\tMaximum resident set size (kbytes): 6544\tAverage resident set size (kbytes): 0\tMajor (requiring I/O) page faults: 10\tMinor (reclaiming a frame) page faults: 778\tVoluntary context switches: 54\tInvoluntary context switches: 9\tSwaps: 0\tFile system inputs: 6528\tFile system outputs: 0\tSocket messages sent: 0\tSocket messages received: 0\tSignals delivered: 0\tPage size (bytes): 4096\tExit status: 0 root@whale:/home/gzzhangyi2015# \\time -v python hello.pyHello World! Love, Python\tCommand being timed: &quot;python hello.py&quot;\tUser time (seconds): 0.01\tSystem time (seconds): 0.00\tPercent of CPU this job got: 84%\tElapsed (wall clock) time (h:mm:ss or m:ss): 0:00.01\tAverage shared text size (kbytes): 0\tAverage unshared data size (kbytes): 0\tAverage stack size (kbytes): 0\tAverage total size (kbytes): 0\tMaximum resident set size (kbytes): 6624\tAverage resident set size (kbytes): 0\tMajor (requiring I/O) page faults: 0\tMinor (reclaiming a frame) page faults: 770\tVoluntary context switches: 1\tInvoluntary context switches: 4\tSwaps: 0\tFile system inputs: 0\tFile system outputs: 0\tSocket messages sent: 0\tSocket messages received: 0\tSignals delivered: 0\tPage size (bytes): 4096\tExit status: 0 总结：Linux有两种方式读取文件，不管以何种方式读文件，都会产生page cache 。 free命令的详细解释1234 total used free shared buffers cachedMem: 49537244 1667532 47869712 146808 21652 421268-/+ buffers/cache: 1224612 48312632Swap: 4194300 0 4194300 buffers&#x2F;cache都是文件系统的缓存，当访问ext3&#x2F;ext4,fat等文件系统中的文件，产生cache。当直接访问裸分区（&#x2F;dev&#x2F;sdax）时，产生buffer。 访问裸分区的用户，主要是应用程序直接打开 or 文件系统本身。dd命令 or 硬盘备份 or sd卡，会访问裸分区，产生的缓存就是buffer。而ext4文件系统把硬盘当作裸分区。 buffer和cache没有本质的区别，只是背景的区别。 -&#x2F;+ buffer&#x2F;cache 的公式used buffers&#x2F;cache &#x3D; used - buffers - cachedfree buffers&#x2F;cache &#x3D; free + buffers + cached 新版freeavailable参数：评估出有多少空闲内存给应用程序使用，free + 可回收的。 File-backed和Anonymous page File-backed映射把进程的虚拟地址空间映射到files 比如 代码段 比如 mmap一个字体文件 Anonymous映射是进程的虚拟地址空间没有映射到任何file Stack Heap CoW pages anonymous pages(没有任何文件背景)分配一个swapfile文件或者一个swap分区，来进行交换到磁盘的动作。 read&#x2F;write和 mmap 本质上都是有文件背景的映射，把进程的虚拟地址空间映射到files。在内存中的副本，只是一个page cache。是page cache就有可能被踢出内存。CPU 内部的cache，当访问新的内存时，也会被踢出cache。 demo：演示进程的代码段是如何被踢出去的？ 12345pidof firefoxcat /proc/&lt;pid&gt;/smaps运行 oom.c swap以及zRAM数据段，在未写过时，有文件背景。在写过之后，变成没有文件背景，就被当作匿名页。linux把swap分区，当作匿名页的文件背景。 12swap(v.)，内存和硬盘之间的颠簸行为。 swap(n.)，swap分区和swap文件，当作内存中匿名页的交换背景。在windows内，被称作虚拟内存。pagefile.sys 页面回收和LRU 回收匿名页和 回收有文件背景的页面。后台慢慢回收：通过kswapd进程，回收到高水位(high)时，才停止回收。从low -&gt; high直接回收：当水位达到min水位，会在两种页面同时进行回收，回收比例通过swappiness越大，越倾向于回收匿名页；swappiness越小，越倾向于回收file-backed的页面。当然，它们的回收方法都是一样的LRU算法。 Linux Page Replacement用LRU算法来进行swap和page cache的页面替换。 123现在cache的大小是4页，前四次，1，2，3，4文件被一次使用，注意第七次，5文件被使用，系统评估最近最少被使用的文件是3，那么不好意思，3被swap出去，5加载进来，依次类推。所以LRU可能会触发page cache或者anonymous页与对应文件的数据交换。 嵌入式系统的zRAM zRAM: 用内存来做swap分区。从内存中开辟一小段出来，模拟成硬盘分区，做交换分区，交换匿名页，自带透明压缩功能。当应用程序往zRAM写数据时，会自动把匿名页进行压缩。当应用程序访问匿名页时，内存页表里不命中，发生page fault（major）。从zRAM中把匿名页透明解压出来，还到内存。"},{"path":"/2024/04/05/linux-docs/内存管理/内存的动态申请和释放/","content":"内存的动态申请和释放内核空间 和用户空间申请的内存最终和buddy怎么交互？以及在页表映射上的区别？虚拟地址到物理地址，什么时候开始映射？ Buddy的问题分配的粒度太大buddy算法把空闲页面分成1，2，4页，buddy算法会明确知道哪一页内存空闲还是被占用？ 4k，8k，16k 无论是在应用还是内核，都需要申请很小的内存。 从buddy要到的内存，会进行slab切割。 slab原理：比如在内核中申请8字节的内存，buddy分配4K，分成很多个小的8个字节，每个都是一个object。 slab，slub，slob 是slab机制的三种不同实现算法。 Linux 会针对一些常规的小的内存申请，数据结构，会做slab申请。 cat &#x2F;proc&#x2F;slabinfo 可以看到内核空间小块内存的申请情况，也是slab分配的情况。 ：每个slab一共可以分出多少个obj， ：还可以分配多少个obj，&lt; pagesperslab&gt;：每个slab对应多少个pages，&lt; objperslab&gt;：每个slab可以分出多少个object，&lt; objsize&gt;：每个obj多大， slab主要分为两类： 一、常用数据结构像 nfsd_drc， UDPv6，TCPv6 ，这些经常申请和释放的数据结构。比如，存在TCPv6的slab，之后申请 TCPv6 数据结构时，会通过这个slab来申请。 二、常规的小内存申请，做的slab。例如 kmalloc-32，kmalloc-64， kmalloc-96， kmalloc-128 注意，slab申请和分配的都是只针对内核空间，与用户空间申请分配内存无关。用户空间的malloc和free调用的是libc。 slab和buddy的关系？1、slab的内存来自于buddy。slab相当于二级管理器。2、slab和buddy在算法上，级别是对等的。 两者都是内存分配器，buddy是把内存条分成多个Zone来管理分配，slab是把从buddy拿到的内存，进行管理分配。 同理，malloc 和free也不找buddy拿内存。 malloc 和free不是系统调用，只是c库中的函数。 mallopt在C库中有一个api是mallopt，可以控制一系列的选项。 M_TRIM_THRESHOLD：控制c库把内存还给内核的阈值。-1UL 代表最大的正整数。 此处代表应用程序把内存还给c库后，c库并不把内存还给内核。 &lt;\\do your RT-thing&gt;程序在此处申请内存，都不需要再和内核交互了，此时程序的实时性比较高。 kmalloc vs. vmalloc&#x2F;ioremap内存空间： 内存+寄存器 register –&gt; LDR&#x2F;STR 所有内存空间的东西，CPU去访问，都要通过虚拟地址。CPU –&gt; virt –&gt; mmu –&gt; phys cpu请求虚拟地址，mmu根据cpu请求的虚拟地址，查页表得物理地址。 buddy算法，管理这一页的使用情况。 两个虚拟地址可以映射到同一个物理地址。 页表 -&gt; 数组， 任何一个虚拟地址，都可以用地址的高20位，作为页表的行号去读对应的页表项。而低12位，是指页内偏移。（由于一页是4K，2^12 足够描述) kmalloc 和 vmalloc 申请的内存，有什么区别？答：申请之后，是否还要去改页表。一般情况，kmalloc申请内存，不需要再去改页表。同一张页表，几个虚拟地址可以同时映射到同一个物理地址。 寄存器，通过ioremap往vmalloc区域，进行映射。然后改进程的虚拟地址页表。 总结：所有的页，最底层都是用buddy算法进行管理，用虚拟地址找物理地址。理解内存分配和映射的区别，无论是lowmem还是highmem 都可以被vmalloc拿走，也可能被用户拿走，只不过拿走之后，还要把虚拟地址往物理地址再映射一遍。但如果是被kmalloc拿走，一般指低端内存，就不需要再改进程的页表。因为这部分低端内存，已经做好了虚实映射。 1cat /proc/vmallocinfo |grep ioremap 可以看到寄存器中的哪个区域，被映射到哪个虚拟地址。 vmalloc区域主要用来，vmalloc申请的内存从这里找虚拟地址 和 寄存器的ioremap映射。 Linux内存分配的lazy行为Linux总是以最lazy的方式，给应用程序分配内存。 malloc 100M内存成功时，其实并没有真实拿到。只有当100M内存中的任何一页，被写一次的时候，才成功。 vss：虚拟地址空间。 rss：常驻内存空间 malloc 100M内存成功时，Linux把100M内存全部以只读的形式，映射到一个全部清0的页面。 当应用程序写100M中每一页任意字节时，会发出page fault。 linux 内核收到缺页中断后，从硬件寄存器中读取到，包括缺页中断发生的原因和虚拟地址。Linux从内存条申请一页内存，执行cow，把页面重新拷贝到新申请的页表，再把进程页表中的虚拟地址，指向一个新的物理地址，权限也被改成R+W。 调用brk 把8k变成 16k。 针对应用程序的堆、代码、栈、等，会使用lazy分配机制，只有当写内存页时，才会真实请求内存分配页表。但，当内核使用kmalloc申请内存时，就真实的分配相应的内存，不使用lazy机制。 内存OOM当真实去写内存时，应用程序并不能拿到真实的内存时。Linux启动OOM，linux在运行时，会对每一个进程进行out-of-memory打分。打分主要基于，耗费的内存。耗费的内存越多，打分越高。 1cat /proc/&lt;pid&gt;/oom_score demo: 12345678910111213141516171819202122#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;int main(int argc, char **argv)&#123; int max = -1; int mb = 0; char *buffer; int i;#define SIZE 2000 unsigned int *p = malloc(1024 * 1024 * SIZE); printf(&quot;malloc buffer: %p &quot;, p); for (i = 0; i &lt; 1024 * 1024 * (SIZE/sizeof(int)); i++) &#123; p[i] = 123; if ((i &amp; 0xFFFFF) == 0) &#123; printf(&quot;%dMB written &quot;, i &gt;&gt; 18); usleep(100000); &#125; &#125; pause(); return 0;&#125; 设定条件： 总内存1G1、swapoff -a 关掉swap交换2、echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;overcommit_memory3、内核不去评估系统还有多少空闲内存 Linux进行OOM打分，主要是看耗费内存情况，此外还会参考用户权限，比如root权限，打分会减少30分。 还有OOM打分因子：&#x2F;proc&#x2F;pid&#x2F;oom_score_adj （加减）和 &#x2F;proc&#x2F;pid&#x2F;oom_adj （乘除）。 总结：1、slab的作用，针对在内核空间小内存分配，和常用数据结构的申请。2、同样的二次分配器，在用户空间是C库。malloc和free的时候，内存不一定从buddy分配和还给buddy。3、kmalloc，vmalloc 和malloc的区别 kmalloc：申请内存，一般在低端内存区。申请到时，内存已经映射过了，不需要再去改进程的页表。所以，申请到的物理页是连续的。 vmalloc：申请内存，申请到就拿到内存，并且已经修改了进程页表的虚拟地址到物理地址的映射。vmalloc()申请的内存并不保证物理地址的连续。 用户空间的malloc：申请内存，申请到并没有拿到，写的时候才去拿到。拿到之后，才去改页表。申请成功，页表只读，只有到写时，发生page fault，才去buddy拿内存。 kmalloc和vmalloc针对内核空间，malloc针对用户空间。这些内存，可以来自任何一个Zone。 无论是kmalloc，vmalloc还是用户空间的malloc，都可以使用内存条的不同Zone，无论是highmem zone、lowmem zone 和 DMA zone。 4、如果在从buddy拿不到内存时，会触发Linux对所有进程进行OOM打分。当Linux出现内存耗尽，就kill一个oom score 最高的那个进程。oom_score，可以根据 oom_adj （-17～25）。 安卓的程序，不停的调整前台和后台进程oom_score，当被切换到后台时，oom_score会被调整的比较大。以保证前台的进程不容易因为oom而kill掉。"},{"path":"/2024/04/05/linux-docs/内存管理/内存管理之分页/","content":"一. 前言 上文分析了内存再用户态的结构体mm_struct及各个区域映射的vm_area_struct以及32位和64位的内核态结构体，本文将基于这些结构来分析Linux的内存管理系统。内存管理系统包括虚拟内存和物理内存的分页以及虚拟内存和物理内存的映射。本文将介绍分页机制，而映射则在下文中说明。本文首先简单介绍SMP和NUMA系统，然后对物理内存的节点、区域、页结构进行分析，在此基础上剖析伙伴系统和Slub Allocator的实现原理，最后介绍页面交换。 二. 内存模型 在计算机的发展历程中，内存模型经历了平坦内存模型、SMP和NUMA三种架构。 平坦内存模型(Flat Memory Model)，又称线性内存模型，在这种模式下，采取连续的物理地址和页，所以非常容易根据地址获取页号，反之亦然。此种布局非常简单，而且是线性增长，利于使用。但是随着内存需求的增大、进程数的变多、为了安全考虑，这种模型渐渐地无法满足要求。 对称多处理 SMP(Symmetric MultiProcessing)，是一种多处理器的电脑硬件架构，在对称多处理架构下，每个处理器的地位都是平等的，对资源的使用权限相同。现代多数的多处理器系统都采用对称多处理架构。在这个系统中，拥有超过一个以上的处理器，这些处理器都连接到同一个共享的主存上，并由单一操作系统来控制。在对称多处理系统上，在操作系统的支持下，无论进程是处于用户空间，或是内核态，都可以分配到任何一个处理器上运行。因此，进程可以在不同的处理器间移动，达到负载平衡，使系统的效率提升。 非均衡访存模型 NUMA(Non-Uniform Memory Access)。在这种模式下，内存不是一整块。每个 CPU 都有自己的本地内存，CPU 访问本地内存不用过总线，因而速度要快很多，每个 CPU 和内存在一起，称为一个 NUMA 节点。但是，在本地内存不足的情况下，每个 CPU 都可以去另外的 NUMA 节点申请内存，这个时候访问延时就会比较长。 三. 节点、区域和页3.1 节点 这里主要分析NUMA模型。我们将内存分为多个节点对应多个CPU，每个节点再被分成一个一个的区域，每个区域有多个页面。由于页需要全局唯一定位，页还是需要有全局唯一的页号的。但是由于物理内存不是连起来的了，因此页号也就不再连续了。于是内存模型就变成了非连续内存模型。NUMA节点对应的结构为pglist_data，主要包括 节点ID node_id 节点区域相关：node_zones，node_zonelists，nr_zones 节点的页数组：node_mam_map 节点的起始页号：node_start_pfn 节点中包含不连续的物理内存地址页数：node_spanned_pages 可用的物理页面数目：node_present_pates 页面回收、交换相关：kswapd_wait，kswapd，kswapd_order，kswapd_failures，kswapd_classzone_idx12345678910111213141516171819202122232425/* * On NUMA machines, each NUMA node would have a pg_data_t to describe * it&#x27;s memory layout. On UMA machines there is a single pglist_data which * describes the whole memory. * * Memory statistics and page replacement data structures are maintained on a * per-zone basis. */typedef struct pglist_data &#123; struct zone node_zones[MAX_NR_ZONES]; struct zonelist node_zonelists[MAX_ZONELISTS]; int nr_zones; struct page *node_mem_map; unsigned long node_start_pfn; unsigned long node_present_pages; /* total number of physical pages */ unsigned long node_spanned_pages; /* total size of physical page range, including holes */ int node_id; wait_queue_head_t kswapd_wait; wait_queue_head_t pfmemalloc_wait; struct task_struct *kswapd;\t/* Protected by mem_hotplug_begin/end() */ int kswapd_order; enum zone_type kswapd_classzone_idx; int kswapd_failures; /* Number of &#x27;reclaimed == 0&#x27; runs */......&#125; pg_data_t; 3.2 区域每个节点可分成一个个的区域，node_zones存储了这些区域，node_zonelist以链表形式存储备用节点和它的内存区域的情况，nr_zones表示区域总数。区域的类型有如下几种，在上节中已简单提过，这里详细说明。 ZONE_DMA 是指可用于作 DMA（Direct Memory Access，直接内存存取）的内存。DMA 是这样一种机制：要把外设的数据读入内存或把内存的数据传送到外设，原来都要通过 CPU 控制完成，但是这会占用 CPU，影响 CPU 处理其他事情，所以有了 DMA 模式。CPU 只需向 DMA 控制器下达指令，让 DMA 控制器来处理数据的传送，数据传送完毕再把信息反馈给 CPU，这样就可以解放 CPU。对于 64 位系统，有两个 DMA 区域。除了上面说的 ZONE_DMA，还有 ZONE_DMA32。 ZONE_NORMAL 是直接映射区，即内核态前896M空间 ZONE_HIGHMEM 是高端内存区，对于 32 位系统来说超过 896M 的地方，对于 64 位没必要有的一段区域。 ZONE_MOVABLE 是可移动区域，通过将物理内存划分为可移动分配区域和不可移动分配区域来避免内存碎片。 1234567891011121314enum zone_type &#123;#ifdef CONFIG_ZONE_DMA ZONE_DMA,#endif#ifdef CONFIG_ZONE_DMA32 ZONE_DMA32,#endif ZONE_NORMAL,#ifdef CONFIG_HIGHMEM ZONE_HIGHMEM,#endif ZONE_MOVABLE, __MAX_NR_ZONES&#125;; 区域的实现数据结构为zone，主要包括 区域初始页zone_start_pfn 区域总共跨多少页 spanned_pages 区域在物理内存中真实存在的页数present_pages 区域被伙伴系统管理的所有页数managed_pages 冷热页区分per_cpu_pagest：如果一个页被加载到 CPU 高速缓存里面，这就是一个热页（Hot Page），CPU 读起来速度会快很多，如果没有就是冷页（Cold Page）。由于每个 CPU 都有自己的高速缓存，因而 per_cpu_pageset 也是每个 CPU 一个。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071struct zone &#123;...... struct pglist_data\t*zone_pgdat; struct per_cpu_pageset __percpu *pageset;...... /* zone_start_pfn == zone_start_paddr &gt;&gt; PAGE_SHIFT */ unsigned long zone_start_pfn; /* * spanned_pages is the total pages spanned by the zone, including * holes, which is calculated as: * spanned_pages = zone_end_pfn - zone_start_pfn; * * present_pages is physical pages existing within the zone, which * is calculated as: *\tpresent_pages = spanned_pages - absent_pages(pages in holes); * * managed_pages is present pages managed by the buddy system, which * is calculated as (reserved_pages includes pages allocated by the * bootmem allocator): *\tmanaged_pages = present_pages - reserved_pages; * * So present_pages may be used by memory hotplug or memory power * management logic to figure out unmanaged pages by checking * (present_pages - managed_pages). And managed_pages should be used * by page allocator and vm scanner to calculate all kinds of watermarks * and thresholds. * * Locking rules: * * zone_start_pfn and spanned_pages are protected by span_seqlock. * It is a seqlock because it has to be read outside of zone-&gt;lock, * and it is done in the main allocator path. But, it is written * quite infrequently. * * The span_seq lock is declared along with zone-&gt;lock because it is * frequently read in proximity to zone-&gt;lock. It&#x27;s good to * give them a chance of being in the same cacheline. * * Write access to present_pages at runtime should be protected by * mem_hotplug_begin/end(). Any reader who can&#x27;t tolerant drift of * present_pages should get_online_mems() to get a stable value. */ atomic_long_t managed_pages; unsigned long spanned_pages; unsigned long present_pages; const char *name;...... int initialized; /* Write-intensive fields used from the page allocator */ ZONE_PADDING(_pad1_) /* free areas of different sizes */ struct free_area\tfree_area[MAX_ORDER]; /* zone flags, see below */ unsigned long flags; /* Primarily protects free_area */ spinlock_t lock; /* Write-intensive fields used by compaction and vmstats. */ ZONE_PADDING(_pad2_) /* * When free pages are below this point, additional steps are taken * when reading the number of free pages to avoid per-cpu counter * drift allowing watermarks to be breached */ unsigned long percpu_drift_mark;...... bool contiguous; ZONE_PADDING(_pad3_) /* Zone statistics */ atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS]; atomic_long_t vm_numa_stat[NR_VM_NUMA_STAT_ITEMS];&#125; ____cacheline_internodealigned_in_smp; 3.3 页每个区域有很多个页，现在让我们将目光投向页的结构page。页中使用了大量的联合，其原因在于页有着多种不同的使用方式。 整页使用方式。在前面介绍过，该种情况也存在两种页，一种是直接映射虚拟地址空间的匿名页（Anonymous Page），另一种则是用于关联文件、然后再和虚拟地址空间建立映射的页，称之为内存映射文件（Memory-mapped File）。对于该种模式，会使用联合里的以下变量 struct address_space *mapping ：用于内存映射，如果是匿名页，最低位为 1；如果是映射文件，最低位为 0 pgoff_t index ：映射区的偏移量 atomic_t _mapcount：指向该页的页表数 struct list_head lru ：表示这一页应该在一个链表上，例如这个页面被换出，就在换出页的链表中； compound 相关的变量用于复合页（Compound Page），就是将物理上连续的两个或多个页看成一个独立的大页。 小块内存使用方式。在很多情况下，我们只需要使用少量内存，因此采用了slab allocator技术用于分配小块内存slab。它的基本原理是从内存管理模块申请一整块页，然后划分成多个小块的存储池，用复杂的队列来维护这些小块的状态（状态包括：被分配了 &#x2F; 被放回池子 &#x2F; 应该被回收）。也正是因为 slab allocator 对于队列的维护过于复杂，后来就有了一种不使用队列的分配器 slub allocator，但是里面还是用了很多 带有slab的API ，因为它保留了 slab 的用户接口，可以看成 slab allocator 的另一种实现。该种模式会使用联合里的以下变量 s_mem ：正在使用的 slab 的第一个对象 freelist ：池子中的空闲对象 rcu_head ：需要释放的列表 小块内存分配器slob，非常简单，常用于小型嵌入式系统 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667struct page &#123; unsigned long flags; /* Atomic flags, some possibly updated asynchronously */ /* * Five words (20/40 bytes) are available in this union. * WARNING: bit 0 of the first word is used for PageTail(). That * means the other users of this union MUST NOT use the bit to * avoid collision and false-positive PageTail(). */ union &#123; struct &#123;\t/* Page cache and anonymous pages */ /** * @lru: Pageout list, eg. active_list protected by * pgdat-&gt;lru_lock. Sometimes used as a generic list * by the page owner. */ struct list_head lru; /* See page-flags.h for PAGE_MAPPING_FLAGS */ struct address_space *mapping; pgoff_t index; /* Our offset within mapping. */ /** * @private: Mapping-private opaque data. * Usually used for buffer_heads if PagePrivate. * Used for swp_entry_t if PageSwapCache. * Indicates order in the buddy system if PageBuddy. */ unsigned long private; &#125;; struct &#123;\t/* page_pool used by netstack */ /** * @dma_addr: might require a 64-bit value even on * 32-bit architectures. */ dma_addr_t dma_addr; &#125;; struct &#123;\t/* slab, slob and slub */ union &#123; struct list_head slab_list;\t/* uses lru */ struct &#123;\t/* Partial pages */ struct page *next;...... &#125;; &#125;; struct kmem_cache *slab_cache; /* not slob */ /* Double-word boundary */ void *freelist; /* first free object */ union &#123; void *s_mem;\t/* slab: first object */ unsigned long counters; /* SLUB */ struct &#123; /* SLUB */ unsigned inuse:16; unsigned objects:15; unsigned frozen:1; &#125;; &#125;; &#125;;...... struct &#123;\t/* ZONE_DEVICE pages */ /** @pgmap: Points to the hosting device page map. */ struct dev_pagemap *pgmap; unsigned long hmm_data; unsigned long _zd_pad_1;\t/* uses mapping */ &#125;; /** @rcu_head: You can use this to free a page by RCU. */ struct rcu_head rcu_head; &#125;;......&#125; _struct_page_alignment; 四. 用户态页的分配 在上节中我们介绍了页按大小分配大致有伙伴系统和小块内存分配的slub分配。本节就这两种展开讨论。 4.1 伙伴系统 对于要分配比较大的内存，例如到分配页级别的，可以使用伙伴系统（Buddy System）。伙伴分配机制可以归纳如下。 采取多个链表将空闲页组织起来，每个链表中的节点包含的页块数目不同，即第i个链表中的每个节点拥有2^i个页。 当向内核请求分配 (2^(i-1)，2^i]数目的页块时，按照 2^i 页块请求处理，并将其分裂。 如果对应的页块链表中没有空闲页块，那我们就在更大的页块链表中去找，并对其进行分裂。如请求128个页，如果128页块链表无空闲则请求256链表，并将其分裂为两个128链表。 源码实现于alloc_pages()中，传参的定义如注释中所示，order表示分配2的order次方个页，gfp_mask表示分配区域，主要有 GFP_USER：用于一个用户进程希望通过内存映射的方式访问某些硬件的缓存，例如显卡缓存 GFP_KERNEL：用于内核中分配页，主要分配 ZONE_NORMAL 区域，也即直接映射区 GFP_HIGHMEM：用于分配高端内存区域 GFP_FS：用于文件映射区 GFP_ATOMIC：表示该页内容不会进入休眠 这里涉及到了一种加快内存速度的技术：交叉存取技术（interleaved memory）。该存储方式会将存储体分为多个模块，每个模块类似于负载均衡逐个写入，这样当前字节被刷新时，可以不影响下一个字节的访问。交叉存储主要补偿DRAM等存储器相对较慢的读写速度。读或写每一个内存块，都需要等待内存块给出ready信号，才能读写下一字节。交叉存储将连续信息分散到各个块中，读写时可以同时等待多个内存块给出ready信号，从而提高了读写的速度。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970static inline struct page *alloc_pages(gfp_t gfp_mask, unsigned int order)&#123; return alloc_pages_current(gfp_mask, order);&#125;/** * alloc_pages_current - Allocate pages. * *\t@gfp: * %GFP_USER user allocation, * %GFP_KERNEL kernel allocation, * %GFP_HIGHMEM highmem allocation, * %GFP_FS don&#x27;t call back into a file system. * %GFP_ATOMIC don&#x27;t sleep. *\t@order: Power of two of allocation size in pages. 0 is a single page. * *\tAllocate a page from the kernel page pool. When not in *\tinterrupt context and apply the current process NUMA policy. *\tReturns NULL when no page can be allocated. */struct page *alloc_pages_current(gfp_t gfp, unsigned order)&#123; struct mempolicy *pol = &amp;default_policy; struct page *page; if (!in_interrupt() &amp;&amp; !(gfp &amp; __GFP_THISNODE)) pol = get_task_policy(current); /* * No reference counting needed for current-&gt;mempolicy * nor system default_policy */ if (pol-&gt;mode == MPOL_INTERLEAVE) page = alloc_page_interleave(gfp, order, interleave_nodes(pol)); else page = __alloc_pages_nodemask(gfp, order, policy_node(gfp, pol, numa_node_id()), policy_nodemask(gfp, pol)); return page;&#125;EXPORT_SYMBOL(alloc_pages_current);```` 交叉存取的alloc_page_interleave()函数实际上通过调用__alloc_page()最终也会调用__alloc_pages_nodemask()函数来实现分配。__alloc_pages_nodemask()是伙伴系统的核心方法，它会调用 get_page_from_freelist()在一个循环中轮询区域，先看当前节点的 zone，如果找不到空闲页则再看备用节点的 zone，直到找到为止。```c/* * This is the &#x27;heart&#x27; of the zoned buddy allocator. */struct page *__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid, nodemask_t *nodemask)&#123; struct page *page; unsigned int alloc_flags = ALLOC_WMARK_LOW; gfp_t alloc_mask; /* The gfp_t that was actually used for allocation */ struct alloc_context ac = &#123; &#125;;...... gfp_mask &amp;= gfp_allowed_mask; alloc_mask = gfp_mask; if (!prepare_alloc_pages(gfp_mask, order, preferred_nid, nodemask, &amp;ac, &amp;alloc_mask, &amp;alloc_flags)) return NULL; finalise_ac(gfp_mask, &amp;ac); /* * Forbid the first pass from falling back to types that fragment * memory until all local zones are considered. */ alloc_flags |= alloc_flags_nofragment(ac.preferred_zoneref-&gt;zone, gfp_mask); /* First allocation attempt */ page = get_page_from_freelist(alloc_mask, order, alloc_flags, &amp;ac);......&#125;EXPORT_SYMBOL(__alloc_pages_nodemask); get_page_from_freelist()函数如下所示，主要调用rmqueue()找寻合适大小的队列，并取出页面。通过rmqueue()-&gt;__rmqueue()-&gt;__rmqueue_smallest()最终来到__rmqueue_smallest()函数 1234567891011121314151617181920212223242526/* * get_page_from_freelist goes through the zonelist trying to allocate * a page. */static struct page *get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags, const struct alloc_context *ac)&#123;\t...... for_next_zone_zonelist_nodemask(zone, z, ac-&gt;zonelist, ac-&gt;high_zoneidx, ac-&gt;nodemask) &#123;...... page = rmqueue(ac-&gt;preferred_zoneref-&gt;zone, zone, order, gfp_mask, alloc_flags, ac-&gt;migratetype); if (page) &#123; prep_new_page(page, order, gfp_mask, alloc_flags); /* * If this is a high-order atomic allocation then check * if the pageblock should be reserved for the future */ if (unlikely(order &amp;&amp; (alloc_flags &amp; ALLOC_HARDER))) reserve_highatomic_pageblock(page, zone, order); return page; &#125; ......&#125; 在__rmqueue_smallest()中，实现了伙伴系统的找寻逻辑。从当前的order开始循环，如果page为空则order++，否则从当前链表lru中删除该页块，通过expand()函数将该区域剩余页块分配到其他对应的order链表中，最后返回该页块。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/* * Go through the free lists for the given migratetype and remove * the smallest available page from the freelists */static __always_inlinestruct page *__rmqueue_smallest(struct zone *zone, unsigned int order, int migratetype)&#123; unsigned int current_order; struct free_area *area; struct page *page; /* Find a page of the appropriate size in the preferred list */ for (current_order = order; current_order &lt; MAX_ORDER; ++current_order) &#123; area = &amp;(zone-&gt;free_area[current_order]); page = list_first_entry_or_null(&amp;area-&gt;free_list[migratetype], struct page, lru); if (!page) continue; list_del(&amp;page-&gt;lru); rmv_page_order(page); area-&gt;nr_free--; expand(zone, page, order, current_order, area, migratetype); set_pcppage_migratetype(page, migratetype); return page; &#125; return NULL;&#125; expand()函数中将页块区域前移，size右移即除2，然后使用list_add加入链表之中/* * The order of subdivision here is critical for the IO subsystem. * Please do not alter this order without good reasons and regression * testing. Specifically, as large blocks of memory are subdivided, * the order in which smaller blocks are delivered depends on the order * they&#x27;re subdivided in this function. This is the primary factor * influencing the order in which pages are delivered to the IO * subsystem according to empirical testing, and this is also justified * by considering the behavior of a buddy system containing a single * large block of memory acted on by a series of small allocations. * This behavior is a critical factor in sglist merging&#x27;s success. * * -- nyc */static inline void expand(struct zone *zone, struct page *page, int low, int high, struct free_area *area, int migratetype)&#123; unsigned long size = 1 &lt;&lt; high; while (high &gt; low) &#123; area--; high--; size &gt;&gt;= 1; VM_BUG_ON_PAGE(bad_range(zone, &amp;page[size]), &amp;page[size]); /* * Mark as guard pages (or page), that will allow to * merge back to allocator when buddy will be freed. * Corresponding page table entries will not be touched, * pages will stay not present in virtual address space */ if (set_page_guard(zone, &amp;page[size], high, migratetype)) continue; list_add(&amp;page[size].lru, &amp;area-&gt;free_list[migratetype]); area-&gt;nr_free++; set_page_order(&amp;page[size], high); &#125;&#125; 最后以一张结构图作为总结 4.2 slub allocator 任务task_struct在派生时，会调用alloc_task_struct_node()分配task_struct对象，其底层调用kmem_cache_alloc_node()函数在task_struct的缓存区域kmem_cache *task_struct_cachep分配内存。在系统初始化的时候，task_struct_cachep 会被 kmem_cache_create() 函数创建。这个函数专门用于分配 task_struct 对象的缓存。这个缓存区的名字就叫 task_struct。 缓存区中每一块的大小正好等于 task_struct 的大小，也即 arch_task_struct_size。有了这个缓存区，每次创建 task_struct 的时候，先调用 kmem_cache_alloc_node()函数在缓存里面看看有没有直接可用的。当一个进程结束，task_struct 也不用直接被销毁，而是调用kmem_cache_free()放回到缓存中。这样，新进程创建的时候，我们就可以直接用现成的缓存中的 task_struct 了，从而加快了申请、释放速度。 123456789101112131415static struct kmem_cache *task_struct_cachep;task_struct_cachep = kmem_cache_create(&quot;task_struct&quot;, arch_task_struct_size, align, SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT, NULL);static inline struct task_struct *alloc_task_struct_node(int node)&#123; return kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL, node);&#125;static inline void free_task_struct(struct task_struct *tsk)&#123; kmem_cache_free(task_struct_cachep, tsk);&#125; 下面分析一下缓冲区kmem_cache结构体。 kmem_cached_cpu和kmem_cached_node：每个NUMA节点都对应一个，分别是缓存分配的fast path和slow path。每次分配的时候，要先从 kmem_cache_cpu 进行分配。如果 kmem_cache_cpu 里面没有空闲的块，那就到 kmem_cache_node 中进行分配；如果还是没有空闲的块，才去伙伴系统分配新的页。 size，object_size和offset：通过链表list_head，所有的小内存块会联系起来存放在缓冲区，用于task_struct、mm_struct、fs_struct等申请小内存块，object_size表示某内存块的大小，size表示该内存块加上指针的总大小，offset表示下一个空闲内存块的指针的偏移量。 1234567891011121314151617181920212223242526272829303132/* * Slab cache management. */struct kmem_cache &#123; struct kmem_cache_cpu __percpu *cpu_slab; /* Used for retrieving partial slabs, etc. */ slab_flags_t flags; unsigned long min_partial; unsigned int size;\t/* The size of an object including metadata */ unsigned int object_size;/* The size of an object without metadata */ unsigned int offset;\t/* Free pointer offset */#ifdef CONFIG_SLUB_CPU_PARTIAL /* Number of per cpu partial objects to keep around */ unsigned int cpu_partial;#endif struct kmem_cache_order_objects oo; /* Allocation and freeing of slabs */ struct kmem_cache_order_objects max; struct kmem_cache_order_objects min; gfp_t allocflags;\t/* gfp flags to use on each alloc */ int refcount; /* Refcount for slab cache destroy */ void (*ctor)(void *); unsigned int inuse; /* Offset to metadata */ unsigned int align; /* Alignment */ unsigned int red_left_pad;\t/* Left redzone padding size */ const char *name;\t/* Name (only for display!) */ struct list_head list;\t/* List of slab caches */...... unsigned int useroffset;\t/* Usercopy region offset */ unsigned int usersize; /* Usercopy region size */ struct kmem_cache_node *node[MAX_NUMNODES];&#125;; 首先来看看快分配方式kmem_cached_cpu，该结构体中page指向内存页块的第一页，freelist指向下一个可用的内存页块，partial表示部分被分配部分为空的页，该项为备用项，仅当page满了才会在partial中寻找，而partial本身指向的是kmem_cached_node中的partial链表。 1234567891011struct kmem_cache_cpu &#123; void **freelist;\t/* Pointer to next available object */ unsigned long tid;\t/* Globally unique transaction id */ struct page *page;\t/* The slab from which we are allocating */#ifdef CONFIG_SLUB_CPU_PARTIAL struct page *partial;\t/* Partially allocated frozen slabs */#endif#ifdef CONFIG_SLUB_STATS unsigned stat[NR_SLUB_STAT_ITEMS];#endif&#125;; kmem_cached_node结构体中也有类似的成员变量，但是相较之下会有更多详细信息，如slab链表总长度、空闲内存块的数量等等。 12345678910111213141516171819202122232425/* * The slab lists for all objects. */struct kmem_cache_node &#123; spinlock_t list_lock;#ifdef CONFIG_SLAB struct list_head slabs_partial;\t/* partial list first, better asm code */ struct list_head slabs_full; struct list_head slabs_free; unsigned long total_slabs;\t/* length of all slab lists */ unsigned long free_slabs;\t/* length of free slab list only */ unsigned long free_objects; unsigned int free_limit; unsigned int colour_next;\t/* Per-node cache coloring */ struct array_cache *shared;\t/* shared per node */ struct alien_cache **alien;\t/* on other nodes */ unsigned long next_reap;\t/* updated without locking */ int free_touched; /* updated without locking */#endif#ifdef CONFIG_SLUB unsigned long nr_partial; struct list_head partial;......#endif&#125;; 下面回到缓冲区的分配函数kmem_cache_alloc_node()和释放函数kmem_cache_free。分配函数kmem_cache_alloc_node()实际调用slab_alloc_node()。从这里可以看出快通道和慢通道的处理逻辑。 快通道：尝试取出kmem_cache_cpu cpu_slab的freelist，如果有空闲则返回，否则进入慢通道 慢通道：调用__slab_alloc()分配新的内存块 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* * Inlined fastpath so that allocation functions (kmalloc, kmem_cache_alloc) * have the fastpath folded into their functions. So no function call * overhead for requests that can be satisfied on the fastpath. * * The fastpath works by first checking if the lockless freelist can be used. * If not then __slab_alloc is called for slow processing. * * Otherwise we can simply pick the next object from the lockless free list. */static __always_inline void *slab_alloc_node(struct kmem_cache *s, gfp_t gfpflags, int node, unsigned long addr)&#123; void *object; struct kmem_cache_cpu *c; struct page *page; unsigned long tid; s = slab_pre_alloc_hook(s, gfpflags); if (!s) return NULL;redo: /* * Must read kmem_cache cpu data via this cpu ptr. Preemption is * enabled. We may switch back and forth between cpus while * reading from one cpu area. That does not matter as long * as we end up on the original cpu again when doing the cmpxchg. * * We should guarantee that tid and kmem_cache are retrieved on * the same cpu. It could be different if CONFIG_PREEMPT so we need * to check if it is matched or not. */ do &#123; tid = this_cpu_read(s-&gt;cpu_slab-&gt;tid); c = raw_cpu_ptr(s-&gt;cpu_slab); &#125; while (IS_ENABLED(CONFIG_PREEMPT) &amp;&amp; unlikely(tid != READ_ONCE(c-&gt;tid))); /* * Irqless object alloc/free algorithm used here depends on sequence * of fetching cpu_slab&#x27;s data. tid should be fetched before anything * on c to guarantee that object and page associated with previous tid * won&#x27;t be used with current tid. If we fetch tid first, object and * page could be one associated with next tid and our alloc/free * request will be failed. In this case, we will retry. So, no problem. */ barrier(); /* * The transaction ids are globally unique per cpu and per operation on * a per cpu queue. Thus they can be guarantee that the cmpxchg_double * occurs on the right processor and that there was no operation on the * linked list in between. */ object = c-&gt;freelist; page = c-&gt;page; if (unlikely(!object || !node_match(page, node))) &#123; object = __slab_alloc(s, gfpflags, node, addr, c); stat(s, ALLOC_SLOWPATH); &#125;...... return object;&#125; __slab_alloc()主要逻辑如下 在此尝试kmem_cache_cpu中的freelist是否可用，因为当前进程可能被中断，等回到该进程继续执行时可能已经有了空闲内存块可以直接使用了，因此先检查一下 跳转到new_slab标签，检查kmem_cache_cpu中的paritial，如果partial不为空则将kmem_cache_cpu中的page替换为partial，跳转至redo标签再次尝试分配 依然失败，则调用new_slab_objects()分配新内存块 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/* * Slow path. The lockless freelist is empty or we need to perform * debugging duties. * * Processing is still very fast if new objects have been freed to the * regular freelist. In that case we simply take over the regular freelist * as the lockless freelist and zap the regular freelist. * * If that is not working then we fall back to the partial lists. We take the * first element of the freelist as the object to allocate now and move the * rest of the freelist to the lockless freelist. * * And if we were unable to get a new slab from the partial slab lists then * we need to allocate a new slab. This is the slowest path since it involves * a call to the page allocator and the setup of a new slab. * * Version of __slab_alloc to use when we know that interrupts are * already disabled (which is the case for bulk allocation). */static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node, unsigned long addr, struct kmem_cache_cpu *c)&#123; void *freelist; struct page *page; page = c-&gt;page; if (!page) goto new_slab;redo: if (unlikely(!node_match(page, node))) &#123; int searchnode = node; if (node != NUMA_NO_NODE &amp;&amp; !node_present_pages(node)) searchnode = node_to_mem_node(node); if (unlikely(!node_match(page, searchnode))) &#123; stat(s, ALLOC_NODE_MISMATCH); deactivate_slab(s, page, c-&gt;freelist, c); goto new_slab; &#125; &#125; /* * By rights, we should be searching for a slab page that was * PFMEMALLOC but right now, we are losing the pfmemalloc * information when the page leaves the per-cpu allocator */ if (unlikely(!pfmemalloc_match(page, gfpflags))) &#123; deactivate_slab(s, page, c-&gt;freelist, c); goto new_slab; &#125; /* must check again c-&gt;freelist in case of cpu migration or IRQ */ freelist = c-&gt;freelist; if (freelist) goto load_freelist; freelist = get_freelist(s, page); if (!freelist) &#123; c-&gt;page = NULL; stat(s, DEACTIVATE_BYPASS); goto new_slab; &#125; stat(s, ALLOC_REFILL);load_freelist: /* * freelist is pointing to the list of objects to be used. * page is pointing to the page from which the objects are obtained. * That page must be frozen for per cpu allocations to work. */ VM_BUG_ON(!c-&gt;page-&gt;frozen); c-&gt;freelist = get_freepointer(s, freelist); c-&gt;tid = next_tid(c-&gt;tid); return freelist;new_slab: if (slub_percpu_partial(c)) &#123; page = c-&gt;page = slub_percpu_partial(c); slub_set_percpu_partial(c, page); stat(s, CPU_PARTIAL_ALLOC); goto redo; &#125; freelist = new_slab_objects(s, gfpflags, node, &amp;c); if (unlikely(!freelist)) &#123; slab_out_of_memory(s, gfpflags, node); return NULL; &#125; page = c-&gt;page; if (likely(!kmem_cache_debug(s) &amp;&amp; pfmemalloc_match(page, gfpflags))) goto load_freelist; /* Only entered in the debug case */ if (kmem_cache_debug(s) &amp;&amp; !alloc_debug_processing(s, page, freelist, addr)) goto new_slab;\t/* Slab failed checks. Next slab needed */ deactivate_slab(s, page, get_freepointer(s, freelist), c); return freelist;&#125; new_slab_objects()函数逻辑如下 调用get_partial()函数，根据node找到对应的kmem_cache_node然后调用get_partial_node()分配内存块。实际分配通过acquire_slab()函数完成，该函数会分配完成返回内存块指针并保存在freelist中，从 kmem_cache_node 的 partial 链表中拿下一大块内存来，并且将 freelist，也就是第一块空闲的缓存块赋值给 t。并且当第一轮循环的时候，将 kmem_cache_cpu 的 page 指向取下来的这一大块内存，返回的 object 就是这块内存里面的第一个缓存块 t。如果 kmem_cache_cpu 也有一个 partial，就会进行第二轮，再次取下一大块内存来，这次调用 put_cpu_partial()，放到 kmem_cache_cpu 的 partial 里面。如果 kmem_cache_node 里面也没有空闲的内存，这就说明原来分配的页里面都放满了，执行下一步。 调用new_slab()函数，向伙伴系统请求2^order个page，将请求的page构建成一个slab。分配的时候，要按 kmem_cache_order_objects 里面的 order 来。如果第一次分配不成功，说明内存已经很紧张了，那就换成 min 版本的 kmem_cache_order_objects。其调用链为new_slab()-&gt;allocate_slab()-&gt;alloc_slab_page()-&gt;__alloc_pages_node()-&gt;__alloc_pages()-&gt;__alloc_pages_nodemask()，从这里回到了伙伴系统，可见上文对该函数的分析。 123456789101112131415161718192021222324252627static inline void *new_slab_objects(struct kmem_cache *s, gfp_t flags, int node, struct kmem_cache_cpu **pc)&#123; void *freelist; struct kmem_cache_cpu *c = *pc; struct page *page; WARN_ON_ONCE(s-&gt;ctor &amp;&amp; (flags &amp; __GFP_ZERO)); freelist = get_partial(s, flags, node, c); if (freelist) return freelist; page = new_slab(s, flags, node); if (page) &#123; c = raw_cpu_ptr(s-&gt;cpu_slab); if (c-&gt;page) flush_slab(s, c); /* * No other reference to the page yet so we can * muck around with it freely without cmpxchg */ freelist = page-&gt;freelist; page-&gt;freelist = NULL; stat(s, ALLOC_SLAB); c-&gt;page = page; *pc = c; &#125; return freelist;&#125; 至此，页的分配就介绍完了。简单概括就是伙伴系统将多个连续页面整合为页块以供大规模使用，slub allocator将从伙伴系统申请的大块切成小块，放在缓存，并分配给其他系统。物理内存分页之后，通过page_address()-&gt;lowmem_page_address()-&gt;page_to_virt()调用链转化为虚拟地址以使用。 五. 内核页表 内核态的页表在系统初始化的时候就需要创建，而非可以等到用的时候再创建页并映射。在 arch&#x2F;x86&#x2F;include&#x2F;asm&#x2F;pgtable_64.h有如下定义 12345678910extern p4d_t level4_kernel_pgt[512];extern p4d_t level4_ident_pgt[512];extern pud_t level3_kernel_pgt[512];extern pud_t level3_ident_pgt[512];extern pmd_t level2_kernel_pgt[512];extern pmd_t level2_fixmap_pgt[512];extern pmd_t level2_ident_pgt[512];extern pte_t level1_fixmap_pgt[512 * FIXMAP_PMD_NUM];extern pgd_t init_top_pgt[];#define swapper_pg_dir init_top_pgt 其中swapper_pg_dir 指向内核最顶级的目录 pgd，同时出现的还有几个页表目录，其中 XXX_ident_pgt 对应的是直接映射区，XXX_kernel_pgt 对应的是内核代码区，XXX_fixmap_pgt 对应的是固定映射区。 初始化的位置位于arch\\x86\\kernel\\head_64.S，以全局变量的方式保存。这里quad 表示声明了一项的内容，org表示跳到了某个位置。 init_top_pgt是内核页的顶级目录，首先将其指向level3_ident_pgt，即直接映射区页表的三级目录。前文中有说过直接映射区物理内存和虚拟内存通过减去偏移量实现，即__START_KERNEL_map，虚拟地址空间的内核代码段的起始地址。通过这种方式，我们得到了其对应的物理地址。接着我们通过PGD_PAGE_OFFSET偏移量和PGD_START_KERNEL偏移量进行两次跳转，其中PGD_PAGE_OFFSET对应__PAGE_OFFSET_BASE，即虚拟地址空间里面内核的起始地址。第二项则指向__START_KERNEL_map，即虚拟地址空间里面内核代码段的起始地址。其他代码也是同理，最终形成如下表所示的整个页表项的初始化工作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051__INITDATANEXT_PAGE(init_top_pgt) .quad level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE .org init_top_pgt + PGD_PAGE_OFFSET*8, 0 .quad level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE .org init_top_pgt + PGD_START_KERNEL*8, 0 /* (2^48-(2*1024*1024*1024))/(2^39) = 511 */ .quad level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLENEXT_PAGE(level3_ident_pgt) .quad level2_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE .fill 511, 8, 0NEXT_PAGE(level2_ident_pgt) /* Since I easily can, map the first 1G. * Don&#x27;t set NX because code runs from these pages. */ PMDS(0, __PAGE_KERNEL_IDENT_LARGE_EXEC, PTRS_PER_PMD)NEXT_PAGE(level3_kernel_pgt) .fill L3_START_KERNEL,8,0 /* (2^48-(2*1024*1024*1024)-((2^39)*511))/(2^30) = 510 */ .quad level2_kernel_pgt - __START_KERNEL_map + _KERNPG_TABLE .quad level2_fixmap_pgt - __START_KERNEL_map + _PAGE_TABLENEXT_PAGE(level2_kernel_pgt) /* * 512 MB kernel mapping. We spend a full page on this pagetable * anyway. * * The kernel code+data+bss must not be bigger than that. * * (NOTE: at +512MB starts the module area, see MODULES_VADDR. * If you want to increase this then increase MODULES_VADDR * too.) */ PMDS(0, __PAGE_KERNEL_LARGE_EXEC, KERNEL_IMAGE_SIZE/PMD_SIZE)NEXT_PAGE(level2_fixmap_pgt) .fill 506,8,0 .quad level1_fixmap_pgt - __START_KERNEL_map + _PAGE_TABLE /* 8MB reserved for vsyscalls + a 2MB hole = 4 + 1 entries */ .fill 5,8,0NEXT_PAGE(level1_fixmap_pgt) .fill 51 PGD_PAGE_OFFSET = pgd_index(__PAGE_OFFSET_BASE)PGD_START_KERNEL = pgd_index(__START_KERNEL_map)L3_START_KERNEL = pud_index(__START_KERNEL_map) 内核页表定义完了，一开始这里面的页表能够覆盖的内存范围比较小。例如内核代码区 512M，直接映射区 1G。这个时候，其实只要能够映射基本的内核代码和数据结构就可以了。可以看出里面还空着很多项，可以用于将来映射巨大的内核虚拟地址空间，等用到的时候再进行映射。如果是用户态进程页表，会有 mm_struct 指向进程顶级目录 pgd，对于内核来讲，也定义了一个 mm_struct，指向 swapper_pg_dir。 1234567891011struct mm_struct init_mm = &#123; .mm_rb = RB_ROOT, .pgd = swapper_pg_dir, .mm_users = ATOMIC_INIT(2), .mm_count = ATOMIC_INIT(1), .mmap_sem = __RWSEM_INITIALIZER(init_mm.mmap_sem), .page_table_lock = __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock), .mmlist = LIST_HEAD_INIT(init_mm.mmlist), .user_ns = &amp;init_user_ns, INIT_MM_CONTEXT(init_mm)&#125;; 内核页表的初始化工作会在系统启动时，start_kernel()通过调用setup_arch()完成。load_cr3(swapper_pg_dir) 说明内核页表要开始起作用了，并且刷新了 TLB，初始化 init_mm 的成员变量，最重要的就是 init_mem_mapping()，通过调用链init_mem_mapping()-&gt;init_memory_mapping()-&gt;kernel_physical_mapping_init()最终通过 __va 将物理地址转换为虚拟地址，然后再创建虚拟地址和物理地址的映射页表。__va和__pa本身可以直接完成物理地址和虚拟地址的转换，但是CPU在保护模式下访问虚拟地址需要通过CR3寄存器，因此必须完成该映射页表的创建工作。 123456789101112131415161718192021void __init setup_arch(char **cmdline_p)&#123; /* * copy kernel address range established so far and switch * to the proper swapper page table */ clone_pgd_range(swapper_pg_dir + KERNEL_PGD_BOUNDARY, initial_page_table + KERNEL_PGD_BOUNDARY, KERNEL_PGD_PTRS); load_cr3(swapper_pg_dir); __flush_tlb_all();...... init_mm.start_code = (unsigned long) _text; init_mm.end_code = (unsigned long) _etext; init_mm.end_data = (unsigned long) _edata; init_mm.brk = _brk_end;...... init_mem_mapping();......&#125; 六. 页面交换 由于虚拟内存是远大于物理内存的，在物理内存中加载所有的虚拟内存页显然是异想天开，因此我们必须要有页面的交换。和CPU调度相似，页的交换也包括主动的交换和被动的交换。 被动页面回收：当分配内存的时候发现物理内存不够用了，则尝试回收。如申请页面会调用get_page_from_freelist()，该函数会通过调用链get_page_from_freelist()-&gt;node_reclaim()-&gt;__node_reclaim()-&gt;shrink_node()尝试是否可以对当前的内存节点执行换出操作，从而腾出空 主动页面管理：在内核中，内核线程kswapd0即负责该部分的功能。下面详细展开分析一下该部分功能。 为了衡量内存的使用情况，kswapd0 定义了三个内存阈值（watermark，也称为水位），分别是页最小阈值（pages_min）、页低阈值（pages_low）和页高阈值（pages_high）。剩余内存，则使用 pages_free 表示。 kswapd0 定期扫描内存的使用情况，并根据剩余内存落在这三个阈值的空间位置，进行内存的回收操作。 剩余内存小于页最小阈值，说明进程可用内存都耗尽了，只有内核才可以分配内存。 剩余内存落在页最小阈值和页低阈值中间，说明内存压力比较大，剩余内存不多了。这时 kswapd0 会执行内存回收，直到剩余内存大于高阈值为止。 剩余内存落在页低阈值和页高阈值中间，说明内存有一定压力，但还可以满足新内存请求。 剩余内存大于页高阈值，说明剩余内存比较多，没有内存压力。 一旦剩余内存小于页低阈值，就会触发内存的回收。这个页低阈值，其实可以通过内核选项 &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;min_free_kbytes 来间接设置。min_free_kbytes 设置了页最小阈值，而其他两个阈值，都是根据页最小阈值计算生成的，计算方法如下 ： 12pages_low = pages_min*5/4pages_high = pages_min*3/2 如下所示为kswapd()源码，核心调用链为balance_pgdat()-&gt;kswapd_shrink_node()-&gt;shrink_node()，所以被动回收和主动管理最后殊途同归，回到了同样的函数shrink_node()。 1234567891011121314151617181920212223242526272829303132333435363738394041/* * The background pageout daemon, started as a kernel thread * from the init process. * * This basically trickles out pages so that we have _some_ * free memory available even if there is no other activity * that frees anything up. This is needed for things like routing * etc, where we otherwise might have all activity going on in * asynchronous contexts that cannot page things out. * * If there are applications that are active memory-allocators * (most normal use), this basically shouldn&#x27;t matter. */static int kswapd(void *p)&#123; unsigned int alloc_order, reclaim_order; unsigned int classzone_idx = MAX_NR_ZONES - 1; pg_data_t *pgdat = (pg_data_t*)p; struct task_struct *tsk = current;...... for ( ; ; ) &#123;......kswapd_try_sleep: kswapd_try_to_sleep(pgdat, alloc_order, reclaim_order, classzone_idx);...... /* * Reclaim begins at the requested order but if a high-order * reclaim fails then kswapd falls back to reclaiming for * order-0. If that happens, kswapd will consider sleeping * for the order it finished reclaiming at (reclaim_order) * but kcompactd is woken to compact for the original * request (alloc_order). */ trace_mm_vmscan_kswapd_wake(pgdat-&gt;node_id, classzone_idx, alloc_order); reclaim_order = balance_pgdat(pgdat, alloc_order, classzone_idx);...... &#125; return 0;&#125; shrink_node()实际调用shrink_node_memcg()。这里面有个 LRU 列表，所有的页面都被挂在 LRU 列表中。LRU 也就是最近最少使用。这个列表里面会按照活跃程度进行排序，这样就容易把不怎么用的内存页拿出来做处理。内存页总共分两类，一类是匿名页，和虚拟地址空间进行关联；一类是内存映射，不但和虚拟地址空间关联，还和文件管理关联。它们每一类都有两个列表，一个是 active，一个是 inactive。顾名思义，active 就是比较活跃的，inactive 就是不怎么活跃的。这两个里面的页会变化，过一段时间，活跃的可能变为不活跃，不活跃的可能变为活跃。如果要换出内存，那就是从不活跃的列表中找出最不活跃的，换出到硬盘上。 shrink_list() 会先缩减活跃页面列表，再压缩不活跃的页面列表。对于不活跃列表的缩减，shrink_inactive_list() 就需要对页面进行回收；对于匿名页来讲，需要分配 swap()，将内存页写入文件系统；对于内存映射关联了文件的，我们需要将在内存中对于文件的修改写回到文件中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/* * This is a basic per-node page freer. Used by both kswapd and direct reclaim. */static void shrink_node_memcg(struct pglist_data *pgdat, struct mem_cgroup *memcg, struct scan_control *sc, unsigned long *lru_pages)&#123; struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, memcg); unsigned long nr[NR_LRU_LISTS]; unsigned long targets[NR_LRU_LISTS]; unsigned long nr_to_scan; enum lru_list lru;...... while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] || nr[LRU_INACTIVE_FILE]) &#123; unsigned long nr_anon, nr_file, percentage; unsigned long nr_scanned; for_each_evictable_lru(lru) &#123; if (nr[lru]) &#123; nr_to_scan = min(nr[lru], SWAP_CLUSTER_MAX); nr[lru] -= nr_to_scan; nr_reclaimed += shrink_list(lru, nr_to_scan, lruvec, memcg, sc); &#125; &#125;......&#125; enum lru_list &#123; LRU_INACTIVE_ANON = LRU_BASE, LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE, LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE, LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE, LRU_UNEVICTABLE, NR_LRU_LISTS&#125;;#define for_each_evictable_lru(lru) for (lru = 0; lru &lt;= LRU_ACTIVE_FILE; lru++)static unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan, struct lruvec *lruvec, struct mem_cgroup *memcg, struct scan_control *sc)&#123; if (is_active_lru(lru)) &#123; if (inactive_list_is_low(lruvec, is_file_lru(lru), memcg, sc, true)) shrink_active_list(nr_to_scan, lruvec, sc, lru); return 0; &#125; return shrink_inactive_list(nr_to_scan, lruvec, sc, lru);&#125; 七. 总结 内存管理可谓非常复杂，本文较为详尽的介绍了内存管理中分页和页的分配相关内容，下文将继续介绍物理内存和虚拟内存的映射。"},{"path":"/2024/04/05/linux-docs/内存管理/多核心Linux内核路径优化的不二法门之-slab与伙伴系统/","content":"Linux内核的slab来自一种很简单的思想，即事先准备好一些会频繁分配，释放的数据结构。然而标准的slab实现太复杂且维护开销巨大，因此便分化 出了更加小巧的slub，因此本文讨论的就是slub，后面所有提到slab的地方，指的都是slub。另外又由于本文主要描述内核优化方面的内容，并不 是基本原理介绍，因此想了解slab细节以及代码实现的请自行百度或者看源码。 单CPU上单纯的slab下图给出了单CPU上slab在分配和释放对象时的情景序列： 可以看出，非常之简单，而且完全达到了slab设计之初的目标。 扩展到多核心CPU现在我们简单的将上面的模型扩展到多核心CPU，同样差不多的分配序列如下图所示： 我们看到，在只有单一slab的时候，如果多个CPU同时分配对象，冲突是不可避免的，解决冲突的几乎是唯一的办法就是加锁排队，然而这将大大增加延迟，我们看到，申请单一对象的整个时延从T0开始，到T4结束，这太久了。 多CPU无锁化并行化操作的直接思路-复制给每个CPU一套相同的数据结构。 不二法门就是增加“每CPU变量”。对于slab而言，可以扩展成下面的样子： 如果以为这么简单就结束了，那这就太没有意义了。 问题首先，我们来看一个简单的问题，如果单独的某个CPU的slab缓存没有对象可分配了，但是其它CPU的slab缓存仍有大量空闲对象的情况，如下图所示： 这 是可能的，因为对单独一种slab的需求是和该CPU上执行的进程&#x2F;线程紧密相关的，比如如果CPU0只处理网络，那么它就会对skb等数据结构有大量的 需求，对于上图最后引出的问题，如果我们选择从伙伴系统中分配一个新的page(或者pages，取决于对象大小以及slab cache的order)，那么久而久之就会造成slab在CPU间分布的不均衡，更可能会因此吃掉大量的物理内存，这都是不希望看到的。 在继续之前，首先要明确的是，我们需要在CPU间均衡slab，并且这些必须靠slab内部的机制自行完成，这个和进程在CPU间负载均衡是完全不同的， 对进程而言，拥有一个核心调度机制，比如基于时间片，或者虚拟时钟的步进速率等，但是对于slab，完全取决于使用者自身，只要对象仍然在使用，就不能剥 夺使用者继续使用的权利，除非使用者自己释放。因此slab的负载均衡必须设计成合作型的，而不是抢占式的。 好了。现在我们知道，从伙伴系统重新分配一个page(s)并不是一个好主意，它应该是最终的决定，在执行它之前，首先要试一下别的路线。 现在，我们引出第二个问题，如下图所示： 谁也不能保证分配slab对象的CPU和释放slab对象的CPU是同一个CPU，谁也不能保证一个CPU在一个slab对象的生命周期内没有分配新的 page(s)，这期间的复杂操作谁也没有规定。这些问题该怎么解决呢？事实上，理解了这些问题是怎么解决的，一个slab框架就彻底理解了。 问题的解决-分层slab cache无级变速总是让人向往。 如果一个CPU的slab缓存满了，直接去抢同级别的别的CPU的slab缓存被认为是一种鲁莽且不道义的做法。那么为何不设置另外一个slab缓存，获 取它里面的对象不像直接获取CPU的slab缓存那么简单且直接，但是难度却又不大，只是稍微增加一点消耗，这不是很好吗？事实上，CPU的 L1，L2，L3 cache不就是这个方案设计的吗？这事实上已经成为cache设计的不二法门。这个设计思想同样作用于slab，就是Linux内核的slub实现。现在可以给出概念和解释了。 Linux kernel slab cache：一个分为3层的对象cache模型。 Level 1 slab cache：一个空闲对象链表，每个CPU一个的独享cache，分配释放对象无需加锁。 Level 2 slab cache：一个空闲对象链表，每个CPU一个的共享page(s) cache，分配释放对象时仅需要锁住该page(s)，与Level 1 slab cache互斥，不互相包容。 Level 3 slab cache：一个page(s)链表，每个NUMA NODE的所有CPU共享的cache，单位为page(s)，获取后被提升到对应CPU的Level 1 slab cache，同时该page(s)作为Level 2的共享page(s)存在。 共享page(s)：该page(s)被一个或者多个CPU占 有，每一个CPU在该page(s)上都可以拥有互相不充图的空闲对象链表，该page(s)拥有一个唯一的Level 2 slab cache空闲链表，该链表与上述一个或多个Level 1 slab cache空闲链表亦不冲突，多个CPU获取该Level 2 slab cache时必须争抢，获取后可以将该链表提升成自己的Level 1 slab cache。 该slab cache的图示如下： 其行为如下图所示： 2个场景对于常规的对象分配过程，下图展示了其细节： 事实上，对于多个CPU共享一个page(s)的情况，还可以有另一种玩法，如下图所示： 伙伴系统前面我们简短的体会了Linux内核的slab设计,不宜过长,太长了不易理解.但是最后,如果Level 3也没有获取page(s)，那么最终会落到终极的伙伴系统。 伙伴系统是为了防内存分配碎片化的，所以它尽可能地做两件事： 1).尽量分配尽可能大的内存2).尽量合并连续的小块内存成一块大内存 我们可以通过下面的图解来理解上面的原则： 注意，本文是关于优化的，不是伙伴系统的科普，所以我假设大家已经理解了伙伴系统。 鉴于slab缓存对象大多数都是不超过1个页面的小结构(不仅仅slab系统，超过1个页面的内存需求相比1个页面的内存需求，很少)，因此会有大量的针 对1个页面的内存分配需求。从伙伴系统的分配原理可知，如果持续大量分配单一页面，会有大量的order大于0的页面分裂成单一页面，在单核心CPU上， 这不是问题，但是在多核心CPU上，由于每一个CPU都会进行此类分配，而伙伴系统的分裂，合并操作会涉及大量的链表操作，这个锁开销是巨大的，因此需要 优化！ Linux内核对伙伴系统针对单一页面的分配需求采取的批量分配“每CPU单一页面缓存”的方式！ 每一个CPU拥有一个单一页面缓存池，需要单一页面的时候，可以无需加锁从当前CPU对应的页面池中获取页面。而当池中页面不足时，系统会批量从伙伴系统中拉取一堆页面到池中，反过来，在单一页面释放的时候，会择优将其释放到每CPU的单一页面缓存中。 为了维持“每CPU单一页面缓存”中页面的数量不会太多或太少(太多会影响伙伴系统，太少会影响CPU的需求)，系统保持了两个值，当缓存页面数量低于 low值的时候，便从伙伴系统中批量获取页面到池中，而当缓存页面数量大于high的时候，便会释放一些页面到伙伴系统中。 小结多 CPU操作系统内核中，关键的开销就是锁的开销。我认为这是一开始的设计导致的，因为一开始，多核CPU并没有出现，单核CPU上的共享保护几乎都是可以 用“禁中断”，“禁抢占”来简单实现的，到了多核时代，操作系统同样简单平移到了新的平台，因此同步操作是在单核的基础上后来添加的。简单来讲，目前的主 流操作系统都是在单核年代创造出来的，因此它们都是顺应单核环境的，对于多核环境，可能它们一开始的设计就有问题。 不管怎么说，优化操作的不二法门就是禁止或者尽量减少锁的操作。随之而来的思路就是为共享的关键数据结构创建”每CPU的缓存“，而这类缓存分为两种类型： 1).数据通路缓存。比如路由表之类的数据结构，你可以用RCU锁来保护，当然如果为每一个CPU都创建一个本地路由表缓存，也是不错的，现在的问题是何时更新它们，因为所有的缓存都是平级的，因此一种批量同步的机制是必须的。2).管理机制缓存。比 如slab对象缓存这类，其生命周期完全取决于使用者，因此不存在同步问题，然而却存在管理问题。采用分级cache的思想是好的，这个非常类似于CPU 的L1&#x2F;L2&#x2F;L3缓存，采用这种平滑的开销逐渐增大，容量逐渐增大的机制，并配合以设计良好的换入&#x2F;换出等算法，效果是非常明显的。"},{"path":"/2024/04/05/linux-docs/内存管理/尽情阅读，技术进阶，详解mmap原理/","content":"1. 一句话概括mmapmmap的作用，在应用这一层，是让你把文件的某一段，当作内存一样来访问。将文件映射到物理内存，将进程虚拟空间映射到那块内存。 这样，进程不仅能像访问内存一样读写文件，多个进程映射同一文件，还能保证虚拟空间映射到同一块物理内存，达到内存共享的作用。 2. 虚拟内存？虚拟空间？其实是一个概念，前一篇对于这个词没有确切的定义，现在定义一下： 虚拟空间就是进程看到的所有地址组成的空间，虚拟空间是某个进程对分配给它的所有物理地址（已经分配的和将会分配的）的重新映射。 而虚拟内存，为啥叫虚拟内存，是因为它就不是真正的内存，是假的，因为它是由地址组成的空间，所以在这里，使用虚拟空间这个词更加确切和易懂。（不过虚拟内存这个词也不算错） 2.1 虚拟空间原理2.1.1物理内存首先，物理地址实际上也不是连续的，通常是包含作为主存的DRAM和IO寄存器 以前的CPU（如X86）是为IO划分单独的地址空间，所以不能用直接访问内存的方式（如指针）IO，只能用专门的方法（in&#x2F;read&#x2F;out&#x2F;write）诸如此类。 现在的CPU利用PCI总线将IO寄存器映射到物理内存，所以出现了基于内存访问的IO。 还有一点补充的，就如同进程空间有一块内核空间一样，物理内存也会有极小一部分是不能访问的，为内核所用。 2.1.2三个总线这里再补充下三个总线的知识，即：地址总线、数据总线、控制总线 地址总线，用来传输地址 数据总线，用来传输数据 控制总线，用来传输命令 比如CPU通过控制总线发送读取命令，同时用地址总线发送要读取的数据虚地址，经过MMU后到内存 内存通过数据总线将数据传输给CPU。 虚拟地址的空间和指令集的地址长度有关，不一定和物理地址长度一致，比如现在的64位处理器，从VA角度看来，可以访问64位的地址，但地址总线长度只有48位，所以你可以访问一个位于2^52这个位置的地址。 2.1.3虚拟内存地址转换（虚地址转实地址）上面已经明确了虚拟内存是虚拟空间，即地址的集合这一概念。基于此，来说说原理。 如果还记得操作系统课程里面提到的虚地址，那么这个虚地址就是虚拟空间的地址了，虚地址通过转换得到实地址，转换方式课程内也讲得很清楚，虚地址头部包含了页号（段地址和段大小，看存储模式：页存储、段存储，段页式），剩下部分是偏移量，经过MMU转换成实地址。 存储方式 如图则是页式存储动态地址变换的方式 虚拟地址头部为页号通过查询页表得到物理页号，假设一页时1K，那么页号*偏移量就得到物理地址 如图所示，段式存储 虚拟地址头部为段号，段表中找到段基地址加上偏移量得到实地址 段页式结合两者，如图所示。 3. mmap映射至此，如果对虚拟空间已经了解了，那么接下来，作为coder，应该自动把虚拟空间无视掉，因为Linux的目的也是要让更多额进程能享用内存，又不让进程做麻烦的事情，是将虚拟空间和MMU都透明化，让进程（和coder）只需要管对内存怎样使用。 所以现在开始不再强调虚拟空间了。 mmap就是将文件映射到内存上，进程直接对内存进行读写，然后就会反映到磁盘上。 虚拟空间获取到一段连续的地址 在没有读写的时候，这个地址指向不存在的地方（所以，上图中起始地址和终止地址是还没分配给进程的） 好了，根据偏移量，进程要读文件数据了，数据占在两个页当中（物理内存着色部分） 这时，进程开始使用内存了，所以OS给这两个页分配了内存（即缺页异常）（其余部分还是没有分配） 然后刚分配的页内是空的，所以再将相同偏移量的文件数据拷贝到物理内存对应页上。"},{"path":"/2024/04/05/linux-docs/内存管理/内存管理之内存映射/","content":"一. 前言 本文为内存部分最后一篇，介绍内存映射。内存映射不仅是物理内存和虚拟内存间的映射，也包括将文件中的内容映射到虚拟内存空间。这个时候，访问内存空间就能够访问到文件里面的数据。而仅有物理内存和虚拟内存的映射，是一种特殊情况。本文首先分析用户态在堆中申请小块内存的brk和申请大块内存的mmap，之后会分析内核态的内存映射机制vmalloc，kmap_atomic，swapper_pg_dir以及内核态缺页异常。 二. 用户态内存映射 用户态调用malloc()会分配堆内存空间，而实际上则是完成了一次用户态的内存映射，根据分配空间的大小，内存映射对应的系统调用主要有brk()和mmap()(当然我们也可以直接调用mmap()来映射文件)。对小块内存（小于 128K），C 标准库使用 brk() 来分配，也就是通过移动堆顶的位置来分配内存。这些内存释放后并不会立刻归还系统，而是被缓存起来，这样就可以重复使用。而大块内存（大于 128K），则直接使用内存映射 mmap() 来分配，也就是在文件映射段找一块空闲内存分配出去。这两种方式，自然各有优缺点。 brk() 方式的缓存，可以减少缺页异常的发生，提高内存访问效率。不过，由于这些内存没有归还系统，在内存工作繁忙时，频繁的内存分配和释放会造成内存碎片。 mmap() 方式分配的内存，会在释放时直接归还系统，所以每次 mmap() 都会发生缺页异常。在内存工作繁忙时，频繁的内存分配会导致大量的缺页异常，使内核的管理负担增大。这也是 malloc() 只对大块内存使用 mmap() 的原因。 2.1 小块内存申请 brk()系统调用为sys_brk()函数，其参数brk是新的堆顶位置，而mm-&gt;brk是原堆顶位置。该函数主要逻辑如下 将原来的堆顶和现在的堆顶按照页对齐地址比较大小，判断是否在同一页中 如果同一页则不需要分配新页，直接跳转至set_brk，设置mm-&gt;brk为新的brk即可 如果不在同一页 如果新堆顶小于旧堆顶，则说明不是新分配内存而是释放内存，由此调用__do_munmap()释放 如果是新分配内存，则调用find_vma()，查找vm_area_struct红黑树中原堆顶所在vm_area_struct的下一个结构体，如果在二者之间有足够的空间分配一个页则调用do_brk_flags()分配堆空间。如果不可以则分配失败。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657SYSCALL_DEFINE1(brk, unsigned long, brk)&#123; unsigned long retval; unsigned long newbrk, oldbrk, origbrk; struct mm_struct *mm = current-&gt;mm; struct vm_area_struct *next;...... newbrk = PAGE_ALIGN(brk); oldbrk = PAGE_ALIGN(mm-&gt;brk); if (oldbrk == newbrk) &#123; mm-&gt;brk = brk; goto success; &#125; /* * Always allow shrinking brk. * __do_munmap() may downgrade mmap_sem to read. */ if (brk &lt;= mm-&gt;brk) &#123; int ret; /* * mm-&gt;brk must to be protected by write mmap_sem so update it * before downgrading mmap_sem. When __do_munmap() fails, * mm-&gt;brk will be restored from origbrk. */ mm-&gt;brk = brk; ret = __do_munmap(mm, newbrk, oldbrk-newbrk, &amp;uf, true); if (ret &lt; 0) &#123; mm-&gt;brk = origbrk; goto out; &#125; else if (ret == 1) &#123; downgraded = true; &#125; goto success; &#125; /* Check against existing mmap mappings. */ next = find_vma(mm, oldbrk); if (next &amp;&amp; newbrk + PAGE_SIZE &gt; vm_start_gap(next)) goto out; /* Ok, looks good - let it rip. */ if (do_brk_flags(oldbrk, newbrk-oldbrk, 0, &amp;uf) &lt; 0) goto out; mm-&gt;brk = brk;success: populate = newbrk &gt; oldbrk &amp;&amp; (mm-&gt;def_flags &amp; VM_LOCKED) != 0; if (downgraded) up_read(&amp;mm-&gt;mmap_sem); else up_write(&amp;mm-&gt;mmap_sem); userfaultfd_unmap_complete(mm, &amp;uf); if (populate) mm_populate(oldbrk, newbrk - oldbrk); return brk;out: retval = origbrk; up_write(&amp;mm-&gt;mmap_sem); return retval;&#125; 在 do_brk_flags() 中，调用 find_vma_links() 找到将来的 vm_area_struct 节点在红黑树的位置，找到它的父节点、前序节点。接下来调用 vma_merge()，看这个新节点是否能够和现有树中的节点合并。如果地址是连着的，能够合并，则不用创建新的 vm_area_struct 了，直接跳到 out，更新统计值即可；如果不能合并，则创建新的 vm_area_struct，既加到 anon_vma_chain 链表中，也加到红黑树中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/* * this is really a simplified &quot;do_mmap&quot;. it only handles * anonymous maps. eventually we may be able to do some * brk-specific accounting here. */static int do_brk_flags(unsigned long addr, unsigned long len, unsigned long flags, struct list_head *uf)&#123; struct mm_struct *mm = current-&gt;mm; struct vm_area_struct *vma, *prev; struct rb_node **rb_link, *rb_parent;...... /* * Clear old maps. this also does some error checking for us */ while (find_vma_links(mm, addr, addr + len, &amp;prev, &amp;rb_link, &amp;rb_parent)) &#123; if (do_munmap(mm, addr, len, uf)) return -ENOMEM; &#125;...... /* Can we just expand an old private anonymous mapping? */ vma = vma_merge(mm, prev, addr, addr + len, flags, NULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX); if (vma) goto out; /* * create a vma struct for an anonymous mapping */ vma = vm_area_alloc(mm); if (!vma) &#123; vm_unacct_memory(len &gt;&gt; PAGE_SHIFT); return -ENOMEM; &#125; vma_set_anonymous(vma); vma-&gt;vm_start = addr; vma-&gt;vm_end = addr + len; vma-&gt;vm_pgoff = pgoff; vma-&gt;vm_flags = flags; vma-&gt;vm_page_prot = vm_get_page_prot(flags); vma_link(mm, vma, prev, rb_link, rb_parent);out: perf_event_mmap(vma); mm-&gt;total_vm += len &gt;&gt; PAGE_SHIFT; mm-&gt;data_vm += len &gt;&gt; PAGE_SHIFT; if (flags &amp; VM_LOCKED) mm-&gt;locked_vm += (len &gt;&gt; PAGE_SHIFT); vma-&gt;vm_flags |= VM_SOFTDIRTY; return 0;&#125; 2.2 大内存块申请 大块内存的申请通过mmap系统调用实现，mmap既可以实现虚拟内存向物理内存的映射，也可以映射文件到自己的虚拟内存空间。映射文件时，实际是映射虚拟内存到物理内存再到文件。 123456789101112SYSCALL_DEFINE6(mmap, unsigned long, addr, unsigned long, len, unsigned long, prot, unsigned long, flags, unsigned long, fd, unsigned long, off)&#123; long error; error = -EINVAL; if (off &amp; ~PAGE_MASK) goto out; error = ksys_mmap_pgoff(addr, len, prot, flags, fd, off &gt;&gt; PAGE_SHIFT);out: return error;&#125; 这里主要调用ksys_mmap_pgoff()函数，这里逻辑如下 判断类型是否为匿名映射，如果不是则为文件映射，调用fget()获取文件描述符 如果是匿名映射，判断是否为大页，如果是则进行对齐处理并调用hugetlb_file_setup()获取文件描述符 调用vm_mmap_pgoff()函数找寻可以映射的区域并建立映射 12345678910111213141516171819202122232425262728293031323334353637383940414243unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len, unsigned long prot, unsigned long flags, unsigned long fd, unsigned long pgoff)&#123; struct file *file = NULL; unsigned long retval; if (!(flags &amp; MAP_ANONYMOUS)) &#123; audit_mmap_fd(fd, flags); file = fget(fd); if (!file) return -EBADF; if (is_file_hugepages(file)) len = ALIGN(len, huge_page_size(hstate_file(file))); retval = -EINVAL; if (unlikely(flags &amp; MAP_HUGETLB &amp;&amp; !is_file_hugepages(file))) goto out_fput; &#125; else if (flags &amp; MAP_HUGETLB) &#123; struct user_struct *user = NULL; struct hstate *hs; hs = hstate_sizelog((flags &gt;&gt; MAP_HUGE_SHIFT) &amp; MAP_HUGE_MASK); if (!hs) return -EINVAL; len = ALIGN(len, huge_page_size(hs)); /* * VM_NORESERVE is used because the reservations will be * taken when vm_ops-&gt;mmap() is called * A dummy user value is used because we are not locking * memory so no accounting is necessary */ file = hugetlb_file_setup(HUGETLB_ANON_FILE, len, VM_NORESERVE, &amp;user, HUGETLB_ANONHUGE_INODE, (flags &gt;&gt; MAP_HUGE_SHIFT) &amp; MAP_HUGE_MASK); if (IS_ERR(file)) return PTR_ERR(file); &#125; flags &amp;= ~(MAP_EXECUTABLE | MAP_DENYWRITE); retval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);out_fput: if (file) fput(file); return retval;&#125; vm_mmap_pgoff()函数调用do_mmap_pgoff()，实际调用do_mmap()函数。这里get_unmapped_area()函数负责寻找可映射的区域，mmap_region()负责映射该区域。 12345678910111213141516171819202122232425/* * The caller must hold down_write(&amp;current-&gt;mm-&gt;mmap_sem). */unsigned long do_mmap(struct file *file, unsigned long addr, unsigned long len, unsigned long prot, unsigned long flags, vm_flags_t vm_flags, unsigned long pgoff, unsigned long *populate, struct list_head *uf)&#123; struct mm_struct *mm = current-&gt;mm; int pkey = 0; *populate = 0;...... /* Obtain the address to map to. we verify (or select) it and ensure * that it represents a valid section of the address space. */ addr = get_unmapped_area(file, addr, len, pgoff, flags);...... addr = mmap_region(file, addr, len, vm_flags, pgoff, uf); if (!IS_ERR_VALUE(addr) &amp;&amp; ((vm_flags &amp; VM_LOCKED) || (flags &amp; (MAP_POPULATE | MAP_NONBLOCK)) == MAP_POPULATE)) *populate = len; return addr;&#125; 首先来看看寻找映射区的函数get_unmapped_area()。 如果是匿名映射，则调用get_umapped_area函数指针，这个函数其实是 arch_get_unmapped_area()。它会调用 find_vma_prev()，在表示虚拟内存区域的 vm_area_struct 红黑树上找到相应的位置。之所以叫 prev，是说这个时候虚拟内存区域还没有建立，找到前一个 vm_area_struct。 如果是映射到一个文件，在 Linux 里面每个打开的文件都有一个 struct file 结构，里面有一个 file_operations用来表示和这个文件相关的操作。如果是我们熟知的 ext4 文件系统，调用的也是get_unmapped_area 函数指针。 1234567891011121314151617181920212223242526272829303132333435unsigned longget_unmapped_area(struct file *file, unsigned long addr, unsigned long len, unsigned long pgoff, unsigned long flags)&#123; unsigned long (*get_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long); unsigned long error = arch_mmap_check(addr, len, flags); if (error) return error; /* Careful about overflows.. */ if (len &gt; TASK_SIZE) return -ENOMEM; get_area = current-&gt;mm-&gt;get_unmapped_area; if (file) &#123; if (file-&gt;f_op-&gt;get_unmapped_area) get_area = file-&gt;f_op-&gt;get_unmapped_area; &#125; else if (flags &amp; MAP_SHARED) &#123; /* * mmap_region() will call shmem_zero_setup() to create a file, * so use shmem&#x27;s get_unmapped_area in case it can be huge. * do_mmap_pgoff() will clear pgoff, so match alignment. */ pgoff = 0; get_area = shmem_get_unmapped_area; &#125; addr = get_area(file, addr, len, pgoff, flags); if (IS_ERR_VALUE(addr)) return addr; if (addr &gt; TASK_SIZE - len) return -ENOMEM; if (offset_in_page(addr)) return -EINVAL; error = security_mmap_addr(addr); return error ? error : addr;&#125; mmap_region()首先会再次检测地址空间是否满足要求，然后清除旧的映射，校验内存的可用性，在一切均满足的情况下调用vma_link()将新创建的vm_area_struct结构挂在mm_struct内的红黑树上。 123456789101112131415161718192021unsigned long mmap_region(struct file *file, unsigned long addr, unsigned long len, vm_flags_t vm_flags, unsigned long pgoff, struct list_head *uf)&#123; struct mm_struct *mm = current-&gt;mm; struct vm_area_struct *vma, *prev; int error; struct rb_node **rb_link, *rb_parent; unsigned long charged = 0;...... vma_link(mm, vma, prev, rb_link, rb_parent); /* Once vma denies write, undo our temporary denial count */ if (file) &#123; if (vm_flags &amp; VM_SHARED) mapping_unmap_writable(file-&gt;f_mapping); if (vm_flags &amp; VM_DENYWRITE) allow_write_access(file); &#125; file = vma-&gt;vm_file;......&#125; vma_link()本身是__vma_link()和__vma_link_file()的包裹函数 12345678910111213141516static void vma_link(struct mm_struct *mm, struct vm_area_struct *vma, struct vm_area_struct *prev, struct rb_node **rb_link, struct rb_node *rb_parent)&#123; struct address_space *mapping = NULL; if (vma-&gt;vm_file) &#123; mapping = vma-&gt;vm_file-&gt;f_mapping; i_mmap_lock_write(mapping); &#125; __vma_link(mm, vma, prev, rb_link, rb_parent); __vma_link_file(vma); if (mapping) i_mmap_unlock_write(mapping); mm-&gt;map_count++; validate_mm(mm);&#125; 其中__vma_link()主要是链表和红黑表的插入，这属于基本数据结构操作，不展开讲解。 12345678static void__vma_link(struct mm_struct *mm, struct vm_area_struct *vma, struct vm_area_struct *prev, struct rb_node **rb_link, struct rb_node *rb_parent)&#123; __vma_link_list(mm, vma, prev, rb_parent); __vma_link_rb(mm, vma, rb_link, rb_parent);&#125; 而__vma_link_file()会对文件映射进行处理，在file结构体中成员f_mapping指向address_space结构体，该结构体中存储红黑树i_mmap挂载vm_area_struct。 123456789101112131415static void __vma_link_file(struct vm_area_struct *vma)&#123; struct file *file; file = vma-&gt;vm_file; if (file) &#123; struct address_space *mapping = file-&gt;f_mapping; if (vma-&gt;vm_flags &amp; VM_DENYWRITE) atomic_dec(&amp;file_inode(file)-&gt;i_writecount); if (vma-&gt;vm_flags &amp; VM_SHARED) atomic_inc(&amp;mapping-&gt;i_mmap_writable); flush_dcache_mmap_lock(mapping); vma_interval_tree_insert(vma, &amp;mapping-&gt;i_mmap); flush_dcache_mmap_unlock(mapping); &#125;&#125; 至此，我们完成了用户态内存的映射，但是此处仅在虚拟内存中建立了新的区域，尚未真正访问物理内存。物理内存的访问只有在调度到该进程时才会真正分配，即发生缺页异常时分配。 三. 用户态缺页异常 一旦开始访问虚拟内存的某个地址，如果我们发现，并没有对应的物理页，那就触发缺页中断，调用 do_page_fault()。这里的逻辑如下 判断是否为内核缺页中断fault_in_kernel_space()，如果是则调用vmalloc_fault() 如果是用户态缺页异常，则调用find_vma()找到地址所在vm_area_struct区域 调用handle_mm_fault()映射找到的区域 1234567891011121314151617181920212223242526272829303132333435363738/* * This routine handles page faults. It determines the address, * and the problem, and then passes it off to one of the appropriate * routines. */asmlinkage void __kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code, unsigned long address)&#123;...... /* * We fault-in kernel-space virtual memory on-demand. The * &#x27;reference&#x27; page table is init_mm.pgd. * * NOTE! We MUST NOT take any locks for this case. We may * be in an interrupt or a critical region, and should * only copy the information from the master page table, * nothing more. */ if (unlikely(fault_in_kernel_space(address))) &#123; if (vmalloc_fault(address) &gt;= 0) return; if (notify_page_fault(regs, vec)) return; bad_area_nosemaphore(regs, error_code, address); return; &#125;...... vma = find_vma(mm, address);...... /* * If for any reason at all we couldn&#x27;t handle the fault, * make sure we exit gracefully rather than endlessly redo * the fault. */ fault = handle_mm_fault(vma, address, flags);......&#125; find_vma()为红黑树查找操作，在此不做展开描述，下面重点看看handle_mm_fault()。这里经过一系列校验之后会根据是否是大页而选择调用hugetlb_fault()或者__handle_mm_fault() 1234567891011vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address, unsigned int flags)&#123;...... if (unlikely(is_vm_hugetlb_page(vma))) ret = hugetlb_fault(vma-&gt;vm_mm, vma, address, flags); else ret = __handle_mm_fault(vma, address, flags);...... return ret;&#125; __handle_mm_fault()完成实际上的映射操作。这里涉及到了由pgd, p4g, pud, pmd, pte组成的五级页表，页表索引填充完后调用handle_pte_fault()创建页表项。 123456789101112131415161718192021222324tatic vm_fault_t __handle_mm_fault(struct vm_area_struct *vma, unsigned long address, unsigned int flags)&#123; struct vm_fault vmf = &#123; .vma = vma, .address = address &amp; PAGE_MASK, .flags = flags, .pgoff = linear_page_index(vma, address), .gfp_mask = __get_fault_gfp_mask(vma), &#125;; unsigned int dirty = flags &amp; FAULT_FLAG_WRITE; struct mm_struct *mm = vma-&gt;vm_mm; pgd_t *pgd; p4d_t *p4d; vm_fault_t ret; pgd = pgd_offset(mm, address); p4d = p4d_alloc(mm, pgd, address);...... vmf.pud = pud_alloc(mm, p4d, address);...... vmf.pmd = pmd_alloc(mm, vmf.pud, address);...... return handle_pte_fault(&amp;vmf);&#125; handle_pte_fault()处理以下三种情况 页表项从未出现过，即新映射页表项 匿名页映射，则映射到物理内存页，调用do_anonymous_page() 文件映射，调用do_fault() 页表项曾出现过，则为从物理内存换出的页，调用do_swap_page()换回来 1234567891011121314151617181920212223242526272829303132333435363738/* * These routines also need to handle stuff like marking pages dirty * and/or accessed for architectures that don&#x27;t do it in hardware (most * RISC architectures). The early dirtying is also good on the i386. * * There is also a hook called &quot;update_mmu_cache()&quot; that architectures * with external mmu caches can use to update those (ie the Sparc or * PowerPC hashed page tables that act as extended TLBs). * * We enter with non-exclusive mmap_sem (to exclude vma changes, but allow * concurrent faults). * * The mmap_sem may have been released depending on flags and our return value. * See filemap_fault() and __lock_page_or_retry(). */static vm_fault_t handle_pte_fault(struct vm_fault *vmf)&#123; pte_t entry;...... /* * A regular pmd is established and it can&#x27;t morph into a huge * pmd from under us anymore at this point because we hold the * mmap_sem read mode and khugepaged takes it in write mode. * So now it&#x27;s safe to run pte_offset_map(). */ vmf-&gt;pte = pte_offset_map(vmf-&gt;pmd, vmf-&gt;address); vmf-&gt;orig_pte = *vmf-&gt;pte;...... if (!vmf-&gt;pte) &#123; if (vma_is_anonymous(vmf-&gt;vma)) return do_anonymous_page(vmf); else return do_fault(vmf); &#125; if (!pte_present(vmf-&gt;orig_pte)) return do_swap_page(vmf);......&#125; 3.1 匿名页映射 对于匿名页映射，流程如下 调用pte_alloc()分配页表项 通过 alloc_zeroed_user_highpage_movable() 分配一个页，该函数会调用 alloc_pages_vma()，并最终调用 __alloc_pages_nodemask()。该函数是伙伴系统的核心函数，用于分配物理页面，在上文中已经详细分析过了。 调用mk_pte()将新分配的页表项指向分配的页 调用set_pte_at()将页表项加入该页 12345678910111213141516171819202122232425262728293031323334353637383940/* * We enter with non-exclusive mmap_sem (to exclude vma changes, * but allow concurrent faults), and pte mapped but not yet locked. * We return with mmap_sem still held, but pte unmapped and unlocked. */static vm_fault_t do_anonymous_page(struct vm_fault *vmf)&#123; struct vm_area_struct *vma = vmf-&gt;vma; struct mem_cgroup *memcg; struct page *page; vm_fault_t ret = 0; pte_t entry;...... /* * Use pte_alloc() instead of pte_alloc_map(). We can&#x27;t run * pte_offset_map() on pmds where a huge pmd might be created * from a different thread. * * pte_alloc_map() is safe to use under down_write(mmap_sem) or when * parallel threads are excluded by other means. * * Here we only have down_read(mmap_sem). */ if (pte_alloc(vma-&gt;vm_mm, vmf-&gt;pmd)) return VM_FAULT_OOM;...... page = alloc_zeroed_user_highpage_movable(vma, vmf-&gt;address);...... entry = mk_pte(page, vma-&gt;vm_page_prot); if (vma-&gt;vm_flags &amp; VM_WRITE) entry = pte_mkwrite(pte_mkdirty(entry)); vmf-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, vmf-&gt;pmd, vmf-&gt;address, &amp;vmf-&gt;ptl);...... set_pte_at(vma-&gt;vm_mm, vmf-&gt;address, vmf-&gt;pte, entry);......&#125;#define __alloc_zeroed_user_highpage(movableflags, vma, vaddr) \\ alloc_page_vma(GFP_HIGHUSER | __GFP_ZERO | movableflags, vma, vaddr) 3.2 文件映射 映射文件do_fault()函数调用了fault函数，该函数实际会根据不同的文件系统调用不同的函数，如ext4文件系统中vm_ops指向ext4_file_vm_ops，实际调用ext4_filemap_fault()函数，该函数会调用filemap_fault()完成实际的文件映射操作。 12345678910111213141516static vm_fault_t do_fault(struct vm_fault *vmf)&#123; struct vm_area_struct *vma = vmf-&gt;vma; struct mm_struct *vm_mm = vma-&gt;vm_mm; vm_fault_t ret; if (!vma-&gt;vm_ops-&gt;fault) &#123;......&#125;vm_fault_t ext4_filemap_fault(struct vm_fault *vmf)&#123;...... ret = filemap_fault(vmf);......&#125; file_map_fault()主要逻辑为 调用find_ge_page()找到映射文件vm_file对应的物理内存缓存页面 如果找到了，则调用do_async_mmap_readahead()，预读一些数据到内存里面 否则调用pagecache_get_page()分配一个缓存页，将该页加入LRU表中，并在address_space中调用123456789101112131415161718192021222324252627282930313233vm_fault_t filemap_fault(struct vm_fault *vmf)&#123; int error; struct file *file = vmf-&gt;vma-&gt;vm_file; struct file *fpin = NULL; struct address_space *mapping = file-&gt;f_mapping; struct file_ra_state *ra = &amp;file-&gt;f_ra; struct inode *inode = mapping-&gt;host;...... /* * Do we have something in the page cache already? */ page = find_get_page(mapping, offset); if (likely(page) &amp;&amp; !(vmf-&gt;flags &amp; FAULT_FLAG_TRIED)) &#123; /* * We found the page, so try async readahead before * waiting for the lock. */ fpin = do_async_mmap_readahead(vmf, page); &#125; else if (!page) &#123; /* No page in the page cache at all */...... &#125; struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset, int fgp_flags, gfp_t gfp_mask)&#123;...... page = __page_cache_alloc(gfp_mask);...... err = add_to_page_cache_lru(page, mapping, offset, gfp_mask);......&#125; 3.3 页交换 前文提到了我们会通过主动回收或者被动回收的方式将物理内存已映射的页面回收至硬盘中，当数据再次访问时，我们又需要通过do_swap_page()将其从硬盘中读回来。do_swap_page() 函数逻辑流程如下：查找 swap 文件有没有缓存页。如果没有，就调用 swapin_readahead()将 swap 文件读到内存中来形成内存页，并通过 mk_pte() 生成页表项。set_pte_at 将页表项插入页表，swap_free 将 swap 文件清理。因为重新加载回内存了，不再需要 swap 文件了。 123456789101112131415161718192021222324252627282930vm_fault_t do_swap_page(struct vm_fault *vmf)&#123;...... entry = pte_to_swp_entry(vmf-&gt;orig_pte);...... page = lookup_swap_cache(entry, vma, vmf-&gt;address); swapcache = page; if (!page) &#123; struct swap_info_struct *si = swp_swap_info(entry); if (si-&gt;flags &amp; SWP_SYNCHRONOUS_IO &amp;&amp; __swap_count(si, entry) == 1) &#123; /* skip swapcache */ page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, vmf-&gt;address);...... &#125; else &#123; page = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE, vmf); swapcache = page; &#125;...... pte = mk_pte(page, vma-&gt;vm_page_prot);...... set_pte_at(vma-&gt;vm_mm, vmf-&gt;address, vmf-&gt;pte, pte); arch_do_swap_page(vma-&gt;vm_mm, vma, vmf-&gt;address, pte, vmf-&gt;orig_pte); vmf-&gt;orig_pte = pte;...... swap_free(entry);......&#125; 通过以上步骤，用户态的缺页异常就处理完毕了。物理内存中有了页面，页表也建立好了映射。接下来，用户程序在虚拟内存空间里面可以通过虚拟地址顺利经过页表映射的访问物理页面上的数据了。页表一般都很大，只能存放在内存中。操作系统每次访问内存都要折腾两步，先通过查询页表得到物理地址，然后访问该物理地址读取指令、数据。为了加快映射速度，我们引入了 TLB（Translation Lookaside Buffer），我们经常称为快表，专门用来做地址映射的硬件设备。它不在内存中，可存储的数据比较少，但是比内存要快。所以我们可以想象，TLB 就是页表的 Cache，其中存储了当前最可能被访问到的页表项，其内容是部分页表项的一个副本。有了 TLB 之后，我们先查块表，块表中有映射关系，然后直接转换为物理地址。如果在 TLB 查不到映射关系时，才会到内存中查询页表。 四. 内核态内存映射及缺页异常 和用户态使用malloc()类似，内核态也有相应的内存映射函数：vmalloc()可用于分配不连续物理页（使用伙伴系统），kmem_cache_alloc()和kmem_cache_create()使用slub分配器分配小块内存，而kmalloc()类似于malloc()，在分配大内存的时候会使用伙伴系统，分配小内存则使用slub分配器。分配内存后会转换为虚拟地址，保存在内核页表中进行映射，有需要时直接访问。由于vmalloc()会带来虚拟连续页和物理不连续页的映射，因此一般速度较慢，使用较少，相比而言kmalloc()使用的更为频繁。而kmem_cache_alloc()和kmem_cache_create()会分配更为精准的小内存块用于特定任务，因此也比较常用。 相对于用户态，内核态还有一种特殊的映射：临时映射。内核态高端内存地区为了节省空间会选择临时映射，采用kmap_atomic()实现。如果是 32 位有高端地址的，就需要调用 set_pte 通过内核页表进行临时映射；如果是 64 位没有高端地址的，就调用 page_address，里面会调用 lowmem_page_address。其实低端内存的映射，会直接使用 __va 进行临时映射。 1234567891011121314151617181920212223void *kmap_atomic_prot(struct page *page, pgprot_t prot)&#123;...... if (!PageHighMem(page)) return page_address(page);...... vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx); set_pte(kmap_pte-idx, mk_pte(page, prot));...... return (void *)vaddr;&#125;void *kmap_atomic(struct page *page)&#123; return kmap_atomic_prot(page, kmap_prot);&#125;static __always_inline void *lowmem_page_address(const struct page *page)&#123; return page_to_virt(page);&#125;#define page_to_virt(x) __va(PFN_PHYS(page_to_pfn(x) kmap_atomic ()发现没有页表的时候会直接创建页表进行映射。而 vmalloc ()只分配了内核的虚拟地址。所以访问它的时候，会产生缺页异常。内核态的缺页异常还是会调用 do_page_fault()，最终进入vmalloc_fault()。在这里会实现内核页表项的关联操作，从而完成分配，整体流程和用户态相似。 123456789101112131415161718192021222324252627static noinline int vmalloc_fault(unsigned long address)&#123; unsigned long pgd_paddr; pmd_t *pmd_k; pte_t *pte_k; /* Make sure we are in vmalloc area: */ if (!(address &gt;= VMALLOC_START &amp;&amp; address &lt; VMALLOC_END)) return -1; /* * Synchronize this task&#x27;s top level page-table * with the &#x27;reference&#x27; page table. * * Do _not_ use &quot;current&quot; here. We might be inside * an interrupt in the middle of a task switch.. */ pgd_paddr = read_cr3_pa(); pmd_k = vmalloc_sync_one(__va(pgd_paddr), address); if (!pmd_k) return -1; pte_k = pte_offset_kernel(pmd_k, address); if (!pte_present(*pte_k)) return -1; return 0&#125; 五. 总结 至此，我们分析了内存物理地址和虚拟地址的映射关系，结合前文页的分配和管理，内存部分的主要功能就算是大致分析清楚了，最后引用极客时间中的一幅图作为总结，算是全部知识点的汇总。 代码资料1、brk 2、mmap 3、page_fault"},{"title":"追番页面配置","path":"/2024/04/05/anzhiyu-docs/page/bilibili/","content":"页面配置📦追番页面配置在博客根目录执行 1npm install hexo-bilibili-bangumi --save 在 hexo 配置文件_config.yml中加入以下配置，注意不是主题配置文件，更多配置请参考hexo-bilibili-bangumi 1234567891011121314151617181920212223242526# 追番插件# https://github.com/HCLonely/hexo-bilibili-bangumibangumi: # 追番设置 enable: true source: bili path: vmid: 372204786 title: &quot;追番列表&quot; quote: &quot;生命不息，追番不止！&quot; show: 1 lazyload: false loading: showMyComment: false pagination: false metaColor: color: webp: progress: extraOrder: proxy: host: &quot;代理host&quot; port: &quot;代理端口&quot; extra_options: top_img: false lazyload: enable: false"},{"title":"分类页配置","path":"/2024/04/05/anzhiyu-docs/page/classify/","content":"页面配置📦分类页配置 前往你的 Hexo 博客的根目录 在 Hexo 博客根目录 [blog]下打开终端，输入 1hexo new page categories 你会找到 source/categories/index.md 这个文件 修改这个文件：记得添加 type: &quot;categories&quot; 1234567---title: 分类date: 2022-02-23 17:56:00aside: falsetop_img: falsetype: &quot;categories&quot;---"},{"title":"我的装备页面配置","path":"/2024/04/05/anzhiyu-docs/page/equipment/","content":"页面配置📦我的装备页面配置前往你的 Hexo 博客的根目录在 Hexo 博客根目录 [blog] 下打开终端，输入 1hexo new page equipment 你会找到 source/equipment/index.md 这个文件 修改这个文件： 记得添加 type: &quot;equipment&quot; 12345title: 我的装备date: 2023-06-10 21:33:24type: equipmentaside: falsetop_img: false 添加数据，新建文件source\\_data\\equipment.yml,没有_data文件夹的话也请自己新建。以下是默认格式示例，打开source\\_data\\equipment.yml，输入 12345678910111213141516171819202122232425262728293031323334353637383940414243444546- class_name: 好物 description: 实物装备推荐 tip: 跟 安知鱼 一起享受科技带来的乐趣 top_background: https://bu.dusays.com/2023/07/05/64a4c38842b7a.webp good_things: - title: 生产力 description: 提升自己生产效率的硬件设备 equipment_list: - name: MacBook Pro 2021 16 英寸 specification: M1 Max 64G / 1TB description: 屏幕显示效果好、色彩准确、对比度强、性能强劲、续航优秀。可以用来开发和设计。 image: https://bu.dusays.com/2023/07/05/64a4c3b191e2e.png link: /posts/571d.html - name: iPad 2020 specification: 深空灰 / 128G description: 事事玩得转，买前生产力，买后爱奇艺。 image: https://bu.dusays.com/2023/07/05/64a4c3b191e2e.png link: https://www.apple.com.cn/ipad-10.2/ - name: iPhone 12 mini specification: 绿色 / 128G description: 超瓷晶面板，玻璃背板搭配铝金属边框，曲线优美的圆角设计，mini大小正好一只手就抓住，深得我心，唯一缺点大概就是续航不够。 image: https://bu.dusays.com/2023/07/05/64a4c3ded6319.webp link: https://www.apple.com.cn/iphone-12/specs/ - name: AirPods（第三代） specification: 标准版 description: 第三代对比第二代提升很大，和我一样不喜欢入耳式耳机的可以入，空间音频等功能确实新颖，第一次使用有被惊艳到。 image: https://bu.dusays.com/2023/07/05/64a4c3ded6319.webp link: https://www.apple.com.cn/airpods-3rd-generation/ - title: 出行 description: 用来出行的实物及设备 equipment_list: - name: Apple Watch Series 8 specification: 黑色 description: 始终为我的健康放哨，深夜弹出站立提醒，不过确实有效的提高了我的运动频率，配合apple全家桶还是非常棒的产品，缺点依然是续航。 image: https://bu.dusays.com/2023/07/05/64a4c40ab698a.webp link: https://www.apple.com.cn/apple-watch-series-8/ - name: NATIONAL GEOGRAPHIC双肩包 specification: 黑色 description: 国家地理黑色大包，正好装下16寸 Macbook Pro，并且背起来很舒适，底部自带防雨罩也好用，各种奇怪的小口袋深得我心。 image: https://bu.dusays.com/2023/07/05/64a4c40ab698a.webp link: https://item.jd.com/100011269828.html - name: NATIONAL GEOGRAPHIC学生书包🎒 specification: 红白色 description: 国家地理黑色大包，冰冰🧊同款，颜值在线且实用。 image: https://bu.dusays.com/2023/07/05/64a4c40ab698a.webp link: https://item.jd.com/100005889786.html 主题配置文件中开启menu中关于和我的装备的注释，导航栏我的装备，注意缩进！！！ 12345678910111213141516171819202122menu: # 文章: # 隧道: /archives/ || anzhiyu-icon-box-archive # 分类: /categories/ || anzhiyu-icon-shapes # 标签: /tags/ || anzhiyu-icon-tags # 友链: # 友人帐: /link/ || anzhiyu-icon-link # 朋友圈: /fcircle/ || anzhiyu-icon-artstation # 留言板: /comments/ || anzhiyu-icon-envelope # 我的: # 音乐馆: /music/ || anzhiyu-icon-music # 追番页: /bangumis/ || anzhiyu-icon-bilibili # 相册集: /album/ || anzhiyu-icon-images # 小空调: /air-conditioner/ || anzhiyu-icon-fan 关于: # 关于本人: /about/ || anzhiyu-icon-paper-plane # 闲言碎语: /essay/ || anzhiyu-icon-lightbulb # 随便逛逛: javascript:toRandomPost() || anzhiyu-icon-shoe-prints1 我的装备: /equipment/ || anzhiyu-icon-dice-d20"},{"title":"首页即刻说说页面配置","path":"/2024/04/05/anzhiyu-docs/page/essay/","content":"页面配置📦首页即刻说说页面配置 前往你的 Hexo 博客的根目录 在 Hexo 博客根目录 [blog]下打开终端，输入 1hexo new page essay 你会找到 source/essay/index.md 这个文件 修改这个文件：记得添加 type: &quot;essay&quot; 12345678---title: 即刻短文date: 2020-07-22 22:06:17comments: trueaside: falsetop_img: falsetype: essay--- 新建source/_data/essay.yml，输入以下内容，具体字段不做解释，可以依葫芦画瓢。 essay.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566- title: 即刻短文 subTitle: 咸鱼的日常生活。 tips: 随时随地，分享生活 buttonText: 关于我 buttonLink: /about/ limit: 30 home_essay: true top_background: https://img02.anheyu.com/adminuploads/1/2022/08/21/630249e2df20f.jpg essay_list: - content: 安知鱼主题指南 date: 2023/09/09 video: - https://player.bilibili.com/player.html?aid=226886152&amp;bvid=BV1Ch41137tR&amp;cid=1081639816&amp;p=1&amp;autoplay=0 - content: 支持了Accesskey快捷键，可以直接按下shift + ?组合键以查看快捷键选项。 date: 2023/07/01 video: - https://cdn.jsdelivr.net/npm/anzhiyu-blog-static@1.0.0/video/%E9%A3%8E%E8%BD%A6%E6%A0%B7%E5%BC%8F%E6%95%88%E6%9E%9C%E9%A2%84%E8%A7%88.mp4 image: - https://img02.anheyu.com/adminuploads/1/2023/07/01/64a033cb2c21e.webp!blogimg address: 长沙 from: 安知鱼 link: /posts/e140.html - content: 音乐支持了参数设置自定义歌单 date: 2023/01/02 link: https://blog.anheyu.com/music/?id=7269231710&amp;server=tencent - content: 关于页的打赏仿了b站的充电功能，使用svg绘图➕一些动画参数移动，应该不会被b站警告吧😜，另外文章也支持了顶部随机b站同款春秋冬banner。 date: 2022/12/18 - content: React中不能直接修改state的一个重要原因是在性能优化时的prueComponment会进行浅层比较会认为是用一个对象且不能进入队列中批量更新 date: 2022/12/10 - content: 好耶，马上就可以放假回家了！好想家里的好吃的😋！才不是想捏妹妹的脸了 date: 2022/12/06 - content: 全局音乐的动画也处理好了, nice! date: 2022/11/13 - content: 把页脚, 首页顶部全都魔改到本地了, 方便后续魔改, 音乐也改成胶囊的样式了, 其实还是想让胶囊可拖拽, 不可点击改变歌词位置的, 但是弄了半天都没弄好就放弃了 date: 2022/11/13 - content: 朋友圈船新版本终于写完了, 耶✌️ date: 2022/11/05 link: https://blog.anheyu.com/album/ - content: 终于把相册集搞定了, 耶✌️, 瀑布流在滑动滚动条一个视口范围上下100的情况执行一次, 到底部停止监听让性能高了好多，再也不会布局混乱🤪了 date: 2022/10/25 link: https://blog.anheyu.com/album/ - content: 搜索🔍支持缩略图显示啦（默认获取文章内容的第一张图片） date: 2022/10/23 08:00:00 from: 安知鱼 - content: 遇见彩虹🌈吃定彩虹 date: 2022/10/23 10:00:00 image: - https://bu.dusays.com/2023/04/09/64329399e285d.webp - https://bu.dusays.com/2023/04/09/64329399aa3bc.webp - https://bu.dusays.com/2023/04/09/6432939996dd7.webp - content: ThreeJs API真多丫 date: 2022/10/19 - content: 妹妹强制要求我买走了她的两幅画 -¥30 date: 2022/10/02 image: - https://bu.dusays.com/2023/04/09/643293997b92b.jpeg - content: 歌曲推荐 date: 2022/09/25 aplayer: server: tencent id: 001FGQba3i10mw - content: 做了一个噩梦, 梦到从楼顶坠下去了。😷 date: 2022/09/24 - content: JOJO是真的好看！ date: 2022/09/21 link: https://www.bilibili.com/bangumi/play/ss39431?spm_id_from=333.337.0.0 【主题版本1.6.7以上支持】其中video对于bilibili进行的额外的支持，视频为player.bilibili.com的将显示为bilibili视频，并且可以在链接后面拼接&amp;autoplay=0控制不自动播放 ::: warning 警告主题配置文件中开启menu中关于和闲言碎语的注释，导航栏闲言碎语，注意缩进！！！::: 123456789101112131415161718192021menu: # 文章: # 隧道: /archives/ || icon-box-archive # 分类: /categories/ || icon-shapes # 标签: /tags/ || icon-tags # 友链: # 友人帐: /link/ || icon-link # 朋友圈: /fcircle/ || icon-artstation # 留言板: /comments/ || icon-envelope # 我的: # 音乐馆: /music/ || icon-music # 追番页: /bangumis/ || icon-bilibili1 # 相册集: /album/ || icon-images # 小空调: /air-conditioner/ || icon-fan 关于: # 关于本人: /about/ || icon-zhifeiji 闲言碎语: /essay/ || icon-lightbulb # 随便逛逛: javascript:toRandomPost() || icon-shoe-prints1 ::: danger 注意示例数据中的图片不保证可用性，请自行更换为您自己的图床链接。图床相关知识&#x3D;&gt;我的图床方案:::"},{"title":"朋友圈页面配置","path":"/2024/04/05/anzhiyu-docs/page/fcircle/","content":"页面配置📦朋友圈页面配置在 Hexo 博客根目录 [blog]下打开终端，输入 1hexo new page fcircle 打开[blog]\\source\\fcircle\\index.md,添加一行type: &#39;fcircle&#39;: 12345678---title: 朋友圈date: 2022-11-21 17:06:17comments: falseaside: falsetop_img: falsetype: &quot;fcircle&quot;--- 主题配置文件中开启menu中友链和朋友圈的注释，导航栏朋友圈，注意缩进！！！ 123456789101112131415161718192021menu: # 文章: # 隧道: /archives/ || icon-box-archive # 分类: /categories/ || icon-shapes # 标签: /tags/ || icon-tags 友链: # 友人帐: /link/ || icon-link 朋友圈: /fcircle/ || icon-artstation # 留言板: /comments/ || icon-envelope # 我的: # 音乐馆: /music/ || icon-music # 追番页: /bangumis/ || icon-bilibili1 # 相册集: /album/ || icon-images # 小空调: /air-conditioner/ || icon-fan 关于: # 关于本人: /about/ || icon-zhifeiji 闲言碎语: /essay/ || icon-lightbulb # 随便逛逛: javascript:toRandomPost() || icon-shoe-prints1 主题配置文件中开启friends_vue.enable，自行设置 朋友圈后端地址 和 顶部模块背景，注意缩进！！！ 123456# 朋友圈配置friends_vue: enable: false vue_js: https://npm.elemecdn.com/anzhiyu-theme-static@1.1.2/friends/index.f9a2b8d2.js apiurl: # 朋友圈后端地址 top_background: 参数 备选值&#x2F;类型 解释 enable boolean 【必须】是否启用 vue_js url 【必须】朋友圈前端构建后的 url apiurl string 【必须】朋友圈后端 url top_background url 【可选】朋友圈顶部背景图 以下是本站配置 12345friends_vue: enable: true vue_js: https://npm.elemecdn.com/anzhiyu-theme-static@1.1.2/friends/index.f9a2b8d2.js apiurl: https://friends.anheyu.com/ # 朋友圈后端地址 top_background: https://img02.anheyu.com/adminuploads/1/2022/08/21/630249e2df20f.jpg 其中vue_js参数，可以将https://npm.elemecdn.com/anzhiyu-theme-static@1.1.2/friends/index.f9a2b8d2.js下载下来后将其中的friends.anheyu.com替换为您的后端 url 然后上传至您的存储端以url的形式使用。 原项目地址：hexo-circle-of-friends-front 第二种办法也可以自行下载项目后，修改代码中的 url 变量路径friends.anheyu.com为你自己的，然后执行npm run build构建后将dist文件夹中的js上传至您的存储端使用 ::: tip注意朋友圈后端爬取需使用common2，否则无法爬取到您的友链数据。::: 主题配置文件中开启menu中友链和朋友圈的注释，注意缩进！！！ 123456789101112131415161718192021menu: # 文章: # 隧道: /archives/ || icon-box-archive # 分类: /categories/ || icon-shapes # 标签: /tags/ || icon-tags 友链: # 友人帐: /link/ || icon-link 朋友圈: /fcircle/ || icon-artstation # 留言板: /comments/ || icon-envelope 我的: 音乐馆: /music/ || icon-music # 追番页: /bangumis/ || icon-bilibili1 # 相册集: /album/ || icon-images # 小空调: /air-conditioner/ || icon-fan 关于: # 关于本人: /about/ || icon-zhifeiji 闲言碎语: /essay/ || icon-lightbulb # 随便逛逛: javascript:toRandomPost() || icon-shoe-prints1"},{"title":"文章页的基本认识","path":"/2024/04/05/anzhiyu-docs/page/front-matter/","content":"页面配置📦🧱 Front-matter 的基本认识Front-matter 是 markdown 文件最上方以 --- 分隔的区域，用于指定个别档案的变数。其中又分为两种 markdown 里 Page Front-matter 用于页面配置 Post Front-matter 用于文章页配置 ::: tip如果标注可选的参数，可根据自己需要添加，不用全部都写在 markdown 里::: :::code-group [Page Front-matter]123456789101112131415title:date:updated:type:comments:description:keywords:top_img:mathjax:katex:aside:aplayer:highlight_shrink:type:top_single_background: [Post Front-matter]1234567891011121314151617181920212223242526title:date:updated:tags:categories:keywords:description:top_img:comments:cover:toc:toc_number:toc_style_simple:copyright:copyright_author:copyright_author_href:copyright_url:copyright_info:mathjax:katex:aplayer:highlight_shrink:aside:swiper_index: 1top_group_index: 1background: &quot;#fff&quot; ::: Page Front-matter 写法 解释 title 【必需】页面标题 date 【必需】页面创建日期 type 【必需】标签、分类、关于、音乐馆、友情链接、相册、相册详情、朋友圈、即刻页面需要配置 updated 【可选】页面更新日期 description 【可选】页面描述 keywords 【可选】页面关键字 comments 【可选】显示页面评论模块(默认 true) top_img 【可选】页面顶部图片 mathjax 【可选】显示 mathjax(当设置 mathjax 的 per_page: false 时，才需要配置，默认 false) katex 【可选】显示 katex(当设置 katex 的 per_page: false 时，才需要配置，默认 false) aside 【可选】显示侧边栏 (默认 true) aplayer 【可选】在需要的页面加载 aplayer 的 js 和 css,请参考文章下面的音乐 配置 highlight_shrink 【可选】配置代码框是否展开(true&#x2F;false)(默认为设置中 highlight_shrink 的配置) top_single_background 【可选】部分页面的顶部模块背景图片 Post Front-matter 写法 解释 title 【必需】文章标题 date 【必需】文章创建日期 updated 【可选】文章更新日期 tags 【可选】文章标签 categories 【可选】文章分类 keywords 【可选】文章关键字 description 【可选】文章描述 top_img 【可选】文章顶部图片 cover 【可选】文章缩略图(如果没有设置 top_img,文章页顶部将显示缩略图，可设为 false&#x2F;图片地址&#x2F;留空) comments 【可选】显示文章评论模块(默认 true) toc 【可选】显示文章 TOC(默认为设置中 toc 的 enable 配置) toc_number 【可选】显示 toc_number(默认为设置中 toc 的 number 配置) toc_style_simple 【可选】显示 toc 简洁模式 copyright 【可选】显示文章版权模块(默认为设置中 post_copyright 的 enable 配置) copyright_author 【可选】文章版权模块的文章作者 copyright_author_href 【可选】文章版权模块的文章作者链接 copyright_url 【可选】文章版权模块的文章链接链接 copyright_info 【可选】文章版权模块的版权声明文字 mathjax 【可选】显示 mathjax(当设置 mathjax 的 per_page: false 时，才需要配置，默认 false) katex 【可选】显示 katex(当设置 katex 的 per_page: false 时，才需要配置，默认 false) aplayer 【可选】在需要的页面加载 aplayer 的 js 和 css,请参考文章下面的音乐 配置 highlight_shrink 【可选】配置代码框是否展开(true&#x2F;false)(默认为设置中 highlight_shrink 的配置) aside 【可选】显示侧边栏 (默认 true) swiper_index 【可选】首页轮播图配置 index 索引，数字越小越靠前 top_group_index 【可选】首页右侧卡片组配置, 数字越小越靠前 ai 【可选】文章ai摘要 main_color 【可选】文章主色，必须是16进制颜色且有6位，不可缩减，例如#ffffff 不可写成#fff 首页轮播图配置: swiper_index, 数字越小越靠前 首页卡片配置: top_group_index, 数字越小越靠前 page 中top_single_background, 可配置部分页面的顶部背景图片 ::: tip只需要在你的文章顶部的Front-matter配置这swiper_index和top_group_index两个字段即可显示轮播图和推荐卡片:::"},{"title":"友情链接配置","path":"/2024/04/05/anzhiyu-docs/page/links/","content":"页面配置📦友情链接配置 前往你的 Hexo 博客的根目录 在 Hexo 博客根目录 [blog]下打开终端，输入 1hexo new page link 你会找到 source/link/index.md 这个文件 修改这个文件：记得添加 type: &quot;link&quot; 12345---title: linkdate: 2020-12-01 22:19:45type: &quot;link&quot;--- 新建文件source\\_data\\link.yml,没有_data文件夹的话也请自己新建。以下是默认友链格式示例(自己写的教程，夹带点私货不过分吧，嘻嘻)。打开[blog]\\source\\_data\\link.yml，输入： 123456789101112131415161718192021222324252627282930313233343536- class_name: 框架 flink_style: flexcard hundredSuffix: &quot;&quot; link_list: - name: Hexo link: https://hexo.io/zh-tw/ avatar: https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg descr: 快速、简单且强大的网站框架 - name: anzhiyu主题 link: https://blog.anheyu.com/ avatar: https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg descr: 生活明朗，万物可爱 siteshot: https://npm.elemecdn.com/anzhiyu-theme-static@1.1.6/img/blog.anheyu.com.jpg- class_name: 推荐博客 flink_style: telescopic hundredSuffix: &quot;&quot; link_list: - name: 安知鱼 link: https://blog.anheyu.com/ avatar: https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg descr: 生活明朗，万物可爱 siteshot: https://npm.elemecdn.com/anzhiyu-theme-static@1.1.6/img/blog.anheyu.com.jpg color: vip tag: 技术- class_name: 小伙伴 class_desc: 那些人，那些事 flink_style: anzhiyu hundredSuffix: &quot;&quot; link_list: - name: 安知鱼 link: https://blog.anheyu.com/ avatar: https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg descr: 生活明朗，万物可爱 recommend: true 参数 解释 class_name 【必填】友链分类名 class_desc 【可选】友链分类描述 flink_style 【必填】flexcard或者anzhiyu或者telescopic hundredSuffix 【可选】解决共同进步板块头像质量问题，配置后共同进步板块的头像会添加该后缀（请确保你的图片加上 hundredSuffix 的配置后依然可以访问）。 例如:hundredSuffix: &quot;!w120&quot; link_list 【必须】友链列表 link_list.name 【必须】友链名称 link_list.link 【必须】友链链接 link_list.avatar 【必须】友链头像 link_list.descr 【必须】友链描述 link_list.siteshot 【可选】flink_style 为 flexcard 或 telescopic 时友链的站点图片 link_list.recommend 【可选】快捷选项，等于color:&quot;&quot; + tag: &quot;荐&quot; link_list.tag 【可选】左上角的 tag，为当前友链打上标签 例如:”推荐” link_list.color 【可选】tag 的十六进制背景颜色例如: “#646cff”，提供了两个快捷颜色选项分别是vip和speed 主题配置文件中开启menu中友链和友人帐的注释，注意缩进！！！ 123456789101112131415161718192021menu: # 文章: # 隧道: /archives/ || icon-box-archive # 分类: /categories/ || icon-shapes # 标签: /tags/ || icon-tags 友链: 友人帐: /link/ || icon-link # 朋友圈: /fcircle/ || icon-artstation # 留言板: /comments/ || icon-envelope # 我的: # 音乐馆: /music/ || icon-music # 追番页: /bangumis/ || icon-bilibili1 # 相册集: /album/ || icon-images # 小空调: /air-conditioner/ || icon-fan 关于: # 关于本人: /about/ || icon-zhifeiji 闲言碎语: /essay/ || icon-lightbulb # 随便逛逛: javascript:toRandomPost() || icon-shoe-prints1 与数百博主共同进步在主题配置文件中_config.anzhiyu.yml中配置 123456# 友情链接顶部相关配置linkPageTop: enable: true title: 与数百名博主无限进步 # 添加博主友链的评论自定义格式 addFriendPlaceholder: &quot;昵称（请勿包含博客等字样）： 网站地址（要求博客地址，请勿提交个人主页）： 头像图片url（请提供尽可能清晰的图片，我会上传到我自己的图床）： 描述： 站点截图（可选）： &quot; 建议超过 30 以上的友链数开启，友链数目不够会导致头像无法铺满。"},{"title":"留言板页面配置","path":"/2024/04/05/anzhiyu-docs/page/message/","content":"页面配置📦留言板页面配置在博客根目录执行 1npm install hexo-butterfly-envelope --save 在站点配置文件_config.yml中添加以下内容配置，更多配置请查看信笺样式留言板 12345678910111213141516171819202122#envelope_comment#seehttps://akilar.top/posts/e2d3c450/envelope_comment: enable: true #控制开关 custom_pic: cover: https://npm.elemecdn.com/hexo-butterfly-envelope/lib/violet.jpg #信笺头部图片 line: https://npm.elemecdn.com/hexo-butterfly-envelope/lib/line.png #信笺底部图片 beforeimg: https://npm.elemecdn.com/hexo-butterfly-envelope/lib/before.png # 信封前半部分 afterimg: https://npm.elemecdn.com/hexo-butterfly-envelope/lib/after.png # 信封后半部分 message: #信笺正文，多行文本，写法如下 - 有什么想问的？ - 有什么想说的？ - 有什么想吐槽的？ - 哪怕是有什么想吃的，都可以告诉我哦~ bottom: 自动书记人偶竭诚为您服务！ #仅支持单行文本 height: #1024px，信封划出的高度 path: #【可选】comments 的路径名称。默认为 comments，生成的页面为 comments/index.html front_matter: #【可选】comments页面的 front_matter 配置 title: 留言板 comments: true top_img: false type: envelope"},{"title":"音乐馆页配置","path":"/2024/04/05/anzhiyu-docs/page/music/","content":"页面配置📦音乐馆页配置 前往你的 Hexo 博客的根目录 在 Hexo 博客根目录 [blog]下打开终端，输入 1hexo new page music 你会找到 source/music/index.md 这个文件 修改这个文件：记得添加 type: &quot;music&quot; 123456789---title: 音乐馆date: 2021-04-24 21:41:30type: musicaplayer: truetop_img: falsecomments: falseaside: false--- 新建 source/json/music.json，此 json 为切换歌单按钮的歌单数据。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128[ &#123; &quot;name&quot;: &quot;青花瓷&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.4/青花瓷/青花瓷.mp3&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000002eFUFm2XYZ7z_2.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.4/青花瓷/青花瓷.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;稻香&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.1/周杰伦/稻香/稻香.mp3&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000002Neh8l0uciQZ_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.1/周杰伦/稻香/稻香.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;晴天&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.2/晴天/晴天.mp3&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000000MkMni19ClKG_3.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.2/晴天/晴天.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;七里香&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.2/七里香/七里香.mp3&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000003DFRzD192KKD_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.2/七里香/七里香.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;花海&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music-jay@1.0.1/花海/花海.flac&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000002Neh8l0uciQZ_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music-jay@1.0.1/花海/花海.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;反方向的钟&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music-jay@1.0.1/反方向的钟/反方向的钟.flac&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000000f01724fd7TH_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music-jay@1.0.1/反方向的钟/反方向的钟.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;兰亭序&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.1/周杰伦/兰亭序/兰亭序.mp3&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000002Neh8l0uciQZ_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.1/周杰伦/兰亭序/兰亭序.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;说好的辛福呢&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.2/说好的辛福呢/说好的辛福呢.mp3&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000002Neh8l0uciQZ_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.2/说好的辛福呢/说好的幸福呢.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;等你下课 (with 杨瑞代)&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.1/周杰伦/等你下课/等你下课.mp3&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000003bSL0v4bpKAx_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.1/周杰伦/等你下课/等你下课.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;我落泪情绪零碎&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.2/我落泪情绪零碎/我落泪情绪零碎.mp3&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000000bviBl4FjTpO_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.2/我落泪情绪零碎/我落泪情绪零碎.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;听妈妈的话&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.2/听妈妈的话/听妈妈的话.mp3&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000002jLGWe16Tf1H_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.2/听妈妈的话/听妈妈的话.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;明明就&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music-jay@1.0.1/明明就/明明就.flac&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000003Ow85E3pnoqi_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music-jay@1.0.1/明明就/明明就.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;我是如此相信&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music-jay@1.0.1/我是如此相信/我是如此相信.flac&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000001hGx1Z0so1YX_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music-jay@1.0.1/我是如此相信/我是如此相信.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;发如雪&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.3/发如雪/发如雪.mp3&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M0000024bjiL2aocxT_3.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.3/发如雪/发如雪.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;以父之名&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.3/以父之名/以父之名.mp3&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000000MkMni19ClKG_3.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.3/以父之名/以父之名.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;园游会&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.3/园游会/园游会.flac&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000003DFRzD192KKD_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.3/园游会/园游会.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;本草纲目&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.4/本草纲目/本草纲目.mp3&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000002jLGWe16Tf1H_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.4/本草纲目/本草纲目.lrc&quot; &#125;, &#123; &quot;name&quot;: &quot;龙卷风&quot;, &quot;artist&quot;: &quot;周杰伦&quot;, &quot;url&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.4/龙卷风/龙卷风.mp3&quot;, &quot;cover&quot;: &quot;https://y.qq.com/music/photo_new/T002R300x300M000000f01724fd7TH_1.jpg?max_age=2592000&quot;, &quot;lrc&quot;: &quot;https://npm.elemecdn.com/anzhiyu-music@1.0.4/龙卷风/龙卷风.lrc&quot; &#125;] hexo 配置文件_config.yml中添加以下配置，注意不是主题配置文件 12345# APlayer# https://github.com/MoePlayer/hexo-tag-aplayer/blob/master/docs/README-zh_cn.mdaplayer: meting: true asset_inject: false 主题配置文件中开启menu中我的和音乐馆的注释，注意缩进！！！ 123456789101112131415161718192021menu: # 文章: # 隧道: /archives/ || icon-box-archive # 分类: /categories/ || icon-shapes # 标签: /tags/ || icon-tags 友链: 友人帐: /link/ || icon-link # 朋友圈: /fcircle/ || icon-artstation # 留言板: /comments/ || icon-envelope 我的: 音乐馆: /music/ || icon-music # 追番页: /bangumis/ || icon-bilibili1 # 相册集: /album/ || icon-images # 小空调: /air-conditioner/ || icon-fan 关于: # 关于本人: /about/ || icon-zhifeiji 闲言碎语: /essay/ || icon-lightbulb # 随便逛逛: javascript:toRandomPost() || icon-shoe-prints1 如何修改默认歌单? 将menu中音乐馆的路径修改为以下格式即可/music/?id=1708664797&amp;server=tencent，支持id和server参数。 id 与 server 的填写请参考MetingJS"},{"title":"标签页配置","path":"/2024/04/05/anzhiyu-docs/page/tags/","content":"页面配置📦标签页配置 前往你的 Hexo 博客的根目录 在 Hexo 博客根目录 [blog]下打开终端，输入 1hexo new page tags 你会找到 source/tags/index.md 这个文件 修改这个文件：记得添加 type: &quot;tags&quot; 1234567---title: 标签date: 2021-04-06 12:01:51type: &quot;tags&quot;comments: falsetop_img: false--- 参数 解释 type 【必须】页面类型，必须为 tags comments 【可选】是否显示评论 top_img 【可选】是否显示顶部图 orderby 【可选】排序方式 ：random&#x2F;name&#x2F;length order 【可选】排序次序： 1, asc for ascending; -1, desc for descending"},{"title":"进阶配置","path":"/2024/04/05/anzhiyu-docs/advanced/index/","content":"进阶配置 🚀评论开启评论需要在 comments-use 中填写你需要的评论。 支持双评论显示，只需要配置两个评论（第一个为默认显示） 12345678910comments: # Up to two comments system, the first will be shown as default # Choose: Valine/Waline/Twikoo/ use: Twikoo,Waline # Twikoo/Waline text: true # Display the comment name next to the button # lazyload: The comment system will be load when comment element enters the browser&#x27;s viewport. # If you set it to true, the comment count will be invalid lazyload: false count: true # Display comment count in post&#x27;s top_img card_post_count: false # Display comment count in Home Page 参数 解释 use 使用的评论（请注意，最多支持两个，如果不需要请留空） text 是否显示评论服务商的名字 lazyload 是否为评论开启 lazyload，开启后，只有滚动到评论位置时才会加载评论所需要的资源（开启 lazyload 后，评论数将不显示） count 是否在文章顶部显示评论数 card_post_count 是否在首页文章卡片显示评论数 TwikooTwikoo 是一个简洁、安全、无后端的静态网站评论系统，基于腾讯云开发。 具体如何配置评论，请查看 Twikoo 文档 你只需要把获取到的 环境 ID (envId) 填写到配置上去就行 修改 主题配置文件 1234567# Twikoo# https://github.com/imaegoo/twikootwikoo: envId: region: visitor: false option: 参数 解释 envId 环境 ID region 环境地域，默认为 ap-shanghai，如果您的环境地域不是上海，需传此参数 visitor 是否显示文章阅读数 option 可选配置 card_post_count 是否在首页文章卡片显示评论数 开启 visitor 后，文章页的访问人数将改为 Twikoo 提供，而不是 不蒜子 Valine遵循 Valine 的指示去配置你的 LeanCloud 应用。以及查看相应的配置说明。 然后修改 主题配置文件: 12345678valine: appId: # leancloud application app id appKey: # leancloud application app key avatar: monsterid # gravatar style https://valine.js.org/#/avatar serverURLs: # This configuration is suitable for domestic custom domain name users, overseas version will be automatically detected (no need to manually fill in) bg: # valine background visitor: false option: 开启 visitor 后，文章页的访问人数将改为 Valine 提供，而不是 不蒜子 Valine 于 v1.4.5 开始支持自定义表情，如果你需要自行配置，请在 emojiCDN 配置表情 CDN。 同时在 Hexo 工作目录下的 source&#x2F;_data&#x2F;创建一个 json 文件 valine.json,等同于 Valine 需要配置的 emojiMaps，valine.json 配置方式可参考如下 valine.json 1234567891011121314151617181920212223&#123; &quot;tv_doge&quot;: &quot;6ea59c827c414b4a2955fe79e0f6fd3dcd515e24.png&quot;, &quot;tv_亲亲&quot;: &quot;a8111ad55953ef5e3be3327ef94eb4a39d535d06.png&quot;, &quot;tv_偷笑&quot;: &quot;bb690d4107620f1c15cff29509db529a73aee261.png&quot;, &quot;tv_再见&quot;: &quot;180129b8ea851044ce71caf55cc8ce44bd4a4fc8.png&quot;, &quot;tv_冷漠&quot;: &quot;b9cbc755c2b3ee43be07ca13de84e5b699a3f101.png&quot;, &quot;tv_发怒&quot;: &quot;34ba3cd204d5b05fec70ce08fa9fa0dd612409ff.png&quot;, &quot;tv_发财&quot;: &quot;34db290afd2963723c6eb3c4560667db7253a21a.png&quot;, &quot;tv_可爱&quot;: &quot;9e55fd9b500ac4b96613539f1ce2f9499e314ed9.png&quot;, &quot;tv_吐血&quot;: &quot;09dd16a7aa59b77baa1155d47484409624470c77.png&quot;, &quot;tv_呆&quot;: &quot;fe1179ebaa191569b0d31cecafe7a2cd1c951c9d.png&quot;, &quot;tv_呕吐&quot;: &quot;9f996894a39e282ccf5e66856af49483f81870f3.png&quot;, &quot;tv_困&quot;: &quot;241ee304e44c0af029adceb294399391e4737ef2.png&quot;, &quot;tv_坏笑&quot;: &quot;1f0b87f731a671079842116e0991c91c2c88645a.png&quot;, &quot;tv_大佬&quot;: &quot;093c1e2c490161aca397afc45573c877cdead616.png&quot;, &quot;tv_大哭&quot;: &quot;23269aeb35f99daee28dda129676f6e9ea87934f.png&quot;, &quot;tv_委屈&quot;: &quot;d04dba7b5465779e9755d2ab6f0a897b9b33bb77.png&quot;, &quot;tv_害羞&quot;: &quot;a37683fb5642fa3ddfc7f4e5525fd13e42a2bdb1.png&quot;, &quot;tv_尴尬&quot;: &quot;7cfa62dafc59798a3d3fb262d421eeeff166cfa4.png&quot;, &quot;tv_微笑&quot;: &quot;70dc5c7b56f93eb61bddba11e28fb1d18fddcd4c.png&quot;, &quot;tv_思考&quot;: &quot;90cf159733e558137ed20aa04d09964436f618a1.png&quot;, &quot;tv_惊吓&quot;: &quot;0d15c7e2ee58e935adc6a7193ee042388adc22af.png&quot;&#125; WalineWaline - 一款从 Valine 衍生的带后端评论系统。可以将 Waline 等价成 With backend Valine。 具体配置可参考 waline 文档 然后修改 主题配置文件: 12345waline: serverURL: # Waline server address url bg: # waline background pageview: false option: 开启 pageview 后，文章页的访问人数将改为 Waline 提供，而不是 不蒜子 在线聊天通用配置这些工具都提供了一个按钮可以打开&#x2F;关闭聊天窗口。主题也提供了一个集合主题特色的按钮来替换这些工具本身的按钮，这个聊天按钮将会出现在右下角里。你只需要把 chat_btn 打开就行。 修改 主题配置文件 123# Chat Button [recommend]# It will create a button in the bottom right corner of website, and hide the origin buttonchat_btn: true 为了不影响访客的体验，主题提供一个 chat_hide_show 配置设为 true 后，使用工具提供的按钮时，只有向上滚动才会显示聊天按钮，向下滚动时会隐藏按钮。 修改 主题配置文件 12# The origin chat button is displayed when scrolling up, and the button is hidden when scrolling downchat_hide_show: true 如果使用工具自带的聊天按钮，按钮位置可能会遮挡右下角图标，请配置 rightside-bottom 调正右下角图标位置 chatra配置 chatra,需要知道 Public key 打开 chatra 并注册账号。你可以在 Preferences 中找到 Public key 修改 主题配置文件 12345# chatra# https://chatra.io/chatra: enable: true id: xxxxxxxx chatra 的样式你可以 Chat Widget 自行配置 tidio配置 tidio,需要知道 Public key 打开 tidio 并注册账号。你可以在 Preferences &gt; Developer 中找到 Public key 修改 主题配置文件 12345# tidio# https://www.tidio.com/tidio: enable: true public_key: XXXX tidio的样式你可以Channels自行配置 daovoice打开 daovoice 和注册帐号 找到你的 app id 修改 主题配置文件 12345# daovoice# http://daovoice.io/daovoice: enable: true app_id: xxxxx 可在聊天设置里配置聊天按钮等样式 crisp打开 crisp 并注册帐号 找到需要的网站 ID 12345# crisp# https://crisp.chat/en/crisp: enable: false website_id: xxxxxxxx messengermessenger 为 Facebook 旗下的聊天服务 具体操作请查看 Facebook 洽谈附加程式 - Messenger 平台 1234messenger: enable: false pageID: xxxxx lang: zh_TW # Language en_US/zh_CN/zh_TW and so on ai 摘要需主题版本大于 1.1.6 版本 修改主题配置文件，其中key和Referer 为 tianli gpt 的key和Referer，可在 https://afdian.net/item/886a79d4db6711eda42a52540025c377 购买 key，购买完成后请立即在前端面板绑定key，以防止被盗用。 适用于AnZhiYu主题项目的Key，前端管理面板 https://summary.tianli0.top/ 每个key限制请求字数50000字，如果是已经请求过的内容不会再次消耗key使用时需要绑定key。 虚拟物品一经发出不支持退款。 关于续费：续费和绑定流程相同，绑定成功后会自动充值到原有的key上，也就是说无需更改网页中的key参数，直接绑定即可。 格式发送，返回的 token 即你的剩余字数 1234567891011# 文章顶部ai摘要post_head_ai_description: enable: true gptName: AnZhiYu mode: local # 默认模式 可选值: tianli/local switchBtn: false # 可以配置是否显示切换按钮 以切换tianli/local btnLink: https://afdian.net/item/886a79d4db6711eda42a52540025c377 randomNum: 3 # 按钮最大的随机次数，也就是一篇文章最大随机出来几种 basicWordCount: 1000 # 最低获取字符数, 最小1000, 最大1999 key: xxxx Referer: https://xx.xx/ 配置完成以后在文章的Front-matter配置ai: true使用 tianli gpt 需将 mode 改为tianli 然后在需要 ai 摘要的文章的Front-matter配置ai: true 如果使用local,需要按照以下方式配置 123456---title: AnZhiYu主题快速开始ai: - 本教程介绍了如何在博客中安装基于Hexo主题的安知鱼主题，并提供了安装、应用主题、修改配置文件、本地启动等详细步骤及技术支持方式。教程的内容针对最新的主题版本进行更新，如果你是旧版本教程会有出入。 - 本文真不错--- mode 对比 tianli 具有完备的训练后端功能，可优秀的完成文章的摘要，方便的快速生成对应摘要 local 本地，需自行在文章顶部添加 ai 摘要，内容自行决定，书写较为麻烦。 控制台信息关于控制台信息，主题不提供修改配置，但是可以在主题配置文件中进行关闭。 123# 控制台打印信息console: enable: true 控制台打印时间相关信息可以配置页脚内容来修改。 12345678910111213footer: owner: enable: true since: 2020 custom_text: copyright: false # Copyright of theme and framework runtime: enable: true launch_time: 04/01/2021 00:00:00 # 网站上线时间 work_img: https://npm.elemecdn.com/anzhiyu-blog@2.0.4/img/badge/安知鱼-上班摸鱼中.svg work_description: 距离月入25k也就还差一个大佬带我~ offduty_img: https://npm.elemecdn.com/anzhiyu-blog@2.0.4/img/badge/安知鱼-下班啦.svg offduty_description: 下班了就该开开心心的玩耍，嘿嘿~ 如果实在有强迫症也可以自行修改内容themes/anzhiyu/layout/includes/anzhiyu/log-js.pug。 如何配置首页顶部右侧不使用轮播图 将主题配置文件中home_top.swiper.enable 修改为 false 即可. 标签卖萌主题配置文件中 12345# 标签卖萌diytitle: enable: true leaveTitle: w(ﾟДﾟ)w 不要走！再看看嘛！ backTitle: ♪(^∇^*)欢迎肥来！ 配置关于页与文章页底部打赏二维码在主题配置文件中 1234567891011# Sponsor/rewardreward: enable: true coinAudio: https://npm.elemecdn.com/akilar-candyassets@1.0.36/audio/aowu.m4a QR_code: - img: https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png link: text: wechat - img: https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png link: text: alipay 主题如何获取文章主色调支持在文章Post Front-matter配置主色main_color，例如 12345---title: AnZhiYu主题安装文档（一）date: 2023-03-26 18:55:44main_color: &quot;#c0e0e0&quot;--- 主题将会优先获取文章Post Front-matter颜色，如果未设置才会使用cdn/api/both 1234567# 主色调相关配置mainTone: enable: true # true or false 文章是否启用获取图片主色调 mode: both # cdn/api/both cdn模式为图片url+imageAve参数获取主色调，api模式为请求API获取主色调，both模式会先请求cdn参数，无法获取的情况下将请求API获取 # 项目地址：https://github.com/anzhiyu-c/img2color-go api: https://color.anheyu.com/api?img= # mode为api时可填写 cover_change: true # 整篇文章跟随cover修改主色调 参数 解释 enable 是否开启主色调 mode cdn&#x2F;api&#x2F;both cdn模式为图片url+imageAve参数获取主色调，api模式为请求API获取主色调，both模式会先请求cdn参数，无法获取的情况下将请求API获取（目前cdn模式支持多吉云和腾讯数据万象两种方式） api mode为api时，需返回16进制颜色，可以参考部署项目: https://github.com/anzhiyu-c/img2color-go cover_change 整篇文章跟随cover修改主色调 双栏如果你需要像我一样首页双栏，修改主题配置文件_config.anzhiyu.yml（主题版本需要 1.1.1 以及以上） 12# 双栏文章article_double_row: true 首页顶部 3 大分类配置首页技能点轮播下的分类，可通过配置主题配置文件home_top 1hexo new page categories 1234---title: Hello Worldcategories: 前端开发--- 只需要在创建分类以后在对应的文章中添加上对应的分类，配置 path 即可，注意一定要对应。 左下角音乐球歌单配置很简单，只需要修改主题配置文件中nav_music即可. 其中id与server配置可以参考MetingJS option default description id require song id &#x2F; playlist id &#x2F; album id &#x2F; search keyword server require music platform: netease, tencent, kugou, xiami, baidu 12345678# 左下角音乐配置项# https://github.com/metowolf/MetingJSnav_music: enable: true console_widescreen_music: false # 宽屏状态控制台显示音乐而不是标签 enable为true 控制台依然会显示 id: 8152976493 server: netease all_playlist: https://y.qq.com/n/ryqq/playlist/8802438608 评论匿名邮箱目前只支持Twikoo 1234# 评论匿名邮箱visitorMail: enable: true mail: visitor@anheyu.com 文章底部工具12345678# ptool 文章底部工具ptool: enable: true share_mobile: true share_weibo: true share_copyurl: true categories: false # 是否显示分类 mode: /wechat/ # 运营模式与责任，不配置不显示 首页技能点配置[blog]代表你的博客根目录。示例数据中的图片链接为本人图床，请自行上传至您自己的图床，（不保证永久可用性）。 主题配置文件，关闭peoplecanvas.enable 1234# 首页随便逛逛people模式 而非技能点模式，关闭后为技能点模式需要配置creativity.ymlpeoplecanvas: enable: false img: https://upload-bbs.miyoushe.com/upload/2023/09/03/125766904/ee23df8517f3c3e3efc4145658269c06_5714860933110284659.png 创建[blog]/source/_data/creativity.yml，输入以下内容 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950- class_name: 开启创造力 creativity_list: - name: Java color: &quot;#fff&quot; icon: https://bu.dusays.com/2023/04/09/643293b1184e9.jpg - name: Docker color: &quot;#57b6e6&quot; icon: https://bu.dusays.com/2023/04/09/643293b0f0abe.png - name: Photoshop color: &quot;#4082c3&quot; icon: https://bu.dusays.com/2022/12/15/639aa3a5c240e.png - name: Node color: &quot;#333&quot; icon: https://npm.elemecdn.com/anzhiyu-blog@2.1.1/img/svg/node-logo.svg - name: Webpack color: &quot;#2e3a41&quot; icon: https://bu.dusays.com/2023/04/09/643293b68026c.png - name: Pinia color: &quot;#fff&quot; icon: https://npm.elemecdn.com/anzhiyu-blog@2.0.8/img/svg/pinia-logo.svg - name: Python color: &quot;#fff&quot; icon: https://bu.dusays.com/2023/04/09/643293b1230f7.png - name: Vite color: &quot;#937df7&quot; icon: https://npm.elemecdn.com/anzhiyu-blog@2.0.8/img/svg/vite-logo.svg - name: Flutter color: &quot;#4499e4&quot; icon: https://bu.dusays.com/2023/04/09/643293b1055c2.png - name: Vue color: &quot;#b8f0ae&quot; icon: https://bu.dusays.com/2023/04/09/643293b6788bd.png - name: React color: &quot;#222&quot; icon: data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9Ii0xMS41IC0xMC4yMzE3NCAyMyAyMC40NjM0OCI+CiAgPHRpdGxlPlJlYWN0IExvZ288L3RpdGxlPgogIDxjaXJjbGUgY3g9IjAiIGN5PSIwIiByPSIyLjA1IiBmaWxsPSIjNjFkYWZiIi8+CiAgPGcgc3Ryb2tlPSIjNjFkYWZiIiBzdHJva2Utd2lkdGg9IjEiIGZpbGw9Im5vbmUiPgogICAgPGVsbGlwc2Ugcng9IjExIiByeT0iNC4yIi8+CiAgICA8ZWxsaXBzZSByeD0iMTEiIHJ5PSI0LjIiIHRyYW5zZm9ybT0icm90YXRlKDYwKSIvPgogICAgPGVsbGlwc2Ugcng9IjExIiByeT0iNC4yIiB0cmFuc2Zvcm09InJvdGF0ZSgxMjApIi8+CiAgPC9nPgo8L3N2Zz4K - name: CSS3 color: &quot;#2c51db&quot; icon: https://bu.dusays.com/2022/12/15/639aa3a5c251e.png - name: JS color: &quot;#f7cb4f&quot; icon: https://bu.dusays.com/2023/04/09/643293b121f02.png - name: HTML color: &quot;#e9572b&quot; icon: https://bu.dusays.com/2022/12/15/639aa3a5c241c.png - name: Git color: &quot;#df5b40&quot; icon: https://bu.dusays.com/2023/04/09/643293b10ccdd.webp - name: Apifox color: &quot;#e65164&quot; icon: https://bu.dusays.com/2022/11/19/6378d6458c6b6.png 此时可以看到首页顶部已经有技能点的轮播了。 文字部分在主题配置文件中home_top配置项修改 配置 nav 顶栏左侧应用列表1234567891011121314151617181920# nav左侧菜单nav: enable: true menu: - title: 网页 item: - name: 个人主页 link: https://index.anheyu.com/ icon: https://bu.dusays.com/2023/08/13/64d8c2748ef34.jpg - name: 博客 link: https://blog.anheyu.com/ icon: https://bu.dusays.com/2023/07/23/64bc72c75319d.png - name: 安知鱼图床 link: https://image.anheyu.com/ icon: https://bu.dusays.com/2023/08/13/64d8c2653332e.ico - title: 项目 item: - name: 安知鱼图床 link: https://image.anheyu.com/ icon: https://bu.dusays.com/2023/08/13/64d8c2653332e.ico 字数统计要为 AnZhiYu 配上字数统计特性, 你需要如下几个步骤: 打开 hexo 工作目录 123npm install hexo-wordcount --save或者yarn add hexo-wordcount 修改 主题配置文件: 12345wordcount: enable: true post_wordcount: true min2read: true total_wordcount: true 网站验证如果需要搜索引擎收录网站，可能需要登录对应搜索引擎的管理平台进行提交。各自的验证码可从各自管理平台拿到 修改 主题配置文件 12345site_verification: # - name: google_site_verification # content: xxxxxx # - name: baidu_site_verification # content: xxxxxxx 搜索系统:::tabs &#x3D;&#x3D; algolia 记得运行 hexo clean 使用 hexo-algoliasearch，请记得配置 fields 参数的 title, permalink 和 content 你需要安装 hexo-algoliasearch. 根据它的说明文档去做相应的配置。 修改 主题配置文件 1234algolia_search: enable: true hits: per_page: 6 hexo 配置文件 _config.yml 添加以下内容 注意algolia搜索出来URL前缀为hexo配置文件_config.yml中的url配置。 12345678910111213141516# algolia搜索: https://github.com/LouisBarranqueiro/hexo-algoliasearchalgolia: appId: &quot;xxxx&quot; apiKey: &quot;xxxx&quot; adminApiKey: &quot;xxxx&quot; chunkSize: 5000 indexName: &quot;hexo&quot; fields: - content:strip:truncate,0,200 - excerpt:strip - gallery - permalink - photos - slug - tags - title &#x3D;&#x3D; 本地搜索 记得运行 hexo clean 你需要安装 hexo-generator-search，根据它的文档去做相应配置 修改 主题配置文件 1234local_search: enable: false preload: false CDN: 参数 解释 enable 是否开启本地搜索 preload 预加载，开启后，进入网页后会自动加载搜索文件。关闭时，只有点击搜索按钮后，才会加载搜索文件 CDN 搜索文件的 CDN 地址（默认使用的本地链接） &#x3D;&#x3D; DocSearchDocSearch 是另一款由 algolia 提供的搜索服务，具体申请和使用请查看 DocSearch 文档 123456docsearch: enable: false appId: apiKey: indexName: option: 参数 解释 enable 【必须】是否开启 docsearch appId 【必须】你的 Algolia 应用 ID apiKey 【必须】你的 Algolia 搜索 API key indexName 【必须】你的 Algolia index name option 【可选】其余的 docsearch 配置 具体配置可查这里 ::: 数学公式:::tabs &#x3D;&#x3D; hexo-renderer-pandoc（推荐） 详情请见 https://www.yyyzyyyz.cn/posts/654d8712aff4/ &#x3D;&#x3D; Mathjax 不要在标题里使用 mathjax 语法，toc 目录不一定能正确显示 mathjax，可能显示 mathjax 代码 建议使用 KaTex 获得更好的效果，下文有介绍！ 修改 主题配置文件: 12345mathjax: enable: true # true 表示每一页都加载mathjax.js # false 需要时加载，须在使用的Markdown Front-matter 加上 mathjax: true per_page: false 如果 per_page 设为 true,则每一页都会加载 Mathjax 服务。设为 false，则需要在文章 Front-matter 添加 mathjax: true,对应的文章才会加载 Mathjax 服务。 然后你需要修改一下默认的 markdown 渲染引擎来实现 MathJax 的效果。 查看: hexo-renderer-kramed 以下操作在你 hexo 博客的目录下 (不是 Anzhiyu 的目录): 安装插件 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 配置 hexo 根目录的配置文件 12345678kramed: gfm: true pedantic: false sanitize: false tables: true breaks: true smartLists: true smartypants: true &#x3D;&#x3D; KaTeX不要在标题里使用 KaTeX 语法，toc 目录不能正确显示 KaTeX。 首先禁用 MathJax（如果你配置过 MathJax 的话），然后修改你的主题配置文件以便加载 katex.min.css: 123456katex: enable: true # true 表示每一页都加载katex.js # false 需要时加载，须在使用的Markdown Front-matter 加上 katex: true per_page: false hide_scrollbar: true 你不需要添加 katex.min.js 来渲染数学方程。相应的你需要卸载你之前的 hexo 的 markdown 渲染器，然后安装其它插件。 因为 KaTeX 更快更轻量，因此没有 MathJax 的功能多（比如右键菜单）。为那些使用 MathJax 的用户，主题也内置了 katex 的 复制 功能。 &#x3D;&#x3D; hexo-renderer-markdown-it卸载掉 marked 插件，安装 hexo-renderer-markdown-it 123456npm un hexo-renderer-marked --save # 如果有安装这个的话，卸载npm un hexo-renderer-kramed --save # 如果有安装这个的话，卸载npm i hexo-renderer-markdown-it --save # 需要安装这个渲染插件npm install katex @renbaoshuo/markdown-it-katex #需要安装这个katex插件 在 hexo 的根目录的_config.yml 中配置 123markdown: plugins: - &quot;@renbaoshuo/markdown-it-katex&quot; 如需配置其它参数，请参考 katex 官网 &#x3D;&#x3D; hexo-renderer-markdown-it-plus 注意，此方法生成的 katex 没有斜体 卸载掉 marked 插件，然后安装新的hexo-renderer-markdown-it-plus: 1234567891011# 替换 `hexo-renderer-kramed` 或者 `hexo-renderer-marked` 等hexo的markdown渲染器# 你可以在你的package.json里找到hexo的markdwon渲染器，并将其卸载npm un hexo-renderer-marked --save# ornpm un hexo-renderer-kramed --save# 然后安装 `hexo-renderer-markdown-it-plus`npm i @upupming/hexo-renderer-markdown-it-plus --save 注意到 hexo-renderer-markdown-it-plus 已经无人持续维护, 所以我们使用 @upupming/hexo-renderer-markdown-it-plus。 这份 fork 的代码使用了 @neilsustc/markdown-it-katex 同时它也是 VSCode 的插件 Markdown All in One 所使用的, 所以我们可以获得最新的 KaTex 功能例如 \\tag{}。 你还可以通过 @neilsustc/markdown-it-katex 控制 KaTeX 的设置，所有可配置的选项参见 https://katex.org/docs/options.html。 比如你想要禁用掉 KaTeX 在命令行上输出的宂长的警告信息，你可以在根目录的 _config.yml 中使用下面的配置将 strict 设置为 false 1234567markdown_it_plus: plugins: - plugin: name: &#x27;@neilsustc/markdown-it-katex&#x27; enable: true options: strict: false 当然，你还可以利用这个特性来定义一些自己常用的 macros。 ::: 分享只能选择一个分享服务商 :::tabs&#x3D;&#x3D; sharejs如果你不知道 sharejs，看看它的説明。 修改 主题配置文件 123sharejs: enable: true sites: facebook,twitter,wechat,weibo,qq #想要显示的内容 &#x3D;&#x3D; addtoany可以到 addtoany 查看使用説明 123addtoany: enable: true item: facebook,twitter,wechat,sina_weibo,facebook_messenger,email,copy_link ::: 欢迎语配置在每次进入首页时根据当前时间弹出欢迎语，为true时必须配置list。 1234567891011121314151617181920212223242526272829# 欢迎语配置greetingBox: enable: true #开启后必须配置下面的list对应的时间段，不然会出现小白条 default: 晚上好👋 list: - greeting: 晚安😴 startTime: 0 endTime: 5 - greeting: 早上好鸭👋, 祝你一天好心情！ startTime: 6 endTime: 9 - greeting: 上午好👋, 状态很好，鼓励一下～ startTime: 10 endTime: 10 - greeting: 11点多啦, 在坚持一下就吃饭啦～ startTime: 11 endTime: 11 - greeting: 午安👋, 宝贝 startTime: 12 endTime: 14 - greeting: 🌈充实的一天辛苦啦！ startTime: 14 endTime: 18 - greeting: 19点喽, 奖励一顿丰盛的大餐吧🍔。 startTime: 19 endTime: 19 - greeting: 晚上好👋, 在属于自己的时间好好放松😌~ startTime: 20 endTime: 24 博客快捷键12345# 快捷键配置shortcutKey: enable: true delay: 100 # 所有键位延时触发而不是立即触发（包括shift，以解决和浏览器键位冲突问题） shiftDelay: 200 # shift按下延时多久开启 无障碍优化123# 无障碍优化（在首页按下「shift + ?」以查看效果）accesskey: enable: true 友情链接顶部相关配置123456# 友情链接顶部相关配置linkPageTop: enable: true title: 与数百名博主无限进步 # 添加博主友链的评论自定义格式 addFriendPlaceholder: &quot;昵称（请勿包含博客等字样）： 网站地址（要求博客地址，请勿提交个人主页）： 头像图片url（请提供尽可能清晰的图片，我会上传到我自己的图床）： 描述： 站点截图（可选）： &quot; 缩略图后缀该配置用于优化缩略图，archive&#x2F;tag&#x2F;category 页面单独开启后缀，可以优化图像质量问题，注意开启后一定要保证你的图片本身可以支持链接后➕pageThumbnailSuffix能够被访问。 12# 缩略图后缀 archive/tag/category 页面单独开启后缀pageThumbnailSuffix: &quot;!page_thumbnail&quot; 隐私协议弹窗该弹窗一个窗口会话只会弹出一次。 1234# 隐私协议弹窗agreementPopup: enable: true url: /privacy 定制化的右键菜单开启rightClickMenu即可。 123# 右键菜单rightClickMenu: enable: true 动效控制12345# 动效dynamicEffect: postTopWave: true # 文章顶部波浪效果 postTopRollZoomInfo: true # 文章顶部滚动时缩放 pageCommentsRollZoom: true # 非文章页面评论滚动时缩放显示（仅仅Twikoo生效） 51A统计可以配置 51A统计 与灵雀 配置后可在关于页面显示统计信息。 12345# 51a统计配置LA: enable: true ck: Jp8wwGQpp21utaFQ LingQueMonitorID: Jp8ztDRrxmTf7LDj ::: warning 警告注意一定要开启数据挂件功能!!!::: 页面卡片顶部气泡升起效果123# 页面卡片顶部气泡升起效果bubble: enable: false 深色模式粒子效果canvas123# 深色模式粒子效果canvasuniverse: enable: true"},{"title":"基础配置","path":"/2024/04/05/anzhiyu-docs/global/base/","content":"站点配置 😋语言修改站点配置文件 _config.yml，不是主题配置文件。 默认语言是 en 主题支持三种语言 default(en) zh-CN (简体中文) zh-TW (繁体中文) 网站资料修改网站各种资料，例如标题、副标题和邮箱等个人资料，请修改博客根目录的_config.yml 。 导航配置修改 主题配置文件 123456789101112131415161718192021menu: 文章: 隧道: /archives/ || anzhiyu-icon-box-archive 分类: /categories/ || anzhiyu-icon-shapes 标签: /tags/ || anzhiyu-icon-tags 友链: 友人帐: /link/ || anzhiyu-icon-link 朋友圈: /fcircle/ || anzhiyu-icon-artstation 留言板: /comments/ || anzhiyu-icon-envelope 我的: 音乐馆: /music/ || anzhiyu-icon-music 追番页: /bangumis/ || anzhiyu-icon-bilibili 相册集: /album/ || anzhiyu-icon-images 小空调: /air-conditioner/ || anzhiyu-icon-fan 关于: 关于本人: /about/ || anzhiyu-icon-paper-plane 闲言碎语: /essay/ || anzhiyu-icon-lightbulb 随便逛逛: javascript:toRandomPost() || anzhiyu-icon-shoe-prints1 必须是 /xxx/，后面||分开，然后写图标名。 如果不希望显示图标，图标名可不写。 ::: tip注意： 导航的文字可自行更改::: 例如： 123456789101112131415161718menu: Article: Tunnel: /archives/ || icon-box-archive Classification: /categories/ || icon-shapes Label: /tags/ || icon-tags Friend: Friends account: /link/ || icon-link Moments: /fcircle/ || icon-artstation Message board: /comments/ || icon-envelope My: Music Hall: /music/ || icon-music Chasing: /bangumis/ || icon-bilibili1 Album Set: /album/ || icon-images Conditioning: /air-conditioner/ || icon-fan About: /about/ || icon-zhifeiji 导航栏设置12345678910111213141516# nav相关配置nav: enable: false travelling: false clock: true menu: - title: 网页 item: - name: 博客 link: https://blog.anheyu.com/ icon: /img/favicon.png - title: 项目 item: - name: 安知鱼图床 link: https://image.anheyu.com/ icon: https://image.anheyu.com/favicon.ico 参数 解释 enable 是否启用 nav 左侧项目按钮，仅控制左侧项目按钮 travelling 是否启用 nav 开往按钮 clock 是否启用 nav 左侧和风天气 menu nav 左侧项目按钮内的菜单 menu.title nav 左侧项目按钮内的菜单标题 menu.item nav 左侧项目按钮内的菜单项 menu.item.name nav 左侧项目按钮内的菜单项标题 menu.item.link nav 左侧项目按钮内的菜单项链接 menu.item.icon nav 左侧项目按钮内的菜单项图标 代码块配置相关信息 代码块中的所有功能只适用于 Hexo 自带的代码渲染 如果使用第三方的渲染器，不一定会有效 代码高亮主题AnZhiYu 支持 6 种代码高亮样式： darker pale night light ocean mac mac light darker pale night light ocean mac mac light 修改 主题配置文件 1highlight_theme: light 代码复制主题支持代码复制功能，修改 主题配置文件 1highlight_copy: true 代码框展开&#x2F;关闭在默认情况下，代码框自动展开，可设置是否所有代码框都关闭状态，点击&gt;可展开代码 true 全部代码框不展开，需点击&gt;打开 false 代码框展开，有&gt;点击按钮 none 不显示&gt;按钮 修改 主题配置文件 1highlight_shrink: true #代码框不展开，需点击 &#x27;&gt;&#x27; 打开 相关信息 你也可以在 post&#x2F;page 页对应的 markdown 文件 front-matter 添加 highlight_shrink 来独立配置。 当主题配置文件中的 highlight_shrink 设为 true 时，可在 front-matter 添加 highlight_shrink: false 来单独配置文章展开代码框。 当主题配置文件中的 highlight_shrink 设为 false 时，可在 front-matter 添加 highlight_shrink: true 来单独配置文章收缩代码框。 代码换行在默认情况下，Hexo 在编译的时候不会实现代码自动换行。如果你不希望在代码块的区域里有横向滚动条的话，那么你可以考虑开启这个功能。 修改 主题配置文件 1code_word_wrap: true 如果你是使用 highlight 渲染，需要找到你站点的 Hexo 配置文件_config.yml，将 line_number 改成 false: 12345highlight: enable: true line_number: false # &lt;- 改这里 auto_detect: false tab_replace: 如果你是使用 prismjs 渲染，需要找到你站点的 Hexo 配置文件_config.yml，将 line_number 改成 false: 12345prismjs: enable: false preprocess: true line_number: false # &lt;- 改这里 tab_replace: &quot;&quot; 代码高度限制可配置代码高度限制，超出的部分会隐藏，并显示展开按钮，默认 330，可配置为 false。 1highlight_height_limit: false # unit: px 注意： 单位是 px，直接添加数字，如 200 实际限制高度为 highlight_height_limit + 30 px ，多增加 30px 限制，目的是避免代码高度只超出 highlight_height_limit 一点时，出现展开按钮，展开没内容。 不适用于隐藏后的代码块（ css 设置 display: none） 图标配置AnZhiYu 支持 阿里图标 (需配置自己的图标)，与 font-awesome v6 图标(需开启fontawesome)，使用阿里图标需配置主题配置文件中icon.ali_iconfont_js字段，默认内置部分图标，修改主题配置文件，视频教程: 安知鱼主题社交图标配置 1234icons: ali_iconfont_js: # 阿里图标symbol 引用链接，主题会进行加载 symbol 引用 fontawesome: false #是否启用fontawesome6图标 fontawesome_animation_css: #fontawesome_animation 如果有就会加载，示例值：https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@1.0.17/lib/assets/font-awesome-animation.min.css 内置阿里图标库：https://www.iconfont.cn/collections/detail?cid=44481 使用方法，将图标库中的图标名复制，然后加上前缀anzhiyu-icon-即可，比如github图标，则为anzhiyu-icon-github。 社交图标书写格式 名称：url || icon名称 123456# social settings (社交图标设置)# formal:# name: link || iconsocial: # Github: https://github.com/anzhiyu-c || anzhiyu-icon-github # BiliBili: https://space.bilibili.com/372204786 || anzhiyu-icon-bilibili 如需 hover 动画生效需配置fontawesome_animation_css 1234icons: ali_iconfont_js: # 阿里图标symbol 引用链接，主题会进行加载 symbol 引用 fontawesome: false #是否启用fontawesome6图标 fontawesome_animation_css: #fontawesome_animation 如果有就会加载，示例值：https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@1.0.17/lib/assets/font-awesome-animation.min.css 社交图标配置完以后个人卡片会出现图标内容，图标左边的内容为你的站点信息，在hexo的配置文件_config.yml中配置author和subtitle字段 个人卡片个人卡片hover后的显示描述，该描述请在侧边栏配置中的aside.card_author.description中修改，支持html显示。 卡片顶部的状态配置： 1234567891011121314# 作者卡片 状态author_status: enable: true # 可以是任何图片，建议放表情包或者emoji图片，效果都很好，[表情包速查](https://emotion.xiaokang.me/) statusImg: &quot;https://bu.dusays.com/2023/08/24/64e6ce9c507bb.png&quot; skills: - 🤖️ 数码科技爱好者 - 🔍 分享与热心帮助 - 🏠 智能家居小能手 - 🔨 设计开发一条龙 - 🤝 专修交互与设计 - 🏃 脚踏实地行动派 - 🧱 团队小组发动机 - 💢 壮汉人狠话不多 顶部图相关信息 如果不要显示顶部图，可直接配置 disable_top_img: true ::: tip 提示顶部图的获取顺序，如果都没有配置，则不显示顶部图。::: 页面顶部图的获取顺序： 各自配置的 top_img &gt; 配置文件的 default_top_img 文章页顶部图的获取顺序： 各自配置的 top_img &gt; cover &gt; 配置文件的 default_top_img 配置中的值： 配置 解释 index_img 主页的 top_img，示例值: index_img: “background: url(https://img02.anheyu.com/xxx) top &#x2F; cover no-repeat” default_top_img 默认的 top_img，当页面的 top_img 没有配置时，会显示 default_top_img archive_img 归档页面的 top_img tag_img tag 子页面 的 默认 top_img tag_per_img tag 子页面的 top_img，可配置每个 tag 的 top_img category_img category 子页面 的 默认 top_img category_per_img category 子页面的 top_img，可配置每个 category 的 top_img 其它页面 （tags&#x2F;categories&#x2F;自建页面）和 文章页 的 top_img ，请到对应的 md 页面设置 front-matter 中的 top_img 以上所有的 top_img 可配置以下值 配置的值 效果 留空 显示默认的 top_img（如有），否则显示默认的顔色（文章页 top_img 留空的话，会显示 cover 的值） img 链接 图片的链接，显示所配置的图片 顔色(HEX 值 - #0000FFRGB 值 - rgb(0,0,255)顔色单词 - orange渐变色 - linear-gradient( 135deg, #E2B0FF 10%, #9F44D3 100%) 对应的顔色 transparent 透明 false 不显示 top_img 并不推荐为每个 tag 和每个 category 都配置不同的顶部图，因为配置太多会拖慢生成速度 1234567tag_per_img： aplayer: https://xxxxxx.png android: ddddddd.pngcategory_per_img： 随想: hdhdh.png 推荐: ddjdjdjd.png 文章置顶【推荐】hexo-generator-index 从 2.0.0 开始，已经支持文章置顶功能。你可以直接在文章的 front-matter 区域里添加 sticky: 1 属性来把这篇文章置顶。数值越大，置顶的优先级越大。 文章封面文章的 markdown 文档上,在 Front-matter 添加 cover ,并填上要显示的图片地址。 如果不配置 cover,可以设置显示默认的 cover。 如果不想在首页显示 cover, 可以设置为 false。 文章封面的获取顺序 Front-matter 的 cover &gt; 配置文件的 default_cover &gt; false 修改 主题配置文件 12345678910cover: # 是否显示文章封面 index_enable: true aside_enable: true archives_enable: true # 封面显示的位置 # 三个值可配置 left , right , both position: both # 当没有设置cover时，默认的封面显示 default_cover: 参数 解释 index_enable 主页是否显示文章封面图 aside_enable 侧栏是否显示文章封面图 archives_enable 归档页面是否显示文章封面图 position 主页卡片文章封面的显示位置- left：全部显示在左边- right：全部显示在右边- both：封面位置以左右左右轮流显示 default_cover 默认的 cover, 可配置图片链接&#x2F;顔色&#x2F;渐变色等 当配置多张图片时,会随机选择一张作为 cover.此时写法应为 1234default_cover: - https://file.crazywong.com/gh/jerryc127/CDN@latest/cover/default_bg.png - https://file.crazywong.com/gh/jerryc127/CDN@latest/cover/default_bg2.png - https://file.crazywong.com/gh/jerryc127/CDN@latest/cover/default_bg3.png 文章 meta 显示这个选项是用来显示文章的相关信息的。 修改 主题配置文件 12345678910111213post_meta: page: date_type: both # created or updated or both 主页文章日期是创建日或者更新日或都显示 date_format: relative # date/relative 显示日期还是相对日期 categories: true # true or false 主页是否显示分类 tags: true # true or false 主页是否显示标签 label: true # true or false 显示描述性文字 post: date_type: both # created or updated or both 文章页日期是创建日或者更新日或都显示 date_format: relative # date/relative 显示日期还是相对日期 categories: true # true or false 文章页是否显示分类 tags: true # true or false 文章页是否显示标签 label: true # true or false 显示描述性文字 主页文章页 date_format配置时间显示明确时间还是相对时间 文章版权为你的博客文章展示文章版权和许可协议。 修改 主题配置文件 123456post_copyright: enable: true decode: false author_href: license: CC BY-NC-SA 4.0 license_url: https://creativecommons.org/licenses/by-nc-sa/4.0/ 由于 Hexo 4.1 开始，默认对网址进行解码，以至于如果是中文网址，会被解码，可设置 decode: true 来显示中文网址。 如果有文章（例如：转载文章）不需要显示版权，可以在文章 Front-matter 单独设置 1copyright: false 支持对单独文章设置版权信息，可以在文章 Front-matter 单独设置 1234copyright_author: xxxxcopyright_author_href: https://xxxxxx.comcopyright_url: https://xxxxxx.comcopyright_info: 此文章版权归xxxxx所有，如有转载，请注明来自原作者 文章打赏在你每篇文章的结尾，可以添加打赏按钮。相关二维码可以自行配置。 对于没有提供二维码的，可配置一张软件的 icon 图片，然后在 link 上添加相应的打赏链接。用户点击图片就会跳转到链接去。 link 可以不写，会默认为图片的链接。coinAudio 为投币的音频。 修改 主题配置文件 12345678910reward: enable: true coinAudio: https://npm.elemecdn.com/akilar-candyassets@1.0.36/audio/aowu.m4a QR_code: - img: https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png link: text: wechat - img: https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png link: text: alipay TOC在文章页，会有一个目录，用于显示 TOC。修改 主题配置文件 123456toc: post: true page: true number: true expand: false style_simple: false # for post 属性 解释 post 文章页是否显示 TOC page 普通页面是否显示 TOC number 是否显示章节数 expand 是否展开 TOC style_simple 简洁模式（侧边栏只显示 TOC, 只对文章页有效 ） 为特定的文章配置 在你的文章 md 文件的头部，加入 toc_number 和 toc，并配置 true 或者 false 即可。 主题会优先判断文章 Markdown 的 Front-matter 是否有配置，如有，则以 Front-matter 的配置为准。否则，以主题配置文件中的配置为准 相关文章警告 当文章封面设置为 false 时，或者没有获取到封面配置，相关文章背景将会显示主题色。 相关文章推荐的原理是根据文章 tags 的比重来推荐 修改 主题配置文件 1234related_post: enable: true limit: 6 # 显示推荐文章数目 date_type: created # or created or updated 文章日期显示创建日或者更新日 文章过期提醒可设置是否显示文章过期提醒，以更新时间为基准。 12345678# Displays outdated notice for a post (文章过期提醒)noticeOutdate: enable: true style: flat # style: simple/flat limit_day: 365 # When will it be shown position: top # position: top/bottom message_prev: It has been message_next: days since the last update, the content of the article may be outdated. limit_day： 距离更新时间多少天才显示文章过期提醒 message_prev： 天数之前的文字 message_next：天数之后的文字 文章编辑按钮在文章标题旁边显示一个编辑按钮，点击会跳转到对应的链接去。 1234567# Post edit# Easily browse and edit blog source code online.post_edit: enable: false # url: https://github.com/user-name/repo-name/edit/branch-name/subdirectory-name/ # For example: https://github.com/jerryc127/butterfly.js.org/edit/main/source/ url: 文章分页按钮::: warning 警告 当文章封面设置为 false 时，或者没有获取到封面配置，分页背景将会显示主题色。 可设置分页的逻辑，也可以关闭分页显示 ::: 123456# post_pagination (分页)# value: 1 || 2 || false# 1: The &#x27;next post&#x27; will link to old post# 2: The &#x27;next post&#x27; will link to new post# false: disable paginationpost_pagination: false 参数 解释 post_pagination: false 关闭分页按钮 post_pagination: 1 下一篇显示的是旧文章 post_pagination: 2 下一篇显示的是新文章 中控台主题配置文件中，默认为true 12# 中控台centerConsole: true 中控台在小屏幕状态下只会显示功能按键 当屏幕足够大（&gt;1200px）的时候，就能够显示兴趣点、最近评论、时间归档、功能按键、音乐等内容"},{"title":"额外配置","path":"/2024/04/05/anzhiyu-docs/global/extra/","content":"额外配置 🚀Footer 设置since 是一个来展示你站点起始时间的选项。它位于页面的最底部。 1234567891011121314151617181920212223242526272829303132# Footer Settings# --------------------------------------footer: owner: enable: true since: 2020 custom_text: runtime: enable: true launch_time: 04/01/2021 00:00:00 # 网站上线时间 work_img: https://npm.elemecdn.com/anzhiyu-blog@2.0.4/img/badge/安知鱼-上班摸鱼中.svg work_description: 距离月入25k也就还差一个大佬带我~ offduty_img: https://npm.elemecdn.com/anzhiyu-blog@2.0.4/img/badge/安知鱼-下班啦.svg offduty_description: 下班了就该开开心心的玩耍，嘿嘿~ # 徽标部分配置项 https://shields.io/ # https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&amp;logo=jsDelivr bdageitem: - link: https://hexo.io/ #徽标指向网站链接 shields: https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Frame-Hexo.svg #徽标API message: 博客框架为Hexo_v5.4.0 #徽标提示语 - link: https://blog.anheyu.com/ shields: https://pan.anheyu.com/d/anzhiyu/svg/Theme-AnZhiYu-2E67D3.svg message: 本站使用AnZhiYu主题 - link: https://www.dogecloud.com/ shields: https://npm.elemecdn.com/anzhiyu-blog@2.2.0/img/badge/CDN-多吉云-3693F3.svg message: 本站使用多吉云为静态资源提供CDN加速 - link: https://github.com/ shields: https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Source-Github.svg message: 本站项目由Github托管 - link: http://creativecommons.org/licenses/by-nc-sa/4.0/ shields: https://npm.elemecdn.com/anzhiyu-blog@2.2.0/img/badge/Copyright-BY-NC-SA.svg message: 本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可 参数 解释 owner 页脚网站所有者@2020-当前年份 owner.enable 页脚网站所有者是否启用 owner.since 页脚年份，控制台中打印的运行时间也来自这里 runtime 运行时间 runtime.enable 运行时间是否启用 runtime.launch_time 网站上线时间 runtime.work_img 页脚上班时间的徽标 runtime.work_description 页脚上班时间的 title 描述 runtime.offduty_img 页脚下班时间的徽标 runtime.offduty_description 页脚下班时间的 title 描述 bdageitem 徽标配置项 bdageitem.link 徽标配置链接 bdageitem.shields 徽标配置徽标 bdageitem.message 徽标配置徽标 title 侧边栏设置可自行决定哪个项目需要显示，可决定位置，也可以设置不显示侧边栏。 修改 主题配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# aside (侧边栏)# --------------------------------------aside: enable: true hide: false button: true mobile: true # display on mobile position: right # left or right display: # 控制对应详情页面是否显示侧边栏 archive: true tag: true category: true card_author: enable: true description: &lt;div style=&quot;line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);&quot;&gt;这有关于&lt;b style=&quot;color:#fff&quot;&gt;产品、设计、开发&lt;/b&gt;相关的问题和看法，还有&lt;b style=&quot;color:#fff&quot;&gt;文章翻译&lt;/b&gt;和&lt;b style=&quot;color:#fff&quot;&gt;分享&lt;/b&gt;。&lt;/div&gt;&lt;div style=&quot;line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);&quot;&gt;相信你可以在这里找到对你有用的&lt;b style=&quot;color:#fff&quot;&gt;知识&lt;/b&gt;和&lt;b style=&quot;color:#fff&quot;&gt;教程&lt;/b&gt;。&lt;/div&gt; name_link: /about card_announcement: enable: false content: 欢迎来看我的博客鸭~ card_weixin: enable: true face: https://img02.anheyu.com/adminuploads/1/2022/09/11/631ddb7c9b250.png backFace: https://img02.anheyu.com/adminuploads/1/2022/09/11/631ddeb0900b7.png card_recent_post: enable: true limit: 5 # if set 0 will show all sort: date # date or updated sort_order: # Don&#x27;t modify the setting unless you know how it works card_categories: enable: false limit: 8 # if set 0 will show all expand: none # none/true/false sort_order: # Don&#x27;t modify the setting unless you know how it works card_tags: enable: true limit: 40 # if set 0 will show all color: false sort_order: # Don&#x27;t modify the setting unless you know how it works highlightTags: - Hexo - 前端 card_archives: enable: true type: monthly # yearly or monthly format: MMMM YYYY # eg: YYYY年MM月 order: -1 # Sort of order. 1, asc for ascending; -1, desc for descending limit: 8 # if set 0 will show all sort_order: # Don&#x27;t modify the setting unless you know how it works card_webinfo: enable: true post_count: true last_push_date: false sort_order: # Don&#x27;t modify the setting unless you know how it works 其中 card_weixin 可以控制微信公众号的图片，face为翻转前的图片，backFace为翻转后的图片。 访问人数 busuanzi访问 busuanzi 的官方网站查看更多的介绍。 修改 主题配置文件 1234busuanzi: site_uv: true site_pv: true page_pv: true 如果需要修改 busuanzi 的 CDN 链接，可通过 主题配置文件 的 CDN 中的 option 进行修改 123CDN: option: busuanzi: xxxxxxxxx 运行时间网页已运行时间 修改 主题配置文件 12345# Time difference between publish date and now (网页运行时间)# Formal: Month/Day/Year Time or Year/Month/Day Timeruntimeshow: enable: true publish_date: 4/1/2021 00:00:00 最新评论最新评论只会在刷新时才会去读取，并不会实时变化 由于 API 有 访问次数限制，为了避免调用太多，主题默认存取期限为 10 分钟。也就是説，调用后资料会存在 localStorage 里，10 分钟内刷新网站只会去 localStorage 读取资料。 10 分钟期限一过，刷新页面时才会去调取 API 读取新的数据。（ 配置 storage，可自行配置缓存时间） 在侧边栏显示最新评论板块 修改 主题配置文件 1234567# Console - Newest Commentsnewest_comments: enable: true sort_order: # Don&#x27;t modify the setting unless you know how it works limit: 6 storage: 10 # unit: mins, save data to localStorage avatar: true 配置 解释 limit 显示的数量 storage 设置缓存时间，单位 分钟 avatar 是否显示头像 右下角按钮简繁转换简体繁体互换 右下角会有简繁转换按钮。 修改 主题配置文件 123456789101112translate: enable: true # 默认按钮显示文字(网站是简体，应设置为&#x27;default: 繁&#x27;) default: 简 #网站默认语言，1: 繁体中文, 2: 简体中文 defaultEncoding: 1 #延迟时间,若不在前, 要设定延迟翻译时间, 如100表示100ms,默认为0 translateDelay: 0 #当文字是简体时，按钮显示的文字 msgToTraditionalChinese: &quot;繁&quot; #当文字是繁体时，按钮显示的文字 msgToSimplifiedChinese: &quot;简&quot; 简体 繁体 阅读模式阅读模式下会去掉除文章外的内容，避免干扰阅读。 只会出现在文章页面，右下角会有阅读模式按钮。 修改 主题配置文件 1readmode: true 夜间模式右下角会有夜间模式按钮 修改 主题配置文件 12345678910111213# dark modedarkmode: enable: true # Toggle Button to switch dark/light mode button: true # Switch dark/light mode automatically (自动切换 dark mode和 light mode) # autoChangeMode: 1 Following System Settings, if the system doesn&#x27;t support dark mode, it will switch dark mode between 6 pm to 6 am # autoChangeMode: 2 Switch dark mode between 6 pm to 6 am # autoChangeMode: false autoChangeMode: 1 # Set the light mode time. The value is between 0 and 24. If not set, the default value is 6 and 18 start: # 8 end: # 22 参数 解释 button 是否在右下角显示日夜模式切换按钮 autoChangeMode 自动切换的模式 autoChangeMode autoChangeMode: 1 跟随系统而变化，不支持的浏览器&#x2F;系统将按照时间 start 到 end 之间切换为 light modeautoChangeMode: 2 只按照时间 start 到 end 之间切换为 light mode ,其余时间为 dark modeautoChangeMode: false 取消自动切换 start light mode 的开始时间 end light mode 的结束时间 按钮排序1234567# Don&#x27;t modify the following settings unless you know how they work (非必要请不要修改 )# Choose: readmode,translate,darkmode,hideAside,toc,chat,comment# Don&#x27;t repeat 不要重复rightside_item_order: enable: false hide: # readmode,translate,darkmode,hideAside show: # toc,chat,comment 短标签 Tag Plugins::: tip短标签是 Hexo 独有的功能，并不是标准的 Markdown 格式。以下的写法，只适用于 AnZhiYu 主题，用在其它主题上不会有效果，甚至可能会报错。使用前请留意::: ::: warning短标签虽然能为主题带来一些额外的功能和 UI 方面的强化，但是，短标签也有明显的限制，使用时请留意。::: 只需要将标签写在md文件内即可使用，详细写法请阅读 安知鱼主题标签 Tag Plugins 查看 评论弹幕 评论弹幕仅支持Twikoo。 token在你博客的控制台获取，打开浏览器开发者工具，找到本地存储空间，找到键名为twikoo-access-token的内容，将值复制下来填写即可，本方法仅限server部署，其他部署方案请自行查看如何获取token。 1234567891011# 留言弹幕配置comment_barrage_config: enable: true # 同时最多显示弹幕数 maxBarrage: 1 # 弹幕显示间隔时间ms barrageTime: 8000 # token accessToken: &quot;xxxxxx&quot; # 博主邮箱md5值 mailMd5: &quot;xxxxxxxxxx&quot; 分析统计百度统计 登录百度统计的官方网站 找到你百度统计的统计代码 修改 主题配置文件1baidu_analytics: 你的代码 谷歌分析 登录谷歌分析的官方网站 找到你的谷歌分析的跟踪 ID 修改 主题配置文件1google_analytics: 你的代码 # 通常以`UA-`打头 Cloudflare 登录 Cloudflare 分析的官方网站 找到 JavaScript 程式码片段 找到你的 token 修改 主题配置文件123# Cloudflare Analytics# https://www.cloudflare.com/zh-tw/web-analytics/cloudflare_analytics: Microsoft Clarity 登录 Clarity 的官方网站 创建 PROJECT 找到你的 ID 修改 主题配置文件123# Microsoft Clarity# https://clarity.microsoft.com/microsoft_clarity: 广告谷歌广告主题已集成谷歌广告（自动广告） 修改 主题配置文件 123456google_adsense: enable: true auto_ads: true js: https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js client: # 填入个人识别码 enable_page_level_ads: true 手动广告配置主题预留了三个位置可供插入广告，分别为主页文章(每三篇文章出现广告)&#x2F;aside公告之后&#x2F;文章页打赏之后。把html代码填写到对应的位置 修改 主题配置文件 1234ad: index: aside: post: 例如: 1index: &lt;ins class=&quot;adsbygoogle&quot; style=&quot;display:block&quot; data-ad-format=&quot;fluid&quot; data-ad-layout-key=&quot;xxxxxxxxxxxx&quot; data-ad-client=&quot;ca-pub-xxxxxxxxxx&quot; data-ad-slot=&quot;xxxxxxxxxx&quot;&gt;&lt;/ins&gt;&lt;script&gt;(adsbygoogle=window.adsbygoogle||[]).push(&#123;&#125;)&lt;/script&gt; 页面加载动画当进入网页时，因为加载速度的问题，可能会导致 top_img 图片出现断层显示，或者网页加载不全而出现等待时间，开启preloader后，会显示加载动画，等页面加载完，加载动画会消失。 主题支持 pace.js 的加载动画，具体可查看 pace.js 修改 主题配置文件，其中avatar可以自定义加载时的头像 1234567891011# Loading Animation (加载动画)preloader: enable: true # source # 1. fullpage-loading # 2. pace (progress bar) # else all source: 3 # pace theme (see https://codebyzach.github.io/pace/) pace_css_url: avatar: # 自定义头像 图片大图查看模式::: warning如果你并不想为某张图片添加大图查看模式，你可以使用 html 格式引用图片，併为图片添加 no-lightbox class 名。::: 修改 主题配置文件 :::code-group [fancybox]12# fancybox http://fancyapps.com/fancybox/3/fancybox: true [medium_zoom]1medium_zoom: true ::: Pjax当用户点击链接，通过ajax更新页面需要变化的部分，然后使用HTML5的pushState修改浏览器的URL地址。 这样可以不用重复加载相同的资源（css&#x2F;js）， 从而提升网页的加载速度。 12345678# Pjax [Beta]# It may contain bugs and unstable, give feedback when you find the bugs.# https://github.com/MoOx/pjaxpjax: enable: true exclude: - /music/ - /no-pjax/ ::: tip对于一些第三方插件，有些并不支持 pjax 。你可以把网页加入到 exclude 里，这个网页会被 pjax 排除在外。点击该网页会重新加载网站 使用pjax后，一些自己DIY的js可能会无效，跳转页面时需要重新调用，请参考Pjax文档 使用pjax后，一些个别页面加载的js&#x2F;css，将会改为所有页面都加载::: ::: warning 主题默认开启Pjax，大量服务依赖于pjax，关闭可能会造成破坏性问题。主题对pjax做了深度适配，关闭可能会造成严重的后果，比如某些链接无法跳转。 安知鱼主题的的Pjax目前仍有一些问题，请留意 使用谷歌广告可能会报错（例如自动广告）如果你在使用中发现问题，欢迎反馈Bugs ::: Snackbar 弹窗Snackbar 弹窗,根据自己爱好开启 修改 主题配置文件 123456789# Snackbar 弹窗# https://github.com/polonel/SnackBar# position 弹窗位置# 可选 top-left / top-center / top-right / bottom-left / bottom-center / bottom-rightsnackbar: enable: true position: bottom-left bg_light: &#x27;#49b1f5&#x27; #light mode时弹窗背景 bg_dark: &#x27;#2d3035&#x27; #dark mode时弹窗背景 Pangu 如果你跟我一样，每次看到网页上的中文字和英文、数字、符号挤在一块，就会坐立难安，忍不住想在它们之间加个空格。这个外挂正是你在网路世界走跳所需要的东西，它会自动替你在网页中所有的中文字和半形的英文、数字、符号之间插入空白。 修改 主题配置文件 12345# https://github.com/vinta/pangu.js# Insert a space between Chinese character and English character (中英文之间添加空格)pangu: enable: false field: post # site/post field只支持两个参数，post(只在文章页生效)和site(全站生效) PWAPWA是web优化的一种手段，主题做了一定的适配，但是依然需要进行配置。要为AnZhiYu配上 PWA 特性, 你可以进行选择以下两种方式来进行适配，hexo-offline适合初学者，hexo-swpp适合已经对service worker有一定解决问题能力的进阶使用。 hexo-offline 打开 hexo 工作目录 npm install hexo-offline --save 或者 yarn add hexo-offline 在根目录创建 hexo-offline.config.cjs 文件，并增加以下内容。 1234567891011121314151617181920212223242526272829303132333435363738// offline config passed to workbox-build.module.exports = &#123; // 静态文件合集，如果你的站点使用了例如 webp 格式的文件，请将文件类型添加进去。 globPatterns: [&quot;404.html&quot;, &quot;css/index.css&quot;], globDirectory: &quot;public&quot;, swDest: &quot;public/service-worker.js&quot;, maximumFileSizeToCacheInBytes: 10485760, // 缓存的最大文件大小，以字节为单位。 skipWaiting: true, clientsClaim: true, runtimeCaching: [ // 如果你需要加载 CDN 资源，请配置该选项，如果没有，可以不配置。 // CDNs - should be CacheFirst, since they should be used specific versions so should not change &#123; urlPattern: /^https:\\/\\/npm\\.elemecdn\\.com\\/anzhiyu-blog/, // 缓存elmentcdn handler: &quot;CacheFirst&quot;, &#125;, ], manifestTransforms: [ async (manifestEntries, compilation) =&gt; &#123; const timestamp = new Date().toISOString().replace(/[-:.TZ]/g, &quot;&quot;); // 获取当前时间戳 manifestEntries.push( &#123; url: &quot;/&quot;, revision: `index-$&#123;timestamp&#125;`, &#125;, &#123; url: &quot;music/&quot;, revision: `music-$&#123;timestamp&#125;`, &#125;, &#123; url: &quot;about/&quot;, revision: `about-$&#123;timestamp&#125;`, &#125; ); return &#123; manifest: manifestEntries &#125;; &#125;, ],&#125;; 更多内容请查看 hexo-offline 的官方文档 在主题配置文件中开启 pwa 选项。 12345678910111213# PWA# See https://github.com/JLHwung/hexo-offline# ---------------pwa: enable: true startup_image_enable: true manifest: /manifest.json theme_color: var(--anzhiyu-main) mask_icon: /img/siteicon/apple-icon-180.png apple_touch_icon: /img/siteicon/apple-icon-180.png bookmark_icon: /img/siteicon/apple-icon-180.png favicon_32_32: /img/siteicon/32.png favicon_16_16: /img/siteicon/16.png 在创建source/目录中创建manifest.json文件。 12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;name&quot;: &quot;安知鱼`Blog&quot;, &quot;short_name&quot;: &quot;安知鱼&quot;, &quot;theme_color&quot;: &quot;#3b70fc&quot;, &quot;background_color&quot;: &quot;#3b70fc&quot;, &quot;display&quot;: &quot;fullscreen&quot;, &quot;scope&quot;: &quot;/&quot;, &quot;start_url&quot;: &quot;/&quot;, &quot;id&quot;: &quot;/&quot;, &quot;icons&quot;: [ &#123; &quot;src&quot;: &quot;img/siteicon/manifest-icon-192.maskable.png&quot;, &quot;sizes&quot;: &quot;192x192&quot;, &quot;type&quot;: &quot;image/png&quot;, &quot;purpose&quot;: &quot;any&quot; &#125;, &#123; &quot;src&quot;: &quot;img/siteicon/manifest-icon-192.maskable.png&quot;, &quot;sizes&quot;: &quot;192x192&quot;, &quot;type&quot;: &quot;image/png&quot;, &quot;purpose&quot;: &quot;maskable&quot; &#125;, &#123; &quot;src&quot;: &quot;img/siteicon/manifest-icon-512.maskable.png&quot;, &quot;sizes&quot;: &quot;512x512&quot;, &quot;type&quot;: &quot;image/png&quot;, &quot;purpose&quot;: &quot;any&quot; &#125;, &#123; &quot;src&quot;: &quot;img/siteicon/manifest-icon-512.maskable.png&quot;, &quot;sizes&quot;: &quot;512x512&quot;, &quot;type&quot;: &quot;image/png&quot;, &quot;purpose&quot;: &quot;maskable&quot; &#125; ], &quot;splash_pages&quot;: null&#125; 你也可以通过 Web App Manifestopen 快速创建manifest.json。（Web App Manifest 要求至少包含一个 512*512 像素的图标） 可以通过Chrome插件Lighthouse检查 PWA 配置是否生效以及配置是否正确。 打开博客页面 启动Lighthouse插件 (Lighthouse插件要求至少包含一个 512*512 像素的图标) 关于 PWA（渐进式增强 Web 应用）的更多内容请参考 Google Tools for Web Developersopen 生成pwa启动图 安装 pwa-asset-generator，执行以下命令1npm install pwa-asset-generator hexo根目录执行hexo g后执行以下命令1npx pwa-asset-generator ./public/img/512.png ./public/img/siteicon --padding &quot;calc(50vh - 20%) calc(50vw - 40%)&quot; -s false -h true 这条命令会使用themes/source/img/512.png这张图片来生成siteicon到目录/public/img/siteicon中，由于hexo cl会清除/public目录中的文件，所以每次hexo d之前都需要执行，如果不想每次hexo d之前都执行的话，可以将主题中的themes/source/img/512.png图片复制移动到根目录/source/img中，然后将生成后的/public/img/siteicon文件夹复制到根目录/source/img中，这样根目录/source/img中就会一直有siteicon，以后执行hexo g时，也会将siteicon生成到public目录中。 hexo-swpp主题版本大于1.5.2支持，这种加载办法为极其迅速，由sw控制其缓存，实现原理以及配置请查看以下三篇文章 hexo-swpp swpp-backends Swpp Backends 官方文档 配置方法： 安装hexo-swpp插件和swpp-backends，在博客根目录执行 12npm install hexo-swpp --savenpm install swpp-backends --save 在主题配置文件中开启 pwa 选项。 12345678910111213# PWA# See https://github.com/JLHwung/hexo-offline# ---------------pwa: enable: true startup_image_enable: true manifest: /manifest.json theme_color: var(--anzhiyu-main) mask_icon: /img/siteicon/apple-icon-180.png apple_touch_icon: /img/siteicon/apple-icon-180.png bookmark_icon: /img/siteicon/apple-icon-180.png favicon_32_32: /img/siteicon/32.png favicon_16_16: /img/siteicon/16.png 在 hexo 的配置文件中添加如下内容即可启用插件： 12345swpp: # 是否启用插件 enable: true # 是否在发布前自动执行脚本 # auto_exec: true 在创建source/目录中创建manifest.json文件。 12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;name&quot;: &quot;安知鱼`Blog&quot;, &quot;short_name&quot;: &quot;安知鱼&quot;, &quot;theme_color&quot;: &quot;#3b70fc&quot;, &quot;background_color&quot;: &quot;#3b70fc&quot;, &quot;display&quot;: &quot;fullscreen&quot;, &quot;scope&quot;: &quot;/&quot;, &quot;start_url&quot;: &quot;/&quot;, &quot;id&quot;: &quot;/&quot;, &quot;icons&quot;: [ &#123; &quot;src&quot;: &quot;img/siteicon/manifest-icon-192.maskable.png&quot;, &quot;sizes&quot;: &quot;192x192&quot;, &quot;type&quot;: &quot;image/png&quot;, &quot;purpose&quot;: &quot;any&quot; &#125;, &#123; &quot;src&quot;: &quot;img/siteicon/manifest-icon-192.maskable.png&quot;, &quot;sizes&quot;: &quot;192x192&quot;, &quot;type&quot;: &quot;image/png&quot;, &quot;purpose&quot;: &quot;maskable&quot; &#125;, &#123; &quot;src&quot;: &quot;img/siteicon/manifest-icon-512.maskable.png&quot;, &quot;sizes&quot;: &quot;512x512&quot;, &quot;type&quot;: &quot;image/png&quot;, &quot;purpose&quot;: &quot;any&quot; &#125;, &#123; &quot;src&quot;: &quot;img/siteicon/manifest-icon-512.maskable.png&quot;, &quot;sizes&quot;: &quot;512x512&quot;, &quot;type&quot;: &quot;image/png&quot;, &quot;purpose&quot;: &quot;maskable&quot; &#125; ], &quot;splash_pages&quot;: null&#125; 你也可以通过 Web App Manifestopen 快速创建manifest.json。（Web App Manifest 要求至少包含一个 512*512 像素的图标） 可以通过Chrome插件Lighthouse检查 PWA 配置是否生效以及配置是否正确。 打开博客页面启动Lighthouse插件 (Lighthouse插件要求至少包含一个 512*512 像素的图标) 关于 PWA（渐进式增强 Web 应用）的更多内容请参考 Google Tools for Web Developersopen 生成pwa启动图安装pwa-asset-generator 1npm install pwa-asset-generator hexo根目录执行hexo g然后执行 1npx pwa-asset-generator ./public/img/512.png ./public/img/siteicon --padding &quot;calc(50vh - 20%) calc(50vw - 40%)&quot; -s false -h true 这条命令会使用themes/anzhiyu/source/img/512.png这张图片来生成siteicon到目录/public/img/siteicon中，由于hexo cl会清除/public目录中的文件，所以每次hexo d之前都需要执行，如果不想每次hexo d之前都执行的话，可以将主题中的themes/anzhiyu/source/img/512.png图片复制移动到根目录``/source/img中，然后将生成后的/public/img/siteicon文件夹复制到根目录``/source/img中，这样根目录``/source/img中就会一直有siteicon，以后执行hexo g时，也会将siteicon生成到public目录中。 在hexo g以后执行一次hexo swpp命令 如果你的网站使用了 CDN 且启用了 CDN 端缓存，请务必将 CDN 缓存时间调整至最大值，然后每次更新网页内容后手动刷新 CDN 缓存。 因为本插件的更新方案要求update.json更新时，其它所有需要更新的资源均已更新，否则客户端拉取时会误以为拉取到了最新的内容，从而导致部分资源“错过”更新。 简而言之，就是update.json必须与需要缓存的资源共享同样的 CDN 缓存周期，但是目前市面上我知道的 CDN 无法做到这一点，所以只能从下列选项中二选一 把所有资源的 CDN 缓存时间拉满，每次更新网站时刷新 CDN 缓存 CDN 不缓存所有需要在客户端缓存的资源 Netlify 构建后自动刷新 CDN 缓存的教程见：《全自动博客部署方案》 请务必注意 CDN 缓存的问题！！！ 默认主题将只缓存404和index.css如果你需要配置自己的缓存策略，请在博客根目录新建sw-rules.js 请注意⚠️定期或不定期检查hexo swpp 是否存在更新，以及查看更新日志对配置做出最新的更改 ::: warning 主题默认内置了一份sw-rules.js规则，位于themes/anzhiyu/sw-rules.js可以开箱即用，如需自定义缓存规则可以复制该文件至hexo根目录然后修改根目录的sw-rules.js。 ::: 安知鱼主题适配hexo-swpp3.2+，swpp-backends2.0+更多内容请参考 Swpp Backends 官方文档 hexo-swpp swpp-backends 解剖SW原理暨博主SW实现 小白也能用的 SW 构建插件 Open Graph在 head 里增加一些 meta 资料，例如缩略图、标题、时间等等。当你分享网页到一些平台时，平台会读取 Open Graph 的内容，展示缩略图，标题等等信息。 修改 主题配置文件 123456789101112# Open graph meta tags# https://developers.facebook.com/docs/sharing/webmasters/Open_Graph_meta: enable: true option: # twitter_card: # twitter_image: # twitter_id: # twitter_site: # google_plus: # fb_admins: # fb_app_id: CSS 前缀有些 CSS 并不是所有浏览器都支持，需要增加对应的前缀才会生效。 开启 css_prefix 后，会自动为一些 CSS 增加前缀。（会增加 20%的体积） 修改 主题配置文件 12# Add the vendor prefixes to ensure compatibilitycss_prefix: true Inject如想添加额外的js&#x2F;css&#x2F;meta等等东西，可以在Inject里添加，支持添加到head(&lt;/body&gt;标签之前)和bottom(&lt;/html&gt;标签之前)。 请注意：以标准的html格式添加内容 12345inject: head: - &lt;link rel=&quot;stylesheet&quot; href=&quot;/self.css&quot;&gt; bottom: - &lt;script src=&quot;xxxx&quot;&gt;&lt;/script&gt; 留意: 如果你的网站根目录不是’&#x2F;‘,使用本地图片时，需加上你的根目录。 例如：网站是 https://yoursite.com/blog,引用css/xx.css，则设置为&lt;link rel=&quot;stylesheet&quot; href=&quot;/blog/css/xx.css&quot;&gt; CDN配置文件中最后一部分CDN，里面是主题所引用到的文件，可自行配置CDN。（非必要请勿修改，配置后请确认链接是否能访问） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# CDN# Don&#x27;t modify the following settings unless you know how they work# 非必要请不要修改CDN: # The CDN provider of internal scripts (主题内部 js 的 cdn 配置) # option: local/elemecdn/jsdelivr/unpkg/cdnjs/custom # Dev version can only choose. ( dev版的主题只能设置为 local ) internal_provider: cbd # The CDN provider of third party scripts (第三方 js 的 cdn 配置) # option: elemecdn/jsdelivr/unpkg/cdnjs/custom third_party_provider: cbd # Add version number to CDN, true or false version: true # Custom format # For example: https://cdn.staticfile.org/$&#123;cdnjs_name&#125;/$&#123;version&#125;/$&#123;min_cdnjs_file&#125; custom_format: # https://npm.elemecdn.com/$&#123;name&#125;@latest/$&#123;file&#125; option: # main_css: # main: # utils: # translate: # random_friends_post_js: # right_click_menu_js: # comment_barrage_js: # local_search: # algolia_js: # algolia_search: # instantsearch: # pjax: # blueimp_md5: # valine: # twikoo: # waline_js: # waline_css: # sharejs: # sharejs_css: # mathjax: # katex: # katex_copytex: # mermaid: # canvas_ribbon: # canvas_fluttering_ribbon: # canvas_nest: # lazyload: # instantpage: # typed: # pangu: # fancybox_css: # fancybox: # medium_zoom: # snackbar_css: # snackbar: # activate_power_mode: # fireworks: # click_heart: # ClickShowText: # fontawesome: # flickr_justified_gallery_js: # flickr_justified_gallery_css: # aplayer_css: # aplayer_js: # meting_js: # meting_api: # prismjs_js: # prismjs_lineNumber_js: # prismjs_autoloader: # artalk_js: # artalk_css: # pace_js: # pace_default_css: # countup_js: # gsap_js: # busuanzi: # rightmenu: # waterfall: # ali_iconfont_css: # accesskey_js: 参数 解释 internal_provider 主题内部文件可选 local&#x2F;jsdelivr&#x2F;unpkg&#x2F;cdnjs&#x2F;customlcoal 为本地加载，custom 为自定义格式，需配置 custom_format注意: 如果使用的是 Dev 版，只能设置为 local third_party_provider 第三方文件可选 local&#x2F;jsdelivr&#x2F;unpkg&#x2F;cdnjs&#x2F;customlcoal 为本地加载，custom 为自定义格式，需配置 custom_format注意: 如果你选择 local 则需要自行将文件都下载至本地，并修改对应的选项，否则会报错。 version true&#x2F;false 为 cdn 加上指定版本号 custom_format 自定义格式 option 你可以在这里更换部分文件,会覆盖原有的配置 version如需修改版本号，可修改主题目录的 ‘plugins.yml’ 中对应插件的 version 请确保你修改的版本号，你所使用的 cdn 有收录 custom_format提供以下参数 参数 解释 name npm 上的包名 file npm 上的文件路径 min_file cdnjs 上的包名 cdnjs_file cdnjs 上的文件路径 min_cdnjs_file cdnjs 上的文件路径（压缩过的文件） version 插件版本号 部分可用的第三方 CDN 列表 请确保你选择的 CDN 有收录主题使用的第三方插件 提供商 格式 备注 Staticfile（七牛云） https://cdn.staticfile.org/${cdnjs_name}/${version}/${min_cdnjs_file} 同步 cdnjs BootCDN https://cdn.bootcdn.net/ajax/libs/${cdnjs_name}/${version}/${min_cdnjs_file} 同步 cdnjs Baomitu（360） 最新版本： https://lib.baomitu.com/${cdnjs_name}/latest/${min_cdnjs_file}指定版本： https://lib.baomitu.com/${cdnjs_name}/${version}/${min_cdnjs_file} 同步 cdnjs Elemecdn 最新版本： https://npm.elemecdn.com/${name}@latest/${file}指定版本： https://npm.elemecdn.com/${name}@${version}/${file} 同步 npm"},{"title":"404页面配置","path":"/2024/04/05/anzhiyu-docs/page/404/","content":"页面配置📦404页面配置主题内置了一个简单的 404 页面，可在设置中开启本地预览时，访问出错的网站是不会跳到 404 页面的。 如需本地预览，请访问 http://localhost:4000/404.html 12345# A simple 404 pageerror_404: enable: true subtitle: &quot;页面没有找到&quot; background:"},{"title":"关于页面配置","path":"/2024/04/05/anzhiyu-docs/page/about/","content":"页面配置 📦关于页面配置 前往你的 Hexo 博客的根目录 在 Hexo 博客根目录 [blog]下打开终端，输入 1hexo new page about 你会找到 source/about/index.md 这个文件 修改这个文件：记得添加 type: &quot;about&quot; 123456789---title: 关于date: 2021-03-30 15:57:51aside: falsetop_img: falsebackground: &quot;#f8f9fe&quot;comments: falsetype: &quot;about&quot;--- 主题配置文件中开启menu中关于和关于本人的注释，注意缩进！！！ 123456789101112131415161718192021menu: # 文章: # 隧道: /archives/ || icon-box-archive # 分类: /categories/ || icon-shapes # 标签: /tags/ || icon-tags 友链: 友人帐: /link/ || icon-link # 朋友圈: /fcircle/ || icon-artstation # 留言板: /comments/ || icon-envelope 我的: 音乐馆: /music/ || icon-music # 追番页: /bangumis/ || icon-bilibili1 # 相册集: /album/ || icon-images # 小空调: /air-conditioner/ || icon-fan 关于: 关于本人: /about/ || icon-zhifeiji 闲言碎语: /essay/ || icon-lightbulb # 随便逛逛: javascript:toRandomPost() || icon-shoe-prints1 新建source/_data/about.yml，输入以下内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152- class_name: 关于页 subtitle: 生活明朗，万物可爱✨ avatarImg: https://npm.elemecdn.com/anzhiyu-blog-static@1.0.0/img/avatar.webp avatarSkills: left: - 🤖️ 数码科技爱好者 - 🔍 分享与热心帮助 - 🏠 智能家居小能手 - 🔨 设计开发一条龙 right: - 专修交互与设计 🤝 - 脚踏实地行动派 🏃 - 团队小组发动机 🧱 - 壮汉人狠话不多 💢 name: 陈志伟 description: 是一名 前端工程师、学生、独立开发者、博主 aboutsiteTips: tips: 追求 title1: 源于 title2: 热爱而去 感受 word: - 学习 - 生活 - 程序 - 体验 helloAbout: Hello there! skillsTips: tips: 技能 title: 开启创造力 careers: tips: 生涯 title: 无限进步 list: - desc: EDU,软件工程专业 color: &quot;#357ef5&quot; - desc: EDU,软件工程专业 color: &quot;#357ef5&quot; - desc: EDU,软件工程专业 color: &quot;#357ef5&quot; img: https://bu.dusays.com/2023/04/21/644287166329b.png statistic: link: /archives text: 文章隧道 cover: https://bu.dusays.com/2023/05/01/644f4b037b930.jpg map: title: 我现在住在 StrengthenTitle: 中国，长沙市 background: https://bu.dusays.com/2023/07/05/64a4c61cb20ef.jpg backgroundDark: https://bu.dusays.com/2023/07/05/64a4c63495ac5.jpg selfInfo: selfInfoTips1: 生于 selfInfoContentYear: 2002 selfInfoTips2: 湖南信息学院 selfInfoContent2: 软件工程 selfInfoTips3: 现在职业 selfInfoContent3: 大三学生👨‍🎓 personalities: author_name: 执政官 personality_type: ESFJ-A photo_url: https://bu.dusays.com/2023/07/05/64a4c63495ac5.jpg personality_img: https://npm.elemecdn.com/anzhiyu-blog@2.0.8/img/svg/ESFJ-A.svg name_url: https://www.16personalities.com/ch/esfj-%E4%BA%BA%E6%A0%BC maxim: maxim_tips: 座右铭 maxim_top: 生活明朗， maxim_bottom: 万物可爱。 buff: buff_tips: 特长 buff_top: 脑回路新奇的 酸菜鱼 buff_bottom: 二次元指数 MAX game: game_tips: 爱好游戏 game_title: 原神 game_uid: &quot;UID: 125766904&quot; game_bg: https://bu.dusays.com/2023/04/22/64433bf26e25d.webp comic: comic_tips: 爱好番剧 comic_title: 追番 comic_list: - name: 约定的梦幻岛 href: https://www.bilibili.com/bangumi/media/md5267750/?spm_id_from=666.25.b_6d656469615f6d6f64756c65.1 cover: https://bu.dusays.com/2023/05/27/647166c44b414.webp - name: 咒术回战 href: https://www.bilibili.com/bangumi/media/md28229899/?spm_id_from=666.25.b_6d656469615f6d6f64756c65.1 cover: https://bu.dusays.com/2023/05/24/646db4398832e.webp - name: 紫罗兰永恒花园 href: https://www.bilibili.com/bangumi/media/md8892/?spm_id_from=666.25.b_6d656469615f6d6f64756c65.1 cover: https://bu.dusays.com/2023/05/24/646db43983d99.webp - name: 鬼灭之刃 href: https://www.bilibili.com/bangumi/media/md22718131/?spm_id_from=666.25.b_6d656469615f6d6f64756c65.1 cover: https://bu.dusays.com/2023/05/24/646db439856a0.webp - name: JOJO的奇妙冒险 黄金之风 href: https://www.bilibili.com/bangumi/media/md135652/?spm_id_from=666.25.b_6d656469615f6d6f64756c65.1 cover: https://bu.dusays.com/2023/05/30/64760e38d651a.webp like: like_tips: 关注偏好 like_title: 数码科技 like_bg: https://bu.dusays.com/2022/12/06/638f5f05ce1f7.jpg like_bottom: 手机、电脑软硬件 music: music_tips: 音乐偏好 music_title: 许嵩、民谣、华语流行 music_bg: https://p2.music.126.net/Mrg1i7DwcwjWBvQPIMt_Mg==/79164837213438.jpg music_link: /music reward_list: - name: 海阔蓝 amount: 8.8 datatime: 2023-03-28 - name: LK66 amount: 66.6 datatime: 2023-03-24 - name: 张时貳 amount: 6.6 datatime: 2023-01-22 - name: ZeroAf amount: 9.9 datatime: 2022-12-14 - name: LuckyWangXi amount: 6.6 datatime: 2022-12-14 - name: 刀中日月长 amount: 10 datatime: 2022-11-16 - name: 鹿啵包 amount: 10 datatime: 2022-11-08 - name: 疾速k amount: 50 datatime: 2022-09-20 - name: 伴舟先生大霖子 amount: 4.03 datatime: 2022-10-27 suffix: 贝壳 - name: Magica_0x0 amount: 3.36 datatime: 2022-10-07 suffix: 贝壳 - name: 名字就是要短像这样 amount: 3.36 datatime: 2022-08-25 suffix: 贝壳 - name: Leviathan520 amount: 1.34 datatime: 2022-08-23 suffix: 贝壳 - name: 托马斯 amount: 10 datatime: 2022-08-19 - name: 哇是猫猫欸 amount: 1.34 datatime: 2022-08-19 suffix: 贝壳 参数 备选值&#x2F;类型 解释 class_name 关于页 【必须】页面类 subtitle string 【必须】副标题 avatarImg url 【必须】头像链接 name string 【必须 作者名称 description string 【必须】描述 aboutsiteTips object 【必须】站点关于提示相关配置 aboutsiteTips.tips string 【必须】站点关于提示性文字 aboutsiteTips.title1 string 【必须】站点关于标题文字 1 aboutsiteTips.title2 string 【必须】站点关于标题文字 2 aboutsiteTips.word list 【必须】站点关于标题滚动文字 helloAbout string 【必须】hello 文字 skillsTips object 【必须】技能相关配置 skillsTips.tips string 【必须】技能提示文字 skillsTips.title string 【必须】技能标题 careers object 【必须】生涯相关配置 careers.tips string 【必须】生涯提示性文字 careers.title string 【必须】生涯标题 careers.list list 【可选】生涯 item careers.list.desc string 【可选】生涯 item 描述 careers.list.color string 【可选】生涯 item 圆圈颜色 careers.img string 【必须】生涯底部图片 statistic object 【必须】统计数据相关配置 statistic.link url 【必须】统计数据按钮前往链接 statistic.text string 【必须】统计数据按钮文字 map object 【必须】地图相关配置 map.title string 【必须】地图标题 map.StrengthenTitle string 【必须】地图大标题 map.background url 【必须】地图亮色模式背景 map.backgroundDark url 【必须】地图暗色模式背景 selfInfo object 【必须】作者相关信息配置 selfInfo.selfInfoTips1 string 【必须】作者相关提示文字 1 selfInfo.selfInfoContentYear number 【必须】作者生日年份 selfInfo.selfInfoTips2 string 【必须】作者相关提示文字 2 selfInfo.selfInfoContent2 string 【必须】作者相关内容 2 selfInfo.selfInfoTips3 string 【必须】作者相关提示文字 3 selfInfo.selfInfoContent3 string 【必须】作者相关内容 3 personalities object 【必须】作者性格相关配置 personalities.author_name string 【必须】作者性格名称 personalities.personality_type string 【必须】作者性格类型 personalities.photo_url url 【必须】作者自拍图片 personalities.personality_img url 【必须】作者性格表述图片 personalities.name_url url 【必须】点击性格跳转到链接 maxim object 【必须】座右铭相关配置 maxim.maxim_tips string 【必须】座右铭相关提示文字 maxim.maxim_top string 【必须】座右铭相关顶部文字 maxim.maxim_bottom string 【必须】座右铭相关底部文字 buff object 【必须】特长相关配置 buff.buff_tips string 【必须】特长相关提示文字 buff.buff_top string 【必须】特长相关顶部文字 buff.buff_bottom string 【必须】特长相关底部文字 game object 【必须】爱好游戏相关配置 game.game_tips string 【必须】爱好游戏提示文字 game.game_title string 【必须】爱好游戏标题 game.game_uid string 【必须】爱好游戏 uid game.game_bg url 【必须】爱好游戏背景 comic object 【必须】追番相关配置，需要 5 条数据 comic.comic_tips string 【必须】追番相关提示文字 comic.comic_title string 【必须】追番相关标题 comic.comic_list list 【必须】追番相关列表 comic.comic_list.name string 【必须】追番 item 名称 comic.comic_list.href url 【必须】追番 item 链接 comic.comic_list.cover url 【必须】追番 item 的 cover like object 【必须】关注偏好相关配置 like.like_tips string 【必须】关注偏好配置提示文字 like.like_title string 【必须】关注偏好配置标题 like.like_bg url 【必须】关注偏好配置背景 like.like_bottom string 【必须】关注偏好配置底部文字 music object 【必须】音乐偏好相关配置 music.music_tips string 【必须】音乐偏好提示性文字 music.music_title string 【必须】音乐偏好标题 music.music_bg url 【必须】音乐偏好背景 music.music_link url 【必须】音乐偏好按钮链接 reward_list object 【可选】打赏相关配置，如果不配置将没有打赏模块 reward_list.name string 【必须】打赏 item 名称 reward_list.amount number 【必须】打赏 item 金额 reward_list.datatime Date 【必须】打赏 item 时间 reward_list.suffix string&#x2F;元 【可选】打赏 item 后缀（默认元）"},{"title":"相册页面配置","path":"/2024/04/05/anzhiyu-docs/page/album/","content":"页面配置📦相册页面配置 前往你的 Hexo 博客的根目录 在 Hexo 博客根目录 [blog]下打开终端，输入 1hexo new page album 你会找到 source/album/index.md 这个文件 修改这个文件：记得添加 type: &quot;album&quot; 1234567---title: 相册集date: 2022-10-23 15:57:51aside: falsetop_img: falsetype: &quot;album&quot;--- 主题配置文件中开启menu中我的和相册集的注释，注意缩进！！！ 123456789101112131415161718192021menu: # 文章: # 隧道: /archives/ || icon-box-archive # 分类: /categories/ || icon-shapes # 标签: /tags/ || icon-tags 友链: 友人帐: /link/ || icon-link # 朋友圈: /fcircle/ || icon-artstation # 留言板: /comments/ || icon-envelope 我的: 音乐馆: /music/ || icon-music # 追番页: /bangumis/ || icon-bilibili1 相册集: /album/ || icon-images # 小空调: /air-conditioner/ || icon-fan 关于: # 关于本人: /about/ || icon-zhifeiji 闲言碎语: /essay/ || icon-lightbulb # 随便逛逛: javascript:toRandomPost() || icon-shoe-prints1 新建文件[blog]\\source\\_data\\album.yml,没有_data文件夹的话也请自己新建。打开[blog]\\source\\_data\\album.yml，输入： 1234567891011121314151617181920212223242526272829303132333435363738394041- class_name: 世界各地夕阳与风景 path_name: /wordScenery type: 2 description: 因为到不了世界各地，所以请网友们发来了各地的夕阳与风景🌇。 cover: https://bu.dusays.com/2023/04/09/64329399d1175.jpg top_background: https://bu.dusays.com/2023/06/30/649e546ada7dd.webp rowHeight: 220 limit: 10 lazyload: true btnLazyload: false url: false top_link: /album top_btn_text: 返回 album_list: - date: 2022/10/26 01:00:00 content: 湘潭的一角。 address: 湖南湘潭 from: 再吃一口就减肥 image: - https://bu.dusays.com/2023/04/09/64329399db122.webp - date: 2022-10-25 content: 洛阳暴雨后的天空。 address: 河南洛阳 from: 紫菜卷 image: - https://bu.dusays.com/2023/04/09/64329399db122.webp - https://bu.dusays.com/2023/04/09/64329399db2e1.webp- class_name: 我的日常 path_name: /dailyPhoto type: 1 description: 这里存放的是有关我自己的一些沙雕生活与有趣的事情。 top_link: /album top_btn_text: 返回 top_background: https://bu.dusays.com/2023/04/09/64329399cea5a.webp cover: https://bu.dusays.com/2023/04/09/64329399cea5a.webp album_list: - date: 2022-10-24 content: 老妹的画 image: - https://bu.dusays.com/2023/04/09/643293997b92b.jpeg 参数 备选值&#x2F;类型 解释 class_name string 【必须】页面类 path_name url 【必须】当前相册路径 type number 【必须】当前相册页面样式类型 description string 【必须】当前相册描述 cover url 【必须】当前相册 cover 图片 rowHeight number 【可选】仅当 type 为 2 时有效，当前相册 rowHeight limit number 【可选】仅当 type 为 2 时有效，当前相册 一次懒加载的数量 lazyload boolean 【可选】仅当 type 为 2 时有效，当前相册 lazyload 是否开启懒加载，默认懒加载为滚动懒加载，type 为 1 时懒加载不可关闭。 btnLazyload boolean 【可选】仅当 type 为 2 且 lazyload 开启 时有效，当前相册 lazyload 懒加载的方式，默认为滚动懒加载，开启本选项后为按钮点击懒加载。 album_list list 【必须】当前相册内图片列表 url url 【可选】仅当 type 为 2 时有效，可以加载远程的 json 数据。 album_list.date date 【必须】当前图片创建时间 album_list.content string 【必须】当前图片描述内容 album_list.image list 【必须】当前图片集，可以多张 album_list.from string 【可选】当前图片的创建人，未填写则不显示 album_list.address string 【必须】当前图片地址 ::: warning注意示例数据中的图片不保证可用性。::: 由于相册页面需要很多的 page，所以在写数据的时候自行写入路径path_name，示例数据中有两个path_name，所以需要再创建两个页面 注意新建的页面必须与path_name一致。 12hexo new page dailyPhotohexo new page wordScenery 你会找到 source/dailyPhoto/index.md 和source/wordScenery/index.md两个文件，这两个为相册集详情页 然后内容为以下内容, 需在详情页加上type: &quot;album_detail&quot; 1234567---title: 日常生活date: 2022-10-23 15:57:51aside: falsetop_img: falsetype: &quot;album_detail&quot;--- 1234567---title: 世界各地风景date: 2022-10-23 15:57:51aside: falsetop_img: falsetype: &quot;album_detail&quot;--- 远程加载json示例数据 12345678910111213141516171819[ &#123; &quot;url&quot;: &quot;https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/IMG_0556.jpg&quot;, &quot;alt&quot;: &quot;IMG_0556.jpg&quot;, &quot;title&quot;: &quot;这是title&quot; &#125;, &#123; &quot;url&quot;: &quot;https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/IMG_0472.jpg&quot;, &quot;alt&quot;: &quot;IMG_0472.jpg&quot; &#125;, &#123; &quot;url&quot;: &quot;https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/IMG_0453.jpg&quot;, &quot;alt&quot;: &quot;&quot; &#125;, &#123; &quot;url&quot;: &quot;https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/IMG_0931.jpg&quot;, &quot;alt&quot;: &quot;&quot; &#125;]"},{"title":"Git 快速入门指南","path":"/2023/03/22/categories/学习/工具/git快速入门/","content":"Git 速成前面的 GitHub 系列文章介绍过，GitHub 是基于 Git 的，所以也就意味着 Git 是基础，如果你不会 Git ，那么接下来你完全继续不下去，所以今天的教程就来说说 Git ，当然关于 Git 的知识单凭一篇文章肯定说不完的，我这篇文章先介绍一些最基本的、最常用的一些 Git 知识，争取让你们 Git 速成。 1.\t什么是Git？Git 是 Linux 发明者 Linus 开发的一款新时代的版本控制系统，那什么是版本控制系统呢？怎么理解？网上一大堆详细的介绍，但是大多枯燥乏味，对于新手也很难理解，这里我只举几个例子来帮助你们理解。熟悉编程的知道，我们在软件开发中源代码其实是最重要的，那么对源代码的管理变得异常重要：比如为了防止代码的丢失，肯定本地机器与远程服务器都要存放一份，而且还需要有一套机制让本地可以跟远程同步；又比如我们经常是好几个人做同一个项目，都要对一份代码做更改，这个时候需要大家互不影响，又需要各自可以同步别人的代码；又比如我们开发的时候免不了有bug，有时候刚发布的功能就出现了严重的bug，这个时候需要紧急对代码进行还原；又比如随着我们版本迭代的功能越来越多，但是我们需要清楚的知道历史每一个版本的代码更改记录，甚至知道每个人历史提交代码的情况；等等等类似以上的情况，这些都是版本控制系统能解决的问题。所以说，版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统，对于软件开发领域来说版本控制是最重要的一环，而 Git 毫无疑问是当下最流行、最好用的版本控制系统。 2.\tGit 安装上面说了，Git 是一个版本控制系统，你也可以理解成是一个工具，跟 Java 类似，使用之前必须得先下载安装，所以第一步必须要安装，我用的是 Mac ， Mac 上其实系统自带 Git 的，不过这里统一提供一下各平台的安装方式，这部分就不过多介绍，相信大家这里搞的定。Mac：https://sourceforge.net/projects/git-osx-installer/ Windows：https://git-for-windows.github.io/ Linux：apt-get install git 3.\t如何学习 Git ？安装好 Git 之后，怎么学习是个问题，其实关于 Git 有很多图形化的软件可以操作，但是我强烈建议大家从命令行开始学习理解，我知道没接触过命令行的人可能会很抵触，但是我的亲身实践证明，只有一开始学习命令行，之后你对 Git 的每一步操作才能理解其意义，而等你熟练之后你想用任何的图形化的软件去操作完全没问题。我一开始教我们团队成员全是基于命令行的，事后证明他们现在已经深深爱上命令行无法自拔，他们很理解 Git 每一步操作的具体含义，以致于在实际项目很少犯错，所以我这里也是基于命令行去教你们学习理解。 4.\tGit 命令列表怎么判断你 Git 有没有安装成功？请在命令行里输入 git ，如果出现以下提示证明你已经安装成功了。 12345678910111213141516171819202122232425262728293031323334353637383940414243root@JSB-002A:~# gitusage: git [--version] [--help] [-C &lt;path&gt;] [-c &lt;name&gt;=&lt;value&gt;] [--exec-path[=&lt;path&gt;]] [--html-path] [--man-path] [--info-path] [-p | --paginate | --no-pager] [--no-replace-objects] [--bare] [--git-dir=&lt;path&gt;] [--work-tree=&lt;path&gt;] [--namespace=&lt;name&gt;] &lt;command&gt; [&lt;args&gt;]These are common Git commands used in various situations:start a working area (see also: git help tutorial) clone Clone a repository into a new directory init Create an empty Git repository or reinitialize an existing onework on the current change (see also: git help everyday) add Add file contents to the index mv Move or rename a file, a directory, or a symlink reset Reset current HEAD to the specified state rm Remove files from the working tree and from the indexexamine the history and state (see also: git help revisions) bisect Use binary search to find the commit that introduced a bug grep Print lines matching a pattern log Show commit logs show Show various types of objects status Show the working tree statusgrow, mark and tweak your common history branch List, create, or delete branches checkout Switch branches or restore working tree files commit Record changes to the repository diff Show changes between commits, commit and working tree, etc merge Join two or more development histories together rebase Reapply commits on top of another base tip tag Create, list, delete or verify a tag object signed with GPGcollaborate (see also: git help workflows) fetch Download objects and refs from another repository pull Fetch from and integrate with another repository or a local branch push Update remote refs along with associated objects&#x27;git help -a&#x27; and &#x27;git help -g&#x27; list available subcommands and someconcept guides. See &#x27;git help &lt;command&gt;&#x27; or &#x27;git help &lt;concept&gt;&#x27;to read about a specific subcommand or concept. Git 所有的操作命令开头都要以 git 开头，上面列举了最常用的一些 Git 命令，紧接着会有一句英文解释这个命令的意义，都不是很难的单词，不妨试着看一下，不过没有实际操作你仍然不好理解，下面我们来以一个实际的操作来介绍下一些常用命令的含义。 5.\tGit 具体命令第一步，我们先新建一个文件夹，在文件夹里新建一个文件（我是用 Linux 命令去新建的，Windows用户可以自己手动新建） 这里提醒下：在进行任何 Git 操作之前，都要先切换到 Git 仓库目录，也就是先要先切换到项目的文件夹目录下。这个时候我们先随便操作一个命令，比如 git status ，可以看到如下提示（别纠结颜色之类的，配置与主题不一样而已）： 意思就是当前目录还不是一个 Git 仓库。 git init这个时候用到了第一个命令，代表初始化 git 仓库，输入 git init 之后会提示： 可以看到初始化成了，至此 test 目录已经是一个 git 仓库了。 git status紧接着我们输入 git status 命令，会有如下提示： 默认就直接在 master 分支，关于分支的概念后面会提，这时最主要的是提示 a.md 文件 Untracked files ，就是说 a.md 这个文件还没有被跟踪，还没有提交在 git 仓库里呢，而且提示你可以使用 git add 去操作你想要提交的文件。git status 这个命令顾名思义就是查看状态，这个命令可以算是使用最频繁的一个命令了，建议大家没事就输入下这个命令，来查看你当前 git 仓库的一些状态。 git add上面提示 a.md 文件还没有提交到 git 仓库里，这个时候我们可以随便编辑下 a.md 文件，然后输入 git add a.md ，然后再输入 git status : 此时提示以下文件 Changes to be committed ， 意思就是 a.md 文件等待被提交，当然你可以使用 git rm –cached 这个命令去移除这个缓存。 git commit接着我们输入 git commit -m ‘first commit’ ，这个命令什么意思呢？ commit 是提交的意思，- m 代表是提交信息，执行了以上命令代表我们已经正式进行了第一次提交。这个时候再输入 git status ，会提示 nothing to commit。 git log这个时候我们输入 git log 命令，会看到如下： git log 命令可以查看所有产生的 commit 记录，所以可以看到已经产生了一条 commit 记录，而提交时候的附带信息叫 ‘first commit’ 。 git add &amp; git commit看到这里估计很多人会有疑问，我想要提交直接进行 commit 不就行了么，为什么先要再 add一次呢？首先 git add 是先把改动添加到一个「暂存区」，你可以理解成是一个缓存区域，临时保存你的改动，而 git commit 才是最后真正的提交。这样做的好处就是防止误提交，当然也有办法把这两步合并成一步，不过后面再介绍，建议新手先按部就班的一步步来。 git branchbranch 即分支的意思，分支的概念很重要，尤其是团队协作的时候，假设两个人都在做同一个项目，这个时候分支就是保证两人能协同合作的最大利器了。举个例子，A, B俩人都在做同一个项目，但是不同的模块，这个时候A新建了一个分支叫a， B新建了一个分支叫b，这样 A、B做的所有代码改动都各自在各自的分支，互不影响，等到俩人都把各自的模块都做完了，最后再统一把分支合并起来。 执行 git init 初始化git仓库之后会默认生成一个主分支 master ，也是你所在的默认分支，也基本是实际开发正式环境下的分支，一般情况下 master 分支不会轻易直接在上面操作的，你们可以输入 git branch 查看下当前分支情况： 如果我们想在此基础上新建一个分支呢，很简单，执行 git branch a 就新建了一个名字叫 a 的分支，这时候分支 a 跟分支 master 是一模一样的内容，我们再输入 git branch 查看的当前分支情况： 但是可以看到 master 分支前有个 * 号，即虽然新建了一个 a 的分支，但是当前所在的分支还是在 master 上，如果我们想在 a 分支上进行开发，首先要先切换到 a 分支上才行，所以下一步要切换分支 执行这个命令，然后再输入 git branch 查看下分支情况： 可以看到当前我们在的分支已经是a了，这个时候 A 同学就可以尽情的在他新建的a分支去进行代码改动了。那有人就说了，我要先新建再切换，未免有点麻烦，有没有一步到位的，聪明： 这个命令的意思就是新建一个a分支，并且自动切换到a分支。 git mergeA同学在a分支代码写的不亦乐乎，终于他的功能完工了，并且测试也都ok了，准备要上线 了，这个时候就需要把他的代码合并到主分支master上来，然后发布。git merge 就是合并分支用到的命令，针对这个情况，需要先做两步，第一步是切换到 master 分支，如果你已经在了就不用切换了，第二步执行 git merge a ，意思就是把a分支的代码合并过来，不出意外，这个时候a分支的代码就顺利合并到 master 分支来了。为什么说不出意外呢？因为这个时候可能会有冲突而合并失败，留个包袱，这个到后面进阶的时候再讲。 git branch -d有新建分支，那肯定有删除分支，假如这个分支新建错了，或者a分支的代码已经顺利合并到 master 分支来了，那么a分支没用了，需要删除，这个时候执行 git branch -d a 就可以把a分支删除了。 git branch -D有些时候可能会删除失败，比如如果a分支的代码还没有合并到master，你执行 git branch -d a 是删除不了的，它会智能的提示你a分支还有未合并的代码，但是如果你非要删除，那就执行 git branch -D a 就可以强制删除a分支。 git tag我们在客户端开发的时候经常有版本的概念，比如v1.0、v1.1之类的，不同的版本肯定对应不同的代码，所以我一般要给我们的代码加上标签，这样假设v1.1版本出了一个新bug，但是又不晓得v1.0是不是有这个bug，有了标签就可以顺利切换到v1.0的代码，重新打个包测试了。所以如果想要新建一个标签很简单，比如 git tag v1.0 就代表我在当前代码状态下新建了一个v1.0的标签，输入 git tag 可以查看历史 tag 记录。 可以看到我新建了两个标签 v1.0、v1.1。 想要切换到某个tag怎么办？也很简单，执行 git checkout v1.0 ，这样就顺利的切换到 v1.0 tag的代码状态了。 向 GitHub 提交代码之前的这篇文章「从0开始学习 GitHub 系列之「Git速成」」相信大家都已经对 Git 的基本操作熟悉了，但是这篇文章只介绍了对本地 Git 仓库的基本操作，今天我就来介绍下如何跟远程仓库一起协作，教你们向 GitHub 上提交你们的第一行代码！ SSH你拥有了一个 GitHub 账号之后，就可以自由的 clone 或者下载其他项目，也可以创建自己的项目，但是你没法提交代码。仔细想想也知道，肯定不可能随意就能提交代码的，如果随意可以提交代码，那么 GitHub 上的项目岂不乱了套了，所以提交代码之前一定是需要某种授权的，而 GitHub 上一般都是基于 SSH 授权的。那么什么是 SSH 呢？ 简单点说，SSH是一种网络协议，用于计算机之间的加密登录。目前是每一台 Linux 电脑的标准配置。而大多数 Git 服务器都会选择使用 SSH 公钥来进行授权，所以想要在 GitHub 提交代码的第一步就是要先添加 SSH key 配置。 生成SSH keyLinux 与 Mac 都是默认安装了 SSH ，而 Windows 系统安装了 Git Bash 应该也是带了 SSH的。大家可以在终端（win下在 Git Bash 里）输入 ssh 如果出现以下提示证明你本机已经安装 SSH， 否则请搜索自行安装下。 紧接着输入 ssh-keygen -t rsa ，什么意思呢？就是指定 rsa 算法生成密钥，接着连续三个回车键（不需要输入密码），然后就会生成两个文件 id_rsa 和 id_rsa.pub ，而 id_rsa 是密钥， id_rsa.pub 就是公钥。这两文件默认分别在如下目录里生成： Linux&#x2F;Mac 系统 在 ~&#x2F;.ssh 下，win系统在 &#x2F;c&#x2F;Documents and Settings&#x2F;username&#x2F;.ssh 下，都是隐藏文件，相信你们有办法查看的。接下来要做的是把 id_rsa.pub 的内容添加到 GitHub 上，这样你本地的 id_rsa 密钥跟 GitHub上的 id_rsa.pub 公钥进行配对，授权成功才可以提交代码。","tags":["Git 工具"],"categories":["工具","Git","快速入门"]},{"title":"Linux 30分钟学会编译linux内核","path":"/2023/01/15/linux-docs/30分钟学会编译linux内核/","content":"1、编译前的准备下载linux源文件：https://www.kernel.org/，我下载的是linux-3.7.4版本，解压到/usr/src/kernels目录中，然后进入/usr/src/kernels/linux-3.7.4中，用make menuconfig命令来选择要编译的模块，但使用make menuconfig（重新编译内核常用的命令，还可以用其他的）报下面的错误： 说缺少ncurses库，然后安装ncurses开发库就可以了，ubuntu下貌似是libncurses-dev包 yum install ncurses-devel.i686 再次使用make menuconfig，出现下面的界面： 然后我直接保存了，都用的默认的选项。 2、编译内核如果你是第一次重新编译内核，先用”make mrproper”命令处理一下内核代码目录中残留的文件，由于我们不知道源代码文件中是否包含像.o之类的文件。 如果不是第一次的话，使用”make clean”命令来清楚.o等编译内核产生的中间文件，但不会删除配置文件。 使用”make bzImage”命令来编译内核，这个内核是经过压缩的。或者使用“make -j10 bzImage”命令，表示用10个线程来编译。 使用”make modules”来编译模块，这个会花费比较长的时间。 上面的命令会花费非常长的时间，编译的动作依据你选择的项目以及你主机硬件的效能而不同。 最后制作出来的数据是被放置在 &#x2F;usr&#x2F;src&#x2F;kernels&#x2F;linux-3.7.4&#x2F; 这个目录下，还没有被放到系统的相关路径中喔！在上面的编译过程当中，如果有发生任何错误的话， 很可能是由于核心模块选择的不好，可能你需要重新以 make menuconfig 再次的检查一下你的相关配置喔！ 如果还是无法成功的话，那么或许将原本系统的内核代码中的 .config 文件，复制到你的内核代码目录下， 然后据以修改，应该就可以顺利的编译出你的核心了。可以发现你的核心已经编译好而且放置在 &#x2F;usr&#x2F;src&#x2F;kernels&#x2F;linux-3.7.4&#x2F;arch&#x2F;x86&#x2F;boot&#x2F;bzImage。 3、安装模块使用命令”make modules_install”安装模块，执行成功后，最终会在 &#x2F;lib&#x2F;modules 底下创建起你这个核心的相关模块，我的模块放在&#x2F;lib&#x2F;modules&#x2F;3.7.4目录下，其中3.7.4就是默认的模块名称。 4、安装内核有两种方法，一种是手工的，一种是自动的。 如果是用手工的，将编译好的内核 &#x2F;usr&#x2F;src&#x2F;kernels&#x2F;linux-3.7.4&#x2F;arch&#x2F;x86&#x2F;boot&#x2F;bzImage复制到&#x2F;boot&#x2F;中，命名为vmlinuz-3.7.4。用命令”mkinitrd -v &#x2F;boot&#x2F;initrd-3.7.4.img 3.7.4”前面一个参数是生成initrd文件，后面一个参数是对应内核模块的名称，mkinitrd回去查找lib&#x2F;modules&#x2F;3.7.4中的模块，将需要的模块插入initrd文件中。为什么我们要制作initrd文件呢？ initrd 文件，他的目的在于提供启动过程中所需要的最重要的核心模块，以让系统启动过程可以顺利完成。 会需要 initrd 的原因，是因为核心模块放至于&#x2F;lib&#x2F;modules&#x2F;$(uname -r)&#x2F;kernel&#x2F; 当中， 这些模块必须要根目录 (&#x2F;) 被挂载时才能够被读取。但是如果核心本身不具备磁盘的驱动程序时， 当然无法挂载根目录，也就没有办法取得驱动程序，因此造成两难的地步，如果没有initrd文件，启动系统时会报下面的错误。 mkinitrd可以将 &#x2F;lib&#x2F;modules&#x2F;…. 内的模块（启动过程当中一定需要的模块）包成一个文件 (就是initrd文件)， 然后在启动时透过主机的 INT 13 硬件功能将该文件读出来解压缩，并且 initrd 在内存内会模拟成为根目录， 由于此虚拟文件系统 (Initial RAM Disk) 主要包含磁盘与文件系统的模块，因此我们的核心最后就能够认识实际的磁盘， 那就能够进行实际根目录的挂载！所以说：initrd 内所包含的模块大多是与启动过程有关，而主要以文件系统及硬盘模块 (如 usb, SCSI 等) 为主！（参考鸟哥的书） 如果是自动的话，直接在 &#x2F;usr&#x2F;src&#x2F;kernels&#x2F;linux-3.7.4目录下输入”make install”就ok了。 5、在启动项中加载新编译的内核由于我用的是fedora16,所以直接输入grub2-mkconfig命令，就会在&#x2F;boot&#x2F;grub2&#x2F;grub.cfg文件中将我们刚编译的模块作为一个启动项了。 其它系统可以通过更改&#x2F;boot&#x2F;grub&#x2F;menu.lst文件来添加启动项。 6、重新启动系统最后就是重新启动系统，选择刚编译的内核那项启动，不过我的系统总是报下面的错误 在网上找了半天，很多都是按我上面写的编译内核的，也没出现这个问题，不知道是不是因为我是在vmware下编译内核的原因。汗，按提示是没有找到根设备，但如果我用原本的内核启动，却不会出错，原来的内核启动也是通过uuid来找根设备的。 版权声明：本文为知乎博主「极致Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/547970302","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux IO 之 IO与网络模型","path":"/2023/01/15/linux-docs/Linux IO 之 IO与网络模型/","content":"Linux内核针对不同并发场景的工具实现 atomic 原子变量x86在多核环境下，多核竞争数据总线时，提供Lock指令进行锁总线操作。保证“读-修改-写”的操作在芯片级的原子性。 spinlock 自旋锁自旋锁将当前线程不停地执行循环体，而不改变线程的运行状态，在CPU上实现忙等，以此保证响应速度更快。这种类型的线程数不断增加时，性能明显下降。所以自旋锁保护的临界区必须小，操作过程必须短。 semaphore 信号量信号量用于保护有限数量的临界资源，信号量在获取和释放时，通过自旋锁保护，当有中断会把中断保存到eflags寄存器，最后再恢复中断。 mutex 互斥锁为了控制同一时刻只有一个线程进入临界区，让无法进入临界区的线程休眠。 rw-lock 读写锁读写锁，把读操作和写操作分别进行加锁处理，减小了加锁粒度，优化了读大于写的场景。 preempt 抢占 时间片用完后调用schedule函数。 由于IO等原因自己主动调用schedule。 其他情况，当前进程被其他进程替换的时候。 per-cpu 变量linux为解决cpu 各自使用的L2 cache 数据与内存中的不一致的问题。 RCU机制 (Read, Copy, Update)用于解决多个CPU同时读写共享数据的场景。它允许多个CPU同时进行写操作，不使用锁，并且实现垃圾回收来处理旧数据。 内存屏障 memory-barrier程序运行过程中，对内存访问不一定按照代码编写的顺序来进行。 编译器对代码进行优化。 多cpu架构存在指令乱序访问内存的可能。 I&#x2F;O 与网络模型介绍各种各样的I&#x2F;O模型，包括以下场景： 阻塞 &amp; 非阻塞 多路复用 Signal IO 异步 IO libevent 现实生活中的场景复杂，Linux CPU和IO行为，他们之间互相等待。例如，阻塞的IO可能会让CPU暂停。 I&#x2F;O模型很难说好与坏，只能说在某些场景下，更适合某些IO模型。其中，1、4 更适合块设备，2、3 更适用于字符设备。 为什么硬盘没有所谓的 多路复用，libevent，signal IO？ 因为select(串口), epoll（socket） 这些都是在监听事件，所以各种各样的IO模型，更多是描述字符设备和网络socket的问题。但硬盘的文件，只有读写，没有 epoll这些。这些IO模型更多是在字符设备，网络socket的场景。 为什么程序要选择正确的IO模型？蓝色代表：cpu，红色代表：io 如上图，某个应用打开一个图片文件，先需要100ms初始化，接下来100ms读这个图片。那打开这个图片就需要200ms。 但是 是否可以开两个线程，同时做这两件事？ 如上图，网络收发程序，如果串行执行，CPU和IO会需要互相等待。为什么CPU和IO可以并行？因为一般硬件，IO通过DMA，cpu消耗比较小，在硬件上操作的时间更长。CPU和硬盘是两个不同的硬件。 再比如开机加速中systemd使用的readahead功能:第一次启动过程，读的文件，会通过Linux inotify监控linux内核文件被操作的情况，记录下来。第二次启动，后台有进程直接读这些文件，而不是等到需要的时候再读。 I&#x2F;O模型会深刻影响应用的最终性能，阻塞 &amp; 非阻塞 、异步 IO 是针对硬盘， 多路复用、signal io、libevent 是针对字符设备和 socket。 简单的IO模型 当一个进程需要读 键盘、触屏、鼠标时，进程会阻塞。但对于大量并发的场景，阻塞IO无法搞定，也可能会被信号打断。 内核程序等待IO，gobal fifo read不到 一般情况select返回，会调用 if signal_pending，进程会返回 ERESTARTSYS；此时，进程的read 返回由singal决定。有可能返回（EINTR），也有可能不返回。 demo:12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;signal.h&gt;#include &lt;sys/types.h&gt;#include &lt;errno.h&gt;#include &lt;string.h&gt;static void sig_handler(int signum)&#123;\tprintf(&quot;int handler %d &quot;, signum);&#125;int main(int argc, char **argv)&#123;\tchar buf[100];\tssize_t ret;\tstruct sigaction oldact;\tstruct sigaction act;\tact.sa_handler = sig_handler;\tact.sa_flags = 0;\t// act.sa_flags |= SA_RESTART;\tsigemptyset(&amp;act.sa_mask);\tif (-1 == sigaction(SIGUSR1, &amp;act, &amp;oldact)) &#123; printf(&quot;sigaction failed!/n&quot;); return -1;\t&#125;\tbzero(buf, 100);\tdo &#123; ret = read(STDIN_FILENO, buf, 10); if ((ret == -1) &amp;&amp; (errno == EINTR)) printf(&quot;retry after eintr &quot;);\t&#125; while((ret == -1) &amp;&amp; (errno == EINTR));\tif (ret &gt; 0) printf(&quot;read %d bytes, content is %s &quot;, ret, buf);\treturn 0;&#125; 一个阻塞的IO，在睡眠等IO时Ready，但中途被信号打断，linux响应信号，read&#x2F;write请求阻塞。配置信号时，在SA_FLAG是不是加“自动”，SA_RESTART指定 被阻塞的IO请求是否重发，并且应用中可以捕捉。加了SA_RESTART重发，就不会返回出错码EINTR。没有加SA_RESTART重发，就会返回出错码（EINTR），这样可以检测read被信号打断时的返回。 但Linux中有一些系统调用，即便你加了自动重发，也不能自动重发。man signal. 当使用阻塞IO时，要小心这部分。 多进程、多线程模型当有多个socket消息需要处理，阻塞IO搞不定，有一种可能是多个进程&#x2F;线程，每当有一个连接建立（accept socket)，都会启动一个线程去处理新建立的连接。但是，这种模型性能不太好，创建多进程、多线程时会有开销。 经典的C10K问题，意思是 在一台服务器上维护1w个连接，需要建立1w个进程或者线程。那么如果维护1亿用户在线，则需要1w台服务器。 IO多路复用，则是解决以上问题的场景。 总结：多进程、多线程模型企图把每一个fd放到不同的线程&#x2F;进程处理，避免阻塞的问题，从而引入了进程创建\\撤销，调度的开销。能否在一个线程内搞定所有IO? – 这就是多路复用的作用。 多路复用 selectselect：效率低，性能不太好。不能解决大量并发请求的问题。 它把1000个fd加入到fd_set（文件描述符集合），通过select监控fd_set里的fd是否有变化。如果有一个fd满足读写事件，就会依次查看每个文件描述符，那些发生变化的描述符在fd_set对应位设为1，表示socket可读或者可写。 Select通过轮询的方式监听，对监听的FD数量 t通过FD_SETSIZE限制。 两个问题： 1、select初始化时，要告诉内核，关注1000个fd， 每次初始化都需要重新关注1000个fd。前期准备阶段长。2、select返回之后，要扫描1000个fd。 后期扫描维护成本大，CPU开销大。 epollepoll ：在内核中的实现不是通过轮询的方式，而是通过注册callback函数的方式。当某个文件描述符发现变化，就主动通知。成功解决了select的两个问题，“epoll 被称为解决 C10K 问题的利器。” 1、select的“健忘症”，一返回就不记得关注了多少fd。api 把告诉内核等哪些文件，和最终监听哪些文件，都是同一个api。而epoll，告诉内核等哪些文件 和具体等哪些文件分开成两个api，epoll的“等”返回后，还是知道关注了哪些fd。2、select在返回后的维护开销很大，而epoll就可以直接知道需要等fd。 epoll获取事件的时候，无须遍历整个被侦听的描述符集，只要遍历那些被内核I&#x2F;O事件异步唤醒而加入就绪队列的描述符集合。 epoll_create: 创建epoll池子。epoll_ctl：向epoll注册事件。告诉内核epoll关心哪些文件，让内核没有健忘症。epoll_wait：等待就绪事件的到来。专门等哪些文件，第2个参数 是输出参数，包含满足的fd，不需要再遍历所有的fd文件。 如上图，epoll在CPU的消耗上，远低于select，这样就可以在一个线程内监控更多的IO。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;sys/stat.h&gt;static void call_epoll(void)&#123; int epfd, fifofd, pipefd; struct epoll_event ev, events[2]; int ret; epfd = epoll_create(2); if (epfd &lt; 0) &#123; perror(&quot;epoll_create()&quot;); return; &#125; ev.events = EPOLLIN|EPOLLET; fifofd = open(&quot;/dev/globalfifo&quot;, O_RDONLY, S_IRUSR); printf(&quot;fifo fd:%d &quot;, fifofd); ev.data.fd = fifofd; ret = epoll_ctl(epfd, EPOLL_CTL_ADD, fifofd, &amp;ev); pipefd = open(&quot;pipe&quot;, O_RDONLY|O_NONBLOCK, S_IRUSR); printf(&quot;pipe fd:%d &quot;, pipefd); ev.data.fd = pipefd; ret = epoll_ctl(epfd, EPOLL_CTL_ADD, pipefd, &amp;ev); while(1) &#123; ret = epoll_wait(epfd, events, 2, 50000); if (ret &lt; 0) &#123; perror(&quot;epoll_wait()&quot;); &#125; else if (ret == 0) &#123; printf(&quot;No data within 50 seconds. &quot;); &#125; else &#123; int i; for(i=0;i&lt;ret;i++) &#123; char buf[100]; read(events[i].data.fd, buf, 100); printf(&quot;%s is available now:, %s &quot;, events[i].data.fd==fifofd? &quot;fifo&quot;:&quot;pipe&quot;, buf); &#125; &#125; &#125;_out: close(epfd);&#125;int main()&#123; call_epoll(); return 0;&#125; 总结：epoll是几乎是大规模并行网络程序设计的代名词，一个线程里可以处理大量的tcp连接，cpu消耗也比较低。很多框架模型，nginx, nodejs, 底层均使用epoll实现。 signal IO目前在linux中很少被用到，Linux内核某个IO事件ready，通过kill出一个signal，应用程序在signal IO上绑定处理函数。 kernel发现设备读写事件变化，调用一个 kill fa_sync ，应用程序绑定signal_io上的事件。 异步IO Linux中 不要把aio串起来， 基于epoll等api进行上层的封装，再基于事件编程。某个事件成立了，就开始去做某件事。 libevent 就像MFC一样，界面上的按钮，VC会产生一个on_button，调对应的函数。是一种典型的事件循环。 本质上还是用了epoll，只是基于事件编程。","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux Kernel内核整体架构(图文详解)","path":"/2023/01/15/linux-docs/Linux Kernel内核整体架构(图文详解)/","content":"1，前言本文是“Linux内核分析”系列文章的第一篇，会以内核的核心功能为出发点，描述Linux内核的整体架构，以及架构之下主要的软件子系统。之后，会介绍Linux内核源文件的目录结构，并和各个软件子系统对应。 注：本文和其它的“Linux内核分析”文章都基于如下约定： a) 内核版本为Linux 3.10.29（该版本是一个long term的版本，会被Linux社区持续维护至少2年)。 b) 鉴于嵌入式系统大多使用ARM处理器，因此涉及到体系结构部分的内容，都以ARM为分析对象 2，Linux内核的核心功能如下图所示，Linux内核只是Linux操作系统一部分。对下，它管理系统的所有硬件设备；对上，它通过系统调用，向Library Routine（例如C库）或者其它应用程序提供接口。 因此，其核心功能就是：管理硬件设备，供应用程序使用。而现代计算机（无论是PC还是嵌入式系统）的标准组成，就是CPU、Memory（内存和外存）、输入输出设备、网络设备和其它的外围设备。所以为了管理这些设备，Linux内核提出了如下的架构。 3，Linux内核的整体架构3.1 整体架构和子系统划分 上图说明了Linux内核的整体架构。根据内核的核心功能，Linux内核提出了5个子系统，分别负责如下的功能：\\1. Process Scheduler，也称作进程管理、进程调度。负责管理CPU资源，以便让各个进程可以以尽量公平的方式访问CPU。\\2. Memory Manager，内存管理。负责管理Memory（内存）资源，以便让各个进程可以安全地共享机器的内存资源。另外，内存管理会提供虚拟内存的机制，该机制可以让进程使用多于系统可用Memory的内存，不用的内存会通过文件系统保存在外部非易失存储器中，需要使用的时候，再取回到内存中。\\3. VFS（Virtual File System），虚拟文件系统。Linux内核将不同功能的外部设备，例如Disk设备（硬盘、磁盘、NAND Flash、Nor Flash等）、输入输出设备、显示设备等等，抽象为可以通过统一的文件操作接口（open、close、read、write等）来访问。这就是Linux系统“一切皆是文件”的体现（其实Linux做的并不彻底，因为CPU、内存、网络等还不是文件，如果真的需要一切皆是文件，还得看贝尔实验室正在开发的”Plan 9”的）。\\4. Network，网络子系统。负责管理系统的网络设备，并实现多种多样的网络标准。\\5. IPC（Inter-Process Communication），进程间通信。IPC不管理任何的硬件，它主要负责Linux系统中进程之间的通信。 3.2 进程调度（Process Scheduler)进程调度是Linux内核中最重要的子系统，它主要提供对CPU的访问控制。因为在计算机中，CPU资源是有限的，而众多的应用程序都要使用CPU资源，所以需要“进程调度子系统”对CPU进行调度管理。进程调度子系统包括4个子模块（见下图），它们的功能如下： \\1. Scheduling Policy，实现进程调度的策略，它决定哪个（或哪几个）进程将拥有CPU。\\2. Architecture-specific Schedulers，体系结构相关的部分，用于将对不同CPU的控制，抽象为统一的接口。这些控制主要在suspend和resume进程时使用，牵涉到CPU的寄存器访问、汇编指令操作等。\\3. Architecture-independent Scheduler，体系结构无关的部分。它会和“Scheduling Policy模块”沟通，决定接下来要执行哪个进程，然后通过“Architecture-specific Schedulers模块”resume指定的进程。\\4. System Call Interface，系统调用接口。进程调度子系统通过系统调用接口，将需要提供给用户空间的接口开放出去，同时屏蔽掉不需要用户空间程序关心的细节。3.3 内存管理（Memory Manager, MM)内存管理同样是Linux内核中最重要的子系统，它主要提供对内存资源的访问控制。Linux系统会在硬件物理内存和进程所使用的内存（称作虚拟内存）之间建立一种映射关系，这种映射是以进程为单位，因而不同的进程可以使用相同的虚拟内存，而这些相同的虚拟内存，可以映射到不同的物理内存上。 内存管理子系统包括3个子模块（见下图），它们的功能如下： \\1. Architecture Specific Managers，体系结构相关部分。提供用于访问硬件Memory的虚拟接口。\\2. Architecture Independent Manager，体系结构无关部分。提供所有的内存管理机制，包括：以进程为单位的memory mapping；虚拟内存的Swapping。\\3. System Call Interface，系统调用接口。通过该接口，向用户空间程序应用程序提供内存的分配、释放，文件的map等功能。 3.4 虚拟文件系统（Virtual Filesystem, VFS）传统意义上的文件系统，是一种存储和组织计算机数据的方法。它用易懂、人性化的方法（文件和目录结构），抽象计算机磁盘、硬盘等设备上冰冷的数据块，从而使对它们的查找和访问变得容易。因而文件系统的实质，就是“存储和组织数据的方法”，文件系统的表现形式，就是“从某个设备中读取数据和向某个设备写入数据”。随着计算机技术的进步，存储和组织数据的方法也是在不断进步的，从而导致有多种类型的文件系统，例如FAT、FAT32、NTFS、EXT2、EXT3等等。而为了兼容，操作系统或者内核，要以相同的表现形式，同时支持多种类型的文件系统，这就延伸出了虚拟文件系统（VFS）的概念。VFS的功能就是管理各种各样的文件系统，屏蔽它们的差异，以统一的方式，为用户程序提供访问文件的接口。我们可以从磁盘、硬盘、NAND Flash等设备中读取或写入数据，因而最初的文件系统都是构建在这些设备之上的。这个概念也可以推广到其它的硬件设备，例如内存、显示器（LCD）、键盘、串口等等。我们对硬件设备的访问控制，也可以归纳为读取或者写入数据，因而可以用统一的文件操作接口访问。Linux内核就是这样做的，除了传统的磁盘文件系统之外，它还抽象出了设备文件系统、内存文件系统等等。这些逻辑，都是由VFS子系统实现。VFS子系统包括6个子模块（见下图），它们的功能如下： \\1. Device Drivers，设备驱动，用于控制所有的外部设备及控制器。由于存在大量不能相互兼容的硬件设备（特别是嵌入式产品），所以也有非常多的设备驱动。因此，Linux内核中将近一半的Source Code都是设备驱动，大多数的Linux底层工程师（特别是国内的企业）都是在编写或者维护设备驱动，而无暇估计其它内容（它们恰恰是Linux内核的精髓所在）。\\2. Device Independent Interface， 该模块定义了描述硬件设备的统一方式（统一设备模型），所有的设备驱动都遵守这个定义，可以降低开发的难度。同时可以用一致的形式向上提供接口。\\3. Logical Systems，每一种文件系统，都会对应一个Logical System（逻辑文件系统），它会实现具体的文件系统逻辑。\\4. System Independent Interface，该模块负责以统一的接口（快设备和字符设备）表示硬件设备和逻辑文件系统，这样上层软件就不再关心具体的硬件形态了。\\5. System Call Interface，系统调用接口，向用户空间提供访问文件系统和硬件设备的统一的接口。3.5 网络子系统（Net）网络子系统在Linux内核中主要负责管理各种网络设备，并实现各种网络协议栈，最终实现通过网络连接其它系统的功能。在Linux内核中，网络子系统几乎是自成体系，它包括5个子模块（见下图），它们的功能如下： \\1. Network Device Drivers，网络设备的驱动，和VFS子系统中的设备驱动是一样的。\\2. Device Independent Interface，和VFS子系统中的是一样的。\\3. Network Protocols，实现各种网络传输协议，例如IP, TCP, UDP等等。\\4. Protocol Independent Interface，屏蔽不同的硬件设备和网络协议，以相同的格式提供接口（socket)。\\5. System Call interface，系统调用接口，向用户空间提供访问网络设备的统一的接口。至于IPC子系统，由于功能比较单纯，这里就不再描述了。 4，Linux内核源代码的目录结构Linux内核源代码包括三个主要部分：\\1. 内核核心代码，包括第3章所描述的各个子系统和子模块，以及其它的支撑子系统，例如电源管理、Linux初始化等\\2. 其它非核心代码，例如库文件（因为Linux内核是一个自包含的内核，即内核不依赖其它的任何软件，自己就可以编译通过）、固件集合、KVM（虚拟机技术）等\\3. 编译脚本、配置文件、帮助文档、版权说明等辅助性文件下图是使用ls命令看到的内核源代码的顶层目录结构，具体描述如下： include&#x2F; —- 内核头文件，需要提供给外部模块（例如用户空间代码）使用。 kernel&#x2F; —- Linux内核的核心代码，包含了3.2小节所描述的进程调度子系统，以及和进程调度相关的模块。 mm&#x2F; —- 内存管理子系统（3.3小节）。 fs&#x2F; —- VFS子系统（3.4小节）。 net&#x2F; —- 不包括网络设备驱动的网络子系统（3.5小节）。 ipc&#x2F; —- IPC（进程间通信）子系统。 arch&#x2F;&#x2F; —- 体系结构相关的代码，例如arm, x86等等。 arch&#x2F;&#x2F;mach- —- 具体的machine&#x2F;board相关的代码。 arch&#x2F;&#x2F;include&#x2F;asm —- 体系结构相关的头文件。 arch&#x2F;&#x2F;boot&#x2F;dts —- 设备树（Device Tree）文件。 init&#x2F; —- Linux系统启动初始化相关的代码。 block&#x2F; —- 提供块设备的层次。 sound&#x2F; —- 音频相关的驱动及子系统，可以看作“音频子系统”。 drivers&#x2F; —- 设备驱动（在Linux kernel 3.10中，设备驱动占了49.4的代码量）。 lib&#x2F; —- 实现需要在内核中使用的库函数，例如CRC、FIFO、list、MD5等。 crypto&#x2F; —– 加密、解密相关的库函数。 security&#x2F; —- 提供安全特性（SELinux）。 virt&#x2F; —- 提供虚拟机技术（KVM等）的支持。 usr&#x2F; —- 用于生成initramfs的代码。 firmware&#x2F; —- 保存用于驱动第三方设备的固件。 samples&#x2F; —- 一些示例代码。 tools&#x2F; —- 一些常用工具，如性能剖析、自测试等。 Kconfig, Kbuild, Makefile, scripts&#x2F; —- 用于内核编译的配置文件、脚本等。 COPYING —- 版权声明。 MAINTAINERS —-维护者名单。 CREDITS —- Linux主要的贡献者名单。 REPORTING-BUGS —- Bug上报的指南。 Documentation, README —- 帮助、说明文档。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/438248184","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核Coding Style整理","path":"/2023/01/15/linux-docs/Linux内核Coding Style整理/","content":"1、缩进了缩进用 Tab, 并且Tab的宽度为8个字符 swich 和 case对齐, 不用缩进 12345678910111213141516switch (suffix) &#123;case &#x27;G&#x27;:case &#x27;g&#x27;: mem &lt;&lt;= 30; break;case &#x27;M&#x27;:case &#x27;m&#x27;: mem &lt;&lt;= 20; break;case &#x27;K&#x27;:case &#x27;k&#x27;: mem &lt;&lt;= 10; /* fall through */default: break;&#125; 一行只有一个表达式 1if (condition) do_this; /* bad example */ 不要用空格来缩进 (除了注释或文档) 2、代码行长度控制在80个字符以内长度过长的行截断时, 注意保持易读性 123456789void fun(int a, int b, int c)&#123; if (condition) printk(KERN_WARNING &quot;Warning this is a long printk with &quot; &quot;3 parameters a: %u b: %u &quot; &quot;c: %u &quot;, a, b, c); else next_statement;&#125; 3、括号和空格的位置函数的大括号另起一行 1234int function(int x)&#123; /* 这个大括号 &#123; 另起了一行 */ body of function&#125; 非函数的语句块(if, switch, for, while, do)不用另起一行 123if (x is true) &#123; /* 这个大括号 &#123; 不用另起一行 */ we do y&#125; 只有一行的语句块不用大括号 12if (condition) action(); 如果if用了大括号, 那么else即使只有一行也要用大括号 123456if (condition) &#123; do_this(); do_that();&#125; else &#123; otherwise();&#125; 下列 keywords 后面追加一个空格 if, switch, case, for, do, while 下列 keywords 后面 不要 追加一个空格 sizeof, typeof, alignof, __attribute 123456if (condition) &#123; do_this(); do_that();&#125; else &#123; otherwise();&#125; 定义指针时, * 紧靠函数名或者变量名 123456if (condition) &#123; do_this(); do_that();&#125; else &#123; otherwise();&#125; 下面的二元和三元操作符左右要留一个空格 &#x3D; + - &lt; &gt; * &#x2F; % | &amp; ^ &lt;&#x3D; &gt;&#x3D; &#x3D;&#x3D; !&#x3D; ? : 下面的一元操作符后面 不要 留空格 &amp; * + - ~ ! sizeof typeof alignof attribute defined 后缀操作符(++ –)前面不要留空格 前缀操作符(++ –)后面不要留空格 结构体成员操作符(. -&gt;)前后都不要留空格 每行代码之后不要有多余的空格 4、命名全局变量或函数(在确实需要时才使用)要有个描述性的名称 12count_active_users() /* good */cntusr() /* cnt */ 局部变量名称要简洁(这个规则比较抽象, 只能多看看内核代码中其他人的命名方式) 5、Typedefs尽量不要使用 typedef, 使用typedef主要为了下面的用途: 完全不透明的类型(访问这些类型也需要对应的访问函数) ex. pid_t, uid_t, pte_t … 等等 避免整型数据的困扰 比如int, long类型的长度在不同体系结构中不一致等等, 使用 u8&#x2F;u16&#x2F;u32 来代替整型定义 当使用kernel的sparse工具做变量类型检查时, 可以typedef一个类型. 定义C99标准中的新类型 为了用户空间的类型安全 内核空间的结构体映射到用户空间时使用typedef, 这样即使内核空间的数据结构有变化, 用户空间也能正常运行 6、函数函数要简短,一个函数只做一件事情 函数长度一般不超过2屏(1屏的大小是 80x24), 也就是48行 如果函数中的 switch 有很多简单的 case语句, 那么超出2屏也可以 函数中局部变量不能超过 5~10 个 函数与函数之间空一行, 但是和EXPORT* 之间不用空 12345678910int one_func(void)&#123; return 0;&#125;int system_is_up(void)&#123; return system_state == SYSTEM_RUNNING;&#125;EXPORT_SYMBOL(system_is_up); 7、函数退出将函数的退出集中在一起, 特别有需要清理内存的时候.(goto 并不是洪水猛兽, 有时也很有用) 1234567891011121314151617181920int fun(int a)&#123; int result = 0; char *buffer = kmalloc(SIZE); if (buffer == NULL) return -ENOMEM; if (condition1) &#123; while (loop1) &#123; ... &#125; result = 1; goto out; &#125; ... out: kfree(buffer); return result;&#125; 8、注释注释code做了什么, 而不是如何做的 使用C89的注释风格(&#x2F;* … *&#x2F;), 不要用C99的注释风格(&#x2F;&#x2F; …) 注释定义的数据, 不管是基本类型还是衍生的类型 9、控制缩进的方法用emacs来保证缩进, .emacs中的配置如下: 12345678910111213141516171819202122232425262728(defun c-lineup-arglist-tabs-only (ignored) &quot;Line up argument lists by tabs, not spaces&quot; (let* ((anchor (c-langelem-pos c-syntactic-element)) (column (c-langelem-2nd-pos c-syntactic-element)) (offset (- (1+ column) anchor)) (steps (floor offset c-basic-offset))) (* (max steps 1) c-basic-offset)))(add-hook &#x27;c-mode-common-hook (lambda () ;; Add kernel style (c-add-style &quot;linux-tabs-only&quot; &#x27;(&quot;linux&quot; (c-offsets-alist (arglist-cont-nonempty c-lineup-gcc-asm-reg c-lineup-arglist-tabs-only))))))(add-hook &#x27;c-mode-hook (lambda () (let ((filename (buffer-file-name))) ;; Enable kernel mode for the appropriate files (when (and filename (string-match (expand-file-name &quot;~/src/linux-trees&quot;) filename)) (setq indent-tabs-mode t) (c-set-style &quot;linux-tabs-only&quot;))))) 使用 indent 脚本来保证缩进. (脚本位置: scripts&#x2F;Lindent) 10、Kconfig配置文件“config” 下一行要缩进一个Tab, “help” 下则缩进2个空格 12345678config AUDIT bool &quot;Auditing support&quot; depends on NET help Enable auditing infrastructure that can be used with another kernel subsystem, such as SELinux (which requires this for logging of avc messages output). Does not do system-call auditing without CONFIG_AUDITSYSCALL. 不稳定的特性要加上 EXPERIMENTAL 1234config SLUB depends on EXPERIMENTAL &amp;&amp; !ARCH_USES_SLAB_PAGE_STRUCT bool &quot;SLUB (Unqueued Allocator)&quot; ... 危险的特性要加上 DANGEROUS 1234config ADFS_FS_RW bool &quot;ADFS write support (DANGEROUS)&quot; depends on ADFS_FS ... 11、数据结构结构体要包含一个引用计数字段 (内核中没有自动垃圾收集, 需要手动释放内存) 需要保证结构体数据一致性时要加锁 结构体中有时甚至会需要多层的引用计数 ex. struc mm_struct, struct super_block 12、宏, 枚举类型和RTL宏定义常量后者标签时使用大写字母 1#define CONSTANT 0x12345 宏定义多行语句时要放入 do - while 中, 此时宏的名称用小写 12345#define macrofun(a, b, c) \\ do &#123; \\ if (a == 5) \\ do_this(b, c); \\ &#125; while (0) 宏定义表达式时要放在 () 中 12#define CONSTANT 0x4000#define CONSTEXP (CONSTANT | 3) 枚举类型 用来定义多个有联系的常量 13、打印内核消息保持打印信息的简明清晰 比如, 不要用 “dont”, 而是使用 “do not” 或者 “don’t” 内核信息不需要使用 “.” 结尾 打印 “(%d)” 之类的没有任何意义, 应该避免 选择合适的打印级别(调试,还是警告,错误等等) 14、分配内存分配内存时sizeof(指针) 而不是 sizeof(结构体) 1p = kmalloc(sizeof(*p), ...); 这样的话, 当p指向其他结构体时, 上面的代码仍然可用 分配内存后返回值转为 void指针是多余的, C语言本身会完成这个步骤 15、内联的弊端如果一个函数有3行以上, 不要使用 inline 来标记它 16、函数返回值和名称如果函数功能是一个动作或者一个命令时, 返回 int型的 error-code 比如, add_work() 函数执行成功时返回 0, 失败时返回对应的error-code(ex. -EBUSY) 如果函数功能是一个判断时, 返回 “0” 表示失败, “1” 表示成功 所有Exported函数, 公用的函数都要上述2条要求 所有私有(static)函数, 不强制要求, 但最好能满足上面2条要求 如果函数返回真实计算结果, 而不是是否成功时, 通过返回计算结果范围外的值来表示失败 比如一个返回指针的函数, 通过返回 NULL 来表示失败 17、不要重复发明内核宏内核定义的宏在头文件 include&#x2F;linux&#x2F;kernel.h 中, 想定义新的宏时, 首先看看其中是否已有类似的宏可用 18、编辑器模式行和其他不要在代码中加入特定编辑器的内容或者其他工具的配置, 比如 emacs 的配置 12345-*- mode: c -*-orLocal Variables:compile-command: &quot;gcc -DMAGIC_DEBUG_FLAG foo.c&quot;End: vim 的配置 1vim:set sw=8 noet 每个人使用的开发工具可能都不一样, 这样的配置对有些人来说就是垃圾 版权声明：本文为知乎博主「极致Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/548067470","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核Makefile系统文件详解","path":"/2023/01/15/linux-docs/Linux内核Makefile系统文件详解/","content":"第一部分、概述什么是makefile?或许很多Winodws的程序员都不知道这个东西,因为那些Windows 的IDE都为你做了这个工作,但我觉得要作一个好的和professional的程序员,makefile 还是要懂。这就好像现在有这么多的HTML的编辑器,但如果你想成为一个专业人士,你还是要了解HTML的标识的含义。特别在Unix下的软件编译,你就不能不自己写makefile了,会不会写makefile,从一个侧面说明了一个人是否具备完成大型工程的能力。 因为, makefile关系到了整个工程的编译规则。一个工程中的源文件不计数,其按类型、功能、模块分别放在若干个目录中,makefile定义了一系列的规则来指定,哪些文件需要先编译,哪些文件需要后编译,哪些文件需要重新编译,甚至于进行更复杂的功能操作,因为makefile就像一个Shell脚本一样,其中也可以执行操作系统的命令。 makefile带来的好处就是——“自动化编译”,一旦写好,只需要一个make命令,整个工程完全自动编译,极大的提高了软件开发的效率。make是一个命令工具,是一个解释makefile中指令的命令工具,一般来说,大多数的IDE都有这个命令,比如:Delphi的make,Visual C++的nmake, Linux下GNU的make。可见,makefile都成为了一种在工程方面的编译方法。 现在讲述如何写makefile的文章比较少,这是我想写这篇文章的原因。当然,不同产商的make各不相同,也有不同的语法,但其本质都是在“文件依赖性”上做文章,这里,我仅对GNU的make进行讲述,我的环境是RedHat Linux 8.0,make的版本是3.80。毕竟,这个make是应用最为广泛的,也是用得最多的。而且其还是最遵循于IEEE 1003.2-1992 标准的(POSIX.2)。 在这篇文档中,将以C&#x2F;C++的源码作为我们基础,所以必然涉及一些关于C&#x2F;C++的编译的知识,相关于这方面的内容,还请各位查看相关的编译器的文档。这里所默认的编译器是UNIX下的GCC和CC。 第二部分、关于程序的编译和链接在此,我想多说关于程序编译的一些规范和方法,一般来说,无论是C、C++、还是pas,首先要把源文件编译成中间代码文件,在Windows下也就是 .obj 文件,UNIX下是 .o 文件,即 Object File,这个动作叫做编译(compile)。然后再把大量的Object File合成执行文件,这个动作叫作链接(link)。 编译时,编译器需要的是语法的正确,函数与变量的声明的正确。对于后者,通常是你需要告诉编译器头文件的所在位置(头文件中应该只是声明,而定义应该放在C&#x2F;C++文件中),只要所有的语法正确,编译器就可以编译出中间目标文件。一般来说,每个源文件都应该对应于一个中间目标文件(O文件或是OBJ文件)。链接时,主要是链接函数和全局变量,所以,我们可以使用这些中间目标文件(O文件或是OBJ 文件)来链接我们的应用程序。链接器并不管函数所在的源文件,只管函数的中间目标文件(Object File),在大多数时候,由于源文件太多,编译生成的中间目标文件太多,而在链 届时需要明显地指出中间目标文件名,这对于编译很不方便,所以,我们要给中间目标文件打个包,在Windows下这种包叫“库文件”(Library File),也就是 .lib 文件,在UNIX 下,是Archive File,也就是 .a 文件。 总结一下,源文件首先会生成中间目标文件,再由中间目标文件生成执行文件。在编译时,编译器只检测程序语法,和函数、变量是否被声明。如果函数未被声明,编译器会给出一个警告,但可以生成Object File。而在链接程序时,链接器会在所有的Object File中找寻函数的实现,如果找不到,那到就会报链接错误码(Linker Error),在VC下,这种错误一般是:Link 2001错误,意思是说,链接器未能找到函数的实现。你需要指定函数的Object File. 好,言归正传,GNU的make有许多的内容,闲言少叙,还是让我们开始吧。 第三部分、Makefile 介绍 make命令执行时,需要一个 Makefile 文件,以告诉make命令需要怎么样的去编译和链接程序。 首先,我们用一个示例来说明Makefile的书写规则。以便给大家一个兴趣认识。这个示例来源于GNU的make使用手册,在这个示例中,我们的工程有8个C文件,和3个头文件,我们要写一个Makefile来告诉make命令如何编译和链接这几个文件。我们的规则是: 1)如果这个工程没有编译过,那么我们的所有C文件都要编译并被链接。 2)如果这个工程的某几个C文件被修改,那么我们只编译被修改的C文件,并链接目标程。 3)如果这个工程的头文件被改变了,那么我们需要编译引用了这几个头文件的C文件,并链接目标程序。 只要我们的Makefile写得够好,所有的这一切,我们只用一个make命令就可以完成, make命令会自动智能地根据当前的文件修改的情况来确定哪些文件需要重编译,从而自己编译所需要的文件和链接目标程序。 一、Makefile的规则在讲述这个Makefile之前,还是让我们先来粗略地看一看Makefile的规则。 1234567target ... : prerequisites ...command...... target也就是一个目标文件,可以是Object File,也可以是执行文件。还可以是一个标签(Label),对于标签这种特性,在后续的“伪目标”章节中会有叙述。 prerequisites就是,要生成那个target所需要的文件或是目标。 command也就是make需要执行的命令。(任意的Shell命令) 这是一个文件的依赖关系,也就是说,target这一个或多个的目标文件依赖于prerequisites中的文件,其生成规则定义在command中。说白一点就是说,prerequisites 中如果有一个以上的文件比target文件要新的话,command所定义的命令就会被执行。这就是Makefile的规则。也就是Makefile中最核心的内容。 说到底,Makefile的东西就是这样一点,好像我的这篇文档也该结束了。呵呵。还不尽然,这是Makefile的主线和核心,但要写好一个Makefile还不够,我会以后面一点一点地结合我的工作经验给你慢慢到来。内容还多着呢。 二、一个示例正如前面所说的,如果一个工程有3个头文件,和8个C文件,我们为了完成前面所述的那三个规则,我们的Makefile应该是下面的这个样子的。 1234567891011121314151617181920212223edit : main.o kbd.o command.o display.o \\insert.o search.o files.o utils.occ -o edit main.o kbd.o command.o display.o \\insert.o search.o files.o utils.omain.o : main.c defs.hcc -c main.ckbd.o : kbd.c defs.h command.hcc -c kbd.ccommand.o : command.c defs.h command.hcc -c command.cdisplay.o : display.c defs.h buffer.hcc -c display.cinsert.o : insert.c defs.h buffer.hcc -c insert.csearch.o : search.c defs.h buffer.hcc -c search.cfiles.o : files.c defs.h buffer.h command.hcc -c files.cutils.o : utils.c defs.hcc -c utils.cclean :rm edit main.o kbd.o command.o display.o \\insert.o search.o files.o utils.o 反斜杠()是换行符的意思。这样比较便于Makefile的易读。我们可以把这个内容保存在文件为“Makefile”或“makefile”的文件中,然后在该目录下直接输入命令“make”就可以生成执行文件edit。如果要删除执行文件和所有的中间目标文件,那么,只要简单地执行一下“make clean”就可以了。 在这个makefile中,目标文件(target)包含:执行文件edit和中间目标文件(*.o),依赖文件(prerequisites)就是冒号后面的那些.c 文件和.h文件。每一个o 每个文件都有一组依赖文件,而这些 .o 文件又是执行文件 edit 的依赖文件。依赖关系的实质上就是说明了目标文件是由哪些文件生成的,换言之,目标文件是由哪些文件更新的。 在定义好依赖关系后,后续的那一行定义了如何生成目标文件的操作系统命令,一定要以一个Tab键作为开头。记住,make并不管命令是怎么工作的,他只管执行所定义的命令。make会比较targets文件和prerequisites文件的修改日期,如果prerequisites文件的日期要比targets文件的日期要新,或者target不存在的话,那么,make就会执行后续定义的命令。 这里要说明一点的是,clean不是一个文件,它只不过是一个动作名字,有点像C语言中的lable一样,其冒号后什么也没有,那么,make就不会自动去找文件的依赖性,也就不会自动执行其后所定义的命令。要执行其后的命令,就要在make命令后明显地指出这个lable的名字。这样的方法非常有用,我们可以在一个makefile中定义不用的编译或是和编译无关的命令,比如程序的打包,程序的备份等等。 三、make是如何工作的在默认的方式下,也就是我们只输入make命令。那么, 1、make会在当前目录下找名字叫“Makefile”或“makefile”的文件。 2、如果找到,它会找文件中的第一个目标文件(target),在上面的例子中,他会找到“edit”这个文件,并把这个文件作为最终的目标文件。 3、如果edit文件不存在,或是edit所依赖的后面的 .o 文件的文件修改时间要比edit 这个文件新,那么,他就会执行后面所定义的命令来生成edit这个文件。 4、如果edit所依赖的.o文件也存在,那么make会在当前文件中找目标为.o文件的依赖性,如果找到则再根据那一个规则生成.o文件。(这有点像一个堆栈的过程) 5、当然,你的C文件和H文件是存在的啦,于是make会生成 .o 文件,然后再用 .o 文件生命make的终极任务,也就是执行文件edit了。 这就是整个make的依赖性,make会一层又一层地去找文件的依赖关系,直到最终编译 出第一个目标文件。在找寻的过程中,如果出现错误,比如最后被依赖的文件找不到,那么make就会直接退出,并报错,而对于所定义的命令的错误,或是编译不成功,make根本不理。make只管文件的依赖性,即,如果在我找了依赖关系之后,冒号后面的文件还是不在,那么对不起,我就不工作啦。 通过上述分析,我们知道,像clean这种,没有被第一个目标文件直接或间接关联,那么它后面所定义的命令将不会被自动执行,不过,我们可以显示要make执行。即命令——“make clean”,以此来清除所有的目标文件,以便重编译。 于是在我们编程中,如果这个工程已被编译过了,当我们修改了其中一个源文件,比如file.c,那么根据我们的依赖性,我们的目标file.o会被重编译(也就是在这个依性关系后面所定义的命令),于是file.o的文件也是最新的啦,于是file.o的文件修改时间要比edit要新,所以edit也会被重新链接了(详见edit目标文件后定义的命令)。 而如果我们改变了“command.h”,那么,kdb.o、command.o和files.o都会被重编译,并且,edit会被重链接。 四、makefile中使用变量在上面的例子中,先让我们看看edit的规则: 1234edit : main.o kbd.o command.o display.o \\insert.o search.o files.o utils.occ -o edit main.o kbd.o command.o display.o \\insert.o search.o files.o utils.o 我们可以看到[.o]文件的字符串被重复了两次,如果我们的工程需要加入一个新的[.o]文件,那么我们需要在两个地方加(应该是三个地方,还有一个地方在clean中)。当然,我们的makefile并不复杂,所以在两个地方加也不累,但如果makefile变得复杂,那么我们就有可能会忘掉一个需要加入的地方,而导致编译失败。所以,为了makefile的易维护,在makefile中我们可以使用变量。makefile的变量也就是一个字符串,理解成C语言中的宏可能会更好。 比如,我们声明一个变量,叫objects, OBJECTS, objs, OBJS, obj, 或是 OBJ,反正不管什么啦,只要能够表示obj文件就行了。我们在makefile一开始就这样定义: 12objects = main.o kbd.o command.o display.o \\insert.o search.o files.o utils.o 于是,我们就可以很方便地在我们的makefile中以“$(objects)”的方式来使用这个变量了,于是我们的改良版makefile就变成下面这个样子: 123456789101112131415161718192021insert.o search.o files.o utils.oedit : $(objects)cc -o edit $(objects)main.o : main.c defs.hcc -c main.ckbd.o : kbd.c defs.h command.hcc -c kbd.ccommand.o : command.c defs.h command.hcc -c command.cdisplay.o : display.c defs.h buffer.hcc -c display.cinsert.o : insert.c defs.h buffer.hcc -c insert.csearch.o : search.c defs.h buffer.hcc -c search.cfiles.o : files.c defs.h buffer.h command.hcc -c files.cutils.o : utils.c defs.hcc -c utils.cclean :rm edit $(objects) 于是如果有新的 .o 文件加入,我们只需简单地修改一下 objects 变量就可以了。 关于变量更多的话题,我会在后续给你一一道来。 五、让make自动推导GNU的make很强大,它可以自动推导文件以及文件依赖关系后面的命令,于是我们就没必要去在每一个[.o]文件后都写上类似的命令,因为,我们的make会自动识别,并自己推导命令。 只要make看到一个[.o]文件,它就会自动地把[.c]文件加在依赖关系中,如果make 找到一个whatever.o,那么whatever.c,就会是whatever.o的依赖文件。并且 cc -c whatever.c 也会被推导出来,于是,我们的makefile再也不用写得这么复杂。我们的是新的makefile又出炉了。 1234567891011121314insert.o search.o files.o utils.oedit : $(objects)cc -o edit $(objects)main.o : defs.hkbd.o : defs.h command.hcommand.o : defs.h command.hdisplay.o : defs.h buffer.hinsert.o : defs.h buffer.hsearch.o : defs.h buffer.hfiles.o : defs.h buffer.h command.hutils.o : defs.h.PHONY : cleanclean :rm edit $(objects) 这种方法,也就是make得“隐晦规则”。上面文件内容中,“.PHONY”表示,clean 是个伪目标文件。 关于更为详细的“隐晦规则”和“伪目标文件”,我会在后续给你一一道来。 六、另类风格的makefile 既然我们的make可以自动推导命令,那么我看到那堆[.o]和[.h]的依赖就有点不爽,那么多的重复的[.h],能不能把其收拢起来,好吧,没有问题,这个对于make来说很容易,谁叫它提供了自动推导命令和文件的功能呢?来看看最新风格的makefile吧。 12345678910objects = main.o kbd.o command.o display.o \\insert.o search.o files.o utils.oedit : $(objects)cc -o edit $(objects)$(objects) : defs.hkbd.o command.o files.o : command.hdisplay.o insert.o search.o files.o : buffer.h.PHONY : cleanclean :rm edit $(objects) 这种风格,让我们的makefile变得很简单,但我们的文件依赖关系就显得有点凌乱了。鱼和熊掌不可兼得。还看你的喜好了。我是不喜欢这种风格的,一是文件的依赖关系看不清楚,二是如果文件一多,要加入几个新的.o文件,那就理不清楚了。 七、清空目标文件的规则每个Makefile中都应该写一个清空目标文件(.o和执行文件)的规则,这不仅便于重编译,也很利于保持文件的清洁。这是一个“修养”(呵呵,还记得我的《编程修养》吗)。一般的风格都是这样的: 123456clean:rm edit $(objects)更为稳健的做法是:.PHONY : cleanclean :-rm edit $(objects) 前面说过,.PHONY意思表示clean是一个“伪目标”,。而在rm命令前面加了一个小减号的意思就是,也许某些文件出现问题,但不要管,继续做后面的事。当然,clean的规则不要放在文件的开头,不然,这就会变成make的默认目标,相信谁也不愿意这样。不成文的规矩是——“clean从来都是放在文件的最后”。 上面就是一个makefile的概貌,也是makefile的基础,下面还有很多makefile的相关细节,准备好了吗?准备好了就来。 四部分、Makefile 总述一、Makefile里有什么?Makefile里主要包含了五个东西:显式规则、隐晦规则、变量定义、文件指示和注释。 1、显式规则。 显式规则说明了,如何生成一个或多的的目标文件。这是由Makefile的书写者明显指出,要生成的文件,文件的依赖文件,生成的命令。 2、隐晦规则。 由于我们的make有自动推导的功能,所以隐晦的规则可以让我们比较粗糙地简略地书写Makefile,这是由make所支持的。 3、变量的定义。 在Makefile中我们要定义一系列的变量,变量一般都是字符串,这个有点你C语言中的宏,当Makefile被执行时,其中的变量都会被扩展到相应的引用位置上。 4、文件指示。 其包括了三个部分,一个是在一个Makefile中引用另一个Makefile,就像C语言中的include一样;另一个是指根据某些情况指定Makefile中的有效部分,就像C语言中的预编译#if一样;还有就是定义一个多行的命令。有关这一部分的内容,我会在后续的部分中讲述。 5、注释。 Makefile中只有行注释,和UNIX的Shell脚本一样,其注释是用“#”字符,这个就像C&#x2F;C++中的“&#x2F;&#x2F;”一样。如果你要在你的Makefile中使用“#”字符,可以用反斜框进行转义,如:“#”。 最后,还值得一提的是,在Makefile中的命令,必须要以[Tab]键开始。 二、Makefile的文件名默认的情况下,make命令会在当前目录下按顺序找寻文件名为“GNUmakefile”、“makefile”、“Makefile”的文件,找到了解释这个文件。在这三个文件名中,最好使用“Makefile”这个文件名,因为,这个文件名第一个字符为大写,这样有一种显目的感觉。最好不要用“GNUmakefile”,这个文件是GNU的make识别的。有另外一些make只对全小写的“makefile”文件名敏感,但是基本上来说,大多数的make都支持“makefile”和“Makefile”这两种默认文件名。 当然,你可以使用别的文件名来书写Makefile,比如:“Make.Linux”,“Make.Solaris”,“Make.AIX”等,如果要指定特定的Makefile,你可以使用make的“-f”和“–file”参数,如:make -f Make.Linux或make –file Make.AIX。 三、引用其它的Makefile在Makefile使用include关键字可以把别的Makefile包含进来,这很像C语言的#include,被包含的文件会原模原样的放在当前文件的包含位置。include的语法是: include filename可以是当前操作系统Shell的文件模式(可以保含路径和通配符) 在include 前面可以有一些空字符,但是绝不能是[Tab]键开始。include和可以用一个或多个空格隔开。举个例子,你有这样几个Makefile:a.mk、b.mk、c.mk,还有一个文件叫foo.make,以及一个变量$(bar),其包含了e.mk和f.mk,那么,下面的语句: include foo.make *.mk $(bar) 等价于: include foo.make a.mk b.mk c.mk e.mk f.mk make命令开始时,会把找寻include所指出的其它Makefile,并把其内容安置在当前的位。就好像C&#x2F;C++的#include指令一样。如果文件都没有指定绝对路径或是相对路径的话, make会在当前目录下首先寻找,如果当前目录下没有找到,那么,make还会在下面的几个目录下找: 1、如果make执行时,有“-I”或“–include-dir”参数,那么make就会在这个参数 所指定的目录下去寻找。 2、如果目录&#x2F;include(一般是:&#x2F;usr&#x2F;local&#x2F;bin或&#x2F;usr&#x2F;include)存在的话, make也会去找。如果有文件没有找到的话,make会生成一条警告信息,但不会马上出现致命错误。它会继续载入其它的文件,一旦完成makefile的读取,make会再重试这些没有找到,或是不能读取的文件,如果还是不行,make才会出现一条致命信息。如果你想让make 不理那些无法读取的文件,而继续执行,你可以在include前加一个减号“-”。 如: -include 其表示,无论include过程中出现什么错误,都不要报错继续执行。和其它版本make 兼 容的相关命令是sinclude,其作用和这一个是一样的。 四、环境变量 MAKEFILES如果你的当前环境中定义了环境变量MAKEFILES,那么,make会把这个变量中的值做一个类似于include的动作。这个变量中的值是其它的Makefile,用空格分隔。只是,它和include 不同的是,从这个环境变中引入的Makefile的“目标”不会起作用,如果环境变量中定义的文件发现错误,make也会不理。 但是在这里我还是建议不要使用这个环境变量,因为只要这个变量一被定义,那么当你使用make时,所有的Makefile都会受到它的影响,这绝不是你想看到的。在这里提这个事,只是为了告诉大家,也许有时候你的Makefile出现了怪事,那么你可以看看当前环境中有没有定义这个变量。 五、make的工作方式GNU的make工作时的执行步骤入下:(想来其它的make也是类似) 1、读入所有的Makefile。 2、读入被include的其它Makefile。 3、初始化文件中的变量。 4、推导隐晦规则,并分析所有规则。 5、为所有的目标文件创建依赖关系链。 6、根据依赖关系,决定哪些目标要重新生成。 7、执行生成命令。 1-5步为第一个阶段,6-7为第二个阶段。第一个阶段中,如果定义的变量被使用了,那么, make会把其展开在使用的位置。但make并不会完全马上展开,make使用的是拖延战术,如果变量出现在依赖关系的规则中,那么仅当这条依赖被决定要使用了,变量才会在其内部展开。 当然,这个工作方式你不一定要清楚,但是知道这个方式你也会对make更为熟悉。有了这个基础,后续部分也就容易看懂了。 第五部分、书写规则规则包含两个部分,一个是依赖关系,一个是生成目标的方法。 在Makefile中,规则的顺序是很重要的,因为,Makefile中只应该有一个最终目标,其它的目标都是被这个目标所连带出来的,所以一定要让make知道你的最终目标是什么。一般来说,定义在Makefile中的目标可能会有很多,但是第一条规则中的目标将被确立为最终的目标。如果第一条规则中的目标有很多个,那么,第一个目标会成为最终的目标。make 所完成的也就是这个目标。 好了,还是让我们来看一看如何书写规则。 一、规则举例12foo.o : foo.c defs.h # foo模块cc -c -g foo.c 看到这个例子,各位应该不是很陌生了,前面也已说过,foo.o是我们的目标,foo.c 和defs.h是目标所依赖的源文件,而只有一个命令“cc -c -g foo.c”(以Tab键开头)。这个规则告诉我们两件事: 1、文件的依赖关系,foo.o依赖于foo.c和defs.h的文件,如果foo.c和defs.h的文件日期要比foo.o文件日期要新,或是foo.o不存在,那么依赖关系发生。 2、如果生成(或更新)foo.o文件。也就是那个cc命令,其说明了,如何生成foo.o 这个文件。(当然foo.c文件include了defs.h文件) 二、规则的语法123targets : prerequisitescommand... 或是这样: 123targets : prerequisites ; commandcommand... targets是文件名,以空格分开,可以使用通配符。一般来说,我们的目标基本上是一个文件,但也有可能是多个文件。 command是命令行,如果其不与“target:prerequisites”在一行,那么,必须以[Tab 键]开头,如果和prerequisites在一行,那么可以用分号做为分隔。(见上) prerequisites 也就是目标所依赖的文件(或依赖目标)。如果其中的某个文件要比目标文件要新,那么,目标就被认为是“过时的”,被认为是需要重生成的。这个在前面已经讲过了。 如果命令太长,你可以使用反斜框(‘\\’)作为换行符。make对一行上有多少个字符没有限制。规则告诉make两件事,文件的依赖关系和如何成成目标文件。 一般来说,make会以UNIX的标准Shell,也就是&#x2F;bin&#x2F;sh来执行命令。 三、在规则中使用通配符如果我们想定义一系列比较类似的文件,我们很自然地就想起使用通配符。make支持三各通配符:“*”,“?”和“[…]”。这是和Unix的B-Shell是相同的。 波浪号(“”)字符在文件名中也有比较特殊的用途。如果是“&#x2F;test”,这就表示当前用户的$HOME目录下的test目录。而“~hchen&#x2F;test”则表示用户hchen的宿主目录下的test目录。(这些都是Unix下的小知识了,make也支持)而在Windows或是MS-DOS下,用户没有宿主目录,那么波浪号所指的目录则根据环境变量“HOME”而定。 通配符代替了你一系列的文件,如“*.c”表示所以后缀为c的文件。一个需要我们注意的是,如果我们的文件名中有通配符,如:“”,那么可以用转义字符“\\”,如“*”来表示真实的“”字符,而不是任意长度的字符串。 好吧,还是先来看几个例子吧: 12clean:rm -f *.o 上面这个例子我不不多说了,这是操作系统Shell所支持的通配符。这是在命令中的通配符。 123print: *.clpr -p $?touch print 上面这个例子说明了通配符也可以在我们的规则中,目标print依赖于所有的[.c]文件。其中的“$?”是一个自动化变量,我会在后面给你讲述。 1objects = *.o 上面这个例子,表示了,通符同样可以用在变量中。并不是说[.o]会展开,不!objects 的值就是“.o”。Makefile中的变量其实就是C&#x2F;C++中的宏。如果你要让通配符在变量中展开,也就是让objects的值是所有[.o]的文件名的集合,那么,你可以这样: 1objects := $(wildcard *.o) 这种用法由关键字“wildcard”指出,关于Makefile的关键字,我们将在后面讨论。 四、文件搜寻在一些大的工程中,有大量的源文件,我们通常的做法是把这许多的源文件分类,并存放在不同的目录中。所以,当make需要去找寻文件的依赖关系时,你可以在文件前加上路径,但最好的方法是把一个路径告诉make,让make在自动去找。 Makefile文件中的特殊变量“VPATH”就是完成这个功能的,如果没有指明这个变量, make只会在当前的目录中去找寻依赖文件和目标文件。如果定义了这个变量,那么,make 就会在当当前目录找不到的情况下,到所指定的目录中去找寻文件了。 1VPATH = src:../headers 上面的的定义指定两个目录,“src”和“..&#x2F;headers”,make会按照这个顺序进行搜索。目录由“冒号”分隔。(当然,当前目录永远是最高优先搜索的地方) 另一个设置文件搜索路径的方法是使用make的“vpath”关键字(注意,它是全小写的),这不是变量,这是一个make的关键字,这和上面提到的那个VPATH变量很类似,但是它更为灵活。它可以指定不同的文件在不同的搜索目录中。这是一个很灵活的功能。它的使用方法有三种: 1、vpath 为符合模式的文件指定搜索目录。 2、vpath 清除符合模式的文件的搜索目录。 3、vpath 清除所有已被设置好了的文件搜索目录。 vapth使用方法中的需要包含“%”字符。“%”的意思是匹配零或若干字符,例如,“%.h”表示所有以“.h”结尾的文件。指定了要搜索的文件集,而则指定了的文件集的搜索的目录。例如: vpath %.h ..&#x2F;headers 该语句表示,要求make在“..&#x2F;headers”目录下搜索所有以“.h”结尾的文件。(如果某文件在当前目录没有找到的话) 我们可以连续地使用vpath语句,以指定不同搜索策略。如果连续的vpath语句中出现了相同的,或是被重复了的,那么,make会按照vpath语句的先后顺序来执行搜索。如: 123vpath %.c foovpath % blishvpath %.c bar 其表示“.c”结尾的文件,先在“foo”目录,然后是“blish”,最后是“bar”目录。 12vpath %.c foo:barvpath % blish 而上面的语句则表示“.c”结尾的文件,先在“foo”目录,然后是“bar”目录,最后才是“blish”目录。 五、伪目标最早先的一个例子中,我们提到过一个“clean”的目标,这是一个“伪目标”, 12clean:rm *.o temp 正像我们前面例子中的“clean”一样,即然我们生成了许多文件编译文件,我们也应该提供一个清除它们的“目标”以备完整地重编译而用。 (以“make clean”来使用该目标) 因为,我们并不生成“clean”这个文件。“伪目标”并不是一个文件,只是一个标签,由于“伪目标”不是文件,所以make无法生成它的依赖关系和决定它是否要执行。我们只有通过显示地指明这个“目标”才能让其生效。当然,“伪目标”的取名不能和文件名重名,不然其就失去了“伪目标”的意义了。 当然,为了避免和文件重名的这种情况,我们可以使用一个特殊的标记“.PHONY”来显示地指明一个目标是“伪目标”,向make说明,不管是否有这个文件,这个目标就是“伪目标”。 1.PHONY : clean 只要有这个声明,不管是否有“clean”文件,要运行“clean”这个目标,只有“make clean”这样。于是整个过程可以这样写: 123.PHONY: cleanclean:rm *.o temp 伪目标一般没有依赖的文件。但是,我们也可以为伪目标指定所依赖的文件。伪目标同样可以作为“默认目标”,只要将其放在第一个。一个示例就是,如果你的Makefile需要一口气生成若干个可执行文件,但你只想简单地敲一个make完事,并且,所有的目标文件都写在一个Makefile中,那么你可以使用“伪目标”这个特性: 12345678all : prog1 prog2 prog3.PHONY : allprog1 : prog1.o utils.occ -o prog1 prog1.o utils.oprog2 : prog2.occ -o prog2 prog2.oprog3 : prog3.o sort.o utils.occ -o prog3 prog3.o sort.o utils.o 我们知道,Makefile中的第一个目标会被作为其默认目标。我们声明了一个“all”的伪目标,其依赖于其它三个目标。由于伪目标的特性是,总是被执行的,所以其依赖的那三个目标就总是不如“all”这个目标新。所以,其它三个目标的规则总是会被决议。也就达到了我们一口气生成多个目标的目的。“.PHONY : all”声明了“all”这个目标为“伪目标”。 随便提一句,从上面的例子我们可以看出,目标也可以成为依赖。所以,伪目标同样也可以成为依赖。看下面的例子: 1234567891011.PHONY: cleanall cleanobj cleandiffcleanall : cleanobj cleandiffrm programcleanobj :rm *.ocleandiff :rm *.diff“make clean”将清除所有要被清除的文件。“cleanobj”和“cleandiff”这两个伪目标有点像“子程序”的意思。我们可以输入“make cleanall”和“make cleanobj”和“make cleandiff”命令来达到清除不同种类文件的目的。 六、多目标Makefile的规则中的目标可以不止一个,其支持多目标,有可能我们的多个目标同时依赖于一个文件,并且其生成的命令大体类似。于是我们就能把其合并起来。当然,多个目标的生成规则的执行命令是同一个,这可能会可我们带来麻烦,不过好在我们的可以使用一个自动化变量“$@”(关于自动化变量,将在后面讲述),这个变量表示着目前规则中所有的目标的集合,这样说可能很抽象,还是看一个例子吧。 12bigoutput littleoutput : text.ggenerate text.g -$(subst output,,$@) &gt; $@ 上述规则等价于: 1234bigoutput : text.ggenerate text.g -big &gt; bigoutputlittleoutput : text.ggenerate text.g -little &gt; littleoutput 其中,-$(subst output,,$@)中的“$”表示执行一个Makefile的函数,函数名为subst,后面的为参数。关于函数,将在后面讲述。这里的这个函数是截取字符串的意思,“$@”表示目标的集合,就像一个数组,“$@”依次取出目标,并执于命令。 七、静态模式静态模式可以更加容易地定义多目标的规则,可以让我们的规则变得更加的有弹性和灵活 我们还是先来看一下语法: 123&lt;targets ...&gt;: &lt;target-pattern&gt;: &lt;prereq-patterns ...&gt;&lt;commands&gt;.... targets定义了一系列的目标文件,可以有通配符。是目标的一个集合。 target-parrtern是指明了targets的模式,也就是的目标集模式。 prereq-parrterns是目标的依赖模式,它对target-parrtern形成的模式再进行一次依赖目标的定义。 这样描述这三个东西,可能还是没有说清楚,还是举个例子来说明一下吧。如果我们的 定义成“%.o”,意思是我们的集合中都是以“.o”结尾的,而如果我们的定义成“%.c”,意思是对所形成的目标集进行二次定义,其计算方法是,取模式中的“%”(也就是去掉了[.o]这个结尾),并为其加上[.c]这个结尾,形成的新集合。 所以,我们的“目标模式”或是“依赖模式”中都应该有“%”这个字符,如果你的文件名中有“%”那么你可以使用反斜杠“\\”进行转义,来标明真实的“%”字符。 看一个例子: 1234objects = foo.o bar.oall: $(objects)$(objects): %.o: %.c$(CC) -c $(CFLAGS) $&lt; -o $@ 上面的例子中,指明了我们的目标从$object中获取,“%.o”表明要所有以“.o”结尾的目标,也就是“foo.o bar.o”,也就是变量$object集合的模式,而依赖模式“%.c”则取模式“%.o”的“%”,也就是“foo bar”,并为其加下“.c”的后缀,于是,我们的依赖目标就是“foo.c bar.c”。而命令中的“$&lt;”和“$@”则是自动化变量,“$&lt;”表示所有的依赖目标集(也就是“foo.c bar.c”),“$@”表示目标集(也就是“foo.o bar.o”)。于是,上面的规则展开后等价于下面的规则: 1234foo.o : foo.c$(CC) -c $(CFLAGS) foo.c -o foo.obar.o : bar.c$(CC) -c $(CFLAGS) bar.c -o bar.o 试想,如果我们的“%.o”有几百个,那种我们只要用这种很简单的“静态模式规则”就可以写完一堆规则,实在是太有效率了。“静态模式规则”的用法很灵活,如果用得好,那会一个很强大的功能。再看一个例子: 12345files = foo.elc bar.o lose.o$(filter %.o,$(files)): %.o: %.c$(CC) -c $(CFLAGS) $&lt; -o $@$(filter %.elc,$(files)): %.elc: %.elemacs -f batch-byte-compile $&lt; $(filter %.o,$(files))表示调用Makefile的filter函数,过滤“$filter”集,只要其中模式为“%.o”的内容。其的它内容,我就不用多说了吧。这个例字展示了Makefile中更大的弹性。 八、自动生成依赖性在Makefile中,我们的依赖关系可能会需要包含一系列的头文件,比如,如果我们的main.c 中有一句“#include “defs.h””,那么我们的依赖关系应该是: main.o : main.c defs.h 但是,如果是一个比较大型的工程,你必需清楚哪些C文件包含了哪些头文件,并且,你在加入或删除头文件时,也需要小心地修改Makefile,这是一个很没有维护性的工作。为了避免这种繁重而又容易出错的事情,我们可以使用C&#x2F;C++编译的一个功能。大多数的C&#x2F;C++编译器都支持一个“-M”的选项,即自动找寻源文件中包含的头文件,并生成一个依赖关系。例如,如果我们执行下面的命令: 1cc -M main.c 其输出是: 1main.o : main.c defs.h 于是由编译器自动生成的依赖关系,这样一来,你就不必再手动书写若干文件的依赖关系,而由编译器自动生成了。需要提醒一句的是,如果你使用GNU的C&#x2F;C++编译器,你得用“-MM”参数,不然,“-M”参数会把一些标准库的头文件也包含进来。 gcc -M main.c的输出是: 1234567891011main.o: main.c defs.h /usr/include/stdio.h /usr/include/features.h \\/usr/include/sys/cdefs.h /usr/include/gnu/stubs.h \\/usr/lib/gcc-lib/i486-suse-linux/2.95.3/include/stddef.h \\/usr/include/bits/types.h /usr/include/bits/pthreadtypes.h \\/usr/include/bits/sched.h /usr/include/libio.h \\/usr/include/_G_config.h /usr/include/wchar.h \\/usr/include/bits/wchar.h /usr/include/gconv.h \\/usr/lib/gcc-lib/i486-suse-linux/2.95.3/include/stdarg.h \\/usr/include/bits/stdio_lim.hgcc -MM main.c的输出则是:main.o: main.c defs.h 那么,编译器的这个功能如何与我们的Makefile联系在一起呢。因为这样一来,我们的Makefile也要根据这些源文件重新生成,让Makefile自已依赖于源文件?这个功能并不现实,不过我们可以有其它手段来迂回地实现这一功能。GNU组织建议把编译器为每一个源 文件的自动生成的依赖关系放到一个文件中,为每一个“name.c”的文件都生成一个“name.d”的Makefile文件,[.d]文件中就存放对应[.c]文件的依赖关系。于是,我们可以写出[.c]文件和[.d]文件的依赖关系,并让make自动更新或自成[.d]文件,并把其包含在我们的主Makefile中,这样,我们就可以自动化地生成每个文件的依赖关系了。 这里,我们给出了一个模式规则来产生[.d]文件: 12345%.d: %.c@set -e; rm -f $@; \\$(CC) -M $(CPPFLAGS) $&lt; &gt; $@.$$$$; \\sed &#x27;s,\\($*\\)\\.o[ :]*,\\1.o $@ : ,g&#x27; &lt; $@.$$$$ &gt; $@; \\rm -f $@.$$$$ 这个规则的意思是,所有的[.d]文件依赖于[.c]文件,“rm -f $@”的意思是删除所有的目标,也就是[.d]文件,第二行的意思是,为每个依赖文件“$&lt;”,也就是[.c]文件生成依赖文件,“$@”表示模式“%.d”文件,如果有一个C文件是name.c,那么“%”就是“name”,“$$$$”意为一个随机编号,第二行生成的文件有可能是“name.d.12345”,第三行使用sed命令做了一个替换,关于sed命令的用法请参看相关的使用文档。第四行就是删除临时文件。 总而言之,这个模式要做的事就是在编译器生成的依赖关系中加入[.d]文件的依赖,即把依赖关系: 1main.o : main.c defs.h 转成: 1main.o main.d : main.c defs.h 于是,我们的[.d]文件也会自动更新了,并会自动生成了,当然,你还可以在这个[.d]文件中加入的不只是依赖关系,包括生成的命令也可一并加入,让每个[.d]文件都包含一个完赖的规则。一旦我们完成这个工作,接下来,我们就要把这些自动生成的规则放进我们的主Makefile中。我们可以使用Makefile的“include”命令,来引入别的Makefile文件(前面讲过),例如: 12sources = foo.c bar.cinclude $(sources:.c=.d) 上述语句中的“$(sources:.c&#x3D;.d)”中的“.c&#x3D;.d”的意思是做一个替换,把变量$(sources)所有[.c]的字串都替换成[.d],关于这个“替换”的内容,在后面我会有更为详细的讲述。当然,你得注意次序,因为include是按次来载入文件,最先载入的[.d]文件中的目标会成为默认目标。 第六部分书写命令每条规则中的命令和操作系统Shell的命令行是一致的。make会一按顺序一条一条的执行命令,每条命令的开头必须以[Tab]键开头,除非,命令是紧跟在依赖规则后面的分号后的。在命令行之间中的空格或是空行会被忽略,但是如果该空格或空行是以Tab键开头的,那么make会认为其是一个空命令。 我们在UNIX下可能会使用不同的Shell,但是make的命令默认是被“&#x2F;bin&#x2F;sh”——UNIX的标准Shell解释执行的。除非你特别指定一个其它的Shell。Makefile中,“#”是注释符,很像C&#x2F;C++中的“&#x2F;&#x2F;”,其后的本行字符都被注释。 一、显示命令通常,make会把其要执行的命令行在命令执行前输出到屏幕上。当我们用“@”字符在命令行前,那么,这个命令将不被make显示出来,最具代表性的例子是,我们用这个功能来像屏幕显示一些信息。如: @echo 正在编译XXX模块…… 当make执行时,会输出“正在编译XXX模块……”字串,但不会输出命令,如果没有“@”,那么,make将输出: echo 正在编译XXX模块…… 正在编译XXX模块…… 如果make执行时,带入make参数“-n”或“–just-print”,那么其只是显示命令,但不会执行命令,这个功能很有利于我们调试我们的Makefile,看看我们书写的命令是执行起来是什么样子的或是什么顺序的。 而make参数“-s”或“–slient”则是全面禁止命令的显示。 二、命令执行当依赖目标新于目标时,也就是当规则的目标需要被更新时,make会一条一条的执行其后的命令。需要注意的是,如果你要让上一条命令的结果应用在下一条命令时,你应该使用分号分隔这两条命令。比如你的第一条命令是cd命令,你希望第二条命令得在cd之后的基础上运行,那么你就不能把这两条命令写在两行上,而应该把这两条命令写在一行上,用分号分隔。如: 示例一: 123exec:cd /home/hchenpwd 示例二: 12exec:cd /home/hchen; pwd 当我们执行“make exec”时,第一个例子中的cd没有作用,pwd会打印出当前的Makefile目录,而第二个例子中,cd就起作用了,pwd会打印出“&#x2F;home&#x2F;hchen”。make 一般是使用环境变量SHELL中所定义的系统Shell来执行命令,默认情况下使用UNIX的 标准Shell——&#x2F;bin&#x2F;sh来执行命令。但在MS-DOS下有点特殊,因为MS-DOS下没有SHELL 环境变量,当然你也可以指定。如果你指定了UNIX风格的目录形式,首先,make会在SHELL 所指定的路径中找寻命令解释器,如果找不到,其会在当前盘符中的当前目录中寻找,如果再找不到,其会在PATH环境变量中所定义的所有路径中寻找。MS-DOS中,如果你定义的命令解释器没有找到,其会给你的命令解释器加上诸如“.exe”、“.com”、“.bat”、“.sh”等后缀。 三、命令出错每当命令运行完后,make会检测每个命令的返回码,如果命令返回成功,那么make会执行下一条命令,当规则中所有的命令成功返回后,这个规则就算是成功完成了。如果一个规则中的某个命令出错了(命令退出码非零),那么make就会终止执行当前规则,这将有可能终止所有规则的执行。 有些时候,命令的出错并不表示就是错误的。例如mkdir命令,我们一定需要建立一个目录,如果目录不存在,那么mkdir就成功执行,万事大吉,如果目录存在,那么就出错了。我们之所以使用mkdir的意思就是一定要有这样的一个目录,于是我们就不希望mkdir出错而终止规则的运行。 为了做到这一点,忽略命令的出错,我们可以在Makefile的命令行前加一个减号“-”(在Tab键之后),标记为不管命令出不出错都认为是成功的。如: 12clean:-rm -f *.o 还有一个全局的办法是,给make加上“-i”或是“–ignore-errors”参数,那么, Makefile中所有命令都会忽略错误。而如果一个规则是以“.IGNORE”作为目标的,那么这个规则中的所有命令将会忽略错误。这些是不同级别的防止命令出错的方法,你可以根据你的不同喜欢设置。 还有一个要提一下的make的参数的是“-k”或是“–keep-going”,这个参数的意思是,如果某规则中的命令出错了,那么就终目该规则的执行,但继续执行其它规则。 四、嵌套执行make在一些大的工程中,我们会把我们不同模块或是不同功能的源文件放在不同的目录中,我们可以在每个目录中都书写一个该目录的Makefile,这有利于让我们的Makefile变得更加地简洁,而不至于把所有的东西全部写在一个Makefile中,这样会很难维护我们的Makefile,这个技术对于我们模块编译和分段编译有着非常大的好处。 例如,我们有一个子目录叫subdir,这个目录下有个Makefile文件,来指明了这个目录下文件的编译规则。那么我们总控的Makefile可以这样书写: 12subsystem:cd subdir &amp;&amp; $(MAKE) 其等价于: 12subsystem:$(MAKE) -C subdir 定义$(MAKE)宏变量的意思是,也许我们的make需要一些参数,所以定义成一个变量比较利于维护。这两个例子的意思都是先进入“subdir”目录,然后执行make命令。 我们把这个Makefile叫做“总控Makefile”,总控Makefile的变量可以传递到下级的Makefile中(如果你显示的声明),但是不会覆盖下层的Makefile中所定义的变量,除非指定了“-e”参数。 如果你要传递变量到下级Makefile中,那么你可以使用这样的声明: export &lt;variable …&gt; 如果你不想让某些变量传递到下级Makefile中,那么你可以这样声明: unexport &lt;variable …&gt; 如: 示例一: 1export variable = value 其等价于: 12variable = valueexport variable 其等价于: 1export variable := value 其等价于: 12variable := valueexport variable 示例二: 1export variable += value 其等价于: 12variable += valueexport variable 如果你要传递所有的变量,那么,只要一个export就行了。后面什么也不用跟,表示传递所有的变量。 需要注意的是,有两个变量,一个是SHELL,一个是MAKEFLAGS,这两个变量不管你是否export,其总是要传递到下层Makefile中,特别是MAKEFILES变量,其中包含了make 的参数信息,如果我们执行“总控Makefile”时有make参数或是在上层Makefile中定义了这个变量,那么MAKEFILES变量将会是这些参数,并会传递到下层Makefile中,这是一个系统级的环境变量。 但是make命令中的有几个参数并不往下传递,它们是“-C”,“-f”,“-h”“-o”和“-W”(有关Makefile参数的细节将在后面说明),如果你不想往下层传递参数,那么,你可以这样来: subsystem: cd subdir &amp;&amp; $(MAKE) MAKEFLAGS&#x3D; 如果你定义了环境变量MAKEFLAGS,那么你得确信其中的选项是大家都会用到的,如果其中有“-t”,“-n”,和“-q”参数,那么将会有让你意想不到的结果,或许会让你异常地恐慌。 还有一个在“嵌套执行”中比较有用的参数,“-w”或是“–print-directory”会在make的过程中输出一些信息,让你看到目前的工作目录。比如,如果我们的下级make目录是“&#x2F;home&#x2F;hchen&#x2F;gnu&#x2F;make”,如果我们使用“make -w”来执行,那么当进入该目录时,我们会看到: make: Entering directory &#96;&#x2F;home&#x2F;hchen&#x2F;gnu&#x2F;make’. 而在完成下层make后离开目录时,我们会看到: make: Leaving directory &#96;&#x2F;home&#x2F;hchen&#x2F;gnu&#x2F;make’ 当你使用“-C”参数来指定make下层Makefile时,“-w”会被自动打开的。如果参数中有“-s”(“–slient”)或是“–no-print-directory”,那么,“-w”总是失效的。 五、定义命令包 如果Makefile中出现一些相同命令序列,那么我们可以为这些相同的命令序列定义一个变量。定义这种命令序列的语法以“define”开始,以“endef”结束,如: 1234define run-yaccyacc $(firstword $^)mv y.tab.c $@endef 这里,“run-yacc”是这个命令包的名字,其不要和Makefile中的变量重名。在“define”和“endef”中的两行就是命令序列。这个命令包中的第一个命令是运行Yacc 程序,因为Yacc程序总是生成“y.tab.c”的文件,所以第二行的命令就是把这个文件改改名字。还是把这个命令包放到一个示例中来看看吧。 foo.c : foo.y $(run-yacc) 我们可以看见,要使用这个命令包,我们就好像使用变量一样。在这个命令包的使用中,命令包“run-yacc”中的“$^”就是“foo.y”,“$@”就是“foo.c”(有关这种以“$”开头的特殊变量,我们会在后面介绍),make在执行命令包时,命令包中的每个命令会被依次独立执行。 第七部分使用变量在Makefile中的定义的变量,就像是C&#x2F;C++语言中的宏一样,他代表了一个文本字串,在Makefile中执行的时候其会自动原模原样地展开在所使用的地方。其与C&#x2F;C++所不同的是,你可以在Makefile中改变其值。在Makefile中,变量可以使用在“目标”,“依赖目标”,“命令”或是Makefile的其它部分中。 变量的命名字可以包含字符、数字,下划线(可以是数字开头),但不应该含有“:”、 “#”、“&#x3D;”或是空字符(空格、回车等)。变量是大小写敏感的,“foo”、“Foo”和“FOO”是三个不同的变量名。传统的Makefile的变量名是全大写的命名方式,但我推荐使用大小写搭配的变量名,如:MakeFlags。这样可以避免和系统的变量冲突,而发生意外的事情。 有一些变量是很奇怪字串,如“$&lt;”、“$@”等,这些是自动化变量,我会在后面介绍。 一、变量的基础变量在声明时需要给予初值,而在使用时,需要给在变量名前加上“$”符号,但最好用小括号“()”或是大括号“{}”把变量给包括起来。如果你要使用真实的“$”字符,那么你需要用“$$”来表示。 变量可以使用在许多地方,如规则中的“目标”、“依赖”、“命令”以及新的变量中。先看一个例子: 1234objects = program.o foo.o utils.oprogram : $(objects)cc -o program $(objects)$(objects) : defs.h 变量会在使用它的地方精确地展开,就像C&#x2F;C++中的宏一样,例如: 123foo = cprog.o : prog.$(foo)$(foo)$(foo) -$(foo) prog.$(foo) 展开后得到: 12prog.o : prog.ccc -c prog.c 当然,千万不要在你的Makefile中这样干,这里只是举个例子来表明Makefile中的变量在使用处展开的真实样子。可见其就是一个“替代”的原理。 另外,给变量加上括号完全是为了更加安全地使用这个变量,在上面的例子中,如果你不想给变量加上括号,那也可以,但我还是强烈建议你给变量加上括号。 二、变量中的变量在定义变量的值时,我们可以使用其它变量来构造变量的值,在Makefile中有两种方式来在用变量定义变量的值。 先看第一种方式,也就是简单的使用“&#x3D;”号,在“&#x3D;”左侧是变量,右侧是变量的值,右侧变量的值可以定义在文件的任何一处,也就是说,右侧中的变量不一定非要是已定义好的值,其也可以使用后面定义的值。如: 12345foo = $(bar)bar = $(ugh)ugh = Huh?all:echo $(foo) 我们执行“make all”将会打出变量$(foo)的值是“Huh?”( $(foo)的值是$(bar), $(bar)的值是$(ugh),$(ugh)的值是“Huh?”)可见,变量是可以使用后面的变量来定义的。 这个功能有好的地方,也有不好的地方,好的地方是,我们可以把变量的真实值推到后面来定义,如: 12CFLAGS = $(include_dirs) -Oinclude_dirs = -Ifoo -Ibar 当“CFLAGS”在命令中被展开时,会是“-Ifoo -Ibar -O”。但这种形式也有不好的地方,那就是递归定义,如: 1CFLAGS = $(CFLAGS) -O 或: 12A = $(B)B = $(A) 这会让make陷入无限的变量展开过程中去,当然,我们的make是有能力检测这样的定义,并会报错。还有就是如果在变量中使用函数,那么,这种方式会让我们的make运行时非常慢,更糟糕的是,他会使用得两个make的函数“wildcard”和“shell”发生不可预知的错误。因为你不会知道这两个函数会被调用多少次。 为了避免上面的这种方法,我们可以使用make中的另一种用变量来定义变量的方法。这种方法使用的是“:&#x3D;”操作符,如: 123x := fooy := $(x) barx := later 其等价于: 12y := foo barx := later 值得一提的是,这种方法,前面的变量不能使用后面的变量,只能使用前面已定义好了的变量。如果是这样: 12y := $(x) barx := foo 那么,y的值是“bar”,而不是“foo bar”。 上面都是一些比较简单的变量使用了,让我们来看一个复杂的例子,其中包括了make 的函数、条件表达式和一个系统变量“MAKELEVEL”的使用: 123456ifeq (0,$&#123;MAKELEVEL&#125;)cur-dir := $(shell pwd)whoami := $(shell whoami)host-type := $(shell arch)MAKE := $&#123;MAKE&#125; host-type=$&#123;host-type&#125; whoami=$&#123;whoami&#125;endif 关于条件表达式和函数,我们在后面再说,对于系统变量“MAKELEVEL”,其意思是,如果我们的make有一个嵌套执行的动作(参见前面的“嵌套使用make”),那么,这个变量会记录了我们的当前Makefile的调用层数。 下面再介绍两个定义变量时我们需要知道的,请先看一个例子,如果我们要定义一个变量,其值是一个空格,那么我们可以这样来: 12nullstring :=space := $(nullstring) # end of the line nullstring是一个Empty变量,其中什么也没有,而我们的space的值是一个空格。因为在 操作符的右边是很难描述一个空格的,这里采用的技术很管用,先用一个Empty变量来标明变量的值开始了,而后面采用“#”注释符来表示变量定义的终止,这样,我们可以定义出其值是一个空格的变量。请注意这里关于“#”的使用,注释符“#”的这种特性值得我 们注意,如果我们这样定义一个变量: 1dir := /foo/bar # directory to put the frobs in dir这个变量的值是“&#x2F;foo&#x2F;bar”,后面还跟了4个空格,如果我们这样使用这样变量来指定别的目录——“$(dir)&#x2F;file”那么就完蛋了。 还有一个比较有用的操作符是“?&#x3D;”,先看示例: 1FOO ?= bar 其含义是,如果FOO没有被定义过,那么变量FOO的值就是“bar”,如果FOO先前被定义过,那么这条语将什么也不做,其等价于: 123ifeq ($(origin FOO), undefined)FOO = barendif 三、变量高级用法这里介绍两种变量的高级使用方法,第一种是变量值的替换。我们可以替换变量中的共有的部分,其格式是“$(var:a&#x3D;b)”或是“${var:a&#x3D;b}”,其意思是,把变量“var”中所有以“a”字串“结尾”的“a”替换成“b”字串。这里的“结尾”意思是“空格”或是“结束符”。 还是看一个示例吧: 12foo := a.o b.o c.obar := $(foo:.o=.c) 这个示例中,我们先定义了一个“$(foo)”变量,而第二行的意思是把“$(foo)”中所有以“.o”字串“结尾”全部替换成“.c”,所以我们的“$(bar)”的值就是“a.c b.c c.c”。 另外一种变量替换的技术是以“静态模式”(参见前面章节)定义的,如: 12foo := a.o b.o c.obar := $(foo:%.o=%.c) 这依赖于被替换字串中的有相同的模式,模式中必须包含一个“%”字符,这个例子同样让$(bar)变量的值为“a.c b.c c.c”。 第二种高级用法是——“把变量的值再当成变量”。先看一个例子: 123x = yy = za := $($(x)) 在这个例子中,$(x)的值是“y”,所以$($(x))就是$(y),于是$(a)的值就是“z”。(注意,是“x&#x3D;y”,而不是“x&#x3D;$(y)”) 我们还可以使用更多的层次: 1234x = yy = zz = ua := $($($(x))) 这里的$(a)的值是“u”,相关的推导留给读者自己去做吧。 让我们再复杂一点,使用上“在变量定义中使用变量”的第一个方式,来看一个例子: 1234x = $(y)y = zz = Helloa := $($(x)) 这里的$($(x))被替换成了$($(y)),因为$(y)值是“z”,所以,最终结果是:a:&#x3D;$(z),也就是“Hello”。 再复杂一点,我们再加上函数: 12345x = variable1variable2 := Helloy = $(subst 1,2,$(x))z = ya := $($($(z))) 这个例子中,“$($($(z)))”扩展为“$($(y))”,而其再次被扩展为“$($(subst 1,2,$(x)))”。$(x)的值是“variable1”,subst函数把“variable1”中的所有“1”字串替换成“2”字串,于是,“variable1”变成“variable2”,再取其值,所以,最终, $(a)的值就是$(variable2)的值——“Hello”。(喔,好不容易) 在这种方式中,或要可以使用多个变量来组成一个变量的名字,然后再取其值: 1234first_second = Helloa = firstb = secondall = $($a_$b) 这里的“$a_$b”组成了“first_second”,于是,$(all)的值就是“Hello”。 再来看看结合第一种技术的例子: 123a_objects := a.o b.o c.o1_objects := 1.o 2.o 3.osources := $($(a1)_objects:.o=.c) 这个例子中,如果$(a1)的值是“a”的话,那么,$(sources)的值就是“a.c b.c c.c”;如果$(a1)的值是“1”,那么$(sources)的值是“1.c 2.c 3.c”。 再来看一个这种技术和“函数”与“条件语句”一同使用的例子: 1234567ifdef do_sortfunc := sortelsefunc := stripendifbar := a d b g q cfoo := $($(func) $(bar)) 这个示例中,如果定义了“do_sort”,那么:foo :&#x3D; $(sort a d b g q c),于是$(foo)的值就是“a b c d g q”,而如果没有定义“do_sort”,那么:foo :&#x3D; $(sort a d bg q c),调用的就是strip函数。 当然,“把变量的值再当成变量”这种技术,同样可以用在操作符的左边: 12345dir = foo$(dir)_sources := $(wildcard $(dir)/*.c)define $(dir)_printlpr $($(dir)_sources)endef 这个例子中定义了三个变量:“dir”,“foo_sources”和“foo_print”。 四、追加变量值我们可以使用“+&#x3D;”操作符给变量追加值,如: 12objects = main.o foo.o bar.o utils.oobjects += another.o 于是,我们的$(objects)值变成:“main.o foo.o bar.o utils.o another.o”(another.o 被追加进去了) 使用“+&#x3D;”操作符,可以模拟为下面的这种例子: 12objects = main.o foo.o bar.o utils.oobjects := $(objects) another.o 所不同的是,用“+&#x3D;”更为简洁。 如果变量之前没有定义过,那么,“+&#x3D;”会自动变成“&#x3D;”,如果前面有变量定义,那么“+&#x3D;”会继承于前次操作的赋值符。如果前一次的是“:&#x3D;”,那么“+&#x3D;”会以“:&#x3D;”作为其赋值符,如: 12variable := valuevariable += more 等价于: 12variable := valuevariable := $(variable) more 但如果是这种情况: 12variable = valuevariable += more 由于前次的赋值符是“&#x3D;”,所以“+&#x3D;”也会以“&#x3D;”来做为赋值,那么岂不会发生变量的递补归定义,这是很不好的,所以make会自动为我们解决这个问题,我们不必担心这个问题。 五、override 指示符如果有变量是通常make的命令行参数设置的,那么Makefile中对这个变量的赋值会被忽略。 如果你想在Makefile中设置这类参数的值,那么,你可以使用“override”指示符。其语法是: 12override &lt;variable&gt; = &lt;value&gt;override &lt;variable&gt; := &lt;value&gt; 当然,你还可以追加: 1override &lt;variable&gt; += &lt;more text&gt; 对于多行的变量定义,我们用define指示符,在define指示符前,也同样可以使用ovveride 指示符,如: 123override define foobarendef 六、多行变量还有一种设置变量值的方法是使用define关键字。使用define关键字设置变量的值可以有换行,这有利于定义一系列的命令(前面我们讲过“命令包”的技术就是利用这个关键字)。 define指示符后面跟的是变量的名字,而重起一行定义变量的值,定义是以endef关键字结束。其工作方式和“&#x3D;”操作符一样。变量的值可以包含函数、命令、文字,或是其它变量。因为命令需要以[Tab]键开头,所以如果你用define定义的命令变量中没有以[Tab]键开头,那么make就不会把其认为是命令。 下面的这个示例展示了define的用法: 1234define two-linesecho fooecho $(bar)endef 七、环境变量make运行时的系统环境变量可以在make开始运行时被载入到Makefile文件中,但是 如果Makefile中已定义了这个变量,或是这个变量由make命令行带入,那么系统的环境变量的值将被覆盖。(如果make指定了“-e”参数,那么,系统环境变量将覆盖Makefile中定义的变量) 因此,如果我们在环境变量中设置了“CFLAGS”环境变量,那么我们就可以在所有的Makefile中使用这个变量了。这对于我们使用统一的编译参数有比较大的好处。如果Makefile中定义了CFLAGS,那么则会使用Makefile中的这个变量,如果没有定义则使用系统环境变量的值,一个共性和个性的统一,很像“全局变量”和“局部变量”的特性。 当make嵌套调用时(参见前面的“嵌套调用”章节),上层Makefile中定义的变量会以系统环境变量的方式传递到下层的Makefile中。当然,默认情况下,只有通过命令行设置的变量会被传递。而定义在文件中的变量,如果要向下层Makefile传递,则需要使用exprot关键字来声明。(参见前面章节) 当然,我并不推荐把许多的变量都定义在系统环境中,这样,在我们执行不用的Makefile时,拥有的是同一套系统变量,这可能会带来更多的麻烦。 八、目标变量前面我们所讲的在Makefile中定义的变量都是“全局变量”,在整个文件,我们都可以访问这些变量。当然,“自动化变量”除外,如“$&lt;”等这种类量的自动化变量就属于“规则型变量”,这种变量的值依赖于规则的目标和依赖目标的定义。当然,我样同样可以为某个目标设置局部变量,这种变量被称为“Target-specific Variable”,它可以和“全局变量”同名,因为它的作用范围只在这条规则以及连带规则中,所以其值也只在作用范围内有效。而不会影响规则链以外的全局变量的值。 其语法是: 12&lt;target ...&gt; : &lt;variable-assignment&gt;&lt;target ...&gt; : overide &lt;variable-assignment&gt; 可以是前面讲过的各种赋值表达式,如“&#x3D;”、“:&#x3D;”、“+&#x3D;”或是“?&#x3D;”。第二个语法是针对于make命令行带入的变量,或是系统环境变量。这个特性非常的有用,当我们设置了这样一个变量,这个变量会作用到由这个目标所引发的所有的规则中去。如: 123456789prog : CFLAGS = -gprog : prog.o foo.o bar.o$(CC) $(CFLAGS) prog.o foo.o bar.oprog.o : prog.c$(CC) $(CFLAGS) prog.cfoo.o : foo.c$(CC) $(CFLAGS) foo.cbar.o : bar.c$(CC) $(CFLAGS) bar.c 在这个示例中,不管全局的$(CFLAGS)的值是什么,在prog目标,以及其所引发的所有规则中(prog.o foo.o bar.o的规则),$(CFLAGS)的值都是“-g” 九、模式变量在GNU的make中,还支持模式变量(Pattern-specific Variable),通过上面的目标变量中,我们知道,变量可以定义在某个目标上。模式变量的好处就是,我们可以给定一种“模式”,可以把变量定义在符合这种模式的所有目标上。 我们知道,make的“模式”一般是至少含有一个“%”的,所以,我们可以以如下方式给所有以[.o]结尾的目标定义目标变量: 1%.o : CFLAGS = -O 同样,模式变量的语法和“目标变量”一样: 12&lt;pattern ...&gt; : &lt;variable-assignment&gt;&lt;pattern ...&gt; : override &lt;variable-assignment&gt; override同样是针对于系统环境传入的变量,或是make命令行指定的变量。 第八部分使用条件判断使用条件判断,可以让make根据运行时的不同情况选择不同的执行分支。条件表达式可以是比较变量的值,或是比较 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/437667448","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核RCU机制","path":"/2023/01/15/linux-docs/Linux内核RCU机制/","content":"1、概述 Read-copy update (RCU) 是一种 2002 年 10 月被引入到内核当中的同步机制。通过允许在更新的同时读数据，RCU 提高了同步机制的可伸缩性（scalability）。相对于传统的在并发线程间不区分是读者还是写者的简单互斥性锁机制，或者是哪些允许并发读但同时不 允许写的读写锁，RCU 支持同时一个更新线程和多个读线程的并发。RCU 通过保存对象的多个副本来保障读操作的连续性，并保证在预定的读方临界区没有完成之前不会释放这个对象。RCU定义并使用高效、可伸缩的机制来发布并读取 对象的新版本，并延长旧版本们的寿命。这些机制将工作分发到了读和更新路径上，以保证读路径可以极快地运行。在某些场合（非抢占内核），RCU 的读方没有任何性能负担。 问题1：seqlock 不是也允许读线程和更新线程并发工作么？ 这个问题可以归结到 “确切地说，什么是RCU？” 这个问题，或许还是 “RCU 可能是如何工作的？” （再或者，不太可能的情况下，问题会变为什么情况下 RCU 不太可能工作）。本文从几个基本的出发点来回答这些问题；之后还会分批地从使用的角度和 API 的角度来看这些问题。最后一篇连载还会给出一组参考文献。 RCU 由三个基本机制组成，第一个用于插入，第二个用于删除，而第三个则用于让读线程可以承受并发的插入或删除。这三个机制将在下面的三节中介绍，讲述如何将 RCU 转化为链表： 订阅发布机制 （用于插入） 等待已有的RCU读者完成 （用于删除） 维护多个最近更新的对象的版本 （为读者维护） 这三个章节之后还有上重点回顾与快速问题答案。 2、订阅发布机制 RCU的一个关键特性是它可以安全地扫描数据，即使数据正被同时改写也没问题。要提供这种并发插入的能力，RCU使用了一种订阅发布机制。举例说，考虑一 个被初始化为 NULL 的全局指针变量 gp 将要被修改为新分配并初始化的数据结构。下面这段代码（使用附加的合适的锁机制）可以用于这个目的： 12345678910111213141 struct foo &#123;2 int a;3 int b;4 int c;5 &#125;;6 struct foo *gp = NULL;78 /* . . . */910 p = kmalloc(sizeof(*p), GFP_KERNEL);11 p-&gt;a = 1;12 p-&gt;b = 2;13 p-&gt;c = 3;14 gp = p; 不幸的是，没有方法强制保证编译器和CPU能顺序执行最后四条语句。如果gp的赋值早于p的各个域的初始化的话，那么并发的读操作将访问到未初始化的变 量。内存屏障（barrier）可以用于保障操作的顺序，但内存屏障以难以使用而闻名。这样我们将他们封装到具有发布语义的 rcu_assign_pointer() 原语之中。最后的四条将成为这样： 12341 p-&gt;a = 1;2 p-&gt;b = 2;3 p-&gt;c = 3;4 rcu_assign_pointer(gp, p); rcu_assign_pointer() 将会发布新的结构，强制编译器和CPU在给p的各个域赋值之后再把指针赋值给gp。然而，仅仅强制更新操作的顺序是不够的，读者也必须强制使用恰当的顺序。考虑下面的这段代码： 12341 p = gp;2 if (p != NULL) &#123;3 do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);4 &#125; 尽管这段代码看起来不会受到顺序错乱的影响，不过十分不幸，DEC Alpha CPU 和投机性编译器优化可能会引发问题，不论你是否相信，这的确有可能会导致 p-&gt;a, p-&gt;b, p-&gt;c 的读取会在读取 p 之前！这种情况在投机性编译器优化的情况中最有可能会出现，编译器会揣测p的值，取出 p-&gt;a, p-&gt;b 和 p-&gt;c，之后取出 p 的真实值来检查拽侧的正确性。这种优化非常激进，或者说疯狂，不过在确实会在profile-driven优化时发生。 毫无疑问，我们需要在CPU和编译器上阻止这种情况的发生。rcu_dereference() 原语使用了必要的内存屏障指令和编译器指令来达到这一目的： 1234561 rcu_read_lock();2 p = rcu_dereference(gp);3 if (p != NULL) &#123;4 do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);5 &#125;6 rcu_read_unlock(); rcu_dereference() 原语可以被看作是订阅了指针指向的值，保证接下来的取值操作将会看到对应的发布操作（rcu_assign_pointer()）发生之前被初始化的值。 rcu_read_lock() 和 rcu_read_unlock() 绝对是必须的：他们定义了 RCU 读方临界区的范围。他们的目的将在下一节 解释，不过，他们不会自旋或阻塞，也不阻止 list_add_rcu() 的并发执行。事实上，对于非抢占内核，它们不产生任何代码。 虽然 rcu_assign_pointer() 和 rcu_dereference() 在理论上可以用于构建任意 RCU 保护的数据结构，但实际上，使用高层构造常常更好。因此，rcu_assign_pointer() 和 rcu_dereference() 原语被嵌入到了 Linux 的链表维护 API 中的特殊 RCU 变量之中了。Linux 有两个双向链表的变种，循环链表 struct list_head 和线性链表 struct hlist_head&#x2F;struct hlist_node。前者的结构如下图所示，绿色的方块表示表头，蓝色的是链表中的元素。 将上面的指针发布例子放到链表的场景中来就是这样： 1234567891011121314151 struct foo &#123;2 struct list_head list;3 int a;4 int b;5 int c;6 &#125;;7 LIST_HEAD(head);89 /* . . . */1011 p = kmalloc(sizeof(*p), GFP_KERNEL);12 p-&gt;a = 1;13 p-&gt;b = 2;14 p-&gt;c = 3;15 list_add_rcu(&amp;p-&gt;list, &amp;head); 第15行被使用某种同步机制保护住了，通常是某种所，以组织多个 list_add() 实例并发执行。然而，这些同步不能组织同时发生的RCU读者。订阅一个 RCU 保护的链表非常直接： 123451 rcu_read_lock();2 list_for_each_entry_rcu(p, head, list) &#123;3 do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);4 &#125;5 rcu_read_unlock(); ist_add_rcu() 原语发布一个节点到制定的链表中去，保证对应的 list_for_each_entry_rcu() 调用都正确的订阅到同一个节点上。 问题2：如果在 list_for_each_entry_rcu() 运行时，刚好进行了一次 list_add_rcu()，如何防止 segfault 的发生呢？ Linux 中的另一个双向链表，hlist，是一个线性表，也就是说，它的头部仅需要一个指针，而不是向循环链表一样需要两个指针。这样，使用 hlist 作为大型哈希表的 hash-bucket 数组的容器将仅消耗一半的内存空间。 将一个新元素添加到一个 RCU 保护的 hlist 里面与添加到循环链表里非常类似： 1234567891011121314151 struct foo &#123;2 struct hlist_node *list;3 int a;4 int b;5 int c;6 &#125;;7 HLIST_HEAD(head);89 /* . . . */1011 p = kmalloc(sizeof(*p), GFP_KERNEL);12 p-&gt;a = 1;13 p-&gt;b = 2;14 p-&gt;c = 3;15 hlist_add_head_rcu(&amp;p-&gt;list, &amp;head); 和上面一样，第15行一定使用了锁或其他某种同步机制。 订阅一个 RCU 保护的 hlist 也和循环链表非常接近。 123451 rcu_read_lock();2 hlist_for_each_entry_rcu(p, q, head, list) &#123;3 do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);4 &#125;5 rcu_read_unlock(); 问题3：为什么我们需要传递两个指针给 hlist_for_each_entry_rcu()， list_for_each_entry_rcu() 可是只需要一个指针的啊？ RCU 发布与订阅原语在如下表中列出，同时给出了 “取消发布”或是撤回的原语 类别发布撤销订阅类别发布撤销订阅指针 123rcu_assign_pointer()rcu_assign_pointer(…, NULL)rcu_dereference() 循环链表 12345list_add_rcu()list_add_tail_rcu()list_replace_rcu()list_del_rcu()list_for_each_entry_rcu() 双向链表 123456hlist_add_after_rcu()hlist_add_before_rcu()hlist_add_head_rcu()hlist_replace_rcu()hlist_del_rcu()hlist_for_each_entry_rcu() 注意，list_replace_rcu(), list_del_rcu(), hlist_replace_rcu(), 以及 hlist_del_rcu() 增加了一些复杂度。什么时候释放被替换或删除掉的数据元素才是安全的呢？具体地说，我们怎么能知道所有的读者都释放了他们手中对数据元素的引用呢？ 这些问题将在下面的章节中得到回答。 3、等待已经存在的RCU读者完成 RCU的最基本的功能就是等待一些事情的完成。当然，还有很多其他方法也是用于等待事情完成的，包括引用计数、读写锁、事件等。RCU最大的好处在于它可 以等待所有（比如说）两万件不同点事情，而无需显式地跟踪它们中的每一个，也不需要担心性能的下降、可伸缩性限制、复杂度死锁场景，以及内存泄露等所有这 些显式跟踪手法所固有的问题。 RCU 中，被等待的东西被叫做“RCU读方临界区”。一个RCU读方临界区始于 rcu_read_lock() 原语，止于 rcu_read_unlock() 原语。RCU 读方临界区可以嵌套，也可以放入很多代码，只要这些代码显式阻塞或睡眠即可（有一种称为“SRCU”的特殊RCU允许在它的读方临界区中睡眠）。只要你遵守这些约定，你就可以使用RCU来等待任何期望的代码段的完成。 正如其他地方对经典RCU和实时RCU的描述，RCU 通过间接确定这些其他事情的完成时间来达到这一目的。 具体地说，如下图所示，RCU是一种等待已经存在的RCU读方临界区结束的方法，包括这些临界区中执行的内存操作。 注意，开始于一个给定宽限期开始之后的RCU读方临界区能够、并可以延续到该宽限期结束之后。 下面的伪码展示了使用RCU等待读者的基本算法形式： 进行改动，比如，替换链表中的一个元素。 等待所有已经存在的RCU读方临界区完成（比如，使用synchronize_rcu()原语）。关键点是接下来的RCU读方临界区将无法得到新近删除的元素的引用了。 清理，比如，释放上述所有被替换的元素。 下面的代码段是从前一节修改而得的，用于说明这一过程，这里面的域a是这个搜索的键值。 1234567891011121314151617181920211 struct foo &#123;2 struct list_head list;3 int a;4 int b;5 int c;6 &#125;;7 LIST_HEAD(head);89 /* . . . */1011 p = search(head, key);12 if (p == NULL) &#123;13 /* Take appropriate action, unlock, and return. */14 &#125;15 q = kmalloc(sizeof(*p), GFP_KERNEL);16 *q = *p;17 q-&gt;b = 2;18 q-&gt;c = 3;19 list_replace_rcu(&amp;p-&gt;list, &amp;q-&gt;list);20 synchronize_rcu();21 kfree(p); 第19、20 和 21 行实现了上面所说的三个步骤。第 16-19行展现了 RCU 的名字（读-复制-更新）：在允许进行并发读操作的同时，第16行进行了复制，而第17-19行进行了更新。 乍一看会觉得 synchronize_rcu() 原语显得比较神秘。毕竟它必须等所有读方临界区完成，而且，正如我们前面看到的，用于限制RCU读方临界区的rcu_read_lock() 和 rcu_read_unlock() 原语在非抢占内核中甚至什么代码都不会生成。 这里有一个小伎俩，经典RCU通过 rcu_read_lock() 和 rcu_read_unlock() 界定的读方临界区是不允许阻塞和休眠的。因此，当一个给定的CPU要进行上下文切换的时候，我们可以确定任何已有的RCU读方临界区都已经完成了。也就是说，只要每个CPU都至少进行了一次上下文切换，那么所有先前的 RCU 读方临界区也就保证都完成了，即 synchronize_rcu() 可以安全返回了。 因此，经典RCU的 synchronize_rcu() 从概念上说可以被简化成这样： 121 for_each_online_cpu(cpu)2 run_on(cpu); 这里，run_on() 将当前线程切换到指定 CPU，来强制该 CPU 进行上下文切换。而 for_each_online_cpu() 循环强制对每个 CPU 进行一次上下文切换。虽然这个简单的方法可以在一个不支持抢占的内核上工作，换句话说，对 non-CONFIG_PREEMPT 和 CONFIG_PREEMPT，但对 CONFIG_PREEMPT_RT 实时 (-rt) 内核无效。因此，实时RCU使用了一个（松散地）基于引用计数的方法。 当然，在真实内核中的实现要复杂得多了，因为它需要管理终端，NMI，CPU热插拔和其他实际内核中的可能有的风险，而且还要维护良好的性能和可伸缩性。RCU的实时实现还必须拥有良好的实时响应能力，这就使得（像上面两行那样）直接禁止抢占变得不可能了。 虽然我们了解到了 synchronize_rcu() 的简单实现原理，不过还有很多其它问题呢。比如，RCU读者们在读一个正在被并发地更新的链表的时候究竟读到了什么呢？这个问题将在下一节讲到。 3、维护多个版本的近期更新的对象 本节将展示 RCU 如何为多个不需要同步的读者维护不同版本的链表。我们使用两个例子来展示一个可能被给定的读者引用的元素必须在该读者处于读方临界区的整个过程中保持完好无损。第一个例子展示了链表元素的删除，而第二个例子则展示了元素的替换。 3.1例1：在删除时维护多个版本 要开始这个“删除”的例子，我们先把上节这个例子的 11-21行改成如下的形式： 1234561 p = search(head, key);2 if (p != NULL) &#123;3 list_del_rcu(&amp;p-&gt;list);4 synchronize_rcu();5 kfree(p);6 &#125; 这个链表以及指针p的最初情况是这样的： 表中每个元素的三元组分别代表域a, b, c。红色的便捷表明读者可以获取它们的指针，而且因为读操作和更新操作不是直接同步的，读者可以在这个删除的过程中同时发生。这里我们为了清晰没有画出双向链表的反向指针。 在第三行的 list_del_rcu() 完成的时候，5,6,7 这个元素已经被从链表中删除了（如下图）。由于读者并不直接和更新操作同步，读者可能同时正在扫描这个链表。由于访问时间不同，这些并发读者可能看到、也 可能没看到新近删除的元素。不过，那些在获取指针之后延迟了读操作的读者（比如因为中断、ECC内存错误，或在 CONFIG_PREEMPT_RT内核中因为抢占而延迟了的）可能仍然会在删除之后的一段时间内看到那个老的链表的版本。下图中 5,6,7 元素的边框仍然是红色的，这意味着仍然有读者可能会引用它。 这里注意，在退出读方临界区之后，读者们就不能再持有 5,6,7 这个元素的引用了。所以，一旦第4行的 synchronize_rcu() 完成了，所有已有读者也就保证都完成了，这样就没有读者会访问这个元素了，下图中，这个元素的边框也变黑了。我们的链表也回到了一个单一的版本了。 这之后，5,6,7 这个元素就可以被安全的释放了： 这里，我们完成了删除 5,6,7 这个元素的操作，下一小节将介绍替换操作。 3.2例2：在替换的过程中维护数据的多个不同版本 在开始替换的例子钱，我们再修改一下前面例子的最后几行： 12345671 q = kmalloc(sizeof(*p), GFP_KERNEL);2 *q = *p;3 q-&gt;b = 2;4 q-&gt;c = 3;5 list_replace_rcu(&amp;p-&gt;list, &amp;q-&gt;list);6 synchronize_rcu();7 kfree(p); 这个链表的初始状态和指针p和删除的那个例子是完全一样的： 和之前一样，每个元素里面的三元组分别代表域 a, b 和 c。红色的边框代表了读者可能会持有这个元素的引用，因为读者和更新者没有直接的同步，读者可能会和整个替换过程并发进行。再次说明，这里我们为了清晰，再次省略了反向指针。 第一行的 kmalloc() 生成了一个替换元素，如下： 第二行把旧的元素的内容拷贝给新的元素： 第三行，将 q-&gt;b 更新为2： 第四行，将 q-&gt;c 更新为3： 现在，第5行进行替换操作，这里，新元素最终对读者可见了。到了这里，如下所示，我们有了这个链表的两个版本。先前已经存在的读者可以看到 5,6,7 元素，而新读者将看到 5,2,3 元素。不过，任何读者都被保证可以看到一个完整的链表。 第6行的 synchronize_rcu() 返回后，宽限期将完成，所有在 list_replace_rcu() 之前开始的读者都将完成。具体地说，任何可能持有 5,6,7 的读者都已经退出了他们的读方临界区，这就保证他们不再持有一个引用。因而也在没有任何读者持有老元素的引用了，途中，5,6,7 元素的边框也就变黑了。对于读者来说，目前又只有一个单一的链表版本了，只是新的元素已经替代了旧元素的位置。 第七行的 kfree() 完成后，链表旧成为了如下的样子： 尽管 RCU 是以替换而命名的，但内核中的大多数使用都是前面小节 中的简单删除的情况。 4、讨论 这个例子假设在更新操作的过程中保存着一个互斥量，也就是说，这个链表在一个给定时间最多有两种版本。 问题4：如何修改删除的例子，来允许超过两个版本的链表可以同时存在？问题5：在某一时刻，RCU最多可以有多少个链表的版本？ 这组例子显示了RCU使用多个版本来保障在存在并发读者的情况下的安全更改数据。当然，一些算法是无法很好地支持多个版本的。 问题6：如果 rcu_read_lock() 与 rcu_read_unlock() 之间没有自旋锁或阻塞，RCU更新者会怎样延迟RCU读者？ 这三个RCU的组成部分允许数据在并发读者访问的同时更新数据，并可以以多种方式实现基于RCU的算法， 版权声明：本文为知乎博主「Linux内核库」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文 出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/516304206","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核fork()函数详解","path":"/2023/01/15/linux-docs/Linux内核fork()函数详解/","content":"从一个比较有意思的题开始说起，最近要找工作无意间看到一个关于unix&#x2F;linux中fork()的面试题： 12345678910111213141516171 #include&lt;sys/types.h&gt; 2 #include&lt;stdio.h&gt; 3 #include&lt;unistd.h&gt; 4 int main(void) 5 &#123; 6 int i; 7 int buf[100]=&#123;1,2,3,4,5,6,7,8,9&#125;; 8 for(i=0;i&lt;2;i++) 9 &#123; 10 fork(); 11 printf(&quot;+&quot;); 12 //write(&quot;/home/pi/code/test_fork/test_fork.txt&quot;,buf,8); 13 write(STDOUT_FILENO,&quot;-&quot;,1); 14 &#125; 15 return 0; 16 17 &#125; 题目要求是从上面的代码中确定输出的“+”的数量，我后面加了一个“-”，再确定输出“-”的数量。 先给答案：“+”8次，“-”6次 11 ---++--++-++++ 上面的这段代码很简单，包含的内容却有很多，有进程产生、系统调用、不带缓冲I&#x2F;O、标准I&#x2F;O。 linux中产生一个进程的调用函数过程如下： fork()———-&gt;sys_fork()————–&gt;do_fork()———-&gt;copy_process() fork()、vfork()、_clone()库函数都根据各自需要的参数标志去调用clone()，然后由clone()去调用do_fork()。do_fork()完成了创建中的大部分工作，该函数调用copy_process()函数， 从用户空间调用fork()函数到执行系统调用产生软件中断陷入内核空间，在内核空间执行do_fork()函数，主要是复制父进程的页表、内核栈等，如果要执行子进程代码还要调用exac()函数拷贝硬盘上的代码到位内存上，由于刚创建的子进程没有申请内存，目前和父进程共用父进程的代码段、数据段等，没有存放子进程自己代码段数据段的内存，此时会产生一个缺页异常，为子进程申请内存，同时定制自己的全局描述GDT、局部描述符LDT、任务状态描述符TSS，下面从代码中分析这个过程然后在回答上面为什么“+”是8次，“-”6次。 调用fork()函数执行到了unistd.h中的宏函数syscall0 123456789101112131415161718192021/* XXX - _foo needs to be __foo, while __NR_bar could be _NR_bar. *//* * Don&#x27;t remove the .ifnc tests; they are an insurance against * any hard-to-spot gcc register allocation bugs. */#define _syscall0(type,name) \\type name(void) \\&#123; \\ register long __a __asm__ (&quot;r10&quot;); \\ register long __n_ __asm__ (&quot;r9&quot;) = (__NR_##name); \\ __asm__ __volatile__ (&quot;.ifnc %0%1,$r10$r9 \\t&quot; \\ &quot;.err \\t&quot; \\ &quot;.endif \\t&quot; \\ &quot;break 13&quot; \\ : &quot;=r&quot; (__a) \\ : &quot;r&quot; (__n_)); \\ if (__a &gt;= 0) \\ return (type) __a; \\ errno = -__a; \\ return (type) -1; \\&#125; 将宏函数展开后变为 12345678910111213141516171819201 /* XXX - _foo needs to be __foo, while __NR_bar could be _NR_bar. */ 2 /* 3 * Don&#x27;t remove the .ifnc tests; they are an insurance against 4 * any hard-to-spot gcc register allocation bugs. 5 */ 7 int fork(void) 8 &#123; 9 register long __a __asm__ (&quot;r10&quot;); \\10 register long __n_ __asm__ (&quot;r9&quot;) = (__NR_##name); \\11 __asm__ __volatile__ (&quot;.ifnc %0%1,$r10$r9 \\t&quot; \\12 &quot;.err \\t&quot; \\13 &quot;.endif \\t&quot; \\14 &quot;break 13&quot; \\15 : &quot;=r&quot; (__a) \\16 : &quot;r&quot; (__n_)); \\17 if (__a &gt;= 0) \\18 return (type) __a; \\19 errno = -__a; \\20 return (type) -1; \\21 &#125; ##的意思就是宏中的字符直接替换 如果name &#x3D; fork，那么在宏中_ NR_##name就替换成了 _ NR_fork了。 _ NR _ ##name是系统调用号，##指的是两次宏展开．即用实际的系统调用名字代替”name”,然后再把_ NR _ …展开．如name &#x3D;&#x3D; ioctl，则为_ NR_ioctl。上面的汇编目前还是没有怎么弄懂——-int $0x80 是所有系统调用函数的总入口，fork()是其中之一，“0”(_NR_fork) 意思是将fork在sys_call_table[]中对应的函数编号_NR_fork也就是2，将2传给eax寄存器。这个编号就是sys_fork()函数在sys_call_table中的偏移值，其他的系统调用在sys_call_table均存在偏移值()。int $0x80 中断返回后，将执行return (type) -1—–&gt;展开就是return (int) __a;产生int $0x80软件中断，CPU从3级特权的进程跳到0特权级内核代码中执行。中断使CPU硬件自动将SS、ESP、EFLAGGS、CS、EIP这五个寄存器的值按照这个顺序压人父进程的内核栈，这些压栈的数据将在后续的copy_process()函数中用来初始化进程1的任务状态描述符TSSCPU自动压栈完成后，跳转到system_call.s中的_system_call处执行，继续将DS、ES、FS、EDX、ECX、EBX压栈(这些压栈仍旧是为了初始化子进程中的任务状态描述符TSS做准备)。最终内核通过刚刚设置的eax的偏移值“2”查询sys_call_table[],知道此次系统调用对应的函数是sys_fork()。跳转到_sys_fork处执行。注意：一个函数的参数不是由函数定义的，而是由函数定义以外的程序通过压栈的方式“做”出来的，是操作系统底层代码与应用程序代码写作手法的差异之一。我们知道在C语言中函数运行时参数是存在栈中的，根据这个原理操作系统设计者可以将前面程序强行压栈的值作为函数的参数，当调用这个函数时这些值就是函数的参数。 sys_fork函数 123456789asmlinkage int sys_fork(void)&#123;#ifndef CONFIG_MMU /* fork almost works, enough to trick you into looking elsewhere:-( */ return -EINVAL;#else return do_fork(SIGCHLD, user_stack(__frame), __frame, 0, NULL, NULL);#endif&#125; do_fork函数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/* * Ok, this is the main fork-routine. * * It copies the process, and if successful kick-starts * it and waits for it to finish using the VM if required. */long do_fork(unsigned long clone_flags, unsigned long stack_start, struct pt_regs *regs, unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr)&#123; struct task_struct *p; int trace = 0; struct pid *pid = alloc_pid(); long nr; if (!pid) return -EAGAIN; nr = pid-&gt;nr; if (unlikely(current-&gt;ptrace)) &#123; trace = fork_traceflag (clone_flags); if (trace) clone_flags |= CLONE_PTRACE; &#125;dup_task_struct p = copy_process(clone_flags, stack_start, regs, stack_size, parent_tidptr, child_tidptr, pid); /* * Do this prior waking up the new thread - the thread pointer * might get invalid after that point, if the thread exits quickly. */ if (!IS_ERR(p)) &#123; struct completion vfork; if (clone_flags &amp; CLONE_VFORK) &#123; p-&gt;vfork_done = &amp;vfork; init_completion(&amp;vfork); &#125; if ((p-&gt;ptrace &amp; PT_PTRACED) || (clone_flags &amp; CLONE_STOPPED)) &#123; /* * We&#x27;ll start up with an immediate SIGSTOP. */ sigaddset(&amp;p-&gt;pending.signal, SIGSTOP); set_tsk_thread_flag(p, TIF_SIGPENDING); &#125; if (!(clone_flags &amp; CLONE_STOPPED)) wake_up_new_task(p, clone_flags); else p-&gt;state = TASK_STOPPED; if (unlikely (trace)) &#123; current-&gt;ptrace_message = nr; ptrace_notify ((trace &lt;&lt; 8) | SIGTRAP); &#125; if (clone_flags &amp; CLONE_VFORK) &#123; freezer_do_not_count(); wait_for_completion(&amp;vfork); freezer_count(); if (unlikely (current-&gt;ptrace &amp; PT_TRACE_VFORK_DONE)) &#123; current-&gt;ptrace_message = nr; ptrace_notify ((PTRACE_EVENT_VFORK_DONE &lt;&lt; 8) | SIGTRAP); &#125; &#125; &#125; else &#123; free_pid(pid); nr = PTR_ERR(p); &#125; return nr;&#125; copy_process函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368/* * This creates a new process as a copy of the old one, * but does not actually start it yet. * * It copies the registers, and all the appropriate * parts of the process environment (as per the clone * flags). The actual kick-off is left to the caller. */static struct task_struct *copy_process(unsigned long clone_flags, unsigned long stack_start, struct pt_regs *regs, unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr, struct pid *pid)&#123; int retval; struct task_struct *p = NULL; if ((clone_flags &amp; (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS)) return ERR_PTR(-EINVAL); /* * Thread groups must share signals as well, and detached threads * can only be started up within the thread group. */ if ((clone_flags &amp; CLONE_THREAD) &amp;&amp; !(clone_flags &amp; CLONE_SIGHAND)) return ERR_PTR(-EINVAL); /* * Shared signal handlers imply shared VM. By way of the above, * thread groups also imply shared VM. Blocking this case allows * for various simplifications in other code. */ if ((clone_flags &amp; CLONE_SIGHAND) &amp;&amp; !(clone_flags &amp; CLONE_VM)) return ERR_PTR(-EINVAL); retval = security_task_create(clone_flags); if (retval) goto fork_out; retval = -ENOMEM; p = dup_task_struct(current); if (!p) goto fork_out;sys_fork rt_mutex_init_task(p);#ifdef CONFIG_TRACE_IRQFLAGS DEBUG_LOCKS_WARN_ON(!p-&gt;hardirqs_enabled); DEBUG_LOCKS_WARN_ON(!p-&gt;softirqs_enabled);#endif retval = -EAGAIN; if (atomic_read(&amp;p-&gt;user-&gt;processes) &gt;= p-&gt;signal-&gt;rlim[RLIMIT_NPROC].rlim_cur) &#123; if (!capable(CAP_SYS_ADMIN) &amp;&amp; !capable(CAP_SYS_RESOURCE) &amp;&amp; p-&gt;user != &amp;root_user) goto bad_fork_free; &#125; atomic_inc(&amp;p-&gt;user-&gt;__count); atomic_inc(&amp;p-&gt;user-&gt;processes); get_group_info(p-&gt;group_info); /* * If multiple threads are within copy_process(), then this check * triggers too late. This doesn&#x27;t hurt, the check is only there * to stop root fork bombs. */ if (nr_threads &gt;= max_threads) goto bad_fork_cleanup_count; if (!try_module_get(task_thread_info(p)-&gt;exec_domain-&gt;module)) goto bad_fork_cleanup_count; if (p-&gt;binfmt &amp;&amp; !try_module_get(p-&gt;binfmt-&gt;module)) goto bad_fork_cleanup_put_domain; p-&gt;did_exec = 0; delayacct_tsk_init(p); /* Must remain after dup_task_struct() */ copy_flags(clone_flags, p); p-&gt;pid = pid_nr(pid); retval = -EFAULT; if (clone_flags &amp; CLONE_PARENT_SETTID) if (put_user(p-&gt;pid, parent_tidptr)) goto bad_fork_cleanup_delays_binfmt; INIT_LIST_HEAD(&amp;p-&gt;children); INIT_LIST_HEAD(&amp;p-&gt;sibling); p-&gt;vfork_done = NULL; spin_lock_init(&amp;p-&gt;alloc_lock); clear_tsk_thread_flag(p, TIF_SIGPENDING); init_sigpending(&amp;p-&gt;pending); p-&gt;utime = cputime_zero; p-&gt;stime = cputime_zero; p-&gt;sched_time = 0;#ifdef CONFIG_TASK_XACCT p-&gt;rchar = 0; /* I/O counter: bytes read */ p-&gt;wchar = 0; /* I/O counter: bytes written */ p-&gt;syscr = 0; /* I/O counter: read syscalls */ p-&gt;syscw = 0; /* I/O counter: write syscalls */#endif task_io_accounting_init(p); acct_clear_integrals(p); p-&gt;it_virt_expires = cputime_zero; p-&gt;it_prof_expires = cputime_zero; p-&gt;it_sched_expires = 0; INIT_LIST_HEAD(&amp;p-&gt;cpu_timers[0]); INIT_LIST_HEAD(&amp;p-&gt;cpu_timers[1]); INIT_LIST_HEAD(&amp;p-&gt;cpu_timers[2]); p-&gt;lock_depth = -1; /* -1 = no lock */ do_posix_clock_monotonic_gettime(&amp;p-&gt;start_time); p-&gt;security = NULL; p-&gt;io_context = NULL; p-&gt;io_wait = NULL; p-&gt;audit_context = NULL; cpuset_fork(p);#ifdef CONFIG_NUMA p-&gt;mempolicy = mpol_copy(p-&gt;mempolicy); if (IS_ERR(p-&gt;mempolicy)) &#123; retval = PTR_ERR(p-&gt;mempolicy); p-&gt;mempolicy = NULL; goto bad_fork_cleanup_cpuset; &#125; mpol_fix_fork_child_flag(p);#endif#ifdef CONFIG_TRACE_IRQFLAGS p-&gt;irq_events = 0;#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW p-&gt;hardirqs_enabled = 1;#else p-&gt;hardirqs_enabled = 0;#endif p-&gt;hardirq_enable_ip = 0; p-&gt;hardirq_enable_event = 0; p-&gt;hardirq_disable_ip = _THIS_IP_; p-&gt;hardirq_disable_event = 0; p-&gt;softirqs_enabled = 1; p-&gt;softirq_enable_ip = _THIS_IP_; p-&gt;softirq_enable_event = 0; p-&gt;softirq_disable_ip = 0; p-&gt;softirq_disable_event = 0; p-&gt;hardirq_context = 0; p-&gt;softirq_context = 0;#endif#ifdef CONFIG_LOCKDEP p-&gt;lockdep_depth = 0; /* no locks held yet */ p-&gt;curr_chain_key = 0; p-&gt;lockdep_recursion = 0;#endif#ifdef CONFIG_DEBUG_MUTEXES p-&gt;blocked_on = NULL; /* not blocked yet */#endif p-&gt;tgid = p-&gt;pid; if (clone_flags &amp; CLONE_THREAD) p-&gt;tgid = current-&gt;tgid; if ((retval = security_task_alloc(p))) goto bad_fork_cleanup_policy; if ((retval = audit_alloc(p))) goto bad_fork_cleanup_security; /* copy all the process information */ if ((retval = copy_semundo(clone_flags, p))) goto bad_fork_cleanup_audit; if ((retval = copy_files(clone_flags, p))) goto bad_fork_cleanup_semundo; if ((retval = copy_fs(clone_flags, p))) goto bad_fork_cleanup_files; if ((retval = copy_sighand(clone_flags, p))) goto bad_fork_cleanup_fs; if ((retval = copy_signal(clone_flags, p))) goto bad_fork_cleanup_sighand; if ((retval = copy_mm(clone_flags, p))) goto bad_fork_cleanup_signal; if ((retval = copy_keys(clone_flags, p))) goto bad_fork_cleanup_mm; if ((retval = copy_namespaces(clone_flags, p))) goto bad_fork_cleanup_keys; retval = copy_thread(0, clone_flags, stack_start, stack_size, p, regs); if (retval) goto bad_fork_cleanup_namespaces; p-&gt;set_child_tid = (clone_flags &amp; CLONE_CHILD_SETTID) ? child_tidptr : NULL; /* * Clear TID on mm_release()? */ p-&gt;clear_child_tid = (clone_flags &amp; CLONE_CHILD_CLEARTID) ? child_tidptr: NULL; p-&gt;robust_list = NULL;#ifdef CONFIG_COMPAT p-&gt;compat_robust_list = NULL;#endif INIT_LIST_HEAD(&amp;p-&gt;pi_state_list); p-&gt;pi_state_cache = NULL; /* * sigaltstack should be cleared when sharing the same VM */ if ((clone_flags &amp; (CLONE_VM|CLONE_VFORK)) == CLONE_VM) p-&gt;sas_ss_sp = p-&gt;sas_ss_size = 0; /* * Syscall tracing should be turned off in the child regardless * of CLONE_PTRACE. */ clear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);#ifdef TIF_SYSCALL_EMU clear_tsk_thread_flag(p, TIF_SYSCALL_EMU);#endif /* Our parent execution domain becomes current domain These must match for thread signalling to apply */ p-&gt;parent_exec_id = p-&gt;self_exec_id; /* ok, now we should be set up.. */ p-&gt;exit_signal = (clone_flags &amp; CLONE_THREAD) ? -1 : (clone_flags &amp; CSIGNAL); p-&gt;pdeath_signal = 0; p-&gt;exit_state = 0; /* * Ok, make it visible to the rest of the system. * We dont wake it up yet. */ p-&gt;group_leader = p; INIT_LIST_HEAD(&amp;p-&gt;thread_group); INIT_LIST_HEAD(&amp;p-&gt;ptrace_children); INIT_LIST_HEAD(&amp;p-&gt;ptrace_list); /* Perform scheduler related setup. Assign this task to a CPU. */ sched_fork(p, clone_flags); /* Need tasklist lock for parent etc handling! */ write_lock_irq(&amp;tasklist_lock); /* for sys_ioprio_set(IOPRIO_WHO_PGRP) */ p-&gt;ioprio = current-&gt;ioprio; /* * The task hasn&#x27;t been attached yet, so its cpus_allowed mask will * not be changed, nor will its assigned CPU. * * The cpus_allowed mask of the parent may have changed after it was * copied first time - so re-copy it here, then check the child&#x27;s CPU * to ensure it is on a valid CPU (and if not, just force it back to * parent&#x27;s CPU). This avoids alot of nasty races. */ p-&gt;cpus_allowed = current-&gt;cpus_allowed; if (unlikely(!cpu_isset(task_cpu(p), p-&gt;cpus_allowed) || !cpu_online(task_cpu(p)))) set_task_cpu(p, smp_processor_id()); /* CLONE_PARENT re-uses the old parent */ if (clone_flags &amp; (CLONE_PARENT|CLONE_THREAD)) p-&gt;real_parent = current-&gt;real_parent; else p-&gt;real_parent = current; p-&gt;parent = p-&gt;real_parent; spin_lock(&amp;current-&gt;sighand-&gt;siglock); /* * Process group and session signals need to be delivered to just the * parent before the fork or both the parent and the child after the * fork. Restart if a signal comes in before we add the new process to * it&#x27;s process group. * A fatal signal pending means that current will exit, so the new * thread can&#x27;t slip out of an OOM kill (or normal SIGKILL). */ recalc_sigpending(); if (signal_pending(current)) &#123; spin_unlock(&amp;current-&gt;sighand-&gt;siglock); write_unlock_irq(&amp;tasklist_lock); retval = -ERESTARTNOINTR; goto bad_fork_cleanup_namespaces; &#125; if (clone_flags &amp; CLONE_THREAD) &#123; p-&gt;group_leader = current-&gt;group_leader; list_add_tail_rcu(&amp;p-&gt;thread_group, &amp;p-&gt;group_leader-&gt;thread_group); if (!cputime_eq(current-&gt;signal-&gt;it_virt_expires, cputime_zero) || !cputime_eq(current-&gt;signal-&gt;it_prof_expires, cputime_zero) || current-&gt;signal-&gt;rlim[RLIMIT_CPU].rlim_cur != RLIM_INFINITY || !list_empty(&amp;current-&gt;signal-&gt;cpu_timers[0]) || !list_empty(&amp;current-&gt;signal-&gt;cpu_timers[1]) || !list_empty(&amp;current-&gt;signal-&gt;cpu_timers[2])) &#123; /* * Have child wake up on its first tick to check * for process CPU timers. */ p-&gt;it_prof_expires = jiffies_to_cputime(1); &#125; &#125; if (likely(p-&gt;pid)) &#123; add_parent(p); if (unlikely(p-&gt;ptrace &amp; PT_PTRACED)) __ptrace_link(p, current-&gt;parent); if (thread_group_leader(p)) &#123; p-&gt;signal-&gt;tty = current-&gt;signal-&gt;tty; p-&gt;signal-&gt;pgrp = process_group(current); set_signal_session(p-&gt;signal, process_session(current)); attach_pid(p, PIDTYPE_PGID, task_pgrp(current)); attach_pid(p, PIDTYPE_SID, task_session(current)); list_add_tail_rcu(&amp;p-&gt;tasks, &amp;init_task.tasks); __get_cpu_var(process_counts)++; &#125; attach_pid(p, PIDTYPE_PID, pid); nr_threads++; &#125; total_forks++; spin_unlock(&amp;current-&gt;sighand-&gt;siglock); write_unlock_irq(&amp;tasklist_lock); proc_fork_connector(p); return p;bad_fork_cleanup_namespaces: exit_task_namespaces(p);bad_fork_cleanup_keys: exit_keys(p);bad_fork_cleanup_mm: if (p-&gt;mm) mmput(p-&gt;mm);bad_fork_cleanup_signal: cleanup_signal(p);bad_fork_cleanup_sighand: __cleanup_sighand(p-&gt;sighand);bad_fork_cleanup_fs: exit_fs(p); /* blocking */bad_fork_cleanup_files: exit_files(p); /* blocking */bad_fork_cleanup_semundo: exit_sem(p);bad_fork_cleanup_audit: audit_free(p);bad_fork_cleanup_security: security_task_free(p);bad_fork_cleanup_policy:#ifdef CONFIG_NUMA mpol_free(p-&gt;mempolicy);bad_fork_cleanup_cpuset:#endif cpuset_exit(p);bad_fork_cleanup_delays_binfmt: delayacct_tsk_free(p); if (p-&gt;binfmt) module_put(p-&gt;binfmt-&gt;module);bad_fork_cleanup_put_domain: module_put(task_thread_info(p)-&gt;exec_domain-&gt;module);bad_fork_cleanup_count: put_group_info(p-&gt;group_info); atomic_dec(&amp;p-&gt;user-&gt;processes); free_uid(p-&gt;user);bad_fork_free: free_task(p);fork_out: return ERR_PTR(retval);&#125; dup_task_struct函数，tsk &#x3D; alloc_task_struct();dup_task_struct()函数主要是为子进程创建一个内核栈，主要赋值语句setup_thread_stack(tsk, orig); 在函数中调用alloc_task_struct()进行内存分配，alloc_task_struct()函数获取内存的方式内核里面有几种： 1、# define alloc_task_struct() kmem_cache_alloc(task_struct_cachep, GFP_KERNEL) 2、 1234567struct task_struct *alloc_task_struct(void)&#123; struct task_struct *p = kmalloc(THREAD_SIZE, GFP_KERNEL); if (p) atomic_set((atomic_t *)(p+1), 1); return p;&#125; 3、#define alloc_task_struct() ((struct task_struct *)_ _ get_free_pages(GFP_KERNEL | __GFP_COMP, KERNEL_STACK_SIZE_ORDER)) 以上3中申请内存的方式最后一种是最底层的，直接分配页，第二种利用了页高速缓存，相当于是对第3中方式进行了封装，第1种在第2中的方式上进行分配，相当于调用了第2种页高速缓存的API进行内存分配的。 12345678910111213141516171819202122232425262728293031323334static struct task_struct *dup_task_struct(struct task_struct *orig)&#123; struct task_struct *tsk; struct thread_info *ti; prepare_to_copy(orig); tsk = alloc_task_struct(); if (!tsk) return NULL; ti = alloc_thread_info(tsk); if (!ti) &#123; free_task_struct(tsk); return NULL; &#125; *tsk = *orig; tsk-&gt;stack = ti; setup_thread_stack(tsk, orig); //主要赋值语句将父进程的进程的thread_info赋值给子进程#ifdef CONFIG_CC_STACKPROTECTOR tsk-&gt;stack_canary = get_random_int();#endif /* One for us, one for whoever does the &quot;release_task()&quot; (usually parent) */ atomic_set(&amp;tsk-&gt;usage,2); atomic_set(&amp;tsk-&gt;fs_excl, 0);#ifdef CONFIG_BLK_DEV_IO_TRACE tsk-&gt;btrace_seq = 0;#endif tsk-&gt;splice_pipe = NULL; return tsk;&#125; 现在我们主要分析copy_process函数，此函数中做了非常重要的，体现linux中父子进程创建机制的工作。 1、调用dup_task_struct()为子进程创建一个内核栈、thread_info结构和task_struct，这些值与当前进程的值相同。此时子进程和父进程的描述符是完全相同的。 p &#x3D; dup_task_struct(current)—-&gt;(struct task_struct *tsk———-&gt;tsk &#x3D; alloc_task_struct()从slab层分配了一个关于进程描述符的slab) 2、检查并确保新创建这个子进程后，当前用户所拥有的进程数目没有超出给它分配的资源的限制。 3、子进程着手使自己与父进程区别开来，为进程的task_struct、tss做个性化设置，进程描述符内的许多成员都要被清0或设置为初始值。那些不是继承而来的进程描述符成员，主要是统计信息。task_struct中的大多数数据都依然未被修改。 4、为子进程创建第一个页表，将进程0的页表项内容赋给这个页表。 copy_process()————&gt;copy_fs(),_copy_fs_struct(current-&gt;fs)中current指针表示当前进程也就是父进程的 copy_fs()函数为子进程复制父进程的页目录项 1234567891011static inline int copy_fs(unsigned long clone_flags, struct task_struct * tsk)&#123; if (clone_flags &amp; CLONE_FS) &#123; atomic_inc(&amp;current-&gt;fs-&gt;count); return 0; &#125; tsk-&gt;fs = __copy_fs_struct(current-&gt;fs); if (!tsk-&gt;fs) return -ENOMEM; return 0;&#125; _copy_fs_struct() 123456789101112131415161718192021222324static inline struct fs_struct *__copy_fs_struct(struct fs_struct *old)&#123; struct fs_struct *fs = kmem_cache_alloc(fs_cachep, GFP_KERNEL); /* We don&#x27;t need to lock fs - think why ;-) */ if (fs) &#123; atomic_set(&amp;fs-&gt;count, 1); rwlock_init(&amp;fs-&gt;lock); fs-&gt;umask = old-&gt;umask; read_lock(&amp;old-&gt;lock); //进行加锁不能被打断 fs-&gt;rootmnt = mntget(old-&gt;rootmnt); fs-&gt;root = dget(old-&gt;root); fs-&gt;pwdmnt = mntget(old-&gt;pwdmnt); fs-&gt;pwd = dget(old-&gt;pwd); if (old-&gt;altroot) &#123; fs-&gt;altrootmnt = mntget(old-&gt;altrootmnt); fs-&gt;altroot = dget(old-&gt;altroot); &#125; else &#123; fs-&gt;altrootmnt = NULL; fs-&gt;altroot = NULL; &#125; read_unlock(&amp;old-&gt;lock); &#125; return fs;&#125; fs_struct数据结构，这个数据结构将VFS层里面的描述页目录对象的结构体进行了实例化，这样就可以为子进程创建一个页目录项，同时这个fs_strcut结构体和为子进程分配内核栈一样都是通过页高速缓存实现的：struct fs_struct *fs &#x3D; kmem_cache_alloc(fs_cachep, GFP_KERNEL); 1234567struct fs_struct &#123; atomic_t count; rwlock_t lock; int umask; struct dentry * root, * pwd, * altroot; //struct denty 页目录项结构体 struct vfsmount * rootmnt, * pwdmnt, * altrootmnt;&#125;; copy_files()函数，为子进程复制父进程的页表，共享父进程的文件 1234567891011121314151617181920212223242526272829303132static int copy_files(unsigned long clone_flags, struct task_struct * tsk)&#123; struct files_struct *oldf, *newf; int error = 0; /* * A background process may not have any files ... */ oldf = current-&gt;files; //将父进程的页表 if (!oldf) goto out; if (clone_flags &amp; CLONE_FILES) &#123; atomic_inc(&amp;oldf-&gt;count); goto out; &#125; /* * Note: we may be using current for both targets (See exec.c) * This works because we cache current-&gt;files (old) as oldf. Don&#x27;t * break this. */ tsk-&gt;files = NULL; newf = dup_fd(oldf, &amp;error); if (!newf) goto out; tsk-&gt;files = newf; error = 0;out: return error;&#125; dup_fd() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091/* * Allocate a new files structure and copy contents from the * passed in files structure. * errorp will be valid only when the returned files_struct is NULL. */ files_structstatic struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)&#123; struct files_struct *newf; struct file **old_fds, **new_fds; int open_files, size, i; struct fdtable *old_fdt, *new_fdt; *errorp = -ENOMEM; newf = alloc_files(); if (!newf) goto out; spin_lock(&amp;oldf-&gt;file_lock); old_fdt = files_fdtable(oldf); new_fdt = files_fdtable(newf); open_files = count_open_files(old_fdt); /* * Check whether we need to allocate a larger fd array and fd set. * Note: we&#x27;re not a clone task, so the open count won&#x27;t change. */ if (open_files &gt; new_fdt-&gt;max_fds) &#123; new_fdt-&gt;max_fds = 0; spin_unlock(&amp;oldf-&gt;file_lock); spin_lock(&amp;newf-&gt;file_lock); *errorp = expand_files(newf, open_files-1); spin_unlock(&amp;newf-&gt;file_lock); if (*errorp &lt; 0) goto out_release; new_fdt = files_fdtable(newf); /* * Reacquire the oldf lock and a pointer to its fd table * who knows it may have a new bigger fd table. We need * the latest pointer. */ spin_lock(&amp;oldf-&gt;file_lock); old_fdt = files_fdtable(oldf); &#125; old_fds = old_fdt-&gt;fd; new_fds = new_fdt-&gt;fd; memcpy(new_fdt-&gt;open_fds-&gt;fds_bits, old_fdt-&gt;open_fds-&gt;fds_bits, open_files/8); memcpy(new_fdt-&gt;close_on_exec-&gt;fds_bits, old_fdt-&gt;close_on_exec-&gt;fds_bits, open_files/8); for (i = open_files; i != 0; i--) &#123; struct file *f = *old_fds++; if (f) &#123; get_file(f); &#125; else &#123; /* * The fd may be claimed in the fd bitmap but not yet * instantiated in the files array if a sibling thread * is partway through open(). So make sure that this * fd is available to the new process. */ FD_CLR(open_files - i, new_fdt-&gt;open_fds); &#125; rcu_assign_pointer(*new_fds++, f); &#125; spin_unlock(&amp;oldf-&gt;file_lock); /* compute the remainder to be cleared */ size = (new_fdt-&gt;max_fds - open_files) * sizeof(struct file *); /* This is long word aligned thus could use a optimized version */ memset(new_fds, 0, size); if (new_fdt-&gt;max_fds &gt; open_files) &#123; int left = (new_fdt-&gt;max_fds-open_files)/8; int start = open_files / (8 * sizeof(unsigned long)); memset(&amp;new_fdt-&gt;open_fds-&gt;fds_bits[start], 0, left); memset(&amp;new_fdt-&gt;close_on_exec-&gt;fds_bits[start], 0, left); &#125; return newf;out_release: kmem_cache_free(files_cachep, newf);out: return NULL;&#125; files_struct结构体，files_struct结构保存了进程打开的所有文件表数据，描述一个正被打开的文件。 123456789101112struct files_struct &#123; atomic_t count; //自动增量 struct fdtable *fdt; struct fdtable fdtab; fd_set close_on_exec_init; //执行exec时需要关闭的文件描述符初值集合 fd_set open_fds_init; //当前打开文件的文件描述符屏蔽字 struct file * fd_array[NR_OPEN_DEFAULT]; spinlock_t file_lock; /* Protects concurrentwriters. Nests inside tsk-&gt;alloc_lock */&#125;; alloc_files()函数 123456789101112131415161718192021222324static struct files_struct *alloc_files(void)&#123; struct files_struct *newf; struct fdtable *fdt; newf = kmem_cache_alloc(files_cachep, GFP_KERNEL); if (!newf) goto out; atomic_set(&amp;newf-&gt;count, 1); spin_lock_init(&amp;newf-&gt;file_lock); newf-&gt;next_fd = 0; fdt = &amp;newf-&gt;fdtab; fdt-&gt;max_fds = NR_OPEN_DEFAULT; fdt-&gt;close_on_exec = (fd_set *)&amp;newf-&gt;close_on_exec_init; fdt-&gt;open_fds = (fd_set *)&amp;newf-&gt;open_fds_init; fdt-&gt;fd = &amp;newf-&gt;fd_array[0]; INIT_RCU_HEAD(&amp;fdt-&gt;rcu); fdt-&gt;next = NULL; rcu_assign_pointer(newf-&gt;fdt, fdt);out: return newf;&#125; 4、子进程的状态被设置为TASK_UNINTERRUPTEIBLE,保证子进程不会投入运行。 前面对于子进程个性化设置没有分析得很清楚，后面自己弄懂了再来补充。 先总结一下fork()的执行流程然后在来解决文章刚开始的问题。 从上面的分析可以看出fork()的流程大概是： 1、p &#x3D; dup_task_struct(current);　为新进程创建一个内核栈、thread_iofo和task_struct,这里完全copy父进程的内容，所以到目前为止，父进程和子进程是没有任何区别的。 2、为新进程在其内存上建立内核堆栈 3、对子进程task_struct任务结构体中部分变量进行初始化设置，检查所有的进程数目是否已经超出了系统规定的最大进程数，如果没有的话，那么就开始设置进程描诉符中的初始值，从这开始，父进程和子进程就开始区别开了。 4、把父进程的有关信息复制给子进程，建立共享关系 5、设置子进程的状态为不可被TASK_UNINTERRUPTIBLE，从而保证这个进程现在不能被投入运行，因为还有很多的标志位、数据等没有被设置 6、复制标志位（falgs成员）以及权限位(PE_SUPERPRIV)和其他的一些标志 7、调用get_pid()给子进程获取一个有效的并且是唯一的进程标识符PID 8、return ret_from_fork;返回一个指向子进程的指针，开始执行 关于文章开始提出的问题，我们可以从前面的分析知道，子进程的产生是从父进程那儿复制的内核栈、页表项以及与父进程共享文件(对于父进程的文件只能读不能写)，所以子进程如果没有执行exac()函数载入自己的可执行代码，他和父进程将共享数据即代码段数据段，这就是为什么fork()一次感觉执行了两次printf()函数，至于为什么不是6次“+”这个和标准I&#x2F;O里面的缓冲有关系，所以后面我用了一个不带缓冲的I&#x2F;O函数进行了测试输出是6次“-”，在子进程复制父进程的内核栈、页表项、页表的时候页把缓存复制到了子进程中，所以多了两次。 可以从下面的图中看明白 总结 linux创建一个新的进程是从复制父进程内核栈、页表项开始的，在系统内核里首先是将父进程的进程描述符进行拷贝，然后再根据自己的情况修改相应的参数，获取自己的进程号，再开始执行。 后续关于线程 在前面我们讲的是在linux中创建一个进程，其实在其中创建线程也和上面的流程一样，只是我们需要设置标志位让子进程与父进程共享数据。linux实现线程的机制非常独特，从内核的角度讲，linux没有线程这个说法，linux把所有的线程都当做进程来实现。内核没有准备特别的调度算法或者是定义特别的数据结构来表征线程。相反，线程仅仅被视为一个与其它进程共享某些资源的进程。每个线程都拥有唯一隶属于自己的task_struct，所以在内核中看起来像一个普通的进程只是线程和其它进程共享某些资源，如地址空间。 所以linux里面实现线程的方法和windows或者sun solaris等操作系统实现差异非常大。这些操作系统在内核里面专门提供了支持线程的机制。对于linux来说线程只是一种共享资源的手段。 线程创建时和普通的进程类似，只不过在调用clone()的时候需要传递一些参数标志来指明需要共享的资源。如： CLONE_FILES:父子进程共享打开的文件 CLONE_FS：父子进程共享打开的文件系统信息 。。。。 后续关于进程终结 一般来说进程的析构是自身引起的。它发生在进程调用exit()系统调用时，既可以显示的调用这个系统调用，也可以隐式的从某个函数返回，C语言编译器会在main函数的返回点后面放置调用exit()的代码。当进程接收到它既不能处理也不能忽略的信号或者异常时，它还能被动的终结。调用do_exit()函数完成进程的终结。进程的终结就是一个释放进程占有的资源的过程。 版权声明：本文为知乎博主「极致Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/547943571","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核堆栈浅谈","path":"/2023/01/15/linux-docs/Linux内核堆栈浅谈/","content":"内核为每个进程分配一个task_struct结构时，实际上分配两个连续的物理页面(8192字节)，如图所示。底部用作task_struct结构(大小约为1K字节)，结构的上面用作内核堆栈(大小约为7K字节)。访问进程自身的task_struct结构，使用宏操作current, 在2.4中定义如下： 根据内核的配置，THREAD_SIZE既可以是4K字节(1个页面)也可以是8K字节(2个页面)。thread_info是52个字节长。下图是当设为8KB时候的内核堆栈：Thread_info在这个内存区的开始处，内核堆栈从末端向下增长。进程描述符不是在这个内存区中，而分别通过task与thread_info指针使thread_info与进程描述符互联。所以获得当前进程描述符的current定义如下: 下面是thread_info结构体的定义： 12345678910111213141516171819struct thread_info &#123; struct task_struct *task; /* main task structure */ struct exec_domain *exec_domain; /* execution domain */ __u32 flags; /* low level flags */ __u32 status; /* thread synchronous flags */ __u32 cpu; /* current CPU */ int preempt_count; /* 0 =&gt; preemptable, &lt;0 =&gt; BUG */ mm_segment_t addr_limit; struct restart_block restart_block; void __user *sysenter_return; #ifdef CONFIG_X86_32 unsigned long previous_esp; /* ESP of the previous stack in case of nested (IRQ) stacks */ __u8 supervisor_stack[0]; #endif unsigned int sig_on_uaccess_error:1; unsigned int uaccess_err:1; /* uaccess failed */ &#125;; 可以看到在thread_info中个task_struct结构体，里面包含的是进程描述符，其他的参数如下：（可以略过） (1) unsigned short used_math; 是否使用FPU。 (2) char comm[16]; 进程正在运行的可执行文件的文件名。 (3) struct rlimit rlim[RLIM_NLIMITS]; 结 构rlimit用于资源管理，定义在linux&#x2F;include&#x2F;linux&#x2F;resource.h中，成员共有两项:rlim_cur是资源的当前最大 数目;rlim_max是资源可有的最大数目。在i386环境中，受控资源共有RLIM_NLIMITS项，即10项，定义在 linux&#x2F;include&#x2F;asm&#x2F;resource.h中，见下表: (4) int errno; 最后一次出错的系统调用的错误号，0表示无错误。系统调用返回时，全程量也拥有该错误号。 (5) long debugreg[8]; 保存INTEL CPU调试寄存器的值，在ptrace系统调用中使用。 (6) struct exec_domain *exec_domain; Linux可以运行由80386平台其它UNIX操作系统生成的符合iBCS2标准的程序。关于此类程序与Linux程序差异的消息就由 exec_domain结构保存。 (7) unsigned long personality; Linux 可以运行由80386平台其它UNIX操作系统生成的符合iBCS2标准的程序。 Personality进一步描述进程执行的程序属于何种UNIX平台的“个性”信息。通常有PER_Linux、PER_Linux_32BIT、 PER_Linux_EM86、PER_SVR3、PER_SCOSVR3、PER_WYSEV386、PER_ISCR4、PER_BSD、 PER_XENIX和PER_MASK等，参见include&#x2F;linux&#x2F;personality.h。 (8) struct linux_binfmt *binfmt; 指向进程所属的全局执行文件格式结构，共有a。out、script、elf和java等四种。结构定义在include&#x2F;linux &#x2F;binfmts.h中(core_dump、load_shlib(fd)、load_binary、use_count)。 (9) int exit_code，exit_signal; 引起进程退出的返回代码exit_code，引起错误的信号名exit_signal。 (10) int dumpable:1; 布尔量，表示出错时是否可以进行memory dump。 (11) int did_exec:1; 按POSIX要求设计的布尔量，区分进程是正在执行老程序代码，还是在执行execve装入的新代码。 (12) int tty_old_pgrp; 进程显示终端所在的组标识。 (13) struct tty_struct *tty; 指向进程所在的显示终端的信息。如果进程不需要显示终端，如0号进程，则该指针为空。结构定义在include&#x2F;linux&#x2F;tty.h中。 (14) struct wait_queue *wait_chldexit; 在进程结束时，或发出系统调用wait4后，为了等待子进程的结束，而将自己(父进程)睡眠在该队列上。结构定义在include&#x2F;linux &#x2F;wait.h中。 13. 进程队列的全局变量 (1) current; 当前正在运行的进程的指针，在SMP中则指向CPU组中正被调度的CPU的当前进程: #define current(0+current_set[smp_processor_id()])&#x2F;sched.h&#x2F; struct task_struct *current_set[NR_CPUS]; (2) struct task_struct init_task; 即0号进程的PCB，是进程的“根”，始终保持初值INIT_TASK。 (3) struct task_struct *task[NR_TASKS]; 进程队列数组，规定系统可同时运行的最大进程数(见kernel&#x2F;sched.c)。NR_TASKS定义在include&#x2F;linux&#x2F;tasks.h 中，值为512。每个进程占一个数组元素(元素的下标不一定就是进程的pid)，task[0]必须指向init_task(0号进程)。可以通过 task[]数组遍历所有进程的PCB。但Linux也提供一个宏定义for_each_task()(见 include&#x2F;linux&#x2F;sched.h)，它通过next_task遍历所有进程的PCB: #define for_each_task(p) \\ for(p&#x3D;&amp;init_task;(p&#x3D;p-&gt;next_task)!&#x3D;&amp;init_task;) (4) unsigned long volatile jiffies; Linux的基准时间(见kernal&#x2F;sched.c)。系统初始化时清0，以后每隔10ms由时钟中断服务程序do_timer()增1。 (5) int need_resched; 重新调度标志位(见kernal&#x2F;sched.c)。当需要Linux调度时置位。在系统调用返回前(或者其它情形下)，判断该标志是否置位。置位的话，马上调用schedule进行CPU调度。 (6) unsigned long intr_count; 记 录中断服务程序的嵌套层数(见kernal&#x2F;softirq.c)。正常运行时，intr_count为0。当处理硬件中断、执行任务队列中的任务或者执 行bottom half队列中的任务时，intr_count非0。这时，内核禁止某些操作，例如不允许重新调度。 接下来写一个模块，打印当前的进程名字： 1234567891011121314151617181920212223242526272829303132#include &lt;linux/init.h&gt;#include &lt;linux/thread_info.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/sched.h&gt;MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;binary_tree&quot;);int test_init()&#123; printk(&quot;hello binary_tree! &quot;); int i=0; struct thread_info * info; struct task_struct * t; unsigned long addr =(unsigned long)&amp;i;// unsigned long base = addr &amp; ~ 0x1fff;//屏蔽低13位 8K info = (struct thread_info *)base;// t= info -&gt; task; printk(&quot;it is name is %s &quot;,t-&gt; comm);//打印出进程名字 return 0;&#125;void test_exit()&#123; printk(&quot;bye! bye ! binary_tree! &quot;);&#125;module_init(test_init);module_exit(test_exit); 在栈中，struct_task的存在方式如下： 可以看到在struct_task中有一个staks的结构体，就是一个循环双向链表，因此，我们可以模拟出一个PS命令： （今天布置的习题） 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;linux/init.h&gt;#include &lt;linux/thread_info.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/sched.h&gt;MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;bunfly&quot;);int test_init()&#123; int i = 0; struct task_struct *t; struct thread_info *info; unsigned long addr = (unsigned long)&amp;i; unsigned long base = addr &amp; ~0x1fff; info = (struct thread_info *)base; t = info-&gt;task; struct task_struct* flag = t; struct task_struct *next = container_of(t-&gt;tasks.next, struct task_struct, tasks); //首地址 子类的类型， 父类 //获得t下一个进程符next; printk(&quot;now comm is %s &quot;,t-&gt;comm); struct task_struct *nnext = container_of(next-&gt;tasks.next, struct task_struct, tasks); //获得next下一个进程nnext while(nnext != flag) &#123; next=nnext; nnext=container_of(next-&gt;tasks.next, struct task_struct, tasks); printk(&quot;next comm is %s &quot;,next-&gt;comm); &#125; //依次循环打印下一个进程 printk(&quot;nnext comm is %s &quot;,nnext-&gt;comm); return 0;&#125;void test_exit()&#123; printk(&quot;exit &quot;);&#125;module_init(test_init);module_exit(test_exit); 在代码中，用到一个函数：container_of(ptr,type,mem),其中三个参数可分别描述为：ptr, type,mem,分别代表： ptr:父类在子类对象中的首地址; type:子类的类型： mem:父类的实体（成员）: 其具体的功能就是求下面是对container_of函数的具体实现：（重点讲解的） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647481 #include&lt;stdio.h&gt; 2 3 #define container_of(ptr,type,mem) (type*)((unsigned long )(ptr)-(unsigned long)(&amp;((type*)0)-&gt;mem)); 4 struct person 5 &#123; 6 int age; 7 struct person* next; 8 &#125;; 9 10 struct man 11 &#123; 12 int len; 13 int size; 14 char name; 15 struct person p; 16 &#125;; 17 18 int main() 19 &#123; 20 struct man haha; 21 22 haha.len=100; 23 haha.p.age=20; 24 struct man* head = &amp;haha.p;//已知父类在子类中的首地址 //3 25 // struct man* tmp=malloc(1); 26 // int size = (unsigned long)(&amp;tmp-&gt;p)-(unsigned long)(tmp); 27 //2 28 // struct man* tmp=0; 29 // int size = (unsigned long)(&amp;tmp-&gt;p); 30 // struct man* m=(struct man*)( (unsigned long )(head)-size ); 31 //1 32 //struct man* tmp=0; 33 //int size = (unsigned long)(&amp;tmp-&gt;p); 34 //struct man* m=(struct man*)( (unsigned long )(head)-size ); 35 36 // struct man* m=(struct man*)((unsigned long )(head)-(unsigned long)(&amp;((struct man*)0)-&gt;p)); 37 struct man* m = container_of(head,struct man,p); 38 39 // printf(&quot;head addr is %p &quot;,&amp;head); 40 // printf(&quot;m addr is %p &quot;,&amp;m); 41 // printf(&quot;head of len is %d &quot;,head-&gt;len); 42 // printf(&quot;head of age is %d &quot;,(head-&gt;p.age) ); 43 44 printf(&quot;m of len is %d &quot;,m-&gt;len); 45 printf(&quot;m of age is %d &quot;,m-&gt;p.age); 46 47 &#125;~ 版权声明：本文为知乎博主「极致Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/549156208","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核操作系统原理与概述(流程图)","path":"/2023/01/15/linux-docs/Linux内核操作系统原理与概述(流程图)/","content":"1、操作系统是什么操作系统(Operating System,OS)是控制应用程序执行和充当硬件系统和应用程序之间的界面的软件。 计算机系统由硬件和软件两部分组成。操作系统(OS，Operating System)是配置在计算机硬件上的第一层软件，是对硬件系统的首次扩充。它在计算机系统中占据了特别重要的地位； 而其它的诸如汇编程序、编译程序、数据库管理系统等系统软件，以及大量的应用软件，都将依赖于操作系统的支持，取得它的服务。操作系统已成为现代计算机系统(大、中、小及微型机)、多处理机系统、计算机网络、多媒体系统以及嵌入式系统中都必须配置的、最重要的系统软件。 1.1初步认识操作系统及其功能当前比较流行的操作系统有： 操作系统具体能做什么？毫无疑问，我们都知道的有 运行程序、控制多个程序并发运行、管理系统资源、监控系统状态、 提供图形化交互界面、 存储文件，读取文件 进程管理 进程控制：创建、暂停、唤醒、撤销等； 进程调度：调度策略、优先级； 进程通信：进程之间怎么通信。 设备管理 设备的分配和调度； 设备的无关性动作； 设备的传输控制； 设备的驱动管理 内存管理 内存分配； 内存共享； 内存保护； 虚拟内存——我们运行程序从来因为内存过小而失败，只会变慢、卡。 文件管理 存储的空间管理； 文件的操作； 目录的操作； 文件和目录的存取权限管理等 1.2操作系统的地位 2，为什么需要操作系统一般地说，在计算机硬件上配置的 OS，其目标有以下几点： 1．有效性 在早期(20 世纪 50～60 年代)，由于计算机系统非常昂贵，操作系统最重要的目标无疑是有效性。事实上，那时有效性是推动操作系统发展最主要的动力。操作系统的有效性可包含如下两方面的含义： (1) 提高系统资源利用率。在未配置 OS 的计算机系统中，诸如 CPU、I&#x2F;O 设备等各种资源，都会因它们经常处于空闲状态而得不到充分利用；内存及外存中所存放的数据太少或者无序而浪费了大量的存储空间。配置了 OS 之后，可使 CPU 和 I&#x2F;O 设备由于能保持稳定的状态而得到有效的利用，且可使内存和外存中存放的数据因有序而节省了存储空间。 (2) 提高系统的吞吐量。操作系统还可以通过合理地组织计算机的工作流程，而进一步改善资源的利用率，加速程序的运行，缩短程序的运行周期，从而提高系统的吞吐量。 2．方便性 配置 OS 后者可使计算机系统更容易使用。一个未配置 OS 计算机系统是极难使用的，因为计算机硬件只能识别 0 和 1 这样的机器代码。用户要直接在计算机硬件上运行自己所编写的程序，就必须用机器语言书写程序；如果我们在计算机硬件上配置了 OS，用户便可通过 OS 所提供的各种命令来使用计算机系统。比如，用编译命令可方便地把用户用高级语言书写的程序翻译成机器代码，大大地方便了用户，从而使计算机变得易学易用。方便性和有效性是设计操作系统时最重要的两个目标。 3．可扩充性 随着 VLSI 技术和计算机技术的迅速发展，计算机硬件和体系结构也随之得到迅速发展，相应地，它们也对 OS 提出了更高的功能和性能要求。此外，多处理机系统、计算机网络，特别是 Internet 的发展，又对 OS 提出了一系列更新的要求。因此，OS 必须具有很好的可扩充性，方能适应计算机硬件、体系结构以及应用发展的要求。这就是说，现代 OS 应采用新的 OS 结构，如微内核结构和客户服务器模式，以便于方便地增加新的功能和模块，并能修改老的功能和模块。 4．开放性 自 20 世纪 80 年代以来，由于计算机网络的迅速发展，特别是 Internet 的应用的日益普及，使计算机操作系统的应用环境已由单机封闭环境转向开放的网络环境。为使来自不同厂家的计算机和设备能通过网络加以集成化，并能正确、有效地协同工作，实现应用的可移植性和互操作性，要求操作系统必须提供统一的开放环境，进而要求 OS 具有开放性。开放性是指系统能遵循世界标准规范，特别是遵循开放系统互连(OSI)国际标准。凡遵循国际标准所开发的硬件和软件，均能彼此兼容，可方便地实现互连。开放性已成为20世纪 90 年代以后计算机技术的一个核心问题，也是一个新推出的系统或软件能否被广泛应用的至关重要的因素。 硬件发展时代划分： 3，操作系统的发展史3.1手工操作时代 3.2 单批道处理系统 3.3 多批道处理系统 3.4分时技术和分时操作系统 分时系统实例 4，典型的操作系统 4.1微机操作系统 4.2实时操作系统 所谓“实时”，是表示“及时”，而实时系统(Real Time System)是指系统能及时(或即时)响应外部事件的请求，在规定的时间内完成对该事件的处理，并控制所有实时任务协调一致地运行。 1、需求： 虽然多道批处理系统和分时系统已能获得较为令人满意的资源利用率和响应时间，从而使计算机的应用范围日益扩大，但它们仍然不能满足以下某些应用领域的需要。 (1) 实时控制。当把计算机用于生产过程的控制，以形成以计算机为中心的控制系统时，系统要求能实时采集现场数据，并对所采集的数据进行及时处理，进而自动地控制相应的执行机构，使某些(个)参数(如温度、压力、方位等)能按预定的规律变化，以保证产品的质量量和提高产量。类似地，也可将计算机用于对武器的控制，如火炮的自动控制系统、飞机自动驾驶系统，以及导弹的制导系统等。此外，随着大规模集成电路的发展，已制作出各种类型的芯片，并可将这些芯片嵌入到各种仪器和设备中，用来对设备的工作进行实施控制，这就构成了所谓的智能仪器和设备。在这些设备中也需要配置某种类型的、能进行实时控制的系统。通常把用于进行实时控制的系统称为实时系统。 (2) 实时信息处理。通常，人们把用于对信息进行实时处理的系统称为实时信息处理系统统。该系统由一台或多台主机通过通信线路连接到成百上千个远程终端上，计算机接收从远程终端上发来的服务请求，根据用户提出的请求对信息进行检索和处理，并在很短的时间内为用户做出正确的响应。典型的实时信息处理系统有早期的飞机或火车的订票系统、情报检索系统等。 2、实时任务 在实时系统中必然存在着若干个实时任务，这些任务通常与某个(些)外部设备相关，能反应或控制相应的外部设备，因而带有某种程度的紧迫性。可以从不同的角度对实时任务加以分类。 按任务执行时是否呈现周期性来划分 (1) 周期性实时任务。外部设备周期性地发出激励信号给计算机，要求它按指定周期循环执行，以便周期性地控制某外部设备。 (2) 非周期性实时任务。外部设备所发出的激励信号并无明显的周期性，但都必须联系着一个截止时间(Deadline)。它又可分为开始截止时间(某任务在某时间以前必须开始执行)和完成截止时间(某任务在某时间以前必须完成)两部分。 根据对截止时间的要求来划分 (1) 硬实时任务(Hard real-time Task)。系统必须满足任务对截止时间的要求，否则可能出现难以预测的结果。 (2) 软实时任务(Soft real-time Task)。它也联系着一个截止时间，但并不严格，若偶尔错过了任务的截止时间，对系统产生的影响也不会太大。 3、实时系统与分时系统特征的比较 实时系统有着与分时系统相似但并不完全相同的特点，下面从五个方面对这两种系统 加以比较。 (1) 多路性。实时信息处理系统也按分时原则为多个终端用户服务。实时控制系统得多路性则主要表现在系统周期性地对多路现场信息进行采集，以及对多个对象或多个执行机构进行控制。而分时系统中的多路性则与用户情况有关，时多时少。 (2) 独立性。实时信息处理系统中的每个终端用户在向实时系统提出服务请求时，是彼此独立地操作，互不干扰；而实时控制系统中，对信息的采集和对对象的控制也都是彼此互不干扰。 (3) 及时性。实时信息处理系统对实时性的要求与分时系统类似，都是所有人所能接受的等待时间来确定的；而实时控制系统的及时性，则是以控制对象所要求的开始截止时间为准完成截止时间来确定的，一般为秒级到毫秒级，甚至有的要低于 100 微秒。 (4) 交互性。实时信息处理系统虽然也具有交互性，但这里人与系统的交互仅限于访问系统中某些特定的专用服务程序。它不像分时系统那样能向终端用户提供数据处理和资源共享等服务。 (5) 可靠性。分时系统虽然也要求系统可靠，但相比之下，实时系统则要求系统更高度的可靠性。因为任何差错都可能带来巨大的经济损失，甚至是无法预料的灾难性后果，所以在实时系统中，往往都采取了多级容错措施来保障系统的安全性及数据的安全性。 4.3嵌入式系统 4.4网络系统 5，操作系统的功能4.1、OS 作为用户与计算机硬件系统之间的接口 OS 作为用户与计算机硬件系统之间接口的含义是：OS 处于用户与计算机硬件系统之间间，用户通过 OS 来使用计算机系统。或者说，用户在 OS 帮助下，能够方便、快捷、安全、可靠地操纵计算机硬件和运行自己的程序。应注意，OS 是一个系统软件，因而这种接口是软件接口。图 1-1 是 OS 作为接口的示意图。由图可看出，用户可通过以下三种方式使用计算机。 1、命令方式。这是指由 OS 提供了一组联机命令接口，以允许用户通过键盘输入有关命令来取得操作系统的服务，并控制用户程序的运行。 2、系统调用方式。OS提供了一组系统调用，用户可在自己的应用程序中通过相应的系统调用，来实现与操作系统的通信，并取得它的服务。 3、 图形、窗口等方式。这是当前使用最为方便、最为广泛的接口，它允许用户通过屏幕上的窗口和图标来实现与操作系统的通信，并取得它的服务。 4.2、OS 作为计算机系统资源的管理者 在一个计算机系统中，通常都含有各种各样的硬件和软件资源。归纳起来可将资源分为四类：处理器、存储器、I&#x2F;O 设备以及信息(数据和程序)。相应地，OS 的主要功能也正是针对这四类资源进行有效的管理，即： 处理机管理，用于分配和控制处理机； 存储器管理，主要负责内存的分配与回收； I&#x2F;O 设备管理，负责 I&#x2F;O 设备的分配与操纵； 文件管理，负责文件的存取、共享和保护。 可见，OS 的确是计算机系统资源的管理者。事实上，当今世界上广为流行的一个关于 OS 作用的观点，正是把 OS 作为计算机系统的资源管理者。值得进一步说明的是，当一个计算机系统同时供多个用户使用时，用户对系统中共享资源的需求(包括数量和时间)可能发生冲突，为了管理好这些共享资源(包括硬件和信息)的使用，操作系统必须记录下各种资源的使用情况，对使用资源的请求进行授权，协调诸用户对共享资源的使用，避免发生冲突，并计算使用资源的费用等 4.3、OS 实现了对计算机资源的抽象 对于一个完全无软件的计算机系统(即裸机)，它向用户提供的是实际硬件接口(物理接口)，用户必须对物理接口的实现细节有充分的了解，并利用机器指令进行编程，因此该物理机器必定是难以使用的。为了方便用户使用 I&#x2F;O 设备，人们在裸机上覆盖上一层 I&#x2F;O 设备管理软件，如图 1-2 所示，由它来实现对 I&#x2F;O 设备操作的细节，并向上提供一组 I&#x2F;O 操作命令，如 Read 和 Write 命令，用户可利用它来进行数据输入或输出，而无需关心 I&#x2F;O 是如何实现的。 6、操作系统的基本特性6.1、并发性并发性是指同一时间间隔内发生两个或多个事件。并行性是指同一时刻内发生两个或多个事件 1．并行与并发：并行性和并发性 (Concurrence) 是既相似又有区别的两个概念，并行性是指两个或多个事件在同一时刻发生；而并发性是指两个或多个事件在同一时间间隔内发生。在多道程序环境下，并发性是指在一段时间内宏观上有多个程序在同时运行，但在单处理机系统中每一时刻却仅能有一道程序执行，故微观上这些程序只能是分时地交替执行。倘若在计算机系统中有多个处理机，则这些可以并发执行的程序便可被分配到多个处理机上，实现并行执行，即利用每个处理机来处理一个可并发执行的程序，这样，多个程序便可同时执行。 2．引入进程：应当指出，通常的程序是静态实体(Passive Entity)，在多道程序系统中，它们是不能独立运行的，更不能和其它程序并发执行。在操作系统中引入进程的目的，就是为了使多个程序能并发执行。例如，在一个未引入进程的系统中，在属于同一个应用程序的计算程序和 I&#x2F;O 程序之间，两者只能是顺序执行，即只有在计算程序执行告一段落后，才允许 I&#x2F;O 程序执行；反之，该程序执行 I&#x2F;O 操作时，计算程序也不能执行，这意味着处理机处于空闲状态 。但在引入进程后，若分别为计算程序和 I&#x2F;O 程序各建立一个进程，则这两个进程便可并发执行。由于在系统中具备计算程序和 I&#x2F;O 程序同时运行的硬件条件，因而可将系统中的 CPU 和 I&#x2F;O 设备同时开动起来，实现并行工作，从而有效地提高了系统资源的利用率和系统吞吐量，并改善了系统的性能。引入进程的好处远不止于此，事实上可以在内存中存放多个用户程序，分别为它们建立进程后，这些进程可以并发执行，亦即实现前面所说的多道程序运行。这样便能极大地提高系统资源的利用率，增加系统的吞吐量。为使多个程序能并发执行，系统必须分别为每个程序建立进程(Process)。简单说来，进程是指在系统中能独立运行并作为资源分配的基本单位，它是由一组机器指令、数据和堆栈等组成的，是一个能独立运行的活动实体。多个进程之间可以并发执行和交换信息。一个进程在运行时需要一定的资源，如 CPU、存储空间及 I&#x2F;O 设备等。OS 中程序的并发执行将使系统复杂化，以致在系统中必须增设若干新的功能模块，分别用于对处理机、内存、I&#x2F;O 设备以及文件系统等资源进行管理，并控制系统中所有作业的运行。事实上，进程和并发是现代操作系统中最重要的基本概念，也是操作系统运行的基础 3．引入线程：长期以来，进程都是操作系统中可以拥有资源并作为独立运行的基本单位。当一个进程因故不能继续运行时，操作系统便调度另一个进程运行。由于进程拥有自己的资源，故使调度付出的开销较大。直到 20 世纪 80 年代中期，人们才又提出了比进程更小的单位——线程(Threads)。通常在一个进程中可以包含若干个线程，它们可以利用进程所拥有的资源。在引入线程的 OS 中，通常都是把进程作为分配资源的基本单位，而把线程作为独立运行和独立调度的基本单位。由于线程比进程更小，基本上不拥有系统资源，故对它的调度所付出的开销就会小得多，能更高效地提高系统内多个程序间并发执行的程度。因而近年来推出的通用操作系统都引入了线程，以便进一步提高系统的并发性，并把它视作现代操作系统的一个重要标致。 6.2、共享性在操作系统环境下，所谓共享 (Sharing)， 是指系统中的资源可供内存中多个并发执行的进程(线程)共同使用，相应地，把这种资源共同使用称为资源共享，或称为资源复用。由于各种资源的属性不同，进程对资源复用的方式也不同，目前主要实现资源共享的方式有如下两种。 1．互斥共享方式：系统中的某些资源，如打印机、磁带机，虽然它们可以提供给多个进程(线程)使用，但为使所打印或记录的结果不致造成混淆，应规定在一段时间内只允许一个进程(线程)访问该资源。为此，系统中应建立一种机制，以保证对这类资源的互斥访问。当一个进程 A 要访问某资源时，必须先提出请求。如果此时该资源空闲，系统便可将之分配给请求进程 A 使用。此后若再有其它进程也要访问该资源时(只要 A 未用完)，则必须等待。仅当 A 进程访问完并释放该资源后，才允许另一方进程对该资源进行访问。我们把这种资源共享方式称为互斥式共享，而把在一段时间内只允许一个进程访问的资源称为临界资源或独占资源。计算机系统中的大多数物理设备，以及某些软件中所用的栈、变量和表格，都属于临界资源，它们要求被互斥地共享。为此，在系统中必须配置某种机制来保证诸进程互斥地使用独占资源。 2．同时访问方式：系统中还有另一类资源，允许在一段时间内由多个进程“同时”对它们进行访问。这里所谓的“同时”，在单处理机环境下往往是宏观上的，而在微观上，这些进程可能是交替地对该资源进行访问。典型的可供多个进程“同时”访问的资源是磁盘设备，一些用重入码编写的文件也可以被“同时”共享，即若干个用户同时访问该文件。并发和共享是操作系统的两个最基本的特征，它们又是互为存在的条件。一方面，资源共享是以程序(进程)的并发执行为条件的，若系统不允许程序并发执行，自然不存在资源共享问题；另一方面，若系统不能对资源共享实施有效管理，协调好诸进程对共享资源的访问，也必然影响到程序并发执行的程度，甚至根本无法并发执行。 6.3、虚拟技术操作系统中的所谓“虚拟” (Virtual) ，是指通过某种技术把一个物理实体变为若干个逻辑上的对应物。物理实体(前者)是实的，即实际存在的，而后者是虚的，仅是用户感觉上的东西。相应地，用于实现拟的技术称为虚拟技术。在操作系统中利用了两种方式实现虚拟技术，即时分复用技术和空分复用技术。 1、时分复用技术 时分复用，亦即分时使用方式，它最早用于电信业中。为了提高信道的利用率，人们利用时分复用方式，将一条物理信道虚拟为多条逻辑信道，将每条信道供一对用户通话。在计算机领域中，广泛利用该技术来实现虚拟处理机、虚拟设备等，以提高资源的利用率。 12虚拟处理机技术虚拟设备技术 2．空分复用技术 早在上世纪初，电信业中就使用频分复用技术来提高信道的利用率。它是将一个频率范围非常宽的信道，划分成多个频率范围较窄的信道，其中的任何一个频道都只供一对用户通话。早期的频分复用只能将一条物理信道划分为十几条到几十条话路，后来又很快发展成上万条话路，每条话路也只供一对用户通话。之后，在计算机中也使用了空分复用技术来提高存储空间的利用率。 12虚拟磁盘技术虚拟存储器技术 应当着重指出：如果虚拟的实现是通过时分复用的方法来实现的，即对某一物理设备进行分时使用，设 N 是某物理设备所对应的虚拟的逻辑设备数，则每台虚拟设备的平均速度必然等于或低于物理设备速度的 1&#x2F;N。类似地，如果是利用空分复用方法来实现虚拟，此时一台虚拟设备平均占用的空间必然也等于或低于物理设备所拥有空间的 1&#x2F;N。 6.4、异步性在多道程序环境下允许多个进程并发执行，但只有进程在获得所需的资源后方能执行。在单处理机环境下，由于系统中只有一台处理机，因而每次只允许一个进程执行，其余进程只能等待。当正在执行的进程提出某种资源要求时，如打印请求，而此时打印机正在为其它某进程打印，由于打印机属于临界资源，因此正在执行的进程必须等待，且放弃处理机，直到打印机空闲，并再次把处理机分配给该进程时，该进程方能继续执行。可见，由于资源等因素的限制，使进程的执行通常都不是“一气呵成”，而是以“停停走走”的方式运行。 内存中的每个进程在何时能获得处理机运行，何时又因提出某种资源请求而暂停，以及进程以怎样的速度向前推进，每道程序总共需多少时间才能完成，等等，这些都是不可预知的。由于各用户程序性能的不同，比如，有的侧重于计算而较少需要 I&#x2F;O，而有的程序其计算少而 I&#x2F;O 多，这样，很可能是先进入内存的作业后完成，而后进入内存的作业先完成。或者说，进程是以人们不可预知的速度向前推进，此即进程的异步性(Asynchronism)。尽管如此，但只要在操作系统中配置有完善的进程同步机制，且运行环境相同，作业经多次运行都会获得完全相同的结果。因此，异步运行方式是允许的，而且是操作系统的一个重要特征。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/446635858","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核架构和工作原理","path":"/2023/01/15/linux-docs/Linux内核架构和工作原理/","content":"前言：作用是将应用层序的请求传递给硬件，并充当底层驱动程序，对系统中的各种设备和组件进行寻址。目前支持模块的动态装卸(裁剪)。Linux内核就是基于这个策略实现的。Linux进程1.采用层次结构，每个进程都依赖于一个父进程。内核启动init程序作为第一个进程。该进程负责进一步的系统初始化操作。init进程是进程树的根，所有的进程都直接或者间接起源于该进程。virt&#x2F; —- 提供虚拟机技术的支持。 1、Linux内核预备工作理解Linux内核最好预备的知识点： 懂C语言懂一点操作系统的知识熟悉少量相关算法懂计算机体系结构 Linux内核的特点： 结合了Unix操作系统的一些基础概念 Linux内核的任务： 1.从技术层面讲，内核是硬件与软件之间的一个中间层。作用是将应用层序的请求传递给硬件，并充当底层驱动程序，对系统中的各种设备和组件进行寻址。 2.从应用程序的层面讲，应用程序与硬件没有联系，只与内核有联系，内核是应用程序知道的层次中的最底层。在实际工作中内核抽象了相关细节。 3.内核是一个资源管理程序。负责将可用的共享资源(CPU时间、磁盘空间、网络连接等)分配得到各个系统进程。 4.内核就像一个库，提供了一组面向系统的命令。系统调用对于应用程序来说，就像调用普通函数一样。 内核实现策略： 1.微内核。最基本的功能由中央内核（微内核）实现。所有其他的功能都委托给一些独立进程，这些进程通过明确定义的通信接口与中心内核通信。 2.宏内核。内核的所有代码，包括子系统（如内存管理、文件管理、设备驱动程序）都打包到一个文件中。内核中的每一个函数都可以访问到内核中所有其他部分。目前支持模块的动态装卸(裁剪)。Linux内核就是基于这个策略实现的。 哪些地方用到了内核机制？ 1.进程（在cpu的虚拟内存中分配地址空间，各个进程的地址空间完全独立;同时执行的进程数最多不超过cpu数目）之间进行通 信，需要使用特定的内核机制。 2.进程间切换(同时执行的进程数最多不超过cpu数目)，也需要用到内核机制。 进程切换也需要像FreeRTOS任务切换一样保存状态，并将进程置于闲置状态&#x2F;恢复状态。 3.进程的调度。确认哪个进程运行多长的时间。 Linux进程 1.采用层次结构，每个进程都依赖于一个父进程。内核启动init程序作为第一个进程。该进程负责进一步的系统初始化操作。init进程是进程树的根，所有的进程都直接或者间接起源于该进程。 2.通过Pstree命令查询。实际上得系统第一个进程是Systemd，而不是init（这也是疑问点） 3.系统中每一个进程都有一个唯一标识符(ID),用户（或其他进程）可以使用ID来访问进程。 Linux内核源代码的目录结构 Linux内核源代码包括三个主要部分： \\1. 内核核心代码，包括第3章所描述的各个子系统和子模块，以及其它的支撑子系统，例如电源管理、Linux初始化等 \\2. 其它非核心代码，例如库文件（因为Linux内核是一个自包含的内核，即内核不依赖其它的任何软件，自己就可以编译通过）、固件集合、KVM（虚拟机技术）等 \\3. 编译脚本、配置文件、帮助文档、版权说明等辅助性文件。 使用ls命令看到的内核源代码的顶层目录结构，具体描述如下： include&#x2F; —- 内核头文件，需要提供给外部模块（例如用户空间代码）使用。 kernel&#x2F; —- Linux内核的核心代码，包含了3.2小节所描述的进程调度子系统，以及和进程调度相关的模块。 mm&#x2F; —- 内存管理子系统（3.3小节）。 fs&#x2F; —- VFS子系统（3.4小节）。 net&#x2F; —- 不包括网络设备驱动的网络子系统（3.5小节）。 ipc&#x2F; —- IPC（进程间通信）子系统。 arch&#x2F;&#x2F; —- 体系结构相关的代码，例如arm, x86等等。 arch&#x2F;&#x2F;mach- —- 具体的machine&#x2F;board相关的代码。 arch&#x2F;&#x2F;include&#x2F;asm —- 体系结构相关的头文件。 arch&#x2F;&#x2F;boot&#x2F;dts —- 设备树（Device Tree）文件。 init&#x2F; —- Linux系统启动初始化相关的代码。 block&#x2F; —- 提供块设备的层次。 sound&#x2F; —- 音频相关的驱动及子系统，可以看作“音频子系统”。 drivers&#x2F; —- 设备驱动（在Linux kernel 3.10中，设备驱动占了49.4的代码量）。 lib&#x2F; —- 实现需要在内核中使用的库函数，例如CRC、FIFO、list、MD5等。 crypto&#x2F; —– 加密、解密相关的库函数。 security&#x2F; —- 提供安全特性（SELinux）。 virt&#x2F; —- 提供虚拟机技术（KVM等）的支持。 usr&#x2F; —- 用于生成initramfs的代码。 firmware&#x2F; —- 保存用于驱动第三方设备的固件。 samples&#x2F; —- 一些示例代码。 tools&#x2F; —- 一些常用工具，如性能剖析、自测试等。 Kconfig, Kbuild, Makefile, scripts&#x2F; —- 用于内核编译的配置文件、脚本等。 COPYING —- 版权声明。 MAINTAINERS —-维护者名单。 CREDITS —- Linux主要的贡献者名单。 REPORTING-BUGS —- Bug上报的指南。 Documentation, README —- 帮助、说明文档。 2、Linux内核体系结构简析简析 Linux系统层次结构 最上面是用户（或应用程序）空间。这是用户应用程序执行的地方。用户空间之下是内核空间，Linux 内核正是位于这里。GNU C Library （glibc）也在这里。它提供了连接内核的系统调用接口，还提供了在用户空间应用程序和内核之间进行转换的机制。这点非常重要，因为内核和用户空间的应用程序使用的是不同的保护地址空间。每个用户空间的进程都使用自己的虚拟地址空间，而内核则占用单独的地址空间。 Linux 内核可以进一步划分成 3 层。最上面是系统调用接口，它实现了一些基本的功能，例如 read 和 write。系统调用接口之下是内核代码，可以更精确地定义为独立于体系结构的内核代码。这些代码是 Linux 所支持的所有处理器体系结构所通用的。在这些代码之下是依赖于体系结构的代码，构成了通常称为 BSP（Board Support Package）的部分。这些代码用作给定体系结构的处理器和特定于平台的代码。 Linux 内核实现了很多重要的体系结构属性。在或高或低的层次上，内核被划分为多个子系统。Linux 也可以看作是一个整体，因为它会将所有这些基本服务都集成到内核中。这与微内核的体系结构不同，后者会提供一些基本的服务，例如通信、I&#x2F;O、内存和进程管理，更具体的服务都是插入到微内核层中的。每种内核都有自己的优点，不过这里并不对此进行讨论。 随着时间的流逝，Linux 内核在内存和 CPU 使用方面具有较高的效率，并且非常稳定。但是对于 Linux 来说，最为有趣的是在这种大小和复杂性的前提下，依然具有良好的可移植性。Linux 编译后可在大量处理器和具有不同体系结构约束和需求的平台上运行。一个例子是 Linux 可以在一个具有内存管理单元（MMU）的处理器上运行，也可以在那些不提供 MMU 的处理器上运行。 Linux 内核的 uClinux 移植提供了对非 MMU 的支持。 Linux内核体系结构 Linux内核的主要组件有：系统调用接口、进程管理、内存管理、虚拟文件系统、网络堆栈、设备驱动程序、硬件架构的相关代码。 （1）系统调用接口 SCI 层提供了某些机制执行从用户空间到内核的函数调用。正如前面讨论的一样，这个接口依赖于体系结构，甚至在相同的处理器家族内也是如此。SCI 实际上是一个非常有用的函数调用多路复用和多路分解服务。在 .&#x2F;linux&#x2F;kernel 中您可以找到 SCI 的实现，并在 .&#x2F;linux&#x2F;arch 中找到依赖于体系结构的部分。 （2）进程管理 进程管理的重点是进程的执行。在内核中，这些进程称为线程，代表了单独的处理器虚拟化（线程代码、数据、堆栈和 CPU 寄存器）。在用户空间，通常使用进程 这个术语，不过 Linux 实现并没有区分这两个概念（进程和线程）。内核通过 SCI 提供了一个应用程序编程接口（API）来创建一个新进程（fork、exec 或 Portable Operating System Interface [POSIX] 函数），停止进程（kill、exit），并在它们之间进行通信和同步（signal 或者 POSIX 机制）。 进程管理还包括处理活动进程之间共享 CPU 的需求。内核实现了一种新型的调度算法，不管有多少个线程在竞争 CPU，这种算法都可以在固定时间内进行操作。这种算法就称为 O(1) 调度程序，这个名字就表示它调度多个线程所使用的时间和调度一个线程所使用的时间是相同的。O(1) 调度程序也可以支持多处理器（称为对称多处理器或 SMP）。您可以在 .&#x2F;linux&#x2F;kernel 中找到进程管理的源代码，在 .&#x2F;linux&#x2F;arch 中可以找到依赖于体系结构的源代码。 （3）内存管理 内核所管理的另外一个重要资源是内存。为了提高效率，如果由硬件管理虚拟内存，内存是按照所谓的内存页 方式进行管理的（对于大部分体系结构来说都是 4KB）。Linux 包括了管理可用内存的方式，以及物理和虚拟映射所使用的硬件机制。不过内存管理要管理的可不止 4KB 缓冲区。Linux 提供了对 4KB 缓冲区的抽象，例如 slab 分配器。这种内存管理模式使用 4KB 缓冲区为基数，然后从中分配结构，并跟踪内存页使用情况，比如哪些内存页是满的，哪些页面没有完全使用，哪些页面为空。这样就允许该模式根据系统需要来动态调整内存使用。为了支持多个用户使用内存，有时会出现可用内存被消耗光的情况。由于这个原因，页面可以移出内存并放入磁盘中。这个过程称为交换，因为页面会被从内存交换到硬盘上。内存管理的源代码可以在 .&#x2F;linux&#x2F;mm 中找到。 （4）虚拟文件系统 虚拟文件系统（VFS）是 Linux 内核中非常有用的一个方面，因为它为文件系统提供了一个通用的接口抽象。VFS 在 SCI 和内核所支持的文件系统之间提供了一个交换层（请参看图4）。 Linux文件系统层次结构 在 VFS 上面，是对诸如 open、close、read 和 write 之类的函数的一个通用 API 抽象。在 VFS 下面是文件系统抽象，它定义了上层函数的实现方式。它们是给定文件系统（超过 50 个）的插件。文件系统的源代码可以在 .&#x2F;linux&#x2F;fs 中找到。文件系统层之下是缓冲区缓存，它为文件系统层提供了一个通用函数集（与具体文件系统无关）。这个缓存层通过将数据保留一段时间（或者随即预先读取数据以便在需要是就可用）优化了对物理设备的访问。缓冲区缓存之下是设备驱动程序，它实现了特定物理设备的接口。 （5）网络堆栈 网络堆栈在设计上遵循模拟协议本身的分层体系结构。回想一下，Internet Protocol (IP) 是传输协议（通常称为传输控制协议或 TCP）下面的核心网络层协议。TCP 上面是 socket 层，它是通过 SCI 进行调用的。socket 层是网络子系统的标准 API，它为各种网络协议提供了一个用户接口。从原始帧访问到 IP 协议数据单元（PDU），再到 TCP 和 User Datagram Protocol (UDP)，socket 层提供了一种标准化的方法来管理连接，并在各个终点之间移动数据。内核中网络源代码可以在 .&#x2F;linux&#x2F;net 中找到。 （6）设备驱动程序 Linux 内核中有大量代码都在设备驱动程序中，它们能够运转特定的硬件设备。Linux 源码树提供了一个驱动程序子目录，这个目录又进一步划分为各种支持设备，例如 Bluetooth、I2C、serial 等。设备驱动程序的代码可以在 .&#x2F;linux&#x2F;drivers 中找到。 （7）依赖体系结构的代码 尽管 Linux 很大程度上独立于所运行的体系结构，但是有些元素则必须考虑体系结构才能正常操作并实现更高效率。.&#x2F;linux&#x2F;arch 子目录定义了内核源代码中依赖于体系结构的部分，其中包含了各种特定于体系结构的子目录（共同组成了 BSP）。对于一个典型的桌面系统来说，使用的是 x86 目录。每个体系结构子目录都包含了很多其他子目录，每个子目录都关注内核中的一个特定方面，例如引导、内核、内存管理等。这些依赖体系结构的代码可以在 .&#x2F;linux&#x2F;arch 中找到。 如果 Linux 内核的可移植性和效率还不够好，Linux 还提供了其他一些特性，它们无法划分到上面的分类中。作为一个生产操作系统和开源软件，Linux 是测试新协议及其增强的良好平台。Linux 支持大量网络协议，包括典型的 TCP&#x2F;IP，以及高速网络的扩展（大于 1 Gigabit Ethernet [GbE] 和 10 GbE）。Linux 也可以支持诸如流控制传输协议（SCTP）之类的协议，它提供了很多比 TCP 更高级的特性（是传输层协议的接替者）。 Linux 还是一个动态内核，支持动态添加或删除软件组件。被称为动态可加载内核模块，它们可以在引导时根据需要（当前特定设备需要这个模块）或在任何时候由用户插入。 Linux 最新的一个增强是可以用作其他操作系统的操作系统（称为系统管理程序）。最近，对内核进行了修改，称为基于内核的虚拟机（KVM）。这个修改为用户空间启用了一个新的接口，它可以允许其他操作系统在启用了 KVM 的内核之上运行。除了运行 Linux 的其他实例之外， Microsoft Windows也可以进行虚拟化。惟一的限制是底层处理器必须支持新的虚拟化指令。 3、Linux体系结构和内核结构区别1．当被问到Linux体系结构（就是Linux系统是怎么构成的）时，我们可以参照下图这么回答：从大的方面讲，Linux体系结构可以分为两块： （1）用户空间：用户空间中又包含了，用户的应用程序，C库 （2）内核空间：内核空间包括，系统调用，内核，以及与平台架构相关的代码 2．Linux体系结构要分成用户空间和内核空间的原因： 1）现代CPU通常都实现了不同的工作模式， 以ARM为例：ARM实现了7种工作模式，不同模式下CPU可以执行的指令或者访问的寄存器不同： （1）用户模式 usr （2）系统模式 sys （3）管理模式 svc （4）快速中断 fiq （5）外部中断 irq （6）数据访问终止 abt （7）未定义指令异常 以（2）X86为例：X86实现了4个不同级别的权限，Ring0—Ring3 ;Ring0下可以执行特权指令，可以访问IO设备；Ring3则有很多的限制 2）所以，Linux从CPU的角度出发，为了保护内核的安全，把系统分成了2部分； 3．用户空间和内核空间是程序执行的两种不同状态，我们可以通过“系统调用”和“硬件中断“来完成用户空间到内核空间的转移 4．Linux的内核结构（注意区分LInux体系结构和Linux内核结构） 4、Linux驱动的platform机制Linux的这种platform driver机制和传统的device_driver机制相比，一个十分明显的优势在于platform机制将本身的资源注册进内核，由内核统一管理，在驱动程序中使用这些资源时通过platform_device提供的标准接口进行申请并使用。这样提高了驱动和资源管理的独立性，并且拥有较好的可移植性和安全性。下面是SPI驱动层次示意图，Linux中的SPI总线可理解为SPI控制器引出的总线： 和传统的驱动一样，platform机制也分为三个步骤： 1、总线注册阶段： 内核启动初始化时的main.c文件中的kernel_init()→do_basic_setup()→driver_init()→platform_bus_init()→bus_register(&amp;platform_bus_type)，注册了一条platform总线（虚拟总线，platform_bus）。 2、添加设备阶段： 设备注册的时候Platform_device_register()→platform_device_add()→(pdev→dev.bus &#x3D; &amp;platform_bus_type)→device_add()，就这样把设备给挂到虚拟的总线上。 3、驱动注册阶段： Platform_driver_register()→driver_register()→bus_add_driver()→driver_attach()→bus_for_each_dev(), 对在每个挂在虚拟的platform bus的设备作__driver_attach()→driver_probe_device(),判断drv→bus→match()是否执行成功，此时通过指针执行platform_match→strncmp(pdev→name , drv→name , BUS_ID_SIZE),如果相符就调用really_probe(实际就是执行相应设备的platform_driver→probe(platform_device)。)开始真正的探测，如果probe成功，则绑定设备到该驱动。 从上面可以看出，platform机制最后还是调用了bus_register() , device_add() , driver_register()这三个关键的函数。 下面看几个结构体： Platform_device结构体描述了一个platform结构的设备，在其中包含了一般设备的结构体struct device dev;设备的资源结构体struct resource * resource;还有设备的名字const char * name。（注意，这个名字一定要和后面platform_driver.driver àname相同，原因会在后面说明。） 该结构体中最重要的就是resource结构，这也是之所以引入platform机制的原因。 名字要一致的原因： 上面说的驱动在注册的时候会调用函数bus_for_each_dev(), 对在每个挂在虚拟的platform bus的设备作__driver_attach()→driver_probe_device(),在此函数中会对dev和drv做初步的匹配，调用的是drv-&gt;bus-&gt;match所指向的函数。platform_driver_register函数中drv-&gt;driver.bus &#x3D; &amp;platform_bus_type，所以drv-&gt;bus-&gt;match就为platform_bus_type→match,为platform_match函数，该函数如下： 是比较dev和drv的name，相同则会进入really_probe（）函数，从而进入自己写的probe函数做进一步的匹配。所以dev→name和driver→drv→name在初始化时一定要填一样的。 不同类型的驱动，其match函数是不一样的，这个platform的驱动，比较的是dev和drv的名字，还记得usb类驱动里的match吗？它比较的是Product ID和Vendor ID。 个人总结Platform机制的好处： 1、提供platform_bus_type类型的总线，把那些不是总线型的soc设备都添加到这条虚拟总线上。使得，总线——设备——驱动的模式可以得到普及。 2、提供platform_device和platform_driver类型的数据结构，将传统的device和driver数据结构嵌入其中，并且加入resource成员，以便于和Open Firmware这种动态传递设备资源的新型bootloader和kernel 接轨。 5、Linux内核体系结构因为Linux内核是单片的，所以它比其他类型的内核占用空间最大，复杂度也最高。这是一个设计特性，在Linux早期引起了相当多的争论，并且仍然带有一些与单内核固有的相同的设计缺陷。 为了解决这些缺陷，Linux内核开发人员所做的一件事就是使内核模块可以在运行时加载和卸载，这意味着您可以动态地添加或删除内核的特性。这不仅可以向内核添加硬件功能，还可以包括运行服务器进程的模块，比如低级别虚拟化，但也可以替换整个内核，而不需要在某些情况下重启计算机。想象一下，如果您可以升级到Windows服务包，而不需要重新启动…… 内核模块 如果Windows已经安装了所有可用的驱动程序，而您只需要打开所需的驱动程序怎么办?这本质上就是内核模块为Linux所做的。内核模块，也称为可加载内核模块(LKM)，对于保持内核在不消耗所有可用内存的情况下与所有硬件一起工作是必不可少的。 模块通常向基本内核添加设备、文件系统和系统调用等功能。lkm的文件扩展名是.ko，通常存储在&#x2F;lib&#x2F;modules目录中。由于模块的特性，您可以通过在启动时使用menuconfig命令将模块设置为load或not load，或者通过编辑&#x2F;boot&#x2F;config文件，或者使用modprobe命令动态地加载和卸载模块，轻松定制内核。 第三方和封闭源码模块在一些发行版中是可用的，比如Ubuntu，默认情况下可能无法安装，因为这些模块的源代码是不可用的。该软件的开发人员(即nVidia、ATI等)不提供源代码，而是构建自己的模块并编译所需的.ko文件以便分发。虽然这些模块像beer一样是免费的，但它们不像speech那样是免费的，因此不包括在一些发行版中，因为维护人员认为它通过提供非免费软件“污染”了内核。 内核并不神奇，但对于任何正常运行的计算机来说，它都是必不可少的。Linux内核不同于OS X和Windows，因为它包含内核级别的驱动程序，并使许多东西“开箱即用”。希望您能对软件和硬件如何协同工作以及启动计算机所需的文件有更多的了解。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/419643250","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核看socket底层的本质(IO)","path":"/2023/01/15/linux-docs/Linux内核看socket底层的本质(IO)/","content":"1、I&#x2F;O 模型一个输入操作通常包括两个阶段： 等待数据准备好 从内核向进程复制数据 对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达。当所等待数据到达时，它被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用进程缓冲区。 Unix 有五种 I&#x2F;O 模型： 阻塞式 I&#x2F;O 非阻塞式 I&#x2F;O I&#x2F;O 复用（select 和 poll） 信号驱动式 I&#x2F;O（SIGIO） 异步 I&#x2F;O（AIO） 1.1阻塞式 I&#x2F;O应用进程被阻塞，直到数据从内核缓冲区复制到应用进程缓冲区中才返回。 应该注意到，在阻塞的过程中，其它应用进程还可以执行，因此阻塞不意味着整个操作系统都被阻塞。因为其它应用进程还可以执行，所以不消耗 CPU 时间，这种模型的 CPU 利用率会比较高。 下图中，recvfrom() 用于接收 Socket 传来的数据，并复制到应用进程的缓冲区 buf 中。这里把 recvfrom() 当成系统调用。 12345ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen); 1.2非阻塞式 I&#x2F;O应用进程执行系统调用之后，内核返回一个错误码。应用进程可以继续执行，但是需要不断的执行系统调用来获知 I&#x2F;O 是否完成，这种方式称为轮询（polling）。 由于 CPU 要处理更多的系统调用，因此这种模型的 CPU 利用率比较低。 1.3I&#x2F;O 复用使用 select 或者 poll 等待数据，并且可以等待多个套接字中的任何一个变为可读。这一过程会被阻塞，当某一个套接字可读时返回，之后再使用 recvfrom 把数据从内核复制到进程中。 它可以让单个进程具有处理多个 I&#x2F;O 事件的能力。又被称为 Event Driven I&#x2F;O，即事件驱动 I&#x2F;O。 如果一个 Web 服务器没有 I&#x2F;O 复用，那么每一个 Socket 连接都需要创建一个线程去处理。如果同时有几万个连接，那么就需要创建相同数量的线程。相比于多进程和多线程技术，I&#x2F;O 复用不需要进程线程创建和切换的开销，系统开销更小。 1.4信号驱动 I&#x2F;O应用进程使用 sigaction 系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。内核在数据到达时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中。 相比于非阻塞式 I&#x2F;O 的轮询方式，信号驱动 I&#x2F;O 的 CPU 利用率更高。 1.5异步 I&#x2F;O应用进程执行 aio_read 系统调用会立即返回，应用进程可以继续执行，不会被阻塞，内核会在所有操作完成之后向应用进程发送信号。 异步 I&#x2F;O 与信号驱动 I&#x2F;O 的区别在于，异步 I&#x2F;O 的信号是通知应用进程 I&#x2F;O 完成，而信号驱动 I&#x2F;O 的信号是通知应用进程可以开始 I&#x2F;O。 1.6五大 I&#x2F;O 模型比较 同步 I&#x2F;O：将数据从内核缓冲区复制到应用进程缓冲区的阶段（第二阶段），应用进程会阻塞。 异步 I&#x2F;O：第二阶段应用进程不会阻塞。 同步 I&#x2F;O 包括阻塞式 I&#x2F;O、非阻塞式 I&#x2F;O、I&#x2F;O 复用和信号驱动 I&#x2F;O ，它们的主要区别在第一个阶段。 非阻塞式 I&#x2F;O 、信号驱动 I&#x2F;O 和异步 I&#x2F;O 在第一阶段不会阻塞。 2、I&#x2F;O 复用select&#x2F;poll&#x2F;epoll 都是 I&#x2F;O 多路复用的具体实现，select 出现的最早，之后是 poll，再是 epoll。 2.1select123int select(int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); select 允许应用程序监视一组文件描述符，等待一个或者多个描述符成为就绪状态，从而完成 I&#x2F;O 操作。 fd_set 使用数组实现，数组大小使用 FD_SETSIZE 定义，所以只能监听少于 FD_SETSIZE 数量的描述符。有三种类型的描述符类型：readset、writeset、exceptset，分别对应读、写、异常条件的描述符集合。 timeout 为超时参数，调用 select 会一直阻塞直到有描述符的事件到达或者等待的时间超过 timeout。 成功调用返回结果大于 0，出错返回结果为 -1，超时返回结果为 0。 123456789101112131415161718192021222324252627282930313233343536fd_set fd_in, fd_out;struct timeval tv;// Reset the setsFD_ZERO( &amp;fd_in );FD_ZERO( &amp;fd_out );// Monitor sock1 for input eventsFD_SET( sock1, &amp;fd_in );// Monitor sock2 for output eventsFD_SET( sock2, &amp;fd_out );// Find out which socket has the largest numeric value as select requires itint largest_sock = sock1 &gt; sock2 ? sock1 : sock2;// Wait up to 10 secondstv.tv_sec = 10;tv.tv_usec = 0;// Call the selectint ret = select( largest_sock + 1, &amp;fd_in, &amp;fd_out, NULL, &amp;tv );// Check if select actually succeedif ( ret == -1 ) // report error and abortelse if ( ret == 0 ) // timeout; no event detectedelse&#123; if ( FD_ISSET( sock1, &amp;fd_in ) ) // input event on sock1 if ( FD_ISSET( sock2, &amp;fd_out ) ) // output event on sock2&#125; 2.2poll1int poll(struct pollfd *fds, unsigned int nfds, int timeout); poll 的功能与 select 类似，也是等待一组描述符中的一个成为就绪状态。 poll 中的描述符是 pollfd 类型的数组，pollfd 的定义如下： 12345struct pollfd &#123; int fd; /* file descriptor */ short events; /* requested events */ short revents; /* returned events */ &#125;; 1234567891011121314151617181920212223242526272829// The structure for two eventsstruct pollfd fds[2];// Monitor sock1 for inputfds[0].fd = sock1;fds[0].events = POLLIN;// Monitor sock2 for outputfds[1].fd = sock2;fds[1].events = POLLOUT;// Wait 10 secondsint ret = poll( &amp;fds, 2, 10000 );// Check if poll actually succeedif ( ret == -1 ) // report error and abortelse if ( ret == 0 ) // timeout; no event detectedelse&#123; // If we detect the event, zero it out so we can reuse the structure if ( fds[0].revents &amp; POLLIN ) fds[0].revents = 0; // input event on sock1 if ( fds[1].revents &amp; POLLOUT ) fds[1].revents = 0; // output event on sock2&#125; 2.3比较1. 功能 select 和 poll 的功能基本相同，不过在一些实现细节上有所不同。 select 会修改描述符，而 poll 不会； select 的描述符类型使用数组实现，FD_SETSIZE 大小默认为 1024，因此默认只能监听少于 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译；而 poll 没有描述符数量的限制； poll 提供了更多的事件类型，并且对描述符的重复利用上比 select 高。 如果一个线程对某个描述符调用了 select 或者 poll，另一个线程关闭了该描述符，会导致调用结果不确定。 2. 速度 select 和 poll 速度都比较慢，每次调用都需要将全部描述符从应用进程缓冲区复制到内核缓冲区。 3. 可移植性 几乎所有的系统都支持 select，但是只有比较新的系统支持 poll。 2.4epoll123int epoll_create(int size);int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); epoll_ctl() 用于向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I&#x2F;O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait() 便可以得到事件完成的描述符。 从上面的描述可以看出，epoll 只需要将描述符从进程缓冲区向内核缓冲区拷贝一次，并且进程不需要通过轮询来获得事件完成的描述符。 epoll 仅适用于 Linux OS。 epoll 比 select 和 poll 更加灵活而且没有描述符数量限制。 epoll 对多线程编程更有友好，一个线程调用了 epoll_wait() 另一个线程关闭了同一个描述符也不会产生像 select 和 poll 的不确定情况。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// Create the epoll descriptor. Only one is needed per app, and is used to monitor all sockets.// The function argument is ignored (it was not before, but now it is), so put your favorite number hereint pollingfd = epoll_create( 0xCAFE );if ( pollingfd &lt; 0 ) // report error// Initialize the epoll structure in case more members are added in futurestruct epoll_event ev = &#123; 0 &#125;;// Associate the connection class instance with the event. You can associate anything// you want, epoll does not use this information. We store a connection class pointer, pConnection1ev.data.ptr = pConnection1;// Monitor for input, and do not automatically rearm the descriptor after the eventev.events = EPOLLIN | EPOLLONESHOT;// Add the descriptor into the monitoring list. We can do it even if another thread is// waiting in epoll_wait - the descriptor will be properly addedif ( epoll_ctl( epollfd, EPOLL_CTL_ADD, pConnection1-&gt;getSocket(), &amp;ev ) != 0 ) // report error// Wait for up to 20 events (assuming we have added maybe 200 sockets before that it may happen)struct epoll_event pevents[ 20 ];// Wait for 10 seconds, and retrieve less than 20 epoll_event and store them into epoll_event arrayint ready = epoll_wait( pollingfd, pevents, 20, 10000 );// Check if epoll actually succeedif ( ret == -1 ) // report error and abortelse if ( ret == 0 ) // timeout; no event detectedelse&#123; // Check if any events detected for ( int i = 0; i &lt; ready; i++ ) &#123; if ( pevents[i].events &amp; EPOLLIN ) &#123; // Get back our connection pointer Connection * c = (Connection*) pevents[i].data.ptr; c-&gt;handleReadEvent(); &#125; &#125;&#125; 2.5工作模式epoll 的描述符事件有两种触发模式：LT（level trigger）和 ET（edge trigger）。 1. LT 模式 当 epoll_wait() 检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait() 会再次通知进程。是默认的一种模式，并且同时支持 Blocking 和 No-Blocking。 2. ET 模式 和 LT 模式不同的是，通知之后进程必须立即处理事件，下次再调用 epoll_wait() 时不会再得到事件到达的通知。 很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。只支持 No-Blocking，以避免由于一个文件句柄的阻塞读&#x2F;阻塞写操作把处理多个文件描述符的任务饿死。 2.6应用场景很容易产生一种错觉认为只要用 epoll 就可以了，select 和 poll 都已经过时了，其实它们都有各自的使用场景。 select 应用场景 select 的 timeout 参数精度为微秒，而 poll 和 epoll 为毫秒，因此 select 更加适用于实时性要求比较高的场景，比如核反应堆的控制。 select 可移植性更好，几乎被所有主流平台所支持。 poll 应用场景 poll 没有最大描述符数量的限制，如果平台支持并且对实时性要求不高，应该使用 poll 而不是 select。 epoll 应用场景 只需要运行在 Linux 平台上，有大量的描述符需要同时轮询，并且这些连接最好是长连接。 需要同时监控小于 1000 个描述符，就没有必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势。 需要监控的描述符状态变化多，而且都是非常短暂的，也没有必要使用 epoll。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率。并且 epoll 的描述符存储在内核，不容易调试。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/477292559","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核编译与开发","path":"/2023/01/15/linux-docs/Linux内核编译与开发/","content":"1、Linux内核简介linux kernel map： linux 系统体系结构： linux kernel体系结构：arm有7种工作模式，x86也实现了4个不同级别RING0-RING3,RING0级别最高，这样linux用户代码运行在RING3下，内核运行在RING0,这样系统本身就得到了充分的保护 用户空间(用户模式)转到内核空间(系统模式)方法：·系统调用·硬件中断 linux kernel 体系结构： 虚拟文件系统VFS:VFS(虚拟文件系统)隐藏各种文件系统的具体细节，为文件操作提供统一的接口 2、Linux内核源代码linux内核下载***http://www.kernel.org***目录结构:解压linux kernel tar后目录·arch:根据cpu体系结构不同而分的代码·block:部分块设备驱动程序·crypto:加密，压缩，CRC校验算法·documentation:内核文档·drivers:设备驱动程序·fs(虚拟文件系统vfs):文件系统·include:内核所需的头文件，(与平台无关的头文件在include&#x2F;linux中)·lib:库文件代码(与平台相关的)·mm:实现内存管理，与硬件体系结构无关的(与硬件体系结构相关的在arch中)·net:网络协议的代码·samples:一些内核编程的范例·scripts:配置内核的脚本·security:SElinux的模块·sound:音频设备的驱动程序·usr:cpio命令实现，用于制作根文件系统的命令(文件系统与内核放到一块的命令)·virt:内核虚拟机 linux DOC 编译生成: linux源根目录&#x2F;Documentation&#x2F;00-INDEX:目录索引linux源根目录&#x2F;Documentation&#x2F;HOWTO:指南·生成linux内核帮助文档:在linux源根目录(Documentation) 执行make htmldocs ubuntu16下需要执行sudo apt-get install xmlto安装插件才可生成doc文档 后面开发中经常要改的是arch，drivers中的代码 3、Linux内核配置与编译清理文件(在linux源码根目录):·make clean:只清理所有产生的文件·make mrproper:清理所有产生的文件与config配置文件·make distclean:清理所有产生的文件与config配置文件，并且编辑过的与补丁文件↓配置(收集硬件信息如cpu型号，网卡等…):·make config:基于文本模式的交互配置·make menuconfig:基于文本模式的菜单模式(推荐使用)·make oldconfig:使用已有的.config,但会询问新增的配置项·make xconfig:图形化的配置(需要安装图形化系统)配置方法：1)使用make menuconfig操作方法：1&gt;按y:编译&gt;连接&gt;镜像文件2&gt;按m:编译3&gt;按n:什么都不做4&gt;按”空格键”:y,n轮换配置完并保存后会在linux源码根目录下生成一个.config文件注意：在ubuntu11上要执行apt-get install libncurses5-dev来安装支持包2)利用已有的配置文件模板(.config)1&gt;linux源码根目录&#x2F;arch&#x2F;&lt;cpu架构&gt;&#x2F;configs&#x2F;&lt;具体某一的CPU文件&gt;，把里面对应的文件copy并改名为.config至linux源码根目录下2&gt;利用当前运行已有的文件(要用ls &#x2F;boot&#x2F; -a查看)把&#x2F;boot&#x2F;config-2.6.18-53.e15拷贝并改名为.config至linux源码根目录下执行以上操作就可以用make menuconfig在拷贝.config文件上面修改文件了↓编译内核:1)make zImage2)make bzImage区别:在X86平台上，zimage只能用于小于512k的内核获取详细编译信息：make zimage V&#x3D;1 或 make bzimage V&#x3D;1编译好的内核在：arch&#x2F;&#x2F;boot&#x2F;目录下注意：在把.config配置文件cp到根目录编译内核前，必须进入make menuconfig并保存退出(否则生不了效)↓编译并安装模块: 1)编译内核模块:make modules2)安装内核模块:make modules_install INSTALL_MOD_PATH&#x3D;&#x2F;lib&#x2F;modules更换本机器内核:将编译好的内核模块从内核源码目录copy至&#x2F;lib&#x2F;modules下制作init ramdisk():输入执行命令mkinitrd initrd-2.6.39(任意) 2.6.39(可通过查询&#x2F;lib&#x2F;modules下的目录得到)注意：mkinitrd命令为redhat里面的,ubuntu的命令为:mkinitramfs -k &#x2F;lib&#x2F;modules&#x2F;模块安装位置 -o initrd-2.6.39(任意) 2.6.39(可通过查询&#x2F;lib&#x2F;modules下的目录得到)如果ubuntu里面没有mkinitramfs命令可以用apt-get install initrd-tools进行安装↓安装内核模块:1)手动1&gt;cp linux根目录&#x2F;arch&#x2F;x86&#x2F;boot&#x2F;bzImage &#x2F;boot&#x2F;mylinux-2.6.392&gt;cp linux根目录&#x2F;initrd-2.6.39 &#x2F;boot&#x2F;initrd-2.6.39最后修改&#x2F;etc&#x2F;grub.conf或&#x2F;etc&#x2F;lilo.conf文件2)自动1&gt;make install:这个命令会自动完成上面的操作(查看当前内核版本：uname -r)-—————————————————————————- 4、linux内核模块开发描述：linux内核组件非常庞大，内核ximage并不包含某组件，而是在该组件需要被使用的时候，动态的添加到正在运行的内核中(也可以卸载)，这种机制叫做“内核模块”的机制。内核模块通常通过使用makefile文件对模块进行编译 模块安装与卸载:1)加载：insmod hello.ko2)卸载：rmmod hello3)查看：lsmod4)加载(自动寻找模块依赖)：modprobe hellomodprobe会根据文件&#x2F;lib&#x2F;modules&#x2F;version&#x2F;modules.dep来查看要加载的模块，看它是否还依赖于其他模块，如果是,会先找到这些模块，把它们先加载到内核 实例分析：1)moduleDep&#x2F;1(一个模块的编译) 12345678910111213141516171819202122232425262728293031#include &lt;linux/module.h&gt;#include &lt;linux/init.h&gt;//模块入口函数//__init:表示代码段中的子段,里面的内容只运行一次并且回收内存.static int __init hello_init(void)&#123; printk(KERN_EMERG &quot;hello world! &quot;); return 0;&#125;//模块卸载函数//__exit:static void __exit hello_exit(void)&#123; printk(KERN_EMERG &quot;hello exit! &quot;);&#125;//内核符号导出 函数int add_integar(int a,int b)&#123; return a+b;&#125;int sub_integar(int a,int b)&#123; return a-b;&#125;module_init(hello_init);module_exit(hello_exit);//函数导出EXPORT_SYMBOL(add_integar);EXPORT_SYMBOL(sub_integar); makefile: 1234567891011121314151617181920#第一次执行KERNELRELEASE是空的,所以执行else里面的ifneq ($(KERNELRELEASE),)obj-m :=hello.o#else块elseKDIR:= /lib/modules/2.6.18-53.el5/buildall:#KDIR 依赖内核模块源代码路径(内核编译安装路径)#PWD 表示内核代码在哪(当前目录)#modules 编译的是模块 make -C $(KDIR) M=$(PWD) modules clean: rm -f *.ko *.o *.mod.o *.mod.c *.symvers *.orderendif 2)moduleDep&#x2F;2(两个模块的编译) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include &lt;linux/module.h&gt;#include &lt;linux/init.h&gt;//模块可选信息MODULE_LICENSE(&quot;GPL&quot;);//许可证声明MODULE_AUTHOR(&quot;liyuan&quot;);//作者声明MODULE_DESCRIPTION(&quot;This module is a param example.&quot;);//模块描述MODULE_VERSION(&quot;V1.0&quot;);//模块别名MODULE_ALIAS(&quot;a simple module&quot;);//模块别名//模块参数static char *name = &quot;liyuan arg&quot;;static int age = 30;//S_IRUGO是参数权限，也可以用数字module_param(age,int,S_IRUGO);module_param(name,charp,S_IRUGO);//使用外部文件函数extern int add(int a,int b);//声明 外部内核符号 函数extern int add_integar(int a,int b);extern int sub_integar(int a,int b);static int __init mains_init(void)&#123; //多文件编译 printk(KERN_EMERG&quot;param hi&quot;); int vle=add(1,2); printk(KERN_EMERG&quot;add value:%d &quot;,vle); //模块参数 printk(KERN_EMERG&quot; name : %s &quot;,name); printk(KERN_EMERG&quot; age : %d &quot;,age); //使用其他模块的函数(内核符号导出) int adds=add_integar(3,1); int subs=sub_integar(3,1); printk(KERN_EMERG&quot; add_integar : %d &quot;,adds); printk(KERN_EMERG&quot; sub_integar : %d &quot;,subs); return 0;&#125;static void __exit mains_exit(void)&#123; printk(&quot;param exit!&quot;);&#125;module_init(mains_init);module_exit(mains_exit); add.c 1234int add(int a,int b)&#123; return a+b;&#125; makefile 123456789101112131415161718ifneq ($(KERNELRELEASE),)#两个以上内核源文件 生成单独的内核模块名ma#内核maobj-m :=ma.o#下面的ma-objs前面必须和上面一样为mama-objs := mains.o add.oelseKDIR:= /lib/modules/2.6.18-53.el5/buildall: make -C $(KDIR) M=$(PWD) modules clean: rm -f *.ko *.o *.mod.o *.mod.c *.symvers *.orderendif 运行带参模块：insmod hello.ko name&#x3D;yuan age&#x3D;12内核符号导出(&#x2F;proc&#x2F;kallsyms记录了内核中所有导出的符号的名字与地址):一个内核模块的运行依赖另一个内核模块的函数实现，必须先运行第一个内核模块，这样就需要进行内核符号导出。 注意：错误信息:disagrees about version of symbol struct_module insmod:error inserting …开发内核模块时会出现，内核模块不匹配的情况.是你当前运行的linux内核与编译连接所依赖的内核版本不匹配，解决方法：·使用modprobe –force-modversion强行插入·可使用uname -r进行查看当前运行的内核版本 printk内核打印:在&lt;linux&#x2F;kernel.h&gt;中printk有8个优先级，按优先级递减的是：·KERN_EMERG 0用于紧急的消息，常常是那些崩溃的消息·KERN_ALERT 1需要立刻行动的消息·KERN_CRIT 2严重情况·KERN_ERR 3错误情况·KERN_WARNING(printk默认级别) 4有问题的警告·KERN_NOTICE 5正常情况，但是仍然值得注意·KERN_INFO 6信息消息·KERN_DEBUG 7用作调试消息 不管是哪个级别的都会在&#x2F;var&#x2F;log&#x2F;messages里面打印出来(messages可以删除后，运行内核进行测试内核打印情况)控制台打印(优先级配置&#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;printk) 6 4 1 7·Console_loglevel·Default_message_loglevel·Minimum_console_level·Default_console_loglevel 在vm+redhat安装2.6.39内核时出现的错误启动时报could not find filesystem ‘&#x2F;dev&#x2F;root’解决方法a.通过make menuconfig选中以下对应的选项General setup –&gt;[] enable deprecated sysfs features to support old userspace tools成功时下面那个也了的b.修改.config文件修改.config文件中CONFIG_SYSFS_DEPRECATED_V2，将原本被注释掉的CONFIG_SYSFS_DEPRECATED_V2 改成CONFIG_SYSFS_DEPRECATED_V2&#x3D;y 版权声明：本文为知乎博主「极致Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/549065925","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核虚拟文件系统","path":"/2023/01/15/linux-docs/Linux内核虚拟文件系统/","content":"1、虚拟文件系统的作用虚拟文件系统(VFS)是linux内核和存储设备之间的抽象层，主要有以下好处。 - 简化了应用程序的开发：应用通过统一的系统调用访问各种存储介质 - 简化了新文件系统加入内核的过程：新文件系统只要实现VFS的各个接口即可，不需要修改内核部分 2、虚拟文件系统的4个主要对象虚拟文件中的4个主要对象，具体每个对象的含义参见如下的详细介绍。 2.1超级块超级块(super_block)主要存储文件系统相关的信息，这是个针对文件系统级别的概念。 它一般存储在磁盘的特定扇区中，但是对于那些基于内存的文件系统(比如proc,sysfs)，超级块是在使用时创建在内存中的。 超级块的定义在：&lt;linux&#x2F;fs.h&gt; 123456789101112131415161718192021222324252627282930313233343536/* * 超级块结构中定义的字段非常多， * 这里只介绍一些重要的属性 */struct super_block &#123; struct list_head s_list; /* 指向所有超级块的链表 */ const struct super_operations *s_op; /* 超级块方法 */ struct dentry *s_root; /* 目录挂载点 */ struct mutex s_lock; /* 超级块信号量 */ int s_count; /* 超级块引用计数 */ struct list_head s_inodes; /* inode链表 */ struct mtd_info *s_mtd; /* 存储磁盘信息 */ fmode_t s_mode; /* 安装权限 */&#125;;/* * 其中的 s_op 中定义了超级块的操作方法 * 这里只介绍一些相对重要的函数 */struct super_operations &#123; struct inode *(*alloc_inode)(struct super_block *sb); /* 创建和初始化一个索引节点对象 */ void (*destroy_inode)(struct inode *); /* 释放给定的索引节点 */ void (*dirty_inode) (struct inode *); /* VFS在索引节点被修改时会调用这个函数 */ int (*write_inode) (struct inode *, int); /* 将索引节点写入磁盘，wait表示写操作是否需要同步 */ void (*drop_inode) (struct inode *); /* 最后一个指向索引节点的引用被删除后，VFS会调用这个函数 */ void (*delete_inode) (struct inode *); /* 从磁盘上删除指定的索引节点 */ void (*put_super) (struct super_block *); /* 卸载文件系统时由VFS调用，用来释放超级块 */ void (*write_super) (struct super_block *); /* 用给定的超级块更新磁盘上的超级块 */ int (*sync_fs)(struct super_block *sb, int wait); /* 使文件系统中的数据与磁盘上的数据同步 */ int (*statfs) (struct dentry *, struct kstatfs *); /* VFS调用该函数获取文件系统状态 */ int (*remount_fs) (struct super_block *, int *, char *); /* 指定新的安装选项重新安装文件系统时，VFS会调用该函数 */ void (*clear_inode) (struct inode *); /* VFS调用该函数释放索引节点，并清空包含相关数据的所有页面 */ void (*umount_begin) (struct super_block *); /* VFS调用该函数中断安装操作 */&#125;; 2.2索引节点索引节点是VFS中的核心概念，它包含内核在操作文件或目录时需要的全部信息。 一个索引节点代表文件系统中的一个文件(这里的文件不仅是指我们平时所认为的普通的文件，还包括目录，特殊设备文件等等)。 索引节点和超级块一样是实际存储在磁盘上的，当被应用程序访问到时才会在内存中创建。 索引节点定义在：&lt;linux&#x2F;fs.h&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344/* * 索引节点结构中定义的字段非常多， * 这里只介绍一些重要的属性 */struct inode &#123; struct hlist_node i_hash; /* 散列表，用于快速查找inode */ struct list_head i_list; /* 索引节点链表 */ struct list_head i_sb_list; /* 超级块链表超级块 */ struct list_head i_dentry; /* 目录项链表 */ unsigned long i_ino; /* 节点号 */ atomic_t i_count; /* 引用计数 */ unsigned int i_nlink; /* 硬链接数 */ uid_t i_uid; /* 使用者id */ gid_t i_gid; /* 使用组id */ struct timespec i_atime; /* 最后访问时间 */ struct timespec i_mtime; /* 最后修改时间 */ struct timespec i_ctime; /* 最后改变时间 */ const struct inode_operations *i_op; /* 索引节点操作函数 */ const struct file_operations *i_fop; /* 缺省的索引节点操作 */ struct super_block *i_sb; /* 相关的超级块 */ struct address_space *i_mapping; /* 相关的地址映射 */ struct address_space i_data; /* 设备地址映射 */ unsigned int i_flags; /* 文件系统标志 */ void *i_private; /* fs 私有指针 */&#125;;/* * 其中的 i_op 中定义了索引节点的操作方法 * 这里只介绍一些相对重要的函数 */struct inode_operations &#123; /* 为dentry对象创造一个新的索引节点 */ int (*create) (struct inode *,struct dentry *,int, struct nameidata *); /* 在特定文件夹中寻找索引节点，该索引节点要对应于dentry中给出的文件名 */ struct dentry * (*lookup) (struct inode *,struct dentry *, struct nameidata *); /* 创建硬链接 */ int (*link) (struct dentry *,struct inode *,struct dentry *); /* 从一个符号链接查找它指向的索引节点 */ void * (*follow_link) (struct dentry *, struct nameidata *); /* 在 follow_link调用之后，该函数由VFS调用进行清除工作 */ void (*put_link) (struct dentry *, struct nameidata *, void *); /* 该函数由VFS调用，用于修改文件的大小 */ void (*truncate) (struct inode *);&#125;; 2.3目录项和超级块和索引节点不同，目录项并不是实际存在于磁盘上的。 在使用的时候在内存中创建目录项对象，其实通过索引节点已经可以定位到指定的文件， 但是索引节点对象的属性非常多，在查找，比较文件时，直接用索引节点效率不高，所以引入了目录项的概念。 路径中的每个部分都是一个目录项，比如路径： &#x2F;mnt&#x2F;cdrom&#x2F;foo&#x2F;bar 其中包含5个目录项，&#x2F; mnt cdrom foo bar 每个目录项对象都有3种状态：被使用，未使用和负状态 - 被使用：对应一个有效的索引节点，并且该对象由一个或多个使用者 - 未使用：对应一个有效的索引节点，但是VFS当前并没有使用这个目录项 - 负状态：没有对应的有效索引节点（可能索引节点被删除或者路径不存在了） 目录项的目的就是提高文件查找，比较的效率，所以访问过的目录项都会缓存在slab中。 slab中缓存的名称一般就是 dentry，可以通过如下命令查看： 12[wangyubin@localhost kernel]$ sudo cat /proc/slabinfo | grep dentrydentry 212545 212625 192 21 1 : tunables 0 0 0 : slabdata 10125 10125 0 目录项定义在：&lt;linux&#x2F;dcache.h&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344/* 目录项对象结构 */struct dentry &#123; atomic_t d_count; /* 使用计数 */ unsigned int d_flags; /* 目录项标识 */ spinlock_t d_lock; /* 单目录项锁 */ int d_mounted; /* 是否登录点的目录项 */ struct inode *d_inode; /* 相关联的索引节点 */ struct hlist_node d_hash; /* 散列表 */ struct dentry *d_parent; /* 父目录的目录项对象 */ struct qstr d_name; /* 目录项名称 */ struct list_head d_lru; /* 未使用的链表 */ /* * d_child and d_rcu can share memory */ union &#123; struct list_head d_child; /* child of parent list */ struct rcu_head d_rcu; &#125; d_u; struct list_head d_subdirs; /* 子目录链表 */ struct list_head d_alias; /* 索引节点别名链表 */ unsigned long d_time; /* 重置时间 */ const struct dentry_operations *d_op; /* 目录项操作相关函数 */ struct super_block *d_sb; /* 文件的超级块 */ void *d_fsdata; /* 文件系统特有数据 */ unsigned char d_iname[DNAME_INLINE_LEN_MIN]; /* 短文件名 */&#125;;/* 目录项相关操作函数 */struct dentry_operations &#123; /* 该函数判断目录项对象是否有效。VFS准备从dcache中使用一个目录项时会调用这个函数 */ int (*d_revalidate)(struct dentry *, struct nameidata *); /* 为目录项对象生成hash值 */ int (*d_hash) (struct dentry *, struct qstr *); /* 比较 qstr 类型的2个文件名 */ int (*d_compare) (struct dentry *, struct qstr *, struct qstr *); /* 当目录项对象的 d_count 为0时，VFS调用这个函数 */ int (*d_delete)(struct dentry *); /* 当目录项对象将要被释放时，VFS调用该函数 */ void (*d_release)(struct dentry *); /* 当目录项对象丢失其索引节点时（也就是磁盘索引节点被删除了），VFS会调用该函数 */ void (*d_iput)(struct dentry *, struct inode *); char *(*d_dname)(struct dentry *, char *, int);&#125;; 2.4文件对象文件对象表示进程已打开的文件，从用户角度来看，我们在代码中操作的就是一个文件对象。 文件对象反过来指向一个目录项对象（目录项反过来指向一个索引节点） 其实只有目录项对象才表示一个已打开的实际文件，虽然一个文件对应的文件对象不是唯一的，但其对应的索引节点和目录项对象却是唯一的。 文件对象的定义在*: &lt;linux&#x2F;fs.h&gt;* 123456789101112131415161718192021222324252627282930313233343536/* * 文件对象结构中定义的字段非常多， * 这里只介绍一些重要的属性 */struct file &#123; union &#123; struct list_head fu_list; /* 文件对象链表 */ struct rcu_head fu_rcuhead; /* 释放之后的RCU链表 */ &#125; f_u; struct path f_path; /* 包含的目录项 */ const struct file_operations *f_op; /* 文件操作函数 */ atomic_long_t f_count; /* 文件对象引用计数 */&#125;;/* * 其中的 f_op 中定义了文件对象的操作方法 * 这里只介绍一些相对重要的函数 */struct file_operations &#123; /* 用于更新偏移量指针,由系统调用lleek()调用它 */ loff_t (*llseek) (struct file *, loff_t, int); /* 由系统调用read()调用它 */ ssize_t (*read) (struct file *, char __user *, size_t, loff_t *); /* 由系统调用write()调用它 */ ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *); /* 由系统调用 aio_read() 调用它 */ ssize_t (*aio_read) (struct kiocb *, const struct iovec *, unsigned long, loff_t); /* 由系统调用 aio_write() 调用它 */ ssize_t (*aio_write) (struct kiocb *, const struct iovec *, unsigned long, loff_t); /* 将给定文件映射到指定的地址空间上,由系统调用 mmap 调用它 */ int (*mmap) (struct file *, struct vm_area_struct *); /* 创建一个新的文件对象,并将它和相应的索引节点对象关联起来 */ int (*open) (struct inode *, struct file *); /* 当已打开文件的引用计数减少时,VFS调用该函数 */ int (*flush) (struct file *, fl_owner_t id);&#125;; 2.5四个对象之间关系图上面分别介绍了4种对象分别的属性和方法,下面用图来展示这4个对象的和VFS之间关系以及4个对象之间的关系。 (这个图是根据我自己的理解画出来的,如果由错误请帮忙指出,谢谢!) 3、文件系统相关的数据结构处理上面4个主要的对象之外，VFS中还有2个专门针对文件系统的2个对象， - struct file_system_type: 用来描述文件系统的类型（比如ext3,ntfs等等） - struct vfsmount : 描述一个安装文件系统的实例 file_system_type 结构体位于：&lt;linux&#x2F;fs.h&gt; 123456789101112131415161718192021struct file_system_type &#123; const char *name; /* 文件系统名称 */ int fs_flags; /* 文件系统类型标志 */ /* 从磁盘中读取超级块,并且在文件系统被安装时,在内存中组装超级块对象 */ int (*get_sb) (struct file_system_type *, int, const char *, void *, struct vfsmount *); /* 终止访问超级块 */ void (*kill_sb) (struct super_block *); struct module *owner; /* 文件系统模块 */ struct file_system_type * next; /* 链表中下一个文件系统类型 */ struct list_head fs_supers; /* 超级块对象链表 */ /* 下面都是运行时的锁 */ struct lock_class_key s_lock_key; struct lock_class_key s_umount_key; struct lock_class_key i_lock_key; struct lock_class_key i_mutex_key; struct lock_class_key i_mutex_dir_key; struct lock_class_key i_alloc_sem_key;&#125;; 每种文件系统,不管由多少个实例安装到系统中,还是根本没有安装到系统中,都只有一个 file_system_type 结构。 当文件系统被实际安装时，会在安装点创建一个 vfsmount 结构体。 结构体代表文件系统的实例，也就是文件系统被安装几次，就会创建几个 vfsmount vfsmount 的定义参见：&lt;linux&#x2F;mount.h&gt; 1234567891011121314151617181920212223242526272829303132333435struct vfsmount &#123; struct list_head mnt_hash; /* 散列表 */ struct vfsmount *mnt_parent; /* 父文件系统，也就是要挂载到哪个文件系统 */ struct dentry *mnt_mountpoint; /* 安装点的目录项 */ struct dentry *mnt_root; /* 该文件系统的根目录项 */ struct super_block *mnt_sb; /* 该文件系统的超级块 */ struct list_head mnt_mounts; /* 子文件系统链表 */ struct list_head mnt_child; /* 子文件系统链表 */ int mnt_flags; /* 安装标志 */ /* 4 bytes hole on 64bits arches */ const char *mnt_devname; /* 设备文件名 e.g. /dev/dsk/hda1 */ struct list_head mnt_list; /* 描述符链表 */ struct list_head mnt_expire; /* 到期链表的入口 */ struct list_head mnt_share; /* 共享安装链表的入口 */ struct list_head mnt_slave_list;/* 从安装链表 */ struct list_head mnt_slave; /* 从安装链表的入口 */ struct vfsmount *mnt_master; /* 从安装链表的主人 */ struct mnt_namespace *mnt_ns; /* 相关的命名空间 */ int mnt_id; /* 安装标识符 */ int mnt_group_id; /* 组标识符 */ /* * We put mnt_count &amp; mnt_expiry_mark at the end of struct vfsmount * to let these frequently modified fields in a separate cache line * (so that reads of mnt_flags wont ping-pong on SMP machines) */ atomic_t mnt_count; /* 使用计数 */ int mnt_expiry_mark; /* 如果标记为到期，则为 True */ int mnt_pinned; /* &quot;钉住&quot;进程计数 */ int mnt_ghosts; /* &quot;镜像&quot;引用计数 */#ifdef CONFIG_SMP int *mnt_writers; /* 写者引用计数 */#else int mnt_writers; /* 写者引用计数 */#endif&#125;; 4、进程相关的数据结构以上介绍的都是在内核角度看到的 VFS 各个结构，所以结构体中包含的属性非常多。 而从进程的角度来看的话，大多数时候并不需要那么多的属性，所有VFS通过以下3个结构体和进程紧密联系在一起。 - struct files_struct ：由进程描述符中的 files 目录项指向，所有与单个进程相关的信息(比如打开的文件和文件描述符)都包含在其中。 - struct fs_struct ：由进程描述符中的 fs 域指向，包含文件系统和进程相关的信息。 - struct mmt_namespace ：由进程描述符中的 mmt_namespace 域指向。 struct files_struct 位于：&lt;linux&#x2F;fdtable.h&gt; 12345678910struct files_struct &#123; atomic_t count; /* 使用计数 */ struct fdtable *fdt; /* 指向其他fd表的指针 */ struct fdtable fdtab;/* 基 fd 表 */ spinlock_t file_lock ____cacheline_aligned_in_smp; /* 单个文件的锁 */ int next_fd; /* 缓存下一个可用的fd */ struct embedded_fd_set close_on_exec_init; /* exec()时关闭的文件描述符链表 */ struct embedded_fd_set open_fds_init; /* 打开的文件描述符链表 */ struct file * fd_array[NR_OPEN_DEFAULT]; /* 缺省的文件对象数组 */&#125;; struct fs_struct 位于：&lt;linux&#x2F;fs_struct.h&gt; 1234567struct fs_struct &#123; int users; /* 用户数目 */ rwlock_t lock; /* 保护结构体的读写锁 */ int umask; /* 掩码 */ int in_exec; /* 当前正在执行的文件 */ struct path root, pwd; /* 根目录路径和当前工作目录路径 */&#125;; struct mmt_namespace 位于：&lt;linux&#x2F;mmt_namespace.h&gt; 但是在2.6内核之后似乎没有这个结构体了，而是用 struct nsproxy 来代替。 以下是 struct task_struct 结构体中关于文件系统的3个属性。 struct task_struct 的定义位于：&lt;linux&#x2F;sched.h&gt; 123456/* filesystem information */ struct fs_struct *fs;/* open file information */ struct files_struct *files;/* namespaces */ struct nsproxy *nsproxy; 5、小结VFS 统一了文件系统的实现框架，使得在linux上实现新文件系统的工作变得简单。 目前linux内核中已经支持60多种文件系统，具体支持的文件系统可以查看 内核源码 fs 文件夹下的内容。 版权声明：本文为知乎博主「极致Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/549601357","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统ARM体系结构处理器机制原理与实现","path":"/2023/01/15/linux-docs/Linux操作系统ARM体系结构处理器机制原理与实现/","content":"ARM 的概念ARM(Advanced RISC Machine)，既可以认为是一个公司的名字，也可以认为是对一类微处理器的通称，还可以认为是一种技术的名字。ARM 公司并不生产芯片也不销售芯片，它只出售芯片技术授权。其合作公司针对不同需求搭配各类硬件部件，比如 UART、SDI、I2C 等，从而设计出不同的 SoC 芯片。 ARM 的应用场景基于 ARM 的处理器具有高速度、低功耗、价格低等优点被广泛应用于以下领域： 为通信、消费电子、成像设备等产品，提供可运行复杂操作系统的开放应用平台； 在海量存储、汽车电子、工业控制和网络应用等领域，提供实时嵌入式应用； 安全系统，比如信用卡、SIM 卡等。 ARM的技术特征ARM 架构支持 32 位的 ARM 指令集和 16 位的 Thumb 指令集（大大减小了代码的存储空间）。 这里先以一个例子解释一下架构、核、处理器和芯片之间的特征：S3C2440，这是一款SoC芯片，注意，它不是cpu。2440和我们熟知的51单片机有点类似，都属于嵌入式，嵌入式的发展到目前经历了三个阶段，分别是SCM、MCU、SoC。51属于SCM或MCU，而2440就属于SoC了，先来看看51单片机的内部结构。 其内部结构可以简单的分成两部分：cpu和外设。 我们再看一下再来看2440的内部结构： arm920t就是它的处理器，处理器和核在我看来在这里是一个概念，只不过一个是硬概念，一个是软概念。这里的920t就既是处理器又是核。而三星做的就是除了这个cpu外其他的东西。 RM版本系列ARM版本Ⅰ：V1版架构。 该版架构只在原型机ARM1出现过，只有26位的寻址空间，没有用于商业产品。 其基本性能有: 基本的数据处理指令（无乘法）； 基于字节、半字和字的Load&#x2F;Store指令; 转移指令，包括子程序调用及链接指令； 供操作系统使用的软件中断指令SWI； 寻址空间：64MB（226）。 ARM版本Ⅱ： V2版架构 该版架构对V1版进行了扩展，例如ARM2和ARM3（V2a）架构。包含了对32位乘法指令和协处理器指令的支持。 版本2a是版本2的变种，ARM3芯片采用了版本2a，是第一片采用片上Cache的ARM处理器。同样为26位寻址空间，现在已经废弃不再使用。 V2版架构与版本V1相比，增加了以下功能： 乘法和乘加指令； 支持协处理器操作指令； 快速中断模式； SWP&#x2F;SWPB的最基本存储器与寄存器交换指令; 寻址空间：64MB。 ARM版本Ⅲ ： V3版架构 ARM作为独立的公司，在1990年设计的第一个微处理器采用的是版本3的ARM6。它作为IP核、独立的处理器、具有片上高速缓存、MMU和写缓冲的集成CPU。 变种版本有3G和3M。版本3G是不与版本2a向前兼容的版本3，版本3M引入了有符号和无符号数乘法和乘加指令，这些指令产生全部64位结果。 V3版架构（ 目前已废弃 ）对ARM体系结构作了较大的改动： 寻址空间增至32位（4GB）； 当前程序状态信息从原来的R15寄存器移到当前程序状态寄存器CPSR（Current Program Status Register）中； 增加了程序状态保存寄存器SPSR（Saved Program Status Register）； 增加了两种异常模式，使操作系统代码可方便地使用数据访问中止异常、指令预取中止异常和未定义指令异常； 增加了MRS&#x2F;MSR指令，以访问新增的CPSR&#x2F;SPSR寄存器； 增加了从异常处理返回的指令功能。 ARM版本Ⅳ ： V4版架构 V4版架构在V3版上作了进一步扩充，V4版架构是目前应用最广的ARM体系结构，ARM7、ARM8、ARM9和StrongARM都采用该架构。 V4不再强制要求与26位地址空间兼容，而且还明确了哪些指令会引起未定义指令异常。 指令集中增加了以下功能： 符号化和非符号化半字及符号化字节的存&#x2F;取指令； 增加了T变种，处理器可工作在Thumb状态，增加了16位Thumb指令集； 完善了软件中断SWI指令的功能； 处理器系统模式引进特权方式时使用用户寄存器操作; 把一些未使用的指令空间捕获为未定义指令 ARM版本Ⅴ ： V5版架构 V5版架构是在V4版基础上增加了一些新的指令，ARM10和Xscale都采用该版架构。 这些新增命令有： 带有链接和交换的转移BLX指令； 计数前导零CLZ指令； BRK中断指令； 增加了数字信号处理指令（V5TE版）； 为协处理器增加更多可选择的指令； 改进了ARM&#x2F;Thumb状态之间的切换效率； E—增强型DSP指令集，包括全部算法操作和16位乘法操作； J—-支持新的JAVA，提供字节代码执行的硬件和优化软件加速功能。 ARM版本Ⅵ ： V6版架构 V6版架构是2001年发布的，首先在2002年春季发布的ARM11处理器中使用。在降低耗电量地同时，还强化了图形处理性能。通过追加有效进行多媒体处理的SIMD(Single Instruction, Multiple Data，单指令多数据 )功能，将语音及图像的处理功能提高到了原型机的4倍。 此架构在V5版基础上增加了以下功能： THUMBTM：35%代码压缩； DSP扩充：高性能定点DSP功能； JazelleTM：Java性能优化，可提高8倍； Media扩充：音&#x2F;视频性能优化，可提高4倍 ARM版本ⅤⅡ：V7版架构 V7架构是在ARMv6架构的基础上诞生的。该架构采用了Thumb-2技术,它是在ARM的Thumb代码压缩技术的基础上发展起来的, 并且保持了对现存ARM解决方案的完整的代码兼容性。 Thumb-2技术比纯32位代码少使用31％的内存,减小了系统开销。同时能够提供比已有的基于Thumb技术的解决方案高出38％的性能。 ARMv7架构还采用了NEON技术,将DSP和媒体处理能力提高了近4倍 , 并支持改良的浮点运算, 满足下一代3D图形、游戏物理应用以及传统嵌入式控制应用的需求。此外,ARMv7还支持改良的运行环境,以迎合不断增加的JIT(Just In Time)和DAC(DynamicAdaptive Compilation)技术的使用。 ARM版本ⅤⅢ：V8版架构 这是一个新的IP核，针对高性能的嵌入式信号处理应用而设计的，v8架构是在32位ARM架构上进行开发的，将被首先用于对扩展虚拟地址和64位数据处理技术有更高要求的产品领域，如企业应用、高档消费电子产品。 ARMv8架构包含两个执行状态：AArch64和AArch32。AArch64执行状态针对64位处理技术，引入了一个全新指令集A64；而AArch32执行状态将支持现有的ARM指令集。 目前的ARMv7架构的主要特性都将在ARMv8架构中得以保留或进一步拓展，如：TrustZone技术、虚拟化技术及NEON advanced SIMD技术，等。 其中左侧的就是架构，右侧的是处理器，也可以叫核。arm首个最成功的cpu是ARM7TDMI，是基于ARMv4的。ARM架构包含了下述RISC特性： 读取&#x2F;储存 架构 不支援地址不对齐内存存取（ARMv6内核现已支援） 正交指令集（任意存取指令可以任意的寻址方式存取数据Orthogonal instruction set） 大量的16 × 32-bit 寄存器阵列（register file） 固定的32 bits 操作码（opcode）长度，降低编码数量所产生的耗费，减轻解码和流水线化的负担。 大多均为一个CPU周期执行。不同版本的架构会有所调整。 和三星相同的其他和arm合作的各大厂商通常会把它的CPU和各类外围IP都放到一起，然后自己拿着图纸去流片，生产出来的也是一个正方形，下面有很多引脚，这个东西不仅包含了CPU，还包含了其他的控制器，这个东西就叫做SOC(system on chip)。从英文来看，所谓的四核SOC什么的，本意就不是单指CPU，而是四核系统。 所以目前各大厂商所做的事情，就是买来ARM的授权，得到ARM处理器的源代码，而后自己搞一些外围IP(或者买或者自己设计)，组成一个SOC后，去流片。不同的SOC，架构不同(就是CPU如何和IP联系起来，有的以总线为核心，有的以DDR为核心)，所以，海思是拥有自主产权的SOC架构。可是，无论任何厂商，再怎么折腾，都没有怎么动过CPU，ARM核心就好好的呆在那里，那就是中央处理器。 理器 ARM Cortex-A ：为传统的、基于虚拟存储的操作系统和应用程序而设计，支持 ARM、Thumb 和 Thumb-2 指令集； ARM Cortex-R：针对实时系统设计，支持 ARM、Thumb 和 Thumb-2 指令集； ARM Cortex-M：为对 价格敏感的产品设计，只支持 Thumb-2 指令集。 ARM命名规则第一个数字：系列名称：eg.ARM7、ARM9 第二个数字：Memory system 1234562：带有MMU4：带有MPU6：无MMU与MPU12345 第三个数字：Memory size 1234560：标准Cache（4-128k）2：减小的Cache6：可变的Cache12345 第四个字符： 12345678910111213T：表示支持Thumb指令集D：表示支持片上调试（Debug）M：表示内嵌硬件乘法器（Multiplier）I ：支持片上断点和调试点E：表示支持增强型DSP功能J ：表示支持Jazelle技术，即Java加速器S：表示全合成式 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/449582466","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统ARM指令集与汇编语言程序设计","path":"/2023/01/15/linux-docs/Linux操作系统ARM指令集与汇编语言程序设计/","content":"一、实验目的1.了解并掌握ARM汇编指令集 2.应用ARM指令集编写一个程序操控开发板上的LED灯 二、实验要求应用ARM汇编指令集编写程序，实现正常状态下开发板上的LED灯不亮，按下一个按键之后开发板上的LED灯进入流水灯模式。 三、实验原理四个LED灯的电路如下图所示： 四个按键电路图如下所示： 将LED灯的控制地址放入一个寄存器中，并将其设置为输出模式: 把按键控制地址的内容全置零，为输入模式（复位值为0，此步骤可省略）。 四、实验结果将生成的二进制代码用烧写脚本烧写到SD卡中，插入开发板的SD卡槽，从SD卡启动，按下按键即开启LED流水灯模式。 五、结果分析通过掩码取出按键数据地址中的值的状态来判断按键是否按下，若按下则跳转到LED流水灯的程序当中。 流水灯程序即顺序的程序结构，依次点亮LED灯，延时并熄灭，达到流水效果。 六、附录：实验源代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546.text.globl _start_start:\t// Set GPM4CON[0:3]as output\tldr r1, =0x110002E0 @GPM4CON address ldr r0, =0x00001111\tstr r0, [r1]\t//Set GPX3CON[2:5] as input\tldr r1, =0x11000C60 @GPX3CON address\tmov r0, #0\tstr r0,[r1] @default is input.So,this section canbe omittedKey_led:\t//Read the state of keys\tldr r1, =0x11000C64 @GPX3DAT address-&gt;keys\tldr r3, =0xFFFFFFC3 @Key mask\tldr r0, [r1] @ Read the state of keys\torr r0, r0,r3 @ get the bits of keys\tmov r0,r0,ROR #2\t@ align to the led bits\tcmp r0, #0xFFFFFFFE\tbeq led_blink\tbl Key_ledled_blink:\tldr r1, =0x110002E4\tmov r0, #0xE\tstr r0, [r1]\tbl delay\tldr r1, =0x110002E4\tmov r0, #0xD\tstr r0, [r1]\tbl delay\tldr r1, =0x110002E4\tmov r0, #0xB\tstr r0, [r1]\tbl delay\tldr r1, =0x110002E4\tmov r0, #0x7\tstr r0, [r1]\tbl delay\tbl Key_leddelay:\tmov r0, #0x200000delay_loop:\tcmp r0, #0\tsub r0, r0, #1\tbne delay_loop\tmov pc, lr 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/449816076","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统IO机制原理(流程图详解)","path":"/2023/01/15/linux-docs/Linux操作系统IO机制原理(流程图详解)/","content":"前言：我们之前的文章提到了操作系统的三个抽象，它们分别是进程、地址空间和文件，除此之外，操作系统还要控制所有的 I&#x2F;O 设备。操作系统必须向设备发送命令，捕捉中断并处理错误。它还应该在设备和操作系统的其余部分之间提供一个简单易用的接口。操作系统如何管理 I&#x2F;O 是我们接下来的重点。 不同的人对 I&#x2F;O 硬件的理解也不同。对于电子工程师而言，I&#x2F;O 硬件就是芯片、导线、电源和其他组成硬件的物理设备。而我们程序员眼中的 I&#x2F;O 其实就是硬件提供给软件的接口，比如硬件接受到的命令、执行的操作以及反馈的错误。我们着重探讨的是如何对硬件进行编程，而不是其工作原理。 1，I&#x2F;O 设备什么是 I&#x2F;O 设备？I&#x2F;O 设备又叫做输入&#x2F;输出设备，它是人类用来和计算机进行通信的外部硬件。输入&#x2F;输出设备能够向计算机发送数据（输出）并从计算机接收数据（输入）。 I&#x2F;O 设备(I&#x2F;O devices)可以分成两种：块设备(block devices) 和 字符设备(character devices)。 2，块设备块设备是一个能存储固定大小块信息的设备，它支持以固定大小的块，扇区或群集读取和（可选）写入数据。每个块都有自己的物理地址。通常块的大小在 512 - 65536 之间。所有传输的信息都会以连续的块为单位。块设备的基本特征是每个块都较为对立，能够独立的进行读写。常见的块设备有 硬盘、蓝光光盘、USB盘与字符设备相比，块设备通常需要较少的引脚。 2.1块设备的缺点基于给定固态存储器的块设备比基于相同类型的存储器的字节寻址要慢一些，因为必须在块的开头开始读取或写入。所以，要读取该块的任何部分，必须寻找到该块的开始，读取整个块，如果不使用该块，则将其丢弃。要写入块的一部分，必须寻找到块的开始，将整个块读入内存，修改数据，再次寻找到块的开头处，然后将整个块写回设备。 字符设备 另一类 I&#x2F;O 设备是字符设备。字符设备以字符为单位发送或接收一个字符流，而不考虑任何块结构。字符设备是不可寻址的，也没有任何寻道操作。常见的字符设备有 打印机、网络设备、鼠标、以及大多数与磁盘不同的设备。 下面显示了一些常见设备的数据速率： 2.2设备控制器首先需要先了解一下设备控制器的概念： 设备控制器是处理 CPU 传入和传出信号的系统。设备通过插头和插座连接到计算机，并且插座连接到设备控制器。设备控制器从连接的设备处接收数据，并将其存储在控制器内部的一些特殊目的寄存器(special purpose registers) 也就是本地缓冲区中。 特殊用途寄存器，顾名思义是仅为一项任务而设计的寄存器。例如，cs，ds，gs 和其他段寄存器属于特殊目的寄存器，因为它们的存在是为了保存段号。 eax，ecx 等是一般用途的寄存器，因为你可以无限制地使用它们。 例如，你不能移动 ds，但是可以移动 eax，ebx。 通用目的寄存器比如有：eax、ecx、edx、ebx、esi、edi、ebp、esp 特殊目的寄存器比如有：cs、ds、ss、es、fs、gs、eip、flag 每个设备控制器都会有一个应用程序与之对应，设备控制器通过应用程序的接口通过中断与操作系统进行通信。设备控制器是硬件，而设备驱动程序是软件。 I&#x2F;O 设备通常由机械组件(mechanical component)和电子组件(electronic component)构成。电子组件被称为 设备控制器(device controller)或者 适配器(adapter)。在个人计算机上，它通常采用可插入（PCIe）扩展插槽的主板上的芯片或印刷电路卡的形式。 机械设备就是它自己，它的组成如下： 控制器卡上通常会有一个连接器，通向设备本身的电缆可以插入到这个连接器中，很多控制器可以操作 2 个、4 个设置 8 个相同的设备。 控制器与设备之间的接口通常是一个低层次的接口。例如，磁盘可能被格式化为 2,000,000 个扇区，每个磁道 512 字节。然而，实际从驱动出来的却是一个串行的比特流，从一个前导符(preamble)开始，然后是一个扇区中的 4096 位，最后是一个校验和 或 ECC（错误码，Error-Correcting Code）。前导符是在对磁盘进行格式化的时候写上去的，它包括柱面数和扇区号，扇区大小以及类似的数据，此外还包含同步信息。 控制器的任务是把串行的位流转换为字节块，并进行必要的错误校正工作。字节块通常会在控制器内部的一个缓冲区按位进行组装，然后再对校验和进行校验并证明字节块没有错误后，再将它复制到内存中。 3，内存映射 I&#x2F;O每个控制器都会有几个寄存器用来和 CPU 进行通信。通过写入这些寄存器，操作系统可以命令设备发送数据，接收数据、开启或者关闭设备等。通过从这些寄存器中读取信息，操作系统能够知道设备的状态，是否准备接受一个新命令等。 为了控制寄存器，许多设备都会有数据缓冲区(data buffer)，来供系统进行读写。例如，在屏幕上显示一个像素的常规方法是使用一个视频 RAM，这一 RAM 基本上只是一个数据缓冲区，用来供程序和操作系统写入数据。 那么问题来了，CPU 如何与设备寄存器和设备数据缓冲区进行通信呢？存在两个可选的方式。第一种方法是，每个控制寄存器都被分配一个 I&#x2F;O 端口(I&#x2F;O port)号，这是一个 8 位或 16 位的整数。所有 I&#x2F;O 端口的集合形成了受保护的 I&#x2F;O 端口空间，以便普通用户程序无法访问它（只有操作系统可以访问）。使用特殊的 I&#x2F;O 指令像是 1IN REG,PORT CPU 可以读取控制寄存器 PORT 的内容并将结果放在 CPU 寄存器 REG 中。类似的，使用 1OUT PORT,REG CPU 可以将 REG 的内容写到控制寄存器中。大多数早期计算机，包括几乎所有大型主机，如 IBM 360 及其所有后续机型，都是以这种方式工作的。 控制寄存器是一个处理器寄存器而改变或控制的一般行为 CPU 或其他数字设备。控制寄存器执行的常见任务包括中断控制，切换寻址模式，分页控制和协处理器控制。 在这一方案中，内存地址空间和 I&#x2F;O 地址空间是不相同的，如下图所示： 指令： 1IN R0,4 和 1MOV R0,4 这一设计中完全不同。前者读取 I&#x2F;O端口 4 的内容并将其放入 R0，而后者读取存储器字 4 的内容并将其放入 R0。这些示例中的 4 代表不同且不相关的地址空间。 第二个方法是 PDP-11 引入的， 什么是 PDP-11? 它将所有控制寄存器映射到内存空间中，如下图所示： 内存映射的 I/O是在 CPU 与其连接的外围设备之间交换数据和指令的一种方式，这种方式是处理器和 IO 设备共享同一内存位置的内存，即处理器和 IO 设备使用内存地址进行映射。 在大多数系统中，分配给控制寄存器的地址位于或者靠近地址的顶部附近。 下面是采用的一种混合方式： 这种方式具有与内存映射 I&#x2F;O 的数据缓冲区，而控制寄存器则具有单独的 I&#x2F;O 端口。x86 采用这一体系结构。在 IBM PC 兼容机中，除了 0 到 64K - 1 的 I&#x2F;O 端口之外，640 K 到 1M - 1 的内存地址保留给设备的数据缓冲区。 这些方案是如何工作的呢？当 CPU 想要读入一个字的时候，无论是从内存中读入还是从 I&#x2F;O 端口读入，它都要将需要的地址放到总线地址线上，然后在总线的一条控制线上调用一个 READ 信号。还有第二条信号线来表明需要的是 I&#x2F;O 空间还是内存空间。如果是内存空间，内存将响应请求。如果是 I&#x2F;O 空间，那么 I&#x2F;O 设备将响应请求。如果只有内存空间，那么每个内存模块和每个 I&#x2F;O 设备都会将地址线和它所服务的地址范围进行比较。如果地址落在这一范围之内，它就会响应请求。绝对不会出现地址既分配给内存又分配给 I&#x2F;O 设备，所以不会存在歧义和冲突。 内存映射 I&#x2F;O 的优点和缺点： 这两种寻址控制器的方案具有不同的优缺点。先来看一下内存映射 I&#x2F;O 的优点。 第一，如果需要特殊的 I&#x2F;O 指令读写设备控制寄存器，那么访问这些寄存器需要使用汇编代码，因为在 C 或 C++ 中不存在执行 IN 和 OUT指令的方法。调用这样的过程增加了 I&#x2F;O 的开销。在内存映射中，控制寄存器只是内存中的变量，在 C 语言中可以和其他变量一样进行寻址。 第二，对于内存映射 I&#x2F;O ，不需要特殊的保护机制就能够阻止用户进程执行 I&#x2F;O 操作。操作系统需要保证的是禁止把控制寄存器的地址空间放在用户的虚拟地址中就可以了。 第三，对于内存映射 I&#x2F;O，可以引用内存的每一条指令也可以引用控制寄存器，便于引用。 在计算机设计中，几乎所有的事情都要权衡。内存映射 I&#x2F;O 也是一样，它也有自己的缺点。首先，大部分计算机现在都会有一些对于内存字的缓存。缓存一个设备控制寄存器的代价是很大的。为了避免这种内存映射 I&#x2F;O 的情况，硬件必须有选择性的禁用缓存，例如，在每个页面上禁用缓存，这个功能为硬件和操作系统增加了额外的复杂性，因此必须选择性的进行管理。 第二点，如果仅仅只有一个地址空间，那么所有的内存模块(memory modules)和所有的 I&#x2F;O 设备都必须检查所有的内存引用来推断出谁来进行响应。 什么是内存模块？在计算中，存储器模块是其上安装有存储器集成电路的印刷电路板。 如果计算机是一种单总线体系结构的话，如下图所示： 让每个内存模块和 I&#x2F;O 设备查看每个地址是简单易行的。 然而，现代个人计算机的趋势是专用的高速内存总线，如下图所示： 装备这一总线是为了优化内存访问速度，x86 系统还可以有多种总线（内存、PCIe、SCSI 和 USB）。如下图所示： 在内存映射机器上使用单独的内存总线的麻烦之处在于，I&#x2F;O 设备无法通过内存总线查看内存地址，因此它们无法对其进行响应。此外，必须采取特殊的措施使内存映射 I&#x2F;O 工作在具有多总线的系统上。一种可能的方法是首先将全部内存引用发送到内存，如果内存响应失败，CPU 再尝试其他总线。 第二种设计是在内存总线上放一个探查设备，放过所有潜在指向所关注的 I&#x2F;O 设备的地址。此处的问题是，I&#x2F;O 设备可能无法以内存所能达到的速度处理请求。 第三种可能的设计是在内存控制器中对地址进行过滤，这种设计与上图所描述的设计相匹配。这种情况下，内存控制器芯片中包含在引导时预装载的范围寄存器。这一设计的缺点是需要在引导时判定哪些内存地址而不是真正的内存地址。因而，每一设计都有支持它和反对它的论据，所以折中和权衡是不可避免的。 直接内存访问 无论一个 CPU 是否具有内存映射 I&#x2F;O，它都需要寻址设备控制器以便与它们交换数据。CPU 可以从 I&#x2F;O 控制器每次请求一个字节的数据，但是这么做会浪费 CPU 时间，所以经常会用到一种称为直接内存访问(Direct Memory Access) 的方案。为了简化，我们假设 CPU 通过单一的系统总线访问所有的设备和内存，该总线连接 CPU 、内存和 I&#x2F;O 设备，如下图所示 现代操作系统实际更为复杂，但是原理是相同的。如果硬件有DMA 控制器，那么操作系统只能使用 DMA。有时这个控制器会集成到磁盘控制器和其他控制器中，但这种设计需要在每个设备上都装有一个分离的 DMA 控制器。单个的 DMA 控制器可用于向多个设备传输，这种传输往往同时进行。 不管 DMA 控制器的物理地址在哪，它都能够独立于 CPU 从而访问系统总线，如上图所示。它包含几个可由 CPU 读写的寄存器，其中包括一个内存地址寄存器，字节计数寄存器和一个或多个控制寄存器。控制寄存器指定要使用的 I&#x2F;O 端口、传送方向（从 I&#x2F;O 设备读或写到 I&#x2F;O 设备）、传送单位（每次一个字节或者每次一个字）以及在一次突发传送中要传送的字节数。 为了解释 DMA 的工作原理，我们首先看一下不使用 DMA 该如何进行磁盘读取。 首先，控制器从磁盘驱动器串行地、一位一位的读一个块（一个或多个扇区），直到将整块信息放入控制器的内部缓冲区。 读取校验和以保证没有发生读错误。然后控制器会产生一个中断，当操作系统开始运行时，它会重复的从控制器的缓冲区中一次一个字节或者一个字地读取该块的信息，并将其存入内存中。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/434511399","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统内存管理RAID磁盘阵列与配置","path":"/2023/01/15/linux-docs/Linux操作系统内存管理RAID磁盘阵列与配置/","content":"1、RAID磁盘阵列简称：独立冗余磁盘阵列 把多块独立的物理硬盘按不同的方式组合起来形成一个硬盘组（逻辑硬盘）。从而提供比单个硬盘更高的存储性能和提供数据备份技术。 1.1RAID级别组成磁盘阵列的不同方式称为RAID级别（RAID Levels） 常用的RAID级别： RAID0、RAID1、RAID5、RAID6、RAID1+0等 ①、RAID 0（条带化存储） RAID 0连续以位或字节为单位分割数据，并行读&#x2F;写于多个磁盘上，因此具有很高的数据传输率，但它没有数据冗余。 RAID 0只是单纯地提高性能，并没有为数据的可靠性提供保证，而且其中的一个磁盘失效将影响到所有数据 RAID 0不能应用于数据安全性要求高的场合 ②、RAID 1（镜像存储） 通过磁盘数据镜像实现数据冗余，在成对的独立磁盘上产生互为备份的数据 当原始数据繁忙时，可直接从镜像拷贝中读取数据，因此RAID 1可以提高读取性能 RAID 1是磁盘阵列中单位成本最高的。但提供了很高的数据安全性和可用性。当一个磁盘失效时，系统可以自动切换到镜像磁盘上读写，而不需要重组失效的数据。 ③、RAID 5 N(N≥3)块盘组成阵列，一份数据产生N-1个条带，同时还有一份校验数据，共N份数据在N块盘上循环均衡存储 N块盘同时读写，读性能很高，但由于有校验机制的问题，写性能相对不高 （N-1）&#x2F;N 磁盘利用率 可靠性高，允许坏一块盘，不影响所有数据 ④、RAID 6 N（N≥4）块盘组成阵列，（N-2）&#x2F;N 磁盘利用率 与RAID 5相比，RAID 6增加了第二块独立的奇偶校验信息块 两个独立的奇偶系统使用不同的算法，即使两块磁盘同时失效也不会影响数据的使用 相对于RAID 5有更大的“写损失”，因此写性能较差 ⑤、RAID 1+0(先做镜像，再做条带) N (偶数，N&gt;&#x3D;4)。块盘两两镜像后，再组合成一个RAID 0 N&#x2F;2磁盘利用率 N&#x2F;2块盘同时写入，N块盘同时读取 性能高，可靠性高 ⑥、RAID 0+1(先做条带，再做镜像) 读写性能与RAID 10相同 安全性低于RAID 10 2、创建软 RAID 磁盘阵列实验1、检查是否已安装mdadm 软件包 2、先关闭虚拟机，然后编辑虚拟机设置，添加4块硬盘，每块分配40G，点击确认后开启虚拟机 3、我们使用xshell来进行连接，使用fdisk -l来查看分区情况 4、对分区进行管理，创建分区并修改分区类型，这里示范一个&#x2F;dev&#x2F;sdb，其余的操作一样，就不示范了 5、使用fdisk -l看一下分区情况，是否全部转换完成 6、验证一下磁盘是否已做raid，然后开始创建raid，这里我们创建一个raid名为md0，级别使用RAID5，然后-l3设置使用三个磁盘，-x1使用一块备份磁盘，再进行查看创建速度。 7、这里已经创建好了，我们开始验证一下 8、我们模拟让它坏掉一个磁盘，来测试一下备份磁盘是否会自动顶上 9、想使用起来得先进行格式化，再进行挂载，我接着尝试了一下在格式化和挂载之后备用磁盘是否还能自动顶上，实验结果：可以。 3、创建软 RAID 磁盘阵列步骤命令1、检查是否已安装mdadm 软件包 12rpm -q mdadmyum install -y mdadm 2、使用fdisk工具将新磁盘设备&#x2F;dev&#x2F;sdb、&#x2F;dev&#x2F;sdc、&#x2F;dev&#x2F;sdd、&#x2F;dev&#x2F;sde划分出主分区sdb1、sdc1、sdd1、sde1，并且把分区类型的 ID 标记号改为“fd” 12fdisk /dev/sdbfdisk /dev/sdc 3、创建 RAID 设备 12#创建RAID5mdadm -C -v /dev/md0 [-a yes] -l5 -n3 /dev/sd[bcd]1 -x1 /dev/sde1 -C：表示新建； -v：显示创建过程中的详细信息。 &#x2F;dev&#x2F;md0：创建 RAID5 的名称。 -a yes：–auto，表示如果有什么设备文件没有存在的话就自动创建，可省略。 -l：指定 RAID 的级别，l5 表示创建 RAID5。 -n：指定使用几块硬盘创建 RAID，n3 表示使用 3 块硬盘创建 RAID。 &#x2F;dev&#x2F;sd[bcd]1：指定使用这四块磁盘分区去创建 RAID。 -x：指定使用几块硬盘做RAID的热备用盘，x1表示保留1块空闲的硬盘作备用 &#x2F;dev&#x2F;sde1：指定用作于备用的磁盘 1234cat /proc/mdstat #还能查看创建RAID的进度或者mdadm -D /dev/md0 #查看RAID磁盘详细信息mdadm -E /dev/sd[b-e]1 #检查磁盘是否已做RAID 4、创建并挂载文件系统 1234567mkfs -t xfs /dev/md0mkdir /myraidmount /dev/md0 /myraid/df -Thcp /etc/fstab /etc/fstab.bakvim /etc/fstab/dev/md0 /myraid xfs defaults 0 0 5、实现故障恢复 12mdadm /dev/md0 -f /dev/sdb1 #模拟/dev/sdb1 故障mdadm -D /dev/md0 #查看发现sde1已顶替sdb1 mdadm命令其它常用选项： -r：移除设备 -a：添加设备 -S：停止RAID -A：启动RAID 12mdadm -S /dev/md0mdadm /dev/md0 -r /dev/sdb1 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/448266123","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统内存管理之磁盘高速缓存机制","path":"/2023/01/15/linux-docs/Linux操作系统内存管理之磁盘高速缓存机制/","content":"前言：相信通过前面的虚拟文件系统VFS及一个具体的Ext2文件系统博文，大家对基本的VFS体系有一个大致的掌握了吧。从本章开始，我们将讨论一些VFS底层的技术细节，磁盘高速缓存就是其中一个重要的技术。磁盘高速缓存是一种软件机制，它允许系统把通常存放在磁盘上的一些数据保留在RAM中，以便对那些数据的进一步访问而不用再访问磁盘。 因为对同一磁盘数据的反复访问频繁发生，所以磁盘高速缓存对系统性能至关重要。与磁盘交互的用户进程有权反复请求读或写同一磁盘数据。此外，不同的进程可能也需要在不同的时间访问相同的磁盘数据。例如，你可以使用cp命令拷贝一个文本文件，然后调用你喜欢的编辑器修改它。为了满足你的请求，命令shell将创建两个不同的进程，它们在不同的时间访问同一个文件。 我们曾在前面的博文中提到过其他的磁盘高速缓存：目录项高速缓存和索引节点高速缓存，前者存放的是描述文件系统路径名的目录项对象，而后者存放的是描述磁盘索引节点的索引节点对象。不过要注意，目录项对象和索引结节点对象不只是存放一些磁盘块内容的缓冲区，而是还加了一些内核感兴趣的其他信息，这些内容并不是从磁盘的某一个块上读取出来的；由此而知，目录项高速缓存和索引节点高速缓存是特殊的磁盘高速缓存，但不是属于我们这里讲的磁盘高速缓存概念的范围。 我们这里介绍的磁盘高速缓存其实是磁盘高速缓存——一种对完整的数据页进行操作的磁盘高速缓存，下面我们就从这个概念开始入手： 1，页高速缓存页高速缓存（page cache）是Linux内核所使用的主要磁盘高速缓存。在绝大多数情况下，内核在读写磁盘时都会引用页面高速缓存。新页被追加到页高速缓存以满足用户态进程的读写请求。如果页不在高速缓存中，新页就被加到高速缓存中，然后用从磁盘读出的数据填充它。如果内存有足够的空闲空间，就让该页在高速缓存中长期保留，使其他进程在使用该页时不再访问磁盘。 同样，在把一页数据写到块设备之前，内核首先检查对应的页是否已经在高速缓存中；如果不在，就要先在其中增加一个新项，并用要写到磁盘中的数据填充该项。I&#x2F;O数据的传送并不是马上开始，而是要延迟几秒之后才对磁盘进行更新，从而使进程有机会对要写入磁盘的数据做进一步的修改（换句话说，就是内核执行延迟的写操作）。 内核的代码和内核数据结构不必从磁盘读，也不必写入磁盘（如果要在关机后恢复系统的所有状态——其实几乎不会出现这种情况，可以执行“挂起到磁盘”操作（hibernation)，RAM的全部内容保存到交换区，时此我们不做更多的讨论。），因此，在高速缓存中的也面可能是下面的类型： 含有普通文件数据的页面。我们会在后面的博文描述内核如何处理它们的读、写和内存映射操作。 含有目录的页。其实，Linux采用与普通文件类似的方式操作目录文件。 含有直接从块设备文件（跳过文件系统层）读出的数据的页。内核处理这种页与处理含有普通文件的页使用相同的函数集合。 含有用户态进程数据的页面，但页中的数据已经被交换到磁盘。内核可能会强行在页高速缓存中保留一些页面，而这些页面中的数据已经被写到交换区（可能是普通文件或磁盘分区）。 -属于特殊文件系统文件的页，如共享内存的进程间通信（Interprocess Communication, IPC）所使用的特殊文件系统shm。 从上面我们可以得出结论，页高速缓存中的每个页所包含的数据肯定属于某个文件。这个文件（或者更准确地说是文件的索引节点）就称为页的所有者（owner）。（即使含有换出数据的页面都属于同一个所有者，即使它们涉及不同的交换区。） 几乎所有的文件读写和写操作都依赖于页面的高速缓存。只有在O_DIRECT标志被置位而进程打开文件的情况下才会出现例外：此时，I&#x2F;O数据的传送绕过了页高速缓而使用了进程用户态地址空间的缓冲区；少数数据库应用软件为了能采用自己的磁盘高速缓存算法而使用了O_DIRECT标志。 页面高速缓存中的信息单位显然是一个完整的数据页。一个页中包含的磁盘块在物理上不一定是相邻的，所以不能用设备号和块号来识别它，取而代之的是，通过页的所有者和所有者数据中的索引（通常是一个索引节点和在相应文件中的偏移量）来识别页高速缓存中的页。 2，address_space对象页高速缓存的核心数据结构是address_space对象，它是一个嵌入在页所有者的索引节点对象中的数据结构（页被换出可能会引起缺页异常，这些被换出的页拥有不在任何索引节点中的公共address_space对象）。高速缓存中的许多页可能属于同一个所有者，从而可能被链接到同一个address_space对象。该对象还在所有者的页面和对这些页面的操作之间建立起链接关系。 每个页描述符都包括两个字段mapping和index，把页链接到页高速缓存的。mapping字段指向拥有页的索引节点的address_space对象，index字段表示在所有者的地址空间中以页大小为单位的偏移量，也就是在所有者的磁盘映像中页中数据的位置。在页面高速缓存中查找页面时使用这两个字段。 值得庆幸的是，页面高速缓存可以包含同一磁盘数据的多个副本。例如，可以用下述方式访问普通文件的同一4KB的数据块： 读文件；因此，数据就包含在普通文件的索引节点所拥有的页面中。 从文件所在的设备文件（磁盘分区）读取块。因此，数据就包含在块设备文件的主索引节点所拥有的页中。 因此，两个不同address_space对象所引用的两个不同的页中出现了相同的磁盘数据。address_space对象包含如下所示： 123456789101112131415161718struct address_space &#123; struct inode *host; /* 指向拥有该对象的索引节点的指针（如果存在） */ struct radix_tree_root page_tree; /* 表示拥有者页的基树（radix tree）的根 */ rwlock_t tree_lock; /* 保护基树的自旋锁 */ unsigned int i_mmap_writable;/* 地址空间中共享内存映射的个数 */ struct prio_tree_root i_mmap; /* radix优先搜索树的根 */ struct list_head i_mmap_nonlinear;/* 地址空间中非线性内存区的链表 */ spinlock_t i_mmap_lock; /* 保护radix优先搜索树的自旋锁 */ unsigned int truncate_count; /* 截断文件时使用的顺序计数器 */ unsigned long nrpages; /* 所有者的页总数 */ pgoff_t writeback_index;/* 最后一次回写操作所作用的页的索引 */ const struct address_space_operations *a_ops; /* 对所有者页进行操作的方法 */ unsigned long flags; /* 错误位和内存分配器的标志 */ struct backing_dev_info *backing_dev_info; /* 指向拥有所有者数据的块设备的backing_dev_info的指针 */ spinlock_t private_lock; /* 通常是管理private_list链表时使用的自旋锁 */ struct list_head private_list; /* 通常是与索引节点相关的间接块的脏缓冲区的链表*/ struct address_space *assoc_mapping; /* 通常是指向间接块所在块设备的address_space对象的指针 */&#125; __attribute__((aligned(sizeof(long)))); 如果页高速缓存中页的所有者是一个文件，address_space对象就嵌入在VFS索引节点对象的i_data字段中。索引节点的i_mapping字段总是指向索引节点的数据页所有者的address_space对象。address_space对象的host字段指向其所有者的索引节点对象。 因此，如果页属于一个文件（存放在Ext3文件系统中），那么页的所有者就是文件的索引节点，而且相应的address_space对象存放在VFS索引节点对象的i_data字段中。索引节点的i_mapping字段指向同一个索引节点的i_data字段，而address_space对象的host字段也指向这个索引节点。 不过，有些时候情况会更复杂。如果页中包含的数据来自块设备文件，即页含有存放着块设备的“原始”数据，那么就把address_space对象嵌入到与该块设备相关的特殊文件系统bdev中文件的“主”索引节点中（块设备描述符的bd_inode字段引用这个索引节点，参见“块设备”博文）。因此，块设备文件对应索引节点的i_mapping字段指向主索引节点中的address_space对象。相应地，address_space对象的host字段指向主索引节点。这样，从块设备读取数据的所有页具有相同的address_space对象，即使这些数据位于不同地块设备文件。 i_mmap, i_mmap_writable, i_mmap_nonlinear和i_mmap_lock字段涉及内存映射和反映射，我们将在后面的博文讨论这些主题。 backing_dev_info字段指向backing_dev_info描述符，后者是对所有者的数据所在块设备进行有关描述的数据结构。 private_list字段是普通链表的首部，文件系统在实现其特定功能时可以随意使用。例如，Ext2文件系统利用这个链表收集与索引节点相关的“间接”块的脏缓冲区。当刷新操作把索引节点强行写入磁盘时，内核也同时刷新该链表中的所有缓冲区。此外，Ext2文件系统在assoc_mapping字段中存放指向间接块所在块设备的address_space对象，并使用assoc_mapping-&gt;private_lock自旋锁保护多处理器系统中的间接块链表。 address_space对象的关键字段是a_ops,它指向一个类型为address_space_operations的表，表中定义了对所有者的页进行处理的各种方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344struct address_space_operations &#123;/* 写操作（从页写到所有者的磁盘映像） */ int (*writepage)(struct page *page, struct writeback_control *wbc);/* 读操作（从所有者的磁盘映像读到页） */ int (*readpage)(struct file *, struct page *);/* 如果对所有者页进行的操作已准备好，则立刻开始I/O数据的传输 */ void (*sync_page)(struct page *); /* Write back some dirty pages from this mapping. *//* 把指定数量的所有者脏页写回磁盘 */ int (*writepages)(struct address_space *, struct writeback_control *); /* Set a page dirty. Return true if this dirtied it *//* 把所有者的页设置为脏页 */ int (*set_page_dirty)(struct page *page);/* 从磁盘中读所有者页的链表 */ int (*readpages)(struct file *filp, struct address_space *mapping, struct list_head *pages, unsigned nr_pages); /* * ext3 requires that a successful prepare_write() call be followed * by a commit_write() call - they must be balanced *//* 为写操作做准备（由磁盘文件系统使用） */ int (*prepare_write)(struct file *, struct page *, unsigned, unsigned);/* 完成写操作（由磁盘文件系统使用） */ int (*commit_write)(struct file *, struct page *, unsigned, unsigned); /* Unfortunately this kludge is needed for FIBMAP. Don&#x27;t use it *//* 从文件块索引中获取逻辑块号 */ sector_t (*bmap)(struct address_space *, sector_t);/* 使所有者的页无效（截断文件时使用） */ void (*invalidatepage) (struct page *, unsigned long);/* 由日志文件系统使用以准备释放页 */ int (*releasepage) (struct page *, gfp_t);/* 所有者页的直接I/O传输（绕过页高速缓存） */ ssize_t (*direct_IO)(int, struct kiocb *, const struct iovec *iov, loff_t offset, unsigned long nr_segs); struct page* (*get_xip_page)(struct address_space *, sector_t, int); /* migrate the contents of a page to the specified target */ int (*migratepage) (struct address_space *, struct page *, struct page *);&#125;; 其中最重要的方法是readpage, writepage, prepare_write和commit_write。我们将在后面的博文对它们进行讨论。在绝大多数情况下，这些方法把所有者的索引节点对象和访问物理设备的低级驱动程序联系起来。例如，为普通文件的索引节点实现readpage方法的函数知道如何确定文件页的对应块在物理磁盘设备上的位置。 3，基树Linux支持大到几个TB的文件。访问大文件时，页面高速缓存中可能充满太多的文件页，以至于顺序扫描这些页要消耗大量的时间。为了实现页高速缓存的高效查找，Linux2.6采用了大量的搜索树，其中每个address_space对象对应一棵搜索树。 address_space对象的page_tree字段是基数（radix tree）的根，它包含指向所有者的描述符的指针。通过它，内核能够通过快速搜索操作来确定所需要的页是否在高速缓存中。当查找所需要的页时，内核把页索（page-&gt;index）引转换为基数中的路径，并快速找到页描述符所（或应当）在的位置。如果找到，内核可以从基树获得页描述符，而且还可以很快确定所找到的页是否是脏页（也就是应当被刷新到磁盘的页），以及其数据的I&#x2F;O传送是否正在进行。 基树的每个节点可以有多到64个指针指向其他节点或页描述符。底层节点存放指向页描述符的指针（叶子节点），而上层的节点存放指向其他节点（孩子节点）的指针。每个节点由radix_tree_node数据结构表示，它包括三个字段：slots是包括64个指针数组，count是记录节点中非空指针数量的计数器，tags是二维的标志数组： 1234567struct radix_tree_node &#123; unsigned int count; void *slots[RADIX_TREE_MAP_SIZE]; unsigned long tags[RADIX_TREE_MAX_TAGS][RADIX_TREE_TAG_LONGS];&#125;;#define RADIX_TREE_MAP_SIZE (1UL &lt;&lt; RADIX_TREE_MAP_SHIFT) /* 64 */#define RADIX_TREE_MAP_SHIFT 3 树根由radix_tree_root数据结构表示，他有三个字段：height表示树的当前深度（不包括叶子节点的层数），gfp_mask指定为新节点请求内存时所用的标志，mode指向与树中第一层节点相应的数据结构radix_tree_node（如果有的话）： 12345struct radix_tree_root &#123; unsigned int height; gfp_t gfp_mask; struct radix_tree_node *rnode;&#125;; 我们来看一个简单的例子。如果树中的索引都小于63，那么树的深度就等于1，因为可能存在的64个叶子可以都存放在第一层的节点中[如图（a）所示]。不过，如果与索引131相应的新页的描述符肯定存放在页高速缓存中，那么树的深度就增加为2，这样基树就可以查找多达4095个索引[如图（b）所示]。 回顾一下分页系统是如何利用页表实现线性地址到物理地址转换的，从而理解如何实现页表查找。线性地址最高20位分成两个10位的字段：第一个字段是页目录中的偏移量，而第二个字段是某个页目录项所指向的页表中的偏移量。 技术中使用类似的方法。页索引相当于线性地址，不过页索引中要考虑的字段的数量依赖于基数的深度。如果基数的深度为1，就只能表示从063范围的索引，因此页索引得第6位被解释为slots数组的下标，每个下标对应第一层的一个节点。如果基数的深度为2，就可以表示从04095范围的索引，页索引得第12位分成两个6位的字段，高位的字段用于表示第一层节点数组的下标，而低位的字段用于表示第二层数组的下标。依此类推，如果深度等于6，页索引得最高两位（因为page-&gt;index是32位的）表示第一层节点数组的下标，接下来的6位表示第二层节点数组的下标，这样一直到最低6位，它们表示第六层节点数组的下标。 如果基树的最大索引小于应该增加的页的索引，那么内核相应地增加树的深度；基树的中间节点依赖于索引得值。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/448258669","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统基础的常用命令","path":"/2023/01/15/linux-docs/Linux操作系统基础的常用命令/","content":"1，Linux简介Linux是一种自由和开放源码的操作系统，存在着许多不同的Linux版本，但它们都使用了Linux内核。Linux可安装在各种计算机硬件设备中，比如手机、平板电脑、路由器、台式计算机。 1.1Linux介绍Linux出现于1991年，是由芬兰赫尔辛基大学学生Linus Torvalds和后来加入的众多爱好者共同开发完成 1.2Linux特点多用户，多任务，丰富的网络功能，可靠的系统安全，良好的可移植性，具有标准兼容性，良好的用户界面，出色的速度性能 1.3开源CentOS 主流：目前的Linux操作系统主要应用于生产环境，主流企业级Linux系统仍旧是RedHat或者CentOS 免费：RedHat 和CentOS差别不大，基于Red Hat Linux 提供的可自由使用源代码的企业CentOS是一个级Linux发行版本 更新方便：CentOS独有的yum命令支持在线升级，可以即时更新系统，不像RedHat 那样需要花钱购买支持服务！ 1.4Linux目录结构 bin (binaries)存放二进制可执行文件 sbin (super user binaries)存放二进制可执行文件，只有root才能访问 etc (etcetera)存放系统配置文件 usr (unix shared resources)用于存放共享的系统资源 home 存放用户文件的根目录 root 超级用户目录 dev (devices)用于存放设备文件 lib (library)存放跟文件系统中的程序运行所需要的共享库及内核模块 mnt (mount)系统管理员安装临时文件系统的安装点 boot 存放用于系统引导时使用的各种文件 tmp (temporary)用于存放各种临时文件 var (variable)用于存放运行时需要改变数据的文件 2，Linux常用命令2.1命令格式：命令 -选项 参数123如：ls -la /usrls：显示文件和目录列表(list) 常用参数： 123-l (long)-a\t(all) 注意隐藏文件、特殊目录.和.. -t (time) 2.2Linux命令的分类内部命令：属于Shell解析器的一部分 123cd 切换目录（change directory）pwd 显示当前工作目录（print working directory）help 帮助 外部命令：独立于Shell解析器之外的文件程序 123ls 显示文件和目录列表（list）mkdir 创建目录（make directoriy）cp 复制文件或目录（copy） 查看帮助文档 12内部命令：help + 命令（help cd）外部命令：man + 命令（man ls） 2.3操作文件或目录常用命令1234567891011121314151617pwd 显示当前工作目录（print working directory）touch 创建空文件 mkdir 创建目录（make directoriy）-p 父目录不存在情况下先生成父目录 （parents） cp 复制文件或目录（copy）-r 递归处理，将指定目录下的文件与子目录一并拷贝（recursive） mv 移动文件或目录、文件或目录改名（move）rm 删除文件（remove）-r 同时删除该目录下的所有文件（recursive）-f 强制删除文件或目录（force）rmdir 删除空目录（remove directoriy）cat显示文本文件内容 （catenate）more、less 分页显示文本文件内容head、tail查看文本中开头或结尾部分的内容head -n 5 a.log 查看a.log文件的前5行tail -F b.log 循环读取（follow） 常用命令 123456789101112131415161718192021222324252627282930313233wc 统计文本的行数、字数、字符数（word count）-m 统计文本字符数-w 统计文本字数-l 统计文本行数find 在文件系统中查找指定的文件find /etc/ -name &quot;aaa&quot;grep 在指定的文本文件中查找指定的字符串ln 建立链接文件（link）-s 对源文件建立符号连接，而非硬连接（symbolic）top 显示当前系统中耗费资源最多的进程 ps 显示瞬间的进程状态-e /-A 显示所有进程，环境变量-f 全格式-a 显示所有用户的所有进程（包括其它用户）-u 按用户名和启动时间的顺序来显示进程-x 显示无控制终端的进程kill 杀死一个进程kill -9 piddf 显示文件系统磁盘空间的使用情况du 显示指定的文件（目录）已使用的磁盘空间的总-h文件大小以K，M，G为单位显示（human-readable）-s只显示各档案大小的总合（summarize）free 显示当前内存和交换空间的使用情况 netstat 显示网络状态信息-a 显示所有连接和监听端口-t (tcp)仅显示tcp相关选项-u (udp)仅显示udp相关选项-n 拒绝显示别名，能显示数字的全部转化成数字。-p 显示建立相关链接的程序名ifconfig 网卡网络配置详解 ping 测试网络的连通性 备份压缩命令 123gzip 压缩（解压）文件或目录，压缩文件后缀为gz bzip2 压缩（解压）文件或目录，压缩文件后缀为bz2 tar 文件、目录打（解）包 gzip命令 12345命令格式：gzip [选项] 压缩（解压缩）的文件名-d将压缩文件解压（decompress）-l显示压缩文件的大小，未压缩文件的大小，压缩比（list）-v显示文件名和压缩比（verbose）-num用指定的数字num调整压缩的速度，-1或--fast表示最快压缩方法（低压缩比），-9或--best表示最慢压缩方法（高压缩比）。系统缺省值为6 bzip2命令 12345命令格式：bzip2 [-cdz] 文档名-c将压缩的过程产生的数据输出到屏幕上-d解压缩的参数（decompress）-z压缩的参数（compress）-num 用指定的数字num调整压缩的速度，-1或--fast表示最快压缩方法（低压缩比），-9或--best表示最慢压缩方法（高压缩比）。系统缺省值为6 tar命令 123456-c 建立一个压缩文件的参数指令（create）-x 解开一个压缩文件的参数指令（extract）-z 是否需要用 gzip 压缩-j 是否需要用 bzip2 压缩-v 压缩的过程中显示文件（verbose）-f 使用档名，在 f 之后要立即接档名（file） 关机&#x2F;重启命令 12345shutdown系统关机 -r 关机后立即重启-h 关机后不重新启动halt 关机后关闭电源 shutdown -hreboot 重新启动 shutdown -r 学习Linux的好习惯 善于查看man page（manual）等帮助文档 利用好Tab键 掌握好一些快捷键 123ctrl + c（停止当前进程）ctrl + r（查看命令历史）ctrl + l（清屏，与clear命令作用相同） 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/434528439","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统处理器调度基本准则和实现","path":"/2023/01/15/linux-docs/Linux操作系统处理器调度基本准则和实现/","content":"1，基本概念在多道程序系统中，进程的数量往往多于处理机的个数，进程争用处理机的情况就在所难免。处理机调度是对处理机进行分配，就是从就绪队列中，按照一定的算法（公平、低效）选择一个进程并将处理机分配给它运行，以实现进程并发地执行。 处理机调度是多道程序操作系统的基础，它是操作系统设计的核心问题。 2，调度的层次一个作业从提交开始直到完成，往往要经历以下三级调度，如图2-4所示。 \\1) 作业调度。又称高级调度，.其主要任务是按一定的原则从外存上处于后备状态的作业中挑选一个（或多个）作业，给它（们）分配内存、输入&#x2F;输出设备等必要的资源，并建立相应的进程，以使它（们）获得竞争处理机的权利。简言之，就是内存与辅存之间的调度。对于每个作业只调入一次、调出一次。 多道批处理系统中大多配有作业调度，而其他系统中通常不需要配置作业调度。作业调度的执行频率较低，通常为几分钟一次。 \\2) 中级调度。又称内存调度。引入中级调度是为了提高内存利用率和系统吞吐量。为此，应使那些暂时不能运行的进程，调至外存等待，把此时的进程状态称为挂起状态。当它们已具备运行条件且内存又稍有空闲时，由中级调度来决定，把内存上的那些已具备运行条件的就绪进程，再重新调入内存，并修改其状态为就绪状态，挂在就绪队列上等待。 \\3) 进程调度。又称为低级调度，其主要任务是按照某种方法和策略从就绪队列中选取一个进程，将处理机分配给它。进程调度是操作系统中最基本的一种调度，在一般操作系统中都必须配置进程调度。进程调度的频率很高，一般几十毫秒一次。 3，三级调度的联系作业调度从外存的后备队列中选择一批作业进入内存，为它们建立进程，这些进程被送入就绪队列，进程调度从就绪队列中选出一个进程，并把其状态改为运行状态，把CPU分配给它。中级调度是为了提高内存的利用率，系统将那些暂时不能运行的进程挂起来。当内存空间宽松时，通过中级调度选择具备运行条件的进程，将其唤醒。 \\1) 作业调度为进程活动做准备，进程调度使进程正常活动起来，中级调度将暂时不能运行的进程挂起，中级调度处于作业调度和进程调度之间。 \\2) 作业调度次数少，中级调度次数略多，进程调度频率最高。 \\3) 进程调度是最基本的，不可或缺的。 4，调度的时机、切换与过程进程调度和切换程序是操作系统内核程序。当请求调度的事件发生后，才可能会运行进程调度程序，当调度了新的就绪进程后，才会去进行进程间的切换。理论上这三件事情应该顺序执行，但在实际设计中，在操作系统内核程序运行时，如果某时发生了引起进程调度的因素，并不一定能够马上进行调度与切换。 现代操作系统中，不能进行进程的调度与切换的情况有以下几种情况。 \\1) 在处理中断的过程中：中断处理过程复杂，在实现上很难做到进程切换，而且中断处理是系统工作的一部分，逻辑上不属于某一进程，不应被剥夺处理机资源。 \\2) 进程在操作系统内核程序临界区中：进入临界区后，需要独占式地访问共享数据，理论上必须加锁，以防止其他并行程序进入，在解锁前不应切换到其他进程运行，以加快该共享数据的释放。 \\3) 其他需要完全屏蔽中断的原子操作过程中：如加锁、解锁、中断现场保护、恢复等原子操作。在原子过程中，连中断都要屏蔽，更不应该进行进程调度与切换。 如果在上述过程中发生了引起调度的条件，并不能马上进行调度和切换，应置系统的请求调度标志，直到上述过程结束后才进行相应的调度与切换。 应该进行进程调度与切换的情况有： \\1) 当发生引起调度条件，且当前进程无法继续运行下去时，可以马上进行调度与切换。如果操作系统只在这种情况下进行进程调度，就是非剥夺调度。 \\2) 当中断处理结束或自动处理结束后，返回被中断进程的用户态程序执行现场前，若置上请求调度标志，即可马上进行进程调度与切换。如果操作系统支持这种情况下的运行调度程序，就实现了剥夺方式的调度。 进程切换往往在调度完成后立刻发生，它要求保存原进程当前切换点的现场信息，恢复被调度进程的现场信息。现场切换时，操作系统内核将原进程的现场信息推入到当前进程的内核堆栈来保存它们，并更新堆栈指针。内核完成从新进程的内核栈中装入新进程的现场信息、更新当前运行进程空间指针、重设PC寄存器等相关工作之后，开始运行新的进程。 5，进程调度方式所谓进程调度方式是指当某一个进程正在处理机上执行时，若有某个更为重要或紧迫的进程需要处理，即有优先权更髙的进程进入就绪队列，此时应如何分配处理机。 通常有以下两种进程调度方式： \\1) 非剥夺调度方式，又称非抢占方式。是指当一个进程正在处理机上执行时，即使有某个更为重要或紧迫的进程进入就绪队列，仍然让正在执行的进程继续执行，直到该进程完成或发生某种事件而进入阻塞状态时，才把处理机分配给更为重要或紧迫的进程。 在非剥夺调度方式下，一旦把CPU分配给一个进程，那么该进程就会保持CPU直到终止或转换到等待状态。这种方式的优点是实现简单、系统开销小，适用于大多数的批处理系统，但它不能用于分时系统和大多数的实时系统。 \\2) 剥夺调度方式，又称抢占方式。是指当一个进程正在处理机上执行时，若有某个更为重要或紧迫的进程需要使用处理机，则立即暂停正在执行的进程，将处理机分配给这个更为重要或紧迫的进程。. 采用剥夺式的调度，对提高系统吞吐率和响应效率都有明显的好处。但“剥夺”不是一种任意性行为，必须遵循一定的原则，主要有：优先权、短进程优先和时间的原则等。 调度的基本准则 不同的调度算法具有不同的特性，在选择调度算法时，必须考虑算法所具有的特性。为了比较处理机调度算法的性能，人们提出很多评价准则，下面介绍主要的几种： \\1) CPU利用率。CPU是计算机系统中最重要和昂贵的资源之一，所以应尽可能使CPU 保持“忙”状态，使这一资源利用率最高。 \\2) 系统吞吐量。表示单位时间内CPU完成作业的数量。长作业需要消耗较长的处理机时间，因此会降低系统的吞吐量。而对于短作业，它们所需要消耗的处理机时间较短，因此能提高系统的吞吐量。调度算法和方式的不同，也会对系统的吞吐量产生较大的影响。 \\3) 周转时间。是指从作业提交到作业完成所经历的时间，包括作业等待、在就绪队列中排队、在处迤机上运行以及进行输入&#x2F;输出操作所花费时间的总和。 作业的周转时间可用公式表示如下： 周转时间 &#x3D; 作业完成时间 - 作业提交时间 平均周转时间是指多个作业周转时间的平均值： 平均周转时间 &#x3D; (作业1的周转时间 + … + 作业 n 的周转时间) &#x2F; n 带权周转时间是指作业周转时间与作业实际运行时间的比值： 平均带权周转时间是指多个作业带权周转时间的平均值： 平均带权周转时间 &#x3D; (作业1的带权周转时间 + … + 作业 n 的带权周转时间) &#x2F; n \\4) 等待时间。&#x3D;开始时间—提交时间。 是指进程处于等待处理机状态时间之和，等待时间越长，用户满意度越低。处理机调度算法实际上并不影响作业执行或输入&#x2F;输出操作的时间，只影响作业在就绪队列中等待所花的时间。因此，衡量一个调度算法的优劣常常只需简单地考察等待时间。 \\5) 响应时间。是指从用户提交请求到系统首次产生响应所用的时间。在交互式系统中，周转时间不可能是最好的评价准则，一般采用响应时间作为衡量调度算法的重要准则之一。从用户角度看，调度策略应尽量降低响应时间，使响应时间处在用户能接受的范围之内。 要想得到一个满足所有用户和系统要求的算法几乎是不可能的。设计调度程序，一方面要满足特定系统用户的要求（如某些实时和交互进程快速响应要求)，另一方面要考虑系统整体效率（如减少整个系统进程平均周转时间），同时还要考虑调度算法的开销。 操作系统典型调度算法 在操作系统中存在多种调度算法，其中有的调度算法适用于作业调度，有的调度算法适用于进程调度，有的调度算法两者都适用。下面介绍几种常用的调度算法。 先来先服务(FCFS)调度算法 FCFS调度算法是一种最简单的调度算法，该调度算法既可以用于作业调度也可以用于进程调度。 在作业调度中，算法每次从后备作业队列中选择最先进入该队列的一个或几个作业，将它们调入内存，分配必要的资源，创建进程并放入就绪队列。 在进程调度中，FCFS调度算法每次从就绪队列中选择最先进入该队列的进程，将处理机分配给它，使之投入运行，直到完成或因某种原因而阻塞时才释放处理机。 下面通过一个实例来说明FCFS调度算法的性能。假设系统中有4个作业，它们的提交时间分别是8、8.4、8.8、9，运行时间依次是2、1、0.5、0.2，系统釆用FCFS调度算法，这组作业的平均等待时间、平均周转时间和平均带权周转时间见表2-3。 平均等待时间 t &#x3D; (0+1.6+2.2+2.5)&#x2F;4&#x3D;1.575 平均周转时间 T &#x3D; (2+2.6+2.7+2.7)&#x2F;4&#x3D;2.5 平均带权周转时间 W &#x3D; (1+2.6+5.牡13.5)&#x2F;4&#x3D;5.625 FCFS调度算法属于不可剥夺算法。从表面上看，它对所有作业都是公平的，但若一个长作业先到达系统，就会使后面许多短作业等待很长时间，因此它不能作为分时系统和实时系统的主要调度策略。但它常被结合在其他调度策略中使用。例如，在使用优先级作为调度策略的系统中，往往对多个具有相同优先级的进程按FCFS原则处理。 FCFS调度算法的特点是算法简单，但效率低；对长作业比较有利，但对短作业不利（相对SJF和高响应比）；有利于CPU繁忙型作业，而不利于I&#x2F;O繁忙型作业。 短作业优先(SJF)调度算法 短作业（进程）优先调度算法（Shortest Job First ）是指对短作业（进程）优先调度的算法。短作业优先(SJF)调度算法是从后备队列中选择一个或若干个估计运行时间最短的作业，将它们调入内存运行。而短进程优先(SPF)调度算法，则是从就绪队列中选择一个估计运行时间最短的进程，将处理机分配给它，使之立即执行，直到完成或发生某事件而阻塞时，才释放处理机。 例如，考虑表2-3中给出的一组作业，若系统采用短作业优先调度算法，其平均等待时间、平均周转时间和平均带权周转时间见表2-4。 平均等待时间 t &#x3D; (0+2.3+1.4+1)&#x2F;4&#x3D;1.175 平均周转时间 T &#x3D; (2+3.3+1.9+1.2)&#x2F;4&#x3D;2.1 平均带权周转时间 W &#x3D; (1+3.3+3.8+6)&#x2F;4&#x3D;3.525 SJF调度算法也存在不容忽视的缺点： 该算法对长作业不利，由表2-3和表2-4可知，SJF调度算法中长作业的周转时间会增加。更严重的是，如果有一长作业进入系统的后备队列，由于调度程序总是优先调度那些 (即使是后进来的）短作业，将导致长作业长期不被调度（“饥饿”现象，注意区分“死锁”。后者是系统环形等待，前者是调度策略问题）。 该算法完全未考虑作业的紧迫程度，因而不能保证紧迫性作业会被及时处理。 由于作业的长短只是根据用户所提供的估计执行时间而定的，而用户又可能会有意或无意地缩短其作业的估计运行时间，致使该算法不一定能真正做到短作业优先调度。 注意，SJF调度算法的平均等待时间、平均周转时间最少。 优先级调度算法 优先级调度算法又称优先权调度算法，该算法既可以用于作业调度，也可以用于进程调度，该算法中的优先级用于描述作业运行的紧迫程度。 在作业调度中，优先级调度算法每次从后备作业队列中选择优先级最髙的一个或几个作业，将它们调入内存，分配必要的资源，创建进程并放入就绪队列。在进程调度中，优先级调度算法每次从就绪队列中选择优先级最高的进程，将处理机分配给它，使之投入运行。 根据新的更高优先级进程能否抢占正在执行的进程，可将该调度算法分为： 非剥夺式优先级调度算法。当某一个进程正在处理机上运行时，即使有某个更为重要或紧迫的进程进入就绪队列，仍然让正在运行的进程继续运行，直到由于其自身的原因而主动让出处理机时（任务完成或等待事件），才把处理机分配给更为重要或紧迫的进程。 剥夺式优先级调度算法。当一个进程正在处理机上运行时，若有某个更为重要或紧迫的进程进入就绪队列，则立即暂停正在运行的进程，将处理机分配给更重要或紧迫的进程。 而根据进程创建后其优先级是否可以改变，可以将进程优先级分为以下两种： 静态优先级。优先级是在创建进程时确定的，且在进程的整个运行期间保持不变。确定静态优先级的主要依据有进程类型、进程对资源的要求、用户要求。 动态优先级。在进程运行过程中，根据进程情况的变化动态调整优先级。动态调整优先级的主要依据为进程占有CPU时间的长短、就绪进程等待CPU时间的长短。 高响应比优先调度算法 高响应比优先调度算法主要用于作业调度，该算法是对FCFS调度算法和SJF调度算法的一种综合平衡，同时考虑每个作业的等待时间和估计的运行时间。在每次进行作业调度时，先计算后备作业队列中每个作业的响应比，从中选出响应比最高的作业投入运行。 响应比的变化规律可描述为： 根据公式可知： 当作业的等待时间相同时，则要求服务时间越短，其响应比越高，有利于短作业。 当要求服务时间相同时，作业的响应比由其等待时间决定，等待时间越长，其响应比越高，因而它实现的是先来先服务。 对于长作业，作业的响应比可以随等待时间的增加而提高，当其等待时间足够长时，其响应比便可升到很高，从而也可获得处理机。克服了饥饿状态，兼顾了长作业。 时间片轮转调度算法 时间片轮转调度算法主要适用于分时系统。在这种算法中，系统将所有就绪进程按到达时间的先后次序排成一个队列，进程调度程序总是选择就绪队列中第一个进程执行，即先来先服务的原则，但仅能运行一个时间片，如100ms。在使用完一个时间片后，即使进程并未完成其运行，它也必须释放出（被剥夺）处理机给下一个就绪的进程，而被剥夺的进程返回到就绪队列的末尾重新排队，等候再次运行。 在时间片轮转调度算法中，时间片的大小对系统性能的影响很大。如果时间片足够大，以至于所有进程都能在一个时间片内执行完毕，则时间片轮转调度算法就退化为先来先服务调度算法。如果时间片很小，那么处理机将在进程间过于频繁切换，使处理机的开销增大，而真正用于运行用户进程的时间将减少。因此时间片的大小应选择适当。 时间片的长短通常由以下因素确定：系统的响应时间、就绪队列中的进程数目和系统的处理能力。 多级反馈队列调度算法（集合了前几种算法的优点） 多级反馈队列调度算法是时间片轮转调度算法和优先级调度算法的综合和发展，如图2-5 所示。通过动态调整进程优先级和时间片大小，多级反馈队列调度算法可以兼顾多方面的系统目标。例如，为提高系统吞吐量和缩短平均周转时间而照顾短进程；为获得较好的I&#x2F;O设备利用率和缩短响应时间而照顾I&#x2F;O型进程；同时，也不必事先估计进程的执行时间。 多级反馈队列调度算法的实现思想如下： 应设置多个就绪队列，并为各个队列赋予不同的优先级，第1级队列的优先级最高，第2级队列次之，其余队列的优先级逐次降低。 赋予各个队列中进程执行时间片的大小也各不相同，在优先级越高的队列中，每个进程的运行时间片就越小。例如，第2级队列的时间片要比第1级队列的时间片长一倍， ……第i+1级队列的时间片要比第i级队列的时间片长一倍。 当一个新进程进入内存后，首先将它放入第1级队列的末尾，按FCFS原则排队等待调度。当轮到该进程执行时，如它能在该时间片内完成，便可准备撤离系统；如果它在一个时间片结束时尚未完成，调度程序便将该进程转入第2级队列的末尾，再同样地按FCFS 原则等待调度执行；如果它在第2级队列中运行一个时间片后仍未完成，再以同样的方法放入第3级队列……如此下去，当一个长进程从第1级队列依次降到第 n 级队列后，在第 n 级队列中便釆用时间片轮转的方式运行。 仅当第1级队列为空时，调度程序才调度第2级队列中的进程运行；仅当第1 ~ (i-1)级队列均为空时，才会调度第i级队列中的进程运行。如果处理机正在执行第i级队列中的某进程时，又有新进程进入优先级较高的队列（第 1 ~ (i-1)中的任何一个队列），则此时新进程将抢占正在运行进程的处理机，即由调度程序把正在运行的进程放回到第i级队列的末尾，把处理机分配给新到的更高优先级的进程。 多级反馈队列的优势有： 终端型作业用户：短作业优先。 短批处理作业用户：周转时间较短。 长批处理作业用户：经过前面几个队列得到部分执行，不会长期得不到处理。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/447737355","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统学习——内核初始化","path":"/2023/01/15/linux-docs/Linux操作系统学习——内核初始化/","content":"一. 前言 前文分析到Linux内核正式启动，完成了实模式到保护模式的切换，并做好了各种准备工作。下来就要看开始内核初始化工作了，源码位置位于init&#x2F;main.c中的start_kernel()，源码如附录所示。这包括了一系列重要的初始化工作，本文会介绍其中一部分较为重要的，但是详细的介绍依然会留在后文各个模块的源码学习中单独进行。本文的目的在于承接上文给出一个从内核启动到各个模块开始运转的过程介绍，而不是详细的各部分内容介绍。 创建0号进程：INIT_TASK(init_task) 异常处理类中断服务程序挂接：trap_init() 内存初始化：mm_init() 调度器初始化sched_init() 剩余初始化：rest_init() 二. 0号进程的创建 start_kernel()上来就会运行 set_task_stack_end_magic(&amp;init_task)创建初始进程。init_task的定义是 struct task_struct init_task &#x3D; INIT_TASK(init_task)。它是系统创建的第一个进程，我们称为 0 号进程。这是唯一一个没有通过 fork 或者 kernel_thread产生的进程，是进程列表的第一个。 如下所示为init_task的定义，这里只节选了部分，采用了gcc的结构体初始化方式为其进行了直接赋值生成。 123456789101112131415161718192021222324252627282930/* * Set up the first task table, touch at your own risk!. Base=0, * limit=0x1fffff (=2MB) */struct task_struct init_task#ifdef CONFIG_ARCH_TASK_STRUCT_ON_STACK __init_task_data#endif= &#123; ...... .state = 0, .stack = init_stack, .usage = REFCOUNT_INIT(2), .flags = PF_KTHREAD, .prio = MAX_PRIO - 20, .static_prio = MAX_PRIO - 20, .normal_prio = MAX_PRIO - 20, .policy = SCHED_NORMAL, .cpus_ptr = &amp;init_task.cpus_mask, .cpus_mask = CPU_MASK_ALL, .nr_cpus_allowed = NR_CPUS, .mm = NULL, .active_mm = &amp;init_mm, ...... .thread_pid = &amp;init_struct_pid, .thread_group = LIST_HEAD_INIT(init_task.thread_group), .thread_node = LIST_HEAD_INIT(init_signals.thread_head), ......&#125;;EXPORT_SYMBOL(init_task); 而 set_task_stack_end_magic(&amp;init_task)函数的源码如下，主要是通过end_of_stack()获取栈边界地址，然后把栈底地址设置为STACK_END_MAGIC，作为栈溢出的标记。每个进程创建的时候，系统会为这个进程创建2个页大小的内核栈。 1234567void set_task_stack_end_magic(struct task_struct *tsk)&#123; unsigned long *stackend; stackend = end_of_stack(tsk); *stackend = STACK_END_MAGIC; /* for overflow detection */&#125; init_task是静态定义的一个进程，也就是说当内核被放入内存时，它就已经存在，它没有自己的用户空间，一直处于内核空间中运行，并且也只处于内核空间运行。0号进程用于包括内存、页表、必要数据结构、信号、调度器、硬件设备等的初始化。当它执行到最后（剩余初始化）时，将start_kernel中所有的初始化执行完成后，会在内核中启动一个kernel_init内核线程和一个kthreadd内核线程，kernel_init内核线程执行到最后会通过execve系统调用执行转变为我们所熟悉的init进程，而kthreadd内核线程是内核用于管理调度其他的内核线程的守护线程。在最后init_task将变成一个idle进程，用于在CPU没有进程运行时运行它，它在此时仅仅用于空转。 三. 中断初始化 由代码可见，trap_init()设置了很多的中断门（Interrupt Gate)，用于处理各种中断，如系统调用的中断门set_system_intr_gate(IA32_SYSCALL_VECTOR, entry_INT80_32)。 12345678910111213141516171819202122232425262728void trap_init(void)&#123; int i; //设置系统的硬件中断 中断位于kernel/asm.s 或 system_call.s set_trap_gate(0,÷_error);//0中断，位于/kernel/asm.s 19行 set_trap_gate(1,&amp;debug); set_trap_gate(2,&amp;nmi); set_system_gate(3,&amp;int3); /* int3-5 can be called from all */ set_system_gate(4,&amp;overflow); set_system_gate(5,&amp;bounds); set_trap_gate(6,&amp;invalid_op); set_trap_gate(7,&amp;device_not_available); set_trap_gate(8,&amp;double_fault); set_trap_gate(9,&amp;coprocessor_segment_overrun); set_trap_gate(10,&amp;invalid_TSS); set_trap_gate(11,&amp;segment_not_present); set_trap_gate(12,&amp;stack_segment); set_trap_gate(13,&amp;general_protection); set_trap_gate(14,&amp;page_fault); set_trap_gate(15,&amp;reserved); set_trap_gate(16,&amp;coprocessor_error); for (i=17;i&lt;48;i++) set_trap_gate(i,&amp;reserved); set_trap_gate(45,&amp;irq13); outb_p(inb_p(0x21)&amp;0xfb,0x21); outb(inb_p(0xA1)&amp;0xdf,0xA1); set_trap_gate(39,¶llel_interrupt);&#125; 四. 内存初始化 内存相关的初始化内容放在mm_init()中进行，代码如下所示 123456789101112131415161718192021// init/main.c/* * Set up kernel memory allocators */static void __init mm_init(void)&#123; /* * page_ext requires contiguous pages, * bigger than MAX_ORDER unless SPARSEMEM. */ page_ext_init_flatmem(); mem_init(); kmem_cache_init(); pgtable_init(); vmalloc_init(); ioremap_huge_init(); /* Should be run before the first non-init thread is created */ init_espfix_bsp(); /* Should be run after espfix64 is set up. */ pti_init();&#125; 调用的函数功能基本如名字所示，主要进行了以下初始化设置： page_ext_init_flatmem()和cgroup的初始化相关，该部分是docker技术的核心部分 mem_init()初始化内存管理的伙伴系统 kmem_cache_init()完成内核slub内存分配体系的初始化，相关的还有buffer_init pgtable_init()完成页表初始化，包括页表锁ptlock_init()和 vmalloc_init()完成vmalloc的初始化 ioremap_huge_init() ioremap实现I&#x2F;O内存资源由物理地址映射到虚拟地址空间，此处为其功能的初始化 init_espfix_bsp()和pti_init()完成PTI（page table isolation）的初始化 此处不展开说明这些函数，留待后面内存管理部分详细分析各个部分。 五. 调度器初始化 调度器初始化通过sched_init()完成，其主要工作包括 对相关数据结构分配内存：如初始化waitqueues数组，根据调度方式FAIR&#x2F;RT设置alloc_size，调用kzalloc分配空间 初始化root_task_group：根据FAIR&#x2F;RT的不同，将kzalloc分配的空间用于其初始化，主要结构task_group包含以下几个重要组成部分：se, rt_se, cfs_rq 以及 rt_rq。其中cfs_rq和rt_rq表示run queue，即一种特殊的per-cpu结构体用于内核调度器存储激活的线程。 调用for_each_possible_cpu()初始化每个possibleCPU（存储于cpu_possible_mask为图中）的runqueue队列(包括其中的cfs队列和实时进程队列)，rq结构体是调度进程的基本数据结构，调度器用rq决定下一个将要被调度的进程。详细介绍会在调度一节进行。 调用set_load_weight(&amp;init_task)，将init_task进程转变为idle进程 需要说明的是init_task在这里会被转变为idle进程，但是它还会继续执行初始化工作，相当于这里只是给init_task挂个idle进程的名号，它其实还是init_task进程，只有到最后init_task进程开启了kernel_init和kthreadd进程之后，才转变为真正意义上的idle进程。 六. 剩余初始化 rest_init是非常重要的一步，主要包括了区分内核态和用户态、初始化1号进程和初始化2号进程。 6.1 内核态和用户态 在运行用户进程之前，尚需要完成一件事：区分内核态和用户态。x86 提供了分层的权限机制，把区域分成了四个 Ring，越往里权限越高，越往外权限越低。操作系统很好地利用了这个机制，将能够访问关键资源的代码放在 Ring0，我们称为内核态（Kernel Mode）；将普通的程序代码放在 Ring3，我们称为用户态（User Mode）。 6.2 初始化1号进程 rest_init() 的一大工作是，用 kernel_thread(kernel_init, NULL, CLONE_FS)创建第二个进程，这个是 1 号进程。1 号进程对于操作系统来讲，有“划时代”的意义，因为它将运行一个用户进程，并从此开始形成用户态进程树。这里主要需要分析的是如何完成从内核态到用户态切换的过程。kernel_thread()代码如下所示，可见其中最主要的是第一个参数指针函数fn决定了栈中的内容，根据fn的不同将生成1号进程和后面的2号进程。 1234567891011121314/* * Create a kernel thread. */pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)&#123; struct kernel_clone_args args = &#123; .flags = ((flags | CLONE_VM | CLONE_UNTRACED) &amp; ~CSIGNAL), .exit_signal = (flags &amp; CSIGNAL), .stack = (unsigned long)fn, .stack_size = (unsigned long)arg, &#125;; return _do_fork(&amp;args);&#125; kernel_thread() 的参数是一个函数 kernel_init()，核心代码如下： 1234567891011if (ramdisk_execute_command) &#123; ret = run_init_process(ramdisk_execute_command); ...... &#125;...... if (!try_to_run_init_process(&quot;/sbin/init&quot;) || !try_to_run_init_process(&quot;/etc/init&quot;) || !try_to_run_init_process(&quot;/bin/init&quot;) || !try_to_run_init_process(&quot;/bin/sh&quot;)) return 0; 这就说明，1 号进程运行的是一个文件。如果我们打开 run_init_process() 函数，会发现它调用的是 do_execve()。 1234567static int run_init_process(const char *init_filename)&#123; argv_init[0] = init_filename; return do_execve(getname_kernel(init_filename), (const char __user *const __user *)argv_init, (const char __user *const __user *)envp_init);&#125; 接着会进行一系列的调用：do_execve-&gt;do_execveat_common-&gt;exec_binprm-&gt;search_binary_handler，这里search_binary_handler()主要是加载ELF文件（Executable and Linkable Format，可执行与可链接格式），代码如下 12345678int search_binary_handler(struct linux_binprm *bprm)&#123; ...... struct linux_binfmt *fmt; ...... retval = fmt-&gt;load_binary(bprm); ......&#125; load_binary先调用load_elf_binary，最后调用start_thread 123456789101112131415voidstart_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)&#123; set_user_gs(regs, 0); regs-&gt;fs = 0; regs-&gt;ds = __USER_DS; regs-&gt;es = __USER_DS; regs-&gt;ss = __USER_DS; regs-&gt;cs = __USER_CS; regs-&gt;ip = new_ip; regs-&gt;sp = new_sp; regs-&gt;flags = X86_EFLAGS_IF; force_iret();&#125;EXPORT_SYMBOL_GPL(start_thread); 这个结构就是在系统调用的时候，内核中保存用户态运行上下文的，里面将用户态的代码段 CS 设置为 __USER_CS，将用户态的数据段 DS 设置为 __USER_DS，以及指令指针寄存器 IP、栈指针寄存器 SP。这里相当于补上了原来系统调用里，保存寄存器的一个步骤。最后的 iret 是干什么的呢？它是用于从系统调用中返回。这个时候会恢复寄存器。从哪里恢复呢？按说是从进入系统调用的时候，保存的寄存器里面拿出。好在上面的函数补上了寄存器。CS 和指令指针寄存器 IP 恢复了，指向用户态下一个要执行的语句。DS 和函数栈指针 SP 也被恢复了，指向用户态函数栈的栈顶。所以，下一条指令，就从用户态开始运行了。 经过上述过程，我们完成了从内核态切换到用户态。而此时代码其实还在运行 kernel_init函数，会调用 12if (!ramdisk_execute_command) ramdisk_execute_command = &quot;/init&quot;; 结合上面的init程序，这里出现了第二个init。这是有其存在的必要性的：上文提到的 init 程序是在文件系统上的，文件系统一定是在一个存储设备上的，例如硬盘。Linux 访问存储设备，要有驱动才能访问。如果存储系统数目很有限，那驱动可以直接放到内核里面，反正前面我们加载过内核到内存里了，现在可以直接对存储系统进行访问。但是存储系统越来越多了，如果所有市面上的存储系统的驱动都默认放进内核，内核就太大了。这该怎么办呢？ 我们只好先弄一个基于内存的文件系统。内存访问是不需要驱动的，这个就是 ramdisk。这个时候，ramdisk 是根文件系统。然后，我们开始运行 ramdisk 上的 &#x2F;init。等它运行完了就已经在用户态了。&#x2F;init 这个程序会先根据存储系统的类型加载驱动，有了驱动就可以设置真正的根文件系统了。有了真正的根文件系统，ramdisk 上的 &#x2F;init 会启动文件系统上的 init。接下来就是各种系统的初始化。启动系统的服务，启动控制台，用户就可以登录进来了。 6.3 初始化2号进程 rest_init 另一大事情就是创建第三个进程，就是 2 号进程。kernel_thread(kthreadd, NULL, CLONE_FS | CLONE_FILES)又一次使用 kernel_thread 函数创建进程。这里需要指出一点，函数名 thread 可以翻译成“线程”，这也是操作系统很重要的一个概念。从内核态来看，无论是进程，还是线程，我们都可以统称为任务（Task），都使用相同的数据结构，平放在同一个链表中。这里的函数kthreadd，负责所有内核态的线程的调度和管理，是内核态所有线程运行的祖先。 kthreadd，即2号进程，用于内核态线程的管理，是一个守护线程。其源码如下所示，运行流程包括 初始化了task结构，并将该线程设置为允许任意CPU运行。 进入循环，将线程状态设置为TASK_INTERRUPTIBLE，如果当前kthread_create_list为空，没有要创建的线程，则执行schedule()让出CPU资源。 如果需要创建，则设置为TASK_RUNNING状态，加上锁spin_lock，从链表中取得kthread_create_info 结构的地址，在上文中已经完成插入操作(将kthread_create_info结构中的 list 成员加到链表中，此时根据成员 list 的偏移获得 create) 调用create_kthread(create)完成线程的创建 12345678910111213141516171819202122232425262728293031323334353637int kthreadd(void *unused)&#123; struct task_struct *tsk = current; /* Setup a clean context for our children to inherit. */ set_task_comm(tsk, &quot;kthreadd&quot;); ignore_signals(tsk); set_cpus_allowed_ptr(tsk, cpu_all_mask); set_mems_allowed(node_states[N_MEMORY]); current-&gt;flags |= PF_NOFREEZE; cgroup_init_kthreadd(); for (;;) &#123; set_current_state(TASK_INTERRUPTIBLE); if (list_empty(&amp;kthread_create_list)) schedule(); __set_current_state(TASK_RUNNING); spin_lock(&amp;kthread_create_lock); while (!list_empty(&amp;kthread_create_list)) &#123; struct kthread_create_info *create; create = list_entry(kthread_create_list.next, struct kthread_create_info, list); list_del_init(&amp;create-&gt;list); spin_unlock(&amp;kthread_create_lock); create_kthread(create); spin_lock(&amp;kthread_create_lock); &#125; spin_unlock(&amp;kthread_create_lock); &#125; return 0;&#125; 而create_kthread(create)函数做了一件让人意外的事情：调用了kernel_thread()，所以又回到了创建1号进程和2号进程的函数上，这次的回调函数为kthread，该函数才会真正意义上分配内存、初始化一个新的内核线程。 123456789101112131415161718192021static void create_kthread(struct kthread_create_info *create)&#123; int pid;#ifdef CONFIG_NUMA current-&gt;pref_node_fork = create-&gt;node;#endif /* We want our own signal handler (we take no signals by default). */ pid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD); if (pid &lt; 0) &#123; /* If user was SIGKILLed, I release the structure. */ struct completion *done = xchg(&amp;create-&gt;done, NULL); if (!done) &#123; kfree(create); return; &#125; create-&gt;result = ERR_PTR(pid); complete(done); &#125;&#125; 下面是kthread的源码，这里有个很重要的地方：新创建的线程由于执行了 schedule() 调度，此时并没有执行，直到我们使用wake_up_process(p)唤醒新创建的线程。线程被唤醒后, 会接着执行最后一段threadfn(data) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051static int kthread(void *_create)&#123; /* Copy data: it&#x27;s on kthread&#x27;s stack */ struct kthread_create_info *create = _create; int (*threadfn)(void *data) = create-&gt;threadfn; void *data = create-&gt;data; struct completion *done; struct kthread *self; int ret; self = kzalloc(sizeof(*self), GFP_KERNEL); set_kthread_struct(self); /* If user was SIGKILLed, I release the structure. */ done = xchg(&amp;create-&gt;done, NULL); if (!done) &#123; kfree(create); do_exit(-EINTR); &#125; if (!self) &#123; create-&gt;result = ERR_PTR(-ENOMEM); complete(done); do_exit(-ENOMEM); &#125; self-&gt;data = data; init_completion(&amp;self-&gt;exited); init_completion(&amp;self-&gt;parked); current-&gt;vfork_done = &amp;self-&gt;exited; /* OK, tell user we&#x27;re spawned, wait for stop or wakeup */ __set_current_state(TASK_UNINTERRUPTIBLE); create-&gt;result = current; /* * Thread is going to call schedule(), do not preempt it, * or the creator may spend more time in wait_task_inactive(). */ preempt_disable(); complete(done); schedule_preempt_disabled(); preempt_enable(); ret = -EINTR; if (!test_bit(KTHREAD_SHOULD_STOP, &amp;self-&gt;flags)) &#123; cgroup_kthread_ready(); __kthread_parkme(self); ret = threadfn(data); &#125; do_exit(ret);&#125; 由此，我们可以总结一下第2号进程的工作流程： 第2号进程kthreadd进程由第0号进程通过kernel_thread()创建，并始终运行在内核空间, 负责所有内核线程的调度和管理 第2号进程会循环检测kthread_create_list全局链表, 当我们调用kernel_thread创建内核线程时，新线程会被加入到此链表中，因此所有的内核线程都是直接或者间接的以kthreadd为父进程 检测到新线程创建，则调用kernel_thread()创建线程，其回调为kthread kthread在创建完后调用schedule()让出CPU资源，而不是直接运行。等待收到wake_up_process(p)的唤醒后再继续执行threadfn(data)。 因此 任何一个内核线程入口都是 kthread() 通过kthread_create()创建的内核线程不会立刻运行，需要手工 wake up. 通过kthread_create() 创建的内核线程有可能不会执行相应线程函数threadfn而直接退出 回到rest_init()，当完成了1号2号进程的创建后，我们将0号进程真正归位idle进程，结束rest_init()，也正事结束了start_kernel()函数，由此，内核初始化全部完成。 七. 总结 本文介绍了内核初始化的几个重要部分，其实还有很多初始化没有介绍，如cgroup初始化、虚拟文件系统初始化、radix树初始化、rcu初始化、计时器和时间初始化、架构初始化等等，这些会在后面有针对性的单独介绍。","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统学习——内核运行","path":"/2023/01/15/linux-docs/Linux操作系统学习——内核运行/","content":"一. 前言 上文中，我们分析了从按下电源键到BootLoader完成加载的过程。加载完成之后，就要正式启动Linux内核了，而在这之前首先要完成从实模式到保护模式的切换。本文主要分析以下几部分内容 新旧中断的交替 打开A20 进入main函数 内核初始化 其实整个过程中还有很多内容，比如检查各种硬件设备等，在此略过不提。下面就开始潜入Linux源码的海洋畅游啦。 二. 新旧中断的交替 在实模式下的中断显然不可以和保护模式的中断同日而语，因此我们需要关闭旧的中断（cli）并确立新的中断（sti）。main函数能够适应保护模式的中断服务体系被重建完毕才会打开中断，而那时候响应中断的服务程序将不再是BIOS提供的中断服务程序，取而代之的是由系统自身提供的中断服务程序。 cli、sti总是在一个完整操作过程的两头出现，目的是避免中断在此期间的介入。接下来的代码将为操作系统进入保护模式做准备。此处即将进行实模式下中断向量表和保护模式下中断描述符表（IDT）的交接工作。试想，如果没有cli，又恰好发生中断，如用户不小心碰了一下键盘，中断就要切进来，就不得不面对实模式的中断机制已经废除、保护模式的中断机制尚未完成的尴尬局面，结果就是系统崩溃。cli、sti保证了这个过程中，IDT能够完整创建，以避免不可预料中断的进入造成IDT创建不完整或新老中断机制混用。 #boot/setup.s …… do _move： mov es,ax！destination segment add ax，#0x1000 cmp ax，#0x9000 jz end_move mov ds,ax！source segment sub di,di sub si,si mov cx，#0x8000 rep movsw jmp do_move 如上代码主要完成了一项工作：将位于0x10000处的内核程序复制至内存地址起始位置0x00000处。在上一节我们分析了实模式中的存储分布图，在此位置原来存放着由BIOS建立的中断向量表及BIOS数据区。这个复制动作将BIOS中断向量表和BIOS数据区完全覆盖，使它们不复存在。这样做的好处如下： 1、废除BIOS的中断向量表，等同于废除了BIOS提供的实模式下的中断服务程序。2、收回刚刚结束使用寿命的程序所占内存空间。3、让内核代码占据内存物理地址最开始的、天然的、有利的位置。 此时，重要角色要登场了，他们就是中断描述符表IDT和全局描述符表GDT。 GDT（Global Descriptor Table，全局描述符表），在系统中唯一的存放段寄存器内容（段描述符）的数组，配合程序进行保护模式下的段寻址。它在操作系统的进程切换中具有重要意义，可理解为所有进程的总目录表，其中存放每一个任务（task）局部描述符表（LDT, Local Descriptor Table）地址和任务状态段（TSS, Task Structure Segment）地址，完成进程中各段的寻址、现场保护与现场恢复。GDTR是GDT基地址寄存器，当程序通过段寄存器引用一个段描述符时，需要取得GDT的入口，GDTR标识的即为此入口。在操作系统对GDT的初始化完成后，可以用LGDT（Load GDT）指令将GDT基地址加载至GDTR。 IDT（Interrupt Descriptor Table，中断描述符表），保存保护模式下所有中断服务程序的入口地址，类似于实模式下的中断向量表。IDTR（IDT基地址寄存器），保存IDT的起始地址。 32位的中断机制和16位的中断机制，在原理上有比较大的差别。最明显的是16位的中断机制用的是中断向量表，中断向量表的起始位置在0x00000处，这个位置是固定的；32位的中断机制用的是中断描述符表（IDT），位置是不固定的，可以由操作系统的设计者根据设计要求灵活安排，由IDTR来锁定其位置。GDT是保护模式下管理段描述符的数据结构，对操作系统自身的运行以及管理、调度进程有重大意义。 此时此刻内核尚未真正运行起来，还没有进程，所以现在创建的GDT第一项为空，第二项为内核代码段描述符，第三项为内核数据段描述符，其余项皆为空。IDT虽然已经设置，实为一张空表，原因是目前已关中断，无需调用中断服务程序。此处反映的是数据“够用即得”的思想。 创建这两个表的过程可理解为是分两步进行的： 1、在设计内核代码时，已经将两个表写好，并且把需要的数据也写好。此处的数据区域是在内核源代码中设定、编译并直接加载至内存形成的一块数据区域。专用寄存器的指向由程序中的lidt和lgdt指令完成2、将专用寄存器（IDTR、GDTR）指向表。 三. A20 A20启用是一个标志性的动作，由上文提到的lzma_decompress.img 调用 real_to_prot启动。打开A20，意味着CPU可以进行32位寻址，最大寻址空间为4 GB。注意图1-19中内存条范围的变化：从5个F扩展到8个F，即0xFFFFFFFF(4 GB)。 实模式下，当程序寻址超过0xFFFFF时，CPU将“回滚”至内存地址起始处寻址（注意，在只有20根地址线的条件下，0xFFFFF+1&#x3D;0x00000，最高位溢出）。例如，系统的段寄存器（如CS）的最大允许地址为0xFFFF，指令指针（IP）的最大允许段内偏移也为0xFFFF，两者确定的最大绝对地址为0x10FFEF，这将意味着程序中可产生的实模式下的寻址范围比1 MB多出将近64 KB（一些特殊寻址要求的程序就利用了这个特点）。这样，此处对A20地址线的启用相当于关闭CPU在实模式下寻址的“回滚”机制。如下所示为利用此特点来验证A20地址线是否确实已经打开。注意此处代码并不在此时运行，而是在后续head运行过程中为了检测是否处于保护模式中使用。 #boot/head.s …… xorl %eax，%eax 1：incl%eax#check that A20 really IS enabled movl %eax，0x000000#loop forever if it isn&#39;t cmpl %eax，0x100000 je 1b …… A20如果没打开，则计算机处于20位的寻址模式，超过0xFFFFF寻址必然“回滚”。一个特例是0x100000会回滚到0x000000，也就是说，地址0x100000处存储的值必然和地址0x000000处存储的值完全相同。通过在内存0x000000位置写入一个数据，然后比较此处和1 MB（0x100000，注意，已超过实模式寻址范围）处数据是否一致，就可以检验A20地址线是否已打开。 四. 进入main函数 这里涉及到一个硬件知识：在X86体系中，采用的终端控制芯片名为8259A，此芯片，是可以用程序控制的中断控制器。单个的8259A能管理8级向量优先级中断，在不增加其他电路的情况下，最多可以级联成64级的向量优先级中断系统。CPU在保护模式下，int 0x00～int 0x1F被Intel保留作为内部（不可屏蔽）中断和异常中断。如果不对8259A进行重新编程，int 0x00～int 0x1F中断将被覆盖。例如，IRQ0（时钟中断）为8号（int 0x08）中断，但在保护模式下此中断号是Intel保留的“Double Fault”（双重故障）。因此，必须通过8259A编程将原来的IRQ0x00～IRQ0x0F对应的中断号重新分布，即在保护模式下，IRQ0x00～IRQ0x0F的中断号是int 0x20～int 0x2F。 setup程序通过下面代码将CPU工作方式设为保护模式。这里涉及到一个CR0寄存器：0号32位控制寄存器，放系统控制标志。第0位为PE（Protected Mode Enable，保护模式使能）标志，置1时CPU工作在保护模式下，置0时为实模式。将CR0寄存器第0位（PE）置1，即设定处理器工作方式为保护模式。CPU工作方式转变为保护模式，一个重要的特征就是要根据GDT决定后续执行哪里的程序。前文提到GDT初始时已写好了数据，这些将用来完成从setup程序到head程序的跳转。 #boot/setup.s mov ax，#0x0001！protected mode（PE）bit lmsw ax！This is it！ jmpi 0，8！jmp offset 0 of segment 8（cs） head程序是进入main之前的最后一步了。head在空间创建了内核分页机制，即在0x000000的位置创建了页目录表、页表、缓冲区、GDT、IDT，并将head程序已经执行过的代码所占内存空间覆盖。这意味着head程序自己将自己废弃，main函数即将开始执行。具体的分页机制因为较为复杂，所以打算放在后续介绍内存管理的部分再单独介绍。 head构造IDT，使中断机制的整体架构先搭建起来（实际的中断服务程序挂接则在main函数中完成），并使所有中断服务程序指向同一段只显示一行提示信息就返回的服务程序。从编程技术上讲，这种初始化操作，既可以防止无意中覆盖代码或数据而引起的逻辑混乱，也可以对开发过程中的误操作给出及时的提示。IDT有256个表项，实际只使用了几十个，对于误用未使用的中断描述符，这样的提示信息可以提醒开发人员注意错误。 除此之外，head程序要废除已有的GDT，并在内核中的新位置重新创建GDT。原来GDT所在的位置是设计代码时在setup.s里面设置的数据，将来这个setup模块所在的内存位置会在设计缓冲区时被覆盖。如果不改变位置，将来GDT的内容肯定会被缓冲区覆盖掉，从而影响系统的运行。这样一来，将来整个内存中唯一安全的地方就是现在head.s所在的位置了。 下来步骤主要包括 1、初始化段寄存器和堆栈 2、主要包括将DS和ES寄存器指向相同的地址，并将DS和CS设置为相同的值。 3、清零eflag寄存器以及内核未初始化数据区 4、调用decompress_kernel()解压内核映像并跳转至0X00100000处。 5、段寄存器初始化为最终值并填充BSS字段为0 初始化临时内核页表 最终完成了分页机制初始化后，PG（Paging) 标志位将会置1，表示地址映射模式采取分页机制，最终跳转至main函数，内核开始初始化工作。 五. 内核初始化 注意，至此为止，我们尚未打开中断，而必须通过main函数完成一系列的初始化后才会打开新的中断，从而使内核正式运行起来。该部分主要包括： 1、为进程0建立内核态堆栈 2、清零eflags寄存器 3、调用setup_idt()用空的中断处理程序填充IDT 4、把BIOS中获得的参数传递给第一个页框 5、用GDT和IDT表填充寄存器 完成这些之后，内核就正式运行，开始创建0号进程了。 六. 总结 本文介绍了实模式到保护模式的整个切换过程，完成了内核的加载并开始正式准备创建0号进程。后续将继续分析启动内核创建0号、1号、2号进程的整个过程。本文介绍过程中忽略了很多汇编代码以及一些虽然很重要但是不属于基本流程的知识，有兴趣了解的可以根据文中链接、文末的源码和参考资料进行更深入的学习研究。","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统学习——启动","path":"/2023/01/15/linux-docs/Linux操作系统学习——启动/","content":"一. 前言 Linux操作系统内核是服务端学习的根基，也是提高编程能力、源码阅读能力和进阶知识学习能力的重要部分，本文开始将记录Linux操作系统中的各个部分源码学习历程。 关于如何学习源码，个人觉得可以从以下角度入手，有效地提高阅读和学习的效率。（学习语言就不说了，这是基本功。学习IDE推荐Source Insight或者Visual Studio，网站源码阅读推荐woboq） 理解代码的组织结构。 以Linux源码举例，首先你得知道操作系统分为哪几个部分，他们单独做了什么功能，如何进行配合完成更为具体的功能。建立整体的印象有助于后续深入学习的时候方便理解，毕竟代码是用的不是看的，理解他的作用有利于理解为什么要这么做。 深入各个模块学习 模块接口： 这里推荐微软的画图工具visio或者思维导图xmind，用其画图可以将各个模块的接口列出，并绘制各个模块之间的关系，通过了解接口可以清楚各个模块之间的关系，即绘制模块组织图 工作流程：通过上面一步得到各模块间的关系，然后实际用断点或log等方式看一看整体的工作流程，在模块组织图的基础上绘制程序流程图 模块粘合层：我们的代码有很多都是用来粘合代码的，比如中间件（middleware）、Promises 模式、回调（Callback）、代理委托、依赖注入等。这些代码模块间的粘合技术是非常重要的，因为它们会把本来平铺直述的代码给分裂开来，让你不容易看明白它们的关系。这些可以作为程序流程图的补充，让其中本来无法顺畅衔接的地方变得通畅无阻。 模块具体实现 ：这是最难得地方，涉及到大量具体源码的学习。深入细节容易迷失在细节的海洋里，因此需要有一些重点去关注，将非重点的内容省略。通过学习绘制模块具体架构图和模块的算法时序图，可以帮助你更好的掌握源码的精髓。 需要关注的包括 代码逻辑。代码有两种逻辑，一种是业务逻辑，这种逻辑是真正的业务处理逻辑；另一种是控制逻辑，这种逻辑只是用控制程序流转的，不是业务逻辑。比如：flag 之类的控制变量，多线程处理的代码，异步控制的代码，远程通讯的代码，对象序列化反序列化的代码等。这两种逻辑你要分开，很多代码之所以混乱就是把这两种逻辑混在一起了。 重要的算法。一般来说，我们的代码里会有很多重要的算法，我说的并不一定是什么排序或是搜索算法，可能会是一些其它的核心算法，比如一些索引表的算法，全局唯一 ID 的算法、信息推荐的算法、统计算法、通读算法（如 Gossip）等。这些比较核心的算法可能会非常难读，但它们往往是最有技术含量的部分。 底层交互。有一些代码是和底层系统的交互，一般来说是和操作系统或是 JVM 的交互。因此，读这些代码通常需要一定的底层技术知识，不然，很难读懂。 可以忽略的包括 出错处理。根据二八原则，20% 的代码是正常的逻辑，80% 的代码是在处理各种错误，所以，你在读代码的时候，完全可以把处理错误的代码全部删除掉，这样就会留下比较干净和简单的正常逻辑的代码。排除干扰因素，可以更高效地读代码。 数据处理。只要你认真观察，就会发现，我们好多代码就是在那里倒腾数据。比如 DAO、DTO，比如 JSON、XML，这些代码冗长无聊，不是主要逻辑，可以不理。 忽略过多的实现细节。在第一遍阅读源码时，已弄懂整体流程为主，至于具体的实现细节先简单的理清处过一遍，不用过于纠结。当梳理清楚全部的框架逻辑后，第二遍再深入的学习研究各个模块的实现，此时应该解决第一遍中的疑惑。第三遍可以跳出代码的实现，来看Linux的设计思路、编程艺术和演进之路。 重在实践。Linux的代码都是可以调试的，看很多遍也许不如跟着调试走一遍，然后再自己修改修改做一些小测试。 传授知识。当你能将知识讲述给别人听，并让别人听懂时，你已经可以自豪的说洞悉了这些知识。所以不妨从一个小的例子开始自说自话，看能不能自圆其说，甚至写成博客、做成PPT给大家讲解。 说了一大堆的废话，下面就正式开始操作系统的深入学习记录之旅了。 二. 混沌初开 本文分析从按下电源键到加载BIOS以及后续bootloader的整个过程。犹如盘古开天辟地一般，该过程将混沌的操作系统世界分为清晰的内核态和用户态，并经历从实模式到保护模式的变化。这里先简单介绍一下名词，便于后续理解。 实模式（Real Mode)：又名 Real Address Mode，在此模式下地址访问的是真实地内存地址所在位置。在此模式下，可以使用20位（1MB）的地址空间，软件可以不受限制的操作所有地址的空间和IO设备。 保护模式（Protected Mode)：又名 Protected Virtual Address Mode，采用虚拟内存、页等机制对内存进行了保护，比起实模式更为安全可靠，同时也增加了灵活性和扩展性。 2.1 从启动电源到BIOS 当我们按下电源键，主板会发向电源组发出信号，接收到信号后，电源会提供合适的电压给计算机。当主板收到电源正常启动的信号后，主板会启动CPU。CPU重置所有寄存器数据，并设置初始化数据，这个初始化数据在X86架构里如下所示： IP 0xfff0 CS selector 0xf000 CS base 0xffff0000 IP/EIP (Instruction Pointer) : 指令指针寄存器，记录将要执行的指令在代码段内的偏移地址 CS（Code Segment Register）：代码段寄存器，指向CPU当前执行代码在内存中的区域（定义了存放代码的存储器的起始地址） 实模式采取内存段来管理 0 - 0xFFFFF的这1M内存空间，但是由于只有16位寄存器，所以最大地址只能表示为0xFFFFF（64KB)，因此不得不采取将内存按段划分为64KB的方式来充分利用1M空间。也就是上所示的，采取段选择子 + 偏移量的表示法。这种方法在保护模式中对于页的设计上也沿用了下来，可谓祖传的智慧了。具体的计算公式如下所示： PhysicalAddress = Segment Selector * 16 + Offset 该部分由硬件完成，通过计算访问0XFFFF0，如果该位置没有可执行代码则计算机无法启动。如果有，则执行该部分代码，这里也就是我们故事的开始，BIOS程序了。 2.2 BIOS到BootLoader BIOS执行程序存储在ROM中，起始位置为0XFFFF0，当CS:IP指向该位置时，BIOS开始执行。BIOS主要包括以下内存映射： 0x00000000 - 0x000003FF - Real Mode Interrupt Vector Table 0x00000400 - 0x000004FF - BIOS Data Area 0x00000500 - 0x00007BFF - Unused 0x00007C00 - 0x00007DFF - Our Bootloader 0x00007E00 - 0x0009FFFF - Unused 0x000A0000 - 0x000BFFFF - Video RAM (VRAM) Memory 0x000B0000 - 0x000B7777 - Monochrome Video Memory 0x000B8000 - 0x000BFFFF - Color Video Memory 0x000C0000 - 0x000C7FFF - Video ROM BIOS 0x000C8000 - 0x000EFFFF - BIOS Shadow Area 0x000F0000 - 0x000FFFFF - System BIOS 其中最重要的莫过于中断向量表和中断服务程序。BIOS程序在内存最开始的位置（0x00000）用1 KB的内存空间（0x00000～0x003FF）构建中断向量表，在紧挨着它的位置用256字节的内存空间构建BIOS数据区（0x00400～0x004FF），并在大约57 KB以后的位置（0x0E05B）加载了8 KB左右的与中断向量表相应的若干中断服务程序。中断向量表中有256个中断向量，每个中断向量占4字节，其中两个字节是CS的值，两个字节是IP的值。每个中断向量都指向一个具体的中断服务程序。 BIOS程序会选择一个启动设备，并将控制权转交给启动扇区中的代码。主要工作即使用中断向量和中断服务程序完成BootLoader的加载，最终将boot.img加载至0X7C00的位置启动。Linux内核通过Boot Protocol定义如何实现该引导程序，有如GRUB 2和syslinux等具体实现方式，这里仅介绍GRUB2。 2.3 BootLoader的工作 boot.img由boot.S编译而成，512字节，安装在启动盘的第一个扇区，即MBR。由于空间有限，其代码十分简单，仅仅是起到一个引导的作用，指向后续的核心镜像文件，即core.img。core.img包括很多重要的部分，如lzma_decompress.img、diskboot.img、kernel.img等，结构如下图。 整个加载流程如下： 1、boot.img加载core.img的第一个扇区，即diskboot.img，对应代码为diskboot.S2、diskboot.img加载core.img的其他部分模块，先是解压缩程序 lzma_decompress.img，再往下是 kernel.img，最后是各个模块 module 对应的映像。这里需要注意，它不是 Linux 的内核，而是 grub 的内核。注意，lzma_decompress.img 对应的代码是 startup_raw.S，本来 kernel.img 是压缩过的，现在执行的时候，需要解压缩。3、加载完core之后，启动grub_main函数。4、grub_main函数初始化控制台，计算模块基地址，设置 root 设备，读取 grub 配置文件，加载模块。最后，将 GRUB 置于 normal 模式，在这个模式中，grub_normal_execute (from grub-core&#x2F;normal&#x2F;main.c) 将被调用以完成最后的准备工作，然后显示一个菜单列出所用可用的操作系统。当某个操作系统被选择之后，grub_menu_execute_entry 开始执行，它将调用 GRUB 的 boot 命令，来引导被选中的操作系统。 在这之前，我们所有遇到过的程序都非常非常小，完全可以在实模式下运行，但是随着我们加载的东西越来越大，实模式这 1M 的地址空间实在放不下了，所以在真正的解压缩之前，lzma_decompress.img 做了一个重要的决定，就是调用 real_to_prot，切换到保护模式，这样就能在更大的寻址空间里面，加载更多的东西。 开机时的16位实模式与内核启动的main函数执行需要的32位保护模式之间有很大的差距，这个差距谁来填补？head.S做的就是这项工作。就像 kernel boot protocol 所描述的，引导程序必须填充 kernel setup header （位于 kernel setup code 偏移 0x01f1 处） 的必要字段，这些均在head.S中定义。在这期间，head程序打开A20，打开pe、pg，废弃旧的、16位的中断响应机制，建立新的32位的IDT……这些工作都做完了，计算机已经处在32位的保护模式状态了，调用32位内核的一切条件已经准备完毕，这时顺理成章地调用main函数。后面的操作就可以用32位编译的main函数完成，从而正式启动内核，进入波澜壮阔的Linux内核操作系统之中。 三. 总结 本文介绍了从按下电源开关至加载完毕BootLoader的整个过程，后续将继续分析从实模式进入保护模式，从而启动内核创建0号、1号、2号进程的整个过程。本文介绍过程中忽略了很多汇编代码以及一些虽然很重要但是不属于基本流程的知识，有兴趣了解的可以根据文中链接、文末的源码和参考资料进行更深入的学习研究。","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统段式存储管理、 段页式存储管理","path":"/2023/01/15/linux-docs/Linux操作系统段式存储管理、 段页式存储管理/","content":"1、段式存储管理1.1分段 进程的地址空间：按照程序自身的逻辑关系划分为若干个段，每个段都有一个段名（在低级语言中，程序员使用段名来编程），每段从0开始编址。 内存分配规则：以段为单位进行分配，每个段在内存中占连续空间，但各段之间可以不相邻。 分段系统的逻辑地址结构由段号（段名）和段内地址（段内偏移量）所组成。 1.2段表 每一个程序设置一个段表，放在内存,属于进程的现场信息 1.3地址变换 1.4段的保护 越界中断处理 1.进程在执行过程中，有时需要扩大分段，如数据段。由于要访问的地址超出原有的段长，所以发越界中断。操作系统处理中断时 ，首先判断该段的“扩充位”，如可扩充，则增加段的长度；否则按出错处理 缺段中断处理 检查内存中是否有足够的空闲空间①若有，则装入该段，修改有关数据结构，中断返回②若没有，检查内存中空闲区的总和是否满足要求，是则应采用紧缩技术，转 ① ；否则，淘汰一（些）段，转① 1.5段的动态连接 为何要进行段的动态链接？ 大型程序由若干程序段，若干数据段组成 进程的某些程序段在进程运行期间可能根本不用 互斥执行的程序段没有必要同时驻留内存 有些程序段执行一次后不再用到 静态链接花费时间，浪费空间 在一个程序运行开始时，只将主程序段装配好并调入主存。其它各段的装配是在主程序段运行过程中逐步进行的。每当需要调用一个新段时，再将这个新段装配好，并与主程序段连接。页式存储管理：难以完成动态链接，其逻辑地址是一维的 1.6信息的保护与共享 这里主要与页式存储管理进行一下对比。 分段比分页更容易实现信息的共享和保护。 纯代码举例：比如，有一个代码段只是简单的输出“Hello World!”。 1.7页式系统与段式系统的对比 补充： 段长是可变的，页的大小是固定的。 分段存储：段内地址W字段溢出将产生越界中断。 分页存储：段内地址W字段溢出会自动加入到页号中。 1.8总结 2、段页式存储管理2.1分页、分段的有缺点分析 2.2基本思想 用户程序划分：按段式划分（对用户来讲，按段的逻辑关系进行划分；对系统讲，按页划分每一段） 逻辑地址： 内存划分：按页式存储管理方案 内存分配：以页为单位进行分配 2.3逻辑地址结构 2.4段表页表 2.5地址转换 2.6评价 优点： 保留了分段和请求分页存储管理的全部优点 提供了虚存空间，能更有效利用主存 缺点： 增加了硬件成本 系统复杂度较大 2.7总结 版权声明：本文为知乎博主「Linux内核库」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文 出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/466602063","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统汇编指令入门级整理知识点","path":"/2023/01/15/linux-docs/Linux操作系统汇编指令入门级整理知识点/","content":"前言我们大都是被高级语言惯坏了的一代，源源不断的新特性正在逐步添加到各类高级语言之中，汇编作为最接近机器指令的低级语言，已经很少被直接拿来写程序了，不过我还真的遇到了一个，那是之前的一个同事，因为在写代码时遇到了成员函数权限及可见性的问题，导致他无法正确调用想执行的函数，结果他就开始在 C++ 代码里嵌入汇编了，绕过了种种限制终于如愿以偿，但是读代码的时候我们傻眼了… 因为项目是跨平台的，代码推送的 Linux 编译的时候他才发现，汇编代码的语法在 Linux 和 Windows 上居然是不一样的，结果他又用一个判断平台的宏定义“完美”地解决了，最终这些代码肯定是重写了啊，因为可读性太差了，最近在学习左值、右值、左引用和右引用的时候，总是有人用程序编译生成的中间汇编代码来解释问题，看得我迷迷糊糊，所以决定熟悉一下简单的汇编指令，边学习边记录，方便今后忘记了可以直接拿来复习。 什么是汇编语言汇编语言是最接近机器语言的编程语言，引用百科中的一段话解释为： 汇编语言（assembly language）是一种用于电子计算机、微处理器、微控制器或其他可编程器件的低级语言，亦称为符号语言。在汇编语言中，用助记符代替机器指令的操作码，用地址符号或标号代替指令或操作数的地址。汇编语言又被称为第二代计算机语言。 汇编语言产生的原因对于绝大多数人来说，二进制程序是不可读的，当然有人可以读，比如第一代程序员，但这类人快灭绝了，直接看二进制不容易看出来究竟做了什么事情，比如最简单的加法指令二进制表示为 00000011，如果它混在一大串01字符串中就很难把它找出来，所以汇编语言主要就是为了解决二进制编码的可读性问题。 汇编与二进制的关系换句话来说，汇编语言就是把给机器看的二进制编码翻译成人话，汇编指令是机器指令的助记符，与机器指令是一一对应的关系，是一种便于阅读和记忆的书写格式。有效地解决了机器指令编写程序难度大的问题，并且使用编译器，可以很方便地把汇编程序转译成机器指令程序，比如之前提到的 00000011 加法指令，对应的汇编指令是 ADD，在调用汇编器时就会把 ADD 翻译成 00000011。 寄存器说到汇编指令不得不提到寄存器，寄存器本身是用来存数据的，因为 CPU 本身只负责逻辑运算，数据需要单独储存在其他的地方，但是对于不熟悉寄存器的人来说会有疑惑，数据不是存在硬盘上吗？或者说数据不是存在内存中吗？这些想法都没错，那么寄存器是用来做什么的呢？ 寄存器作用其实硬盘、内存都是用来存储数据的，但是 CPU 的运算速度远高于内存的读写速度，更不用说从硬盘上取数据了，所以为了避免被拖慢速度影响效率，CPU 都自带一级缓存和二级缓存，一些 CPU 甚至增加了三级缓存，从这些缓存中读写数据要比内存快很多，但是还是无法使用飞速运转的 CPU，所以才会有寄存器的存在。 寄存器不是后来增加的，在最初的计算中就已经设计出来，相比而言，多级缓存出现得更晚一些，通常那些最频繁读写的数据都会被放在寄存器里面，CPU 优先读写寄存器，再通过寄存器、缓存跟内存来交换数据，达到缓冲的目的，因为可以通过名称访问寄存器，这样访问速度是最快的，因此也被称为零级缓存。 存取速度比较通过上面的叙述我们可以知道存取速度从高到低分别是: 寄存器 &gt; 1级缓存 &gt; 2级缓存 &gt; 3级缓存 &gt; 内存 &gt; 硬盘，关于它们的存取速度，举个例子很容易就能明白了，比如我们做菜（CPU工作）时，取手中（寄存器）正拿着的肉和蔬菜肯定是最快的，如果没有就需要把案板上（1级缓存）处理好的菜拿过来，如果案板上没有就在更远一点的洗菜池（2级缓存）中找一找，还没找到的话就要到冰箱（3级缓存）中看一看了，这时发现家里真没有，那去楼下的菜店（内存）去买点吧，转了一圈发现没有想要的，最后还是开车去农贸市场（硬盘）买吧。 通过上面这个例子应该能明白它们的速度关系了，既然缓存这么快，为什么不用缓存代替内存，或者将2、3级缓存都换成1级缓存呢？这里边有一个成本问题，速度越快对应着价格越高，如果你买过机械硬盘和固态硬盘应该很容易就理解了。 寄存器分类 常用的 x86 CPU 寄存器有8个：EAX 、EBX、ECX、EDX、EDI、ESI、EBP、ESP，据说现在寄存器总数已经超过100个了，等我找到相关资料再来补充，上面这几个寄存器是最常用的，这些名字也常常出现在汇编的代码中。 我们常说的32位、64位 CPU 是指数据总线的宽度或根数，而寄存器是暂存数据和中间结果的单元，因此寄存器的位数也就是处理数据的长度与数据总线的根数是相同的，所以32位 CPU 对应的寄存器也应该是32位的。 常用寄存器用途 上面提到大8个寄存器都有其特定的用途，我们以32位 CPU 为例简单说明下这些寄存器的作用，整理如下表： 寄存器EAX、AX、AH、AL的关系在上面的图标中每个常用寄存器后面还有其他的名字，它们是同一个寄存器不同用法下的不同名字，比如在32位 CPU 上，EAX是32位的寄存器，而AX是EAX的低16位，AH是AX的高8位，而AL是AX的低8位，它们的对照关系如下: 1234500000000 00000000 00000000 00000000|===============EAX===============|---4个字节 |======AX=======|---2个字节 |==AH===|-----------1个字节 |===AL==|---1个字节 汇编语言指令终于说到汇编常用指令了，因为 linux 和 windows 下面的汇编语法是有些不同的，所以下面我们先通过 windows 下的汇编指令来简单学习一下，后续再来比较两者的不同。 数据传送指令 算术运算指令 逻辑运算指令 循环控制指令 循环控制指令 linux 和 windows 下汇编的区别 前面说到 linux 和 windows 下面的汇编语法是不同的，其实两种语法的不同和系统不同没有绝对的关系，一般在 linux 上会使用 gcc&#x2F;g++ 编译器，而在 windows 上会使用微软的 cl 也就是 MSBUILD，所以产生不同的代码是因为编译器不同，gcc 下采用的是AT&amp;T的汇编语法格式，MSBUILD 采用的是Intel汇编语法格式。 总结 汇编指令是机器指令的助记符，与机器指令是一一对应的 AT&amp;T的汇编语法格式和Intel汇编语法格式的是不同的 常用寄存器：EAX 、EBX、ECX、EDX、EDI、ESI、EBP、ESP 存取速度从高到低分别是: 寄存器 &gt; 1级缓存 &gt; 2级缓存 &gt; 3级缓存 &gt; 内存 &gt; 硬盘 常用的汇编指令：mov、je、jmp、call、add、sub、inc、dec、and、or 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/449787476","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统汇编语言基础知识(图文代码)","path":"/2023/01/15/linux-docs/Linux操作系统汇编语言基础知识(图文代码)/","content":"1、什么是汇编语言，它在计算机语言中的地位？1汇编语言是程序设计语言的基础语言，是唯一可以直接与计算机硬件打交道的语言 2、汇编语言与源程序、汇编程序、汇编的关系？ 3、汇编语言的特点 \\1) 汇编语言与机器指令一一对应，可充分理解计算机的操作过程汇编语言指令是机器指令的符号表示 \\2) 汇编语言是靠近机器的语言编程时要求熟悉机器硬件系统，可充分利用机器硬件中的全部功能，发挥机器的特点在计算机系统中，某些功能由汇编语言程序实现：实时过程控制系统、系统初始化、实际的输入输出设备操作 \\3) 汇编语言程序的效率高于高级语言效率，指的是用汇编语言编写的源程序在汇编后所得的目标程序效率高时间域的高效率：运行速度快；空间域的高效率：目标代码占用存储空间少 4、汇编语言与高级语言的比较 5、进制转换1（略） 6、数据组织单位 \\1) 位（bit）是计算机中表示信息的最小单位，符号b，是一个二进制位，每一位用0或1表示\\2) 字节（Byte）8位二进制数为一个字节\\3) 字（Word）若干个字节为一个字，一般一个字包含两个字节范围0000HFFFFH\\4) 双字（Double Word）两个字节为一个字，四个字节为连个字，称为双字范围00000000HFFFFFFFFH\\5) 字长机器字的长度为字长，即计算机中每个字所包含的位数，由机器数据总线数决定例如，数据总线数为64位，机器字长为64位，即每个字有8个字节\\6) 数据字与指令字数据字：在存储单元中存储的是数据指令字：在存储单元中存储的是指令无论是数据字还是指令字，在存储单元中都是以二进制的形式存放的 7、BCD码 两种存储方式：组合型（1个字节表示2个BCD码）；非组合型（1个字节表示1个BCD码） 8、80X86计算机组织结构 微型计算机的硬件系统主要由3个主要部分组成： 1)中央处理器CPU（运算器、控制器、寄存器） 2)输入输出设备 3)存储器 9、80X86 CPU的寄存器 寄存器分为3类： 1)通用寄存器 2)控制寄存器 3)段寄存器 8个8位通用寄存器：AL,AH,BL,BH,CL,CH,DL,DH 8个16位通用寄存器：AX,BX,CX,DX,SI,DI,BP,SP 8个32位通用寄存器：EAX,EBX,ECX,EDX,ESI,EDI,EBP,ESP 说明：1）指针寄存器（SP,ESP,BP,EBP）SP,ESP为堆栈指针寄存器，存放当前堆栈段栈顶的偏移地址，是根据指令自动移动的，要想随机读取 堆栈段中的数据，必须通过BP或EBP基址指针寄存器来读取。2）控制寄存器（IP,EIP,FLAGS,EFLAGS）IP,EIP为指令指针寄存器，用于存放当前正在执行的指令的下一条指令的偏移地址，该寄存器所指的为代码段的偏移地址。FLAGS为标识寄存器，表示程序运行时的状态和一些特殊控制 3）段寄存器 代码和数据是分开存放，代码存放在代码段，数据存放在数据段 10、内存组织结构 1）内存的地址在存储器中内存单元的基本单位是字节，每个字节都有一个唯一的地址 2）存储单元的内容 一个存储单元存放的信息为存储单元的内容 分为：字节单元、字节单元、双字单元 双字：需要两个16位寄存器，通常为DX:AX,DX高位，AX低位 12345673)堆栈堆栈是内存中一块特定的区域，其中数据按照*先进后出*原则作用：暂存数据、子程序调用与返回、调用中断处理程序、从中断处理程序返回位置：堆栈段地址存放于SS寄存器中，偏移地址存放在堆栈指针寄存器（SP(16位)/ESP(32位)），他们永远指向栈顶\t初始化：堆栈的初始化时通过设置SS及SP/ESP值来完成的，可以由编译系统自动完成，也可以在程序中通过伪指令显示地定义 11、实模式 123456789101112131415161718192021222324252627282930313233343536371)介绍 只有8086/8088工作在实模式下；\t80286以上的微处理器工作在实模式和保护模式下；\t在实模式下微处理器只能寻址1MB的存储空间；\t80286以上系统的微处理器在加点或复位时都以实模式方式开始工作2）内存地址的分段\t*为什么要分段？*\t8086/8088地址总线为20根，可访问的地址为：2^20=1048576=1M\t8086/8088内部寄存器都是16位的，可以直接处理16位长度的存储地址，16位地址的寻址2^16=64K\t为了把寻址范围扩大到1MB，实模式存储器地址均采用存储空间的分段技术来解决寻址1MB的存储空间\t提出了段地址和偏移地址合成20位物理地址的概念 *分段方法？*\t16位段地址+16位段内地址---&gt;20位物理地址\t地址的组合：物理地址=段地址*16D(或10H)+偏移地址，（段地址*16D--二进制段地址左移4位）\t存放段地址：16位段地址寄存器（CS、DS、SS、ES）\t存放偏移地址：16位指针寄存器（IP、SP）\t在1MB存储器中可以有64K个段，每个段最多64KB，最小为16KB *物理地址、段地址、段内地址、逻辑地址的区别？*\t物理地址：与内存单元一一对应的20位二进制数,1MB=00000H~FFFFFH 每个物理地址代表一个唯一的内存单元 段地址：将1MB的内存空间分为长64KB的程序区和数据区称为段 每个段用1个16位二进制地址表示 段地址存放在段寄存器中 代码段：用于存放源程序的二进制程序代码，该段的段地址放在CS中 数据段：存放操作数据的，该段的段地址放在DS中 堆栈段：堆栈用的存储区，该段的段地址放在SS中 附加段：该段的段地址放在ES中 段内地址：16位二进制段内地址为偏移地址 （偏移地址）不同段内的偏移地址存放在不同的寄存器中，段寄存器与装偏移地址的寄存器按一定要求组合 1234567逻辑地址：用段地址和偏移地址来表示内存单元的地址为逻辑地址，例如，段地址：偏移地址\t*逻辑地址与物理地址的换算关系？*\t物理地址 = 段地址*16D（10H）+偏移地址\t逻辑地址 = 段地址：偏移地址\t例子：逻辑地址，1111H:2222H物理地址，1111H*10H+2222H = 13332H假设1111H为代码段地址，2222H在指针寄存器IP中，示意图如下： 内存分配方法？ 代码段、数据段、堆栈段的大小，是以节为最小单位分配内存区域的16字节&#x3D;2个字&#x3D;1节，节的边界地址就是能够被16整除的地址偏移地址（段内地址）是从0000H开始的例子：假设程序分配的内存区从6100H开始，程序长度1020字节，操作数510字节，堆栈段250字节则代码段长度为1024D&#x3D;400H，数据段长度为512D&#x3D;200H，堆栈段长度为256D&#x3D;100H 示意图如下： 段与段之间的关系？ 8088&#x2F;8086 CPU把1MB的存储空间划分成若干逻辑段每个段的起始地址必须是能够被16整除的数逻辑段的最大长度为64KB 1MB的存储空间最多可以分成64K个逻辑段，当每个逻辑段为16KB时段与段之间可以相邻、分离、重叠、部分重叠 12、保护模式 123456789101112131415161）保护模式存储器寻址机制\t在保护模式下，逻辑地址=选择符+偏移地址\t与实模式不同，实模式的段寄存器存放段基地址，而保护模式的段寄存器存放选择符\t保护模式下，通过选择描述符表中的描述符，间接地形成段基地址\t保护模式的偏移地址最大可以是32位，最大段长可以从16KB扩展到4GB2)描述符\t描述符包括，段在寄存器中的位置，段的长度，访问权限\t由基地址、段界限、访问权限、附加字段组成 基地址：指定段的起始地址 段界限：存放该段的最大偏移地址 访问权限：说明该段在系统中的功能和一些控制信息 附加字段：描述该段的一些属性\t描述符的内容是由系统自动设置的\t由于段寄存器是16位的，描述符是64位的\t故将64位的段描述符放按顺序存放形成一个段描述符表，放在内存中\t而在段寄存器中实际存放的是要选择的段描述符表的序号，类似于数组中的下标 13、存储器管理机制 1234567891011121314151617181）分段管理机制 ①虚拟存储器：在有限的物理存储器上获取更大的使用空间 *虚拟存储器是如何实现存储的？* 在程序执行期间的任意时刻，虚拟存储器系统自动吧程序分成许多小块即程序段 将某个程序段存放到物理存储器中，其他程序段放在磁盘中 当程序要访问到哪个程序段时，就把哪个程序段引导到物理存储器中 ②分段管理：将4GB的存储空间分成若干独立的受保护的存储空间块 每个应用程序可以使用这些存储空间块 2）分页管理机制①线性地址空间：每个进程都有相同大小的4GB线性空间用分段管理机制实现虚拟地址空间到线性地址空间的映射，实现把二维的虚拟地址转换为一维的线性地址②分页存储管理：把线性地址空间和物理地址空间分别划分为大小相同的块，每块长为4KB这样的块称为页，通过分页管理机制实现线性地址空间到物理地址空间的映射，实现线性地址到物理地址的转换 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/449157752","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统进程同步的几种方式及基本原理","path":"/2023/01/15/linux-docs/Linux操作系统进程同步的几种方式及基本原理/","content":"1，进程同步的几种方式1.1信号量用于进程间传递信号的一个整数值。在信号量上只有三种操作可以进行：初始化，P操作和V操作，这三种操作都是原子操作。 P操作(递减操作)可以用于阻塞一个进程，V操作(增加操作)可以用于解除阻塞一个进程。 基本原理是两个或多个进程可以通过简单的信号进行合作，一个进程可以被迫在某一位置停止，直到它接收到一个特定的信号。该信号即为信号量s。 为通过信号量s传送信号，进程可执行原语semSignal(s);为通过信号量s接收信号，进程可执行原语semWait(s);如果相应的信号仍然没有发送，则进程会被阻塞，直到发送完为止。 可把信号量视为一个具有整数值的变量，在它之上定义三个操作： 一个信号量可以初始化为非负数 semWait操作使信号量s减1.若值为负数，则执行semWait的进程被阻塞。否则进程继续执行。 semSignal操作使信号量加1，若值大于或等于零，则被semWait操作阻塞的进程被解除阻塞。 1.2管程管程是由一个或多个过程、一个初始化序列和局部数据组成的软件模块，其主要特点如下： 局部数据变量只能被管程的过程访问，任何外部过程都不能访问。 一个进程通过调用管程的一个过程进入管程。 在任何时候，只能有一个进程在管程中执行，调用管程的任何其他进程都被阻塞，以等待管程可用。 管程通过使用条件变量提供对同步的支持，这些条件变量包含在管程中，并且只有在管程中才能被访问。有两个函数可以操作条件变量： cwait(c)：调用进程的执行在条件c上阻塞，管程现在可被另一个进程使用。 csignal(c)：恢复执行在cwait之后因为某些条件而阻塞的进程。如果有多个这样的进程，选择其中一个；如果没有这样的进程，什么以不做。 1.3消息传递消息传递的实际功能以一对原语的形式提供： send(destination,message) receive(source,message) 这是进程间进程消息传递所需要的最小操作集。 一个进程以消息的形式给另一个指定的目标进程发送消息； 进程通过执行receive原语接收消息，receive原语中指明发送消息的源进程和消息。 2、进程互斥由于进程具有独立性和异步性等并发特征，计算机的资源有限，导致了进程之间的资源竞争和共享，也导致了对进程执行过程的制约。 1.临界区相关概念：临界资源：也就是一次只允许一个进程操作使用的计算机资源，这里的资源可以是物理资源，也可以是逻辑上的变量等。临界区：把不允许多个并发进程交叉执行的一段程序称为临界区（critical region）或临界部分（critical section）。当一个进程使用该临界资源时，其他需要访问该资源的进程必须阻塞，直到占用者释放该资源。 2、间接制约把这种由于共享某一公有资源而引起的在临界区内不允许并发进程交叉执行的现象，称为由共享公有资源而造成的对并发进程执行速度的间接制约。这里的“间接”二字主要是指各种并发进程的速度受公有资源的制约，而非进程之间的直接制约。 3.进程互斥在并发进程中，一个或多个进程要对公用资源进行访问时，必须确保该资源处于空闲状态，也就是说，在并发进程中，临界区只允许一个进程进入，而其他进程阻塞，等待该共享临界资源释放。 4.并发进程之间必须满足以下特征： 平等竞争：即各并发进程享有平等地、独立地竞争共有资源的权利，且在不采取任何措施的条件下，在临界区内任意指令结束时，其他并发进程可以进入临界区。 互斥使用：当并发进程中的时候 多个进程同时申请进入临界区时，它只允许一个进程进入临界区。 不可独占：当进程不在临界区后，它不能阻止其他进程进入临界区。 有限等待：也就是在就绪队列中的进程等待资源时间必须是有限的。并发进程中的某个进程从申请进入临界区时开始，应在有限时间内得以进入临界区。 3、互斥的实现2.1互斥的加锁实现对互斥的临界区进行加锁处理，即当一个进程进入了 临界区之后，对此临界区进行加锁，直到该进程退出临界区为止。而其他并发进程在申请进入临界区之前，必须测试该临界区是否加锁，如果是，则阻塞等待！加锁实现是系统的原语：lock(key[S])和Unlock(key([S]))均保持原子操作。系统实现时锁定位key[S]总是设置在公有资源所对应的数据结构中的。 2.2互斥加锁实现的缺点1.在进行锁测试和定位中将耗费CPU资源2、进程加锁实现可能会对进程不公平，例如： 123456789101112进程A:lock(key[S])&lt;S&gt;unlock(key[S])Goto A进程B：lock(key[S])&lt;S&gt;unlock(key[S])Goto B 如上面所示，进程A和B之间的一个进程运行到Goto之后，会使得另一个进程无法得到处理机资源运行。而处于永久饥饿状态（starvation）。 分析可以知道，一个进程能否进入临界区取决于进程自己调用lock过程去测试相应的锁定位。也就是说，每个进程能否进入临界区是依靠进程自己的测试判断。这样，没有获得执行机会的进程当然无法判断，从而出现不公平现象。 那么是否有办法解决这个问题呢？当然，很明显，办法是有的，我们可以为临界区设置一个管理员，由这个管理员来管理相应临界区的公有资源，它代表可用资源的实体，这个管理员就是信号量。 2.3信号量和P、V操作信号量和P、V原语是荷兰科学家E. W. Dijkstra提出来的。**P原语：**P是荷兰语Proberen（**测试）的首字母。为阻塞原因，负责把当前进程由运行状态转换为阻塞状态，直到另外一个进程唤醒它。操作方法：申请一个空闲资源（把信号量减1），若成功，则退出；若失败，则该进程会被阻塞； V原语：V是荷兰语Verhogen（增加）的首字母。为唤醒原语，负责把一个被阻塞的进程唤醒，它有一个参数表，存放着等待被唤醒的进程信息。操作为：释放一个被占用的资源（把信号量加1），如果发现有被阻塞的进程，则选择一个唤醒之。 【信号量semaphore】 在操作系统中，信号量sem是一个整数。 sem &gt;&#x3D; 0时，代表可供并发进程使用的资源实体数； sem &lt; 0时，表示正在等待使用临界区的进程数。 显然，用于互斥的信号量sem的初值应该大于0，而建立一个信号量必须说明所建信号量代表的意义，赋初值，以及建立相应的数据结构，以便指向那些等待使用该临界区的进程。sem初值为1。 【P、V原语】信号量的数值仅能由P、V原语操作改变。采用P、V原语，可以把类名为S的临界区描述为：When S do P(sem) 临界区 V(sem) od。 一次P原语操作使信号量sem减1 一次V原语操作使信号量sem加1 P原语操作： sem减1；若sem减1后仍大于或等于0，则P原语返回，该进程继续执行；若sem减1后小于0，则该进程被阻塞后进入与该信号相对应的队列中，然后转进程调度。 V原语操作： sem加1；若相加结果大于0，V原语停止执行，该进程返回调用处，继续执行；若相加结果小于或等于0，则从该信号的等待队列中唤醒一个等待进程，然后再返回原进程继续执行或转进程调度。 这里给出一个使用加锁法的软件实现方法来实现P、V原语： 1234567891011121314151617181920212223242526P(sem): begin 封锁中断; lock(lockbit) val[sem]=val[sem]-1 if val[sem]&lt;0 保护当前进程CPU现场 当前进程状态置为“等待” 将当前进程插入信号sem等待队列 转进程调度 fi unlock(lockbit);开放中断 endV(sem): begin 封锁中断; lock(lockbit) val[sem]=val[sem]+1 if val[sem]&lt;=0 local k 从sem等待队列中选取一个等待进程，将其指针置入k中 将k插入就绪队列 进程状态置位“就绪” fi unlock(lockbit);开放中断 end 2.3用P、V原语实现进程互斥设信号量sem是用于互斥的信号量，且其初始值为1表示没有并发进程使用该临界区。显然，由前面论述可知，只要把临界区置于P(sem)和V(sem)之间，即可实现进程之间的互斥。 用信号量实现两个并发进程PA和PB互斥的描述如下：（1）设sem为互斥信号量，其取值范围为（1，0，-1）。其中sem&#x3D;1表示进程PA和PB都未进入类名为S的临界区，sem&#x3D;0表示进程PA或PB已进入类名为S的临界区，sem&#x3D;-1表示进程PA和PB中，一个进程已进入临界区，而另一个进程等待进入该临界区。（2）实现过程： 1234567891011121314Pa: P(sem) &lt;S&gt; V(sem) . . .Pb: P(sem) &lt;S&gt; V(sem) . . . 4，进程互斥的软件实现方法：1，单标志法 1）在进入区只检查，不上锁 2）在退出区把临界资源的使用权交给另一个进程 3）主要问题：不遵循空闲让进的原则 2，双标志先检查 1）在进入区先检查后上锁，退出区解锁 2）主要问题：不遵循原则等待原则 3，双标志后检查 1）在进入区先 上锁后检查，退出区解锁 2）主要问题：不遵循空闲让进，有限等待原则，可能导致饥饿 4，Peterson算法 1）在进入区主动争取——》主动谦让——》检查对方是否想进，己方是否谦让 2）主要问题：不遵循让则等待原则，会发送忙等 5、进程同步【进程间的直接制约】：一组在异步环境下的并发进程，各自的执行结果互为对方的执行条件，从而限制各进程的执行速度的过程称为并发进程间的直接制约。这里的异步环境主要是指各并发进程的执行起始时间的随机性和执行速度的独立性。 【进程间的同步】：把异步环境下的一组并发进程因直接制约而互相发送消息而进行互相合作、互相等待，使得各进程按一定的速度执行的过程称为进程间的同步。具有同步关系的一组并发进程称为合作进程，合作进程间相互发送的信号称为消息或事件。用消息实现进程同步： 用wait(消息名)表示进程等待合作进程发来的消息。用signal(消息名)表示向合作进程发送消息。过程wait的功能是等待到消息名为true的进程继续执行，而signal的功能则是向合作进程发送合作进程所需要的消息名，并将其值置为true。 进程互斥和进程同步】：进程同步不同于进程互斥，进程互斥时它们的执行顺序可以是任意的。一般来说，也可以把个进程之间发送的消息作为信号量看待。与进程互斥时不同的是，这里的信号量只与制约进程及被制约进程有关，而不是与整租并发进程有关。因此，称该信号量为私用信号量（private semaphore）。一个进程Pi的私用信号量semi是从制约进程发送来的进程Pi的执行条件所需要的信息。与私用信号量相对应，称互斥时使用的信号量为公用信号量。 【用P、V原语实现进程同步】：首先为各并发进程设置私用信号量，然后为私用信号量赋初值，最后利用P、V原语和私用信号量规定各进程的执行顺序。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/447720544","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统进程的状态和转换(五态模型)","path":"/2023/01/15/linux-docs/Linux操作系统进程的状态和转换(五态模型)/","content":"1、进程的状态和装换1.1进程的三态模型按进程在执行过程中的不同情况至少要定义三种状态： 运行（running）态：进程占有处理器正在运行的状态。进程已获得CPU，其程序正在执行。在单处理机系统中，只有一个进程处于执行状态； 在多处理机系统中，则有多个进程处于执行状态。 就绪（ready）态：进程具备运行条件，等待系统分配处理器以便运行的状态。当进程已分配到除CPU以外的所有必要资源后，只要再获得CPU，便可立即执行，进程这时的状态称为就绪状态。在一个系统中处于就绪状态的进程可能有多个，通常将它们排成一个队列，称为就绪队列。 等待（wait）态：又称阻塞态或睡眠态，指进程不具备运行条件，正在等待某个时间完成的状态。也称为等待或睡眠状态，一个进程正在等待某一事件发生（例如请求I&#x2F;O而等待I&#x2F;O完成等）而暂时停止运行，这时即使把处理机分配给进程也无法运行，故称该进程处于阻塞状态。 引起进程状态转换的具体原因如下： 运行态→等待态：等待使用资源； 如等待外设传输；等待人工干预。 等待态→就绪态：资源得到满足； 如外设传输结束；人工干预完成。 运行态→就绪态：运行时间片到； 出现有更高优先权进程。就绪态—→运行态：CPU 空闲时选择一个就绪进程。 1.2 进程的五态模型五态模型在三态模型的基础上增加了新建态（new）和终止态（exit）： 新建态：对应于进程被创建时的状态，尚未进入就绪队列。创建一个进程需要通过两个步骤：1.为新进程分配所需要的资源和建立必要的管理信息。2.设置该进程为就绪态，并等待被调度执行。 终止态：指进程完成任务到达正常结束点，或出现无法克服的错误而异常终止，或被操作系统及有终止权的进程所终止时所处的状态。处于终止态的进程不再被调度执行，下一步将被系统撤销，最终从系统中消失。终止一个进程需要两个步骤：1.先对操作系统或相关的进程进行善后处理（如抽取信息）。2.然后回收占用的资源并被系统删除。 引起进程状态转换的具体原因如下： NULL→新建态：执行一个程序，创建一个子进程。 新建态→就绪态：当操作系统完成了进程创建的必要操作，并且当前系统的性能和虚拟内存的容量均允许。 运行态→终止态：当一个进程到达了自然结束点，或是出现了无法克服的错误，或是被操作系统所终结，或是被其他有终止权的进程所终结。 运行态→就绪态：运行时间片到；出现有更高优先权进程。 运行态→等待态：等待使用资源；如等待外设传输；等待人工干预。 就绪态→终止态：未在状态转换图中显示，但某些操作系统允许父进程终结子进程。 等待态→终止态：未在状态转换图中显示，但某些操作系统允许父进程终结子进程。 终止态→NULL：完成善后操作。 1.3 进程的七态模型三态模型和五态模型都是假设所有进程都在内存中的事实上有序不断的创建进程，当系统资源尤其是内存资源已经不能满足进程运行的要求时，必须把某些进程挂起（suspend），对换到磁盘对换区中，释放它占有的某些资源，暂时不参与低级调度。起到平滑系统操作负荷的目的。 引起进程挂起的原因是多样的，主要有：1.终端用户的请求。当终端用户在自己的程序运行期间发现有可疑问题时，希望暂停使自己的程序静止下来。亦即，使正在执行的进程暂停执行；若此时用户进程正处于就绪状态而未执行，则该进程暂不接受调度，以便用户研究其执行情况或对程序进行修改。我们把这种静止状态成为“挂起状态”。 2.父进程的请求。有时父进程希望挂起自己的某个子进程，以便考察和修改子进程，或者协调各子进程间的活动。 3.负荷调节的需要。当实时系统中的工作负荷较重，已可能影响到对实时任务的控制时，可由系统把一些不重要的进程挂起，以保证系统能正常运行。 4.操作系统的需要。操作系统有时希望挂起某些进程，以便检查运行中的资源使用情况或进行记账。 5.对换的需要。为了缓和内存紧张的情况，将内存中处于阻塞状态的进程换至外存上。 七态模型在五态模型的基础上增加了挂起就绪态（ready suspend）和挂起等待态（blocked suspend）。 挂起就绪态：进程具备运行条件，但目前在外存中，只有它被对换到内存才能被调度执行。 挂起等待态：表明进程正在等待某一个事件发生且在外存中。 引起进程状态转换的具体原因如下： 等待态→挂起等待态：操作系统根据当前资源状况和性能要求，可以决定把等待态进程对换出去成为挂起等待态。 挂起等待态→挂起就绪态：引起进程等待的事件发生之后，相应的挂起等待态进程将转换为挂起就绪态挂起就绪态→就绪态：当内存中没有就绪态进程，或者挂起就绪态进程具有比就绪态进程更高的优先级，系统将把挂起就绪态进程转换成就绪态。 就绪态→挂起就绪态：操作系统根据当前资源状况和性能要求，也可以决定把就绪态进程对换出去成为挂起就绪态。 挂起等待态→等待态：当一个进程等待一个事件时，原则上不需要把它调入内存。但是在下面一种情况下，这一状态变化是可能的。当一个进程退出后，主存已经有了一大块自由空间,而某个挂起等待态进程具有较高的优先级并且操作系统已经得知导致它阻塞的事件即将结束，此时便发生了这一状态变化。 运行态→挂起就绪态：当一个具有较高优先级的挂起等待态进程的等待事件结束后，它需要抢占 CPU，而此时主存空间不够，从而可能导致正在运行的进程转化为挂起就绪态。另外处于运行态的进程也可以自己挂起自己。 新建态→挂起就绪态：考虑到系统当前资源状况和性能要求，可以决定新建的进程将被对换出去成为挂起就绪态。 挂起进程等同于不在内存中的进程，因此挂起进程将不参与低级调度直到它们被调换进内存。 挂起进程具有如下特征： 该进程不能立即被执行 挂起进程可能会等待一个事件，但所等待的事件是独立于挂起条件的，事件结束并不能导致进程具备执行条件。 （等待事件结束后进程变为挂起就绪态） 进程进入挂起状态是由于操作系统、父进程或进程本身阻止它的运行。 结束进程挂起状态的命令只能通过操作系统或父进程发出。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/447668827","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux操作系统通过实战理解CPU上下文切换","path":"/2023/01/15/linux-docs/Linux操作系统通过实战理解CPU上下文切换/","content":"前言：Linux是一个多任务的操作系统，可以支持远大于CPU数量的任务同时运行，但是我们都知道这其实是一个错觉，真正是系统在很短的时间内将CPU轮流分配给各个进程，给用户造成多任务同时运行的错觉。所以这就是有一个问题，在每次运行进程之前CPU都需要知道进程从哪里加载、从哪里运行，也就是说需要系统提前帮它设置好CPU寄存器和程序计数器。 1、CPU上下文CPU上下文其实是一些环境正是有这些环境的支撑，任务得以运行，而这些环境的硬件条件便是CPU寄存器和程序计数器。CPU寄存器是CPU内置的容量非常小但是速度极快的存储设备，程序计数器则是CPU在运行任何任务时必要的，里面记录了当前运行任务的行数等信息，这就是CPU上下文。 2、CPU上下文切换根据任务的不同，CPU的上下文切换就可以分为进程上下文切换、线程上下文切换、中断上下文切换，进程上下文切换。 在Linux中，Linux按照特权等级，将进程的运行空间分为内核空间和用户空间： 内核空间具有最高权限，可以直接访问所有资源 用户空间只能访问受限资源，不能直接访问内存等硬件设备，要想访问这些特权资源，必须通过系统调用 对于一个进程来说，一般是运行在用户态的，但是当需要访问内存、磁盘等硬件设备的时候需要陷入到内核态中，也就是要从用户态到内核态的转变，而这种转变需要通过系统调用来实现，例如一个打开文件的操作，需要调用open()打开文件，read()读取文件内容，write()将文件内容输出到控制台，最后close()关闭文件，这就是系统调用 在系统调用的过程中同样发发生了CPU上下文切换： CPU寄存器里面原来用户态的指令位置，需要先保存起来，接着运行内核态代码 CPU寄存器需要更新为内核态指令的位置，执行内核态代码 系统调用结束后，CPU寄存器需要恢复原来保存的用户态，然后切换为用户空间，所以一次系统调用的过程，会发生两次的CPU上下文切换但是我们一般说系统调用是特权模式切换而不是上下文切换，因为这里没有涉及到虚拟内存等这些进程用户态的资源，也不会切换进程是属于进程之内的上下文切换，进程是由内核来管理和调度的，进程的切换只能发生在内核态，所以进程的上下文包含了虚拟内存、栈、全局变量等用户空间的资源，还包含了内核堆栈、寄存器等内核空间的状态，所以进程的上下文切换要比系统调用更多一步，保存该进程的虚拟内存、栈等用户空间的资源，进程上下文切换一般需要几十纳秒到数微秒的CPU时间，当进程上下文切换次数比较多的情况下爱，将导致CPU将大量的时间耗费在寄存器、内核栈即虚拟内存等资源的保存和恢复上，另外，Linux通过TLB快表来管理虚拟内存到物理内存的映射关系，当虚拟内存更新之后，需要刷新缓存，在这多处理系统上是很复杂的，因为多个处理器共享一个缓存。 下面再来说说什么时候会进行进程的上下文切换，其实就是进程在被调度的时候需要切换上下文，可能是主动地，也有可能是被动的 系统进程正常调度算法导致进程上下文切换，例如目前使用的时间片轮转算法，当一个进程的时间片耗尽之后，CPU会进项进程的调度切换到其他进程 进程在资源不足的时候，会被挂起例如在等待IO或者内存不足的时候，会主动挂起，并且等待系统调度其他进程 当进程通过一些睡眠函数sleep()主动挂起的时候，也会重新调度 当有高优先级的进程运行时，当前进程也会被挂起 当发生硬件中断时，CPU上的进程会被中断挂起 3、线程上下文切换线程是调度的基本单位，而进程则是资源拥有的基本单位，也就是说对于内核中的任务调度是以线程为单位，但是进程只是给线程提供了虚拟内存、全局变量等资源，进程与线程之间的区别这里不再介绍那么线程上下文的切换，其实分为两种情况： 前后两个线程属于不同进程，因为资源不共享，所以这时候的线程上下文切换和进程上下文切换是一致的 前后两个线程属于同一个进程，因为虚拟内存是共享的，所以在切换的时候，虚拟内存这些资源保持不动，只有切换线程的私有数据、寄存器等不共享的资源 所以同进程内的线程切换要比多进程内的线程切换消耗更少的资源 4、中断上下文切换中断是为了快速响应硬件的事件，简单来shu就是计算机停下当前的事情，去处理其他的事情，然后在回来继续执行之前的任务，例如我们在调用print函数的时候，其实汇编的底层会帮我们调用一条 int 0x80的指令，便是调用0x80号中断当然，中断要先将当前进程的状态保存下来，这样中断结束后进程仍然可以从原来的状态恢复运行，中断上下文的切换并不涉及进程的用户态，所以当中断程序打断了正在处于用户态的进程，不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源，只需要保存和恢复这个进程的内核态中的资源包括CPU寄存器、内核堆栈等对于同一个CPU来说，中断处理比进程拥有更高的优先级，所以中断上下文切换并不会与进程上下文切换同时发生，一般来说中断程序都执行比较快短小精悍，以便快速结束执行之前的任务。当中断上下文切换次数比较多的时候，会耗费大量的CPU怎么查看系统上下文上面已经介绍到CPU上下文切换分为进程上下文切换、线程上下文切换、中断上下文切换，那么过多的上下文切换会把CPU的时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，缩短进程真正运行的时间，成为系统性能大幅下降的一个因素所以我们可以使用vmstat这个工具来查询系统的上下文切换情况，vmstat是一个常用的系统性能分析工具，可以用来分析CPU上下文切换和中断的次数 需要特别关注的是： cs(context switch)：每秒上下文切换的次数 in(interrupt)：每秒中断的次数 r(Running or Runnable)：就绪队列的长度，也就是正在运行和等待CPU的进程 b(Blocked)：处于不可中断睡眠状态的进程数 vmstat是给出整个系统总体的上下文切换情况，要想查看每个进程的详细情况就需要使用pidstat，加上-w选项就可以查看进程上下文切换的情况 需要特别关注的是： cswch(voluntary context switches)：表示每秒自愿上下文切换的次数 nvcswch(non voluntary context switches)：表示每秒非自愿上下文切换的次数 这两个概念的分别含义： 自愿上下文切换：进程无法获取所需的资源，导致的上下文切换，例如IO、内存等资源不足时，就会发生自愿上下文切换 非自愿上下文切换：进程由于时间片已到等时间，被系统强制调度，进而发生的上下文切换，例如大量的进程都在争抢CPU时，就容易发生非自愿上下文切换 实战分析通过上面的工具已经可以初步查看到系统上下文切换的次数，但是当系统上下文切换的次数为多少时是不正常的呢？案例使用sysbench工具来模拟多线程调度切换的情况，sysbench是一个多线程的基准测试工具，可以模拟上下文切换过多的问题首先在第一个终端运行stsbench，模拟多线程切换问题# 以 10 个线程运行 5 分钟的基准测试，模拟多线程切换的问题 sysbench –threads&#x3D;10 –max-time&#x3D;300 threads run然后在第二个终端运行vmstat，每1秒查看上下文切换的情况 可以观察到如下指标： r列：就绪队列的长度已经到了8左右，已经超过了2个cpu，所以会有大量的CPU竞争 us(user)列和sy(system)列，这两列的CPU使用率已经到达100%，并且大量是由sy造成的，说明CPU主要是被内核占用了 in(interrupt)：in列的数值也到了解决1万，所以中断处理也是一个问题 那我们接着使用pidstat来查看是那一个进程出现了问题，由于pidstat默认是显示进程的指标数据，但是我们使用sysbench模拟的线程的数据，所以需要加上-t选项gpw@gopuwe:~$ pidstat -wt 所以到这里可以分析出是sysbench的子线程的上下文切换次数有很多还有一个问题，在使用vmstat的时候，发现in指标的数据也比较多，那么我们需要找出是什么类型的中断导致了中断上升，中断肯定是发生在内核态，但是pidstat只是一个进程的性能分析工具，并不提供任何关于中断的详细信息我们可以从&#x2F;proc&#x2F;interrupts这个只读文件中读取，&#x2F;proc是一个虚拟文件系统，用于内核空间和用户空间之间的通信，&#x2F;proc&#x2F;interrupts则提供了一个只读的中断使用情况，可以使用cat命令查看&#x2F;proc&#x2F;interrupts可以发现变化速度最快的是重调度中断RES，这个中断类型表示唤醒空闲状态的CPU来调度新的任务运行，也被成为处理器中断那么到底上下文切换的次数为多少合适呢？这个数值其实取决于系统本身的 CPU 性能，在我看来，如果系统的上下文切换次数比较稳定，那么从数百到一万以内，都应该算是正常的。但当上下文切换次数超过一万次，或者切换次数出现数量级的增长时，就很可能已经出现了性能问题，这个时候还要根据上下文切换的类型，做具体的分析，例如： 自愿上下文切换变多了，说明进程都在等待资源，有可能发生了 I&#x2F;O 等其他问题； 非自愿上下文切换变多了，说明进程都在被强制调度，也就是都在争抢 CPU，说明 CPU的确成了瓶颈； 中断次数变多了，说明 CPU 被中断处理程序占用，还需要通过查看 &#x2F;proc&#x2F;interrupts 文件 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/452839400","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux用户空间与内核空间通信(Netlink通信机制)","path":"/2023/01/15/linux-docs/Linux用户空间与内核空间通信(Netlink通信机制)/","content":"1，什么是Netlink通信机制Netlink是linux提供的用于内核和用户态进程之间的通信方式。但是注意虽然Netlink主要用于用户空间和内核空间的通信，但是也能用于用户空间的两个进程通信。只是进程间通信有其他很多方式，一般不用Netlink。除非需要用到Netlink的广播特性时。 那么Netlink有什么优势呢？ 一般来说用户空间和内核空间的通信方式有三种：&#x2F;proc、ioctl、Netlink。而前两种都是单向的，但是Netlink可以实现双工通信。Netlink协议基于BSD socket和AF_NETLINK地址簇(address family)，使用32位的端口号寻址(以前称作PID)，每个Netlink协议(或称作总线，man手册中则称之为netlink family)，通常与一个或一组内核服务&#x2F;组件相关联，如NETLINK_ROUTE用于获取和设置路由与链路信息、NETLINK_KOBJECT_UEVENT用于内核向用户空间的udev进程发送通知等。 netlink具有以下特点： ① 支持全双工、异步通信(当然同步也支持) ② 用户空间可使用标准的BSD socket接口(但netlink并没有屏蔽掉协议包的构造与解析过程，推荐使用libnl等第三方库) ③ 在内核空间使用专用的内核API接口 ④ 支持多播(因此支持“总线”式通信，可实现消息订阅) ⑤ 在内核端可用于进程上下文与中断上下文 2，用户态数据结构首先看一下几个重要的数据结构的关系： 1.struct msghdr msghdr这个结构在socket变成中就会用到，并不算Netlink专有的，这里不在过多说明。只说明一下如何更好理解这个结构的功能。我们知道socket消息的发送和接收函数一般有这几对：recv／send、readv／writev、recvfrom／sendto。当然还有recvmsg／sendmsg，前面三对函数各有各的特点功能，而recvmsg／sendmsg就是要囊括前面三对的所有功能，当然还有自己特殊的用途。msghdr的前两个成员就是为了满足recvfrom／sendto的功能，中间两个成员msg_iov和msg_iovlen则是为了满足readv／writev的功能，而最后的msg_flags则是为了满足recv／send中flag的功能，剩下的msg_control和msg_controllen则是满足recvmsg／sendmsg特有的功能。 2.struct sockaddr_ln struct sockaddr_ln为Netlink的地址，和我们通常socket编程中的sockaddr_in作用一样，他们的结构对比如下： struct sockaddr_nl的详细定义和描述如下： 1234567struct sockaddr_nl&#123; sa_family_t nl_family; /*该字段总是为AF_NETLINK */ unsigned short nl_pad; /* 目前未用到，填充为0*/ __u32 nl_pid; /* process pid */ __u32 nl_groups; /* multicast groups mask */&#125;; (1) nl_pid：在Netlink规范里，PID全称是Port-ID(32bits)，其主要作用是用于唯一的标识一个基于netlink的socket通道。通常情况下nl_pid都设置为当前进程的进程号。前面我们也说过，Netlink不仅可以实现用户-内核空间的通信还可使现实用户空间两个进程之间，或内核空间两个进程之间的通信。该属性为0时一般指内核。 (2) nl_groups：如果用户空间的进程希望加入某个多播组，则必须执行bind()系统调用。该字段指明了调用者希望加入的多播组号的掩码(注意不是组号，后面我们会详细讲解这个字段)。如果该字段为0则表示调用者不希望加入任何多播组。对于每个隶属于Netlink协议域的协议，最多可支持32个多播组(因为nl_groups的长度为32比特)，每个多播组用一个比特来表示。 3.struct nlmsghdr Netlink的报文由消息头和消息体构成，struct nlmsghdr即为消息头。消息头定义在文件里，由结构体nlmsghdr表示： 12345678struct nlmsghdr&#123; __u32 nlmsg_len; /* Length of message including header */ __u16 nlmsg_type; /* Message content */ __u16 nlmsg_flags; /* Additional flags */ __u32 nlmsg_seq; /* Sequence number */ __u32 nlmsg_pid; /* Sending process PID */&#125;; 消息头中各成员属性的解释及说明： (1) nlmsg_len：整个消息的长度，按字节计算。包括了Netlink消息头本身。 (2) nlmsg_type：消息的类型，即是数据还是控制消息。目前(内核版本2.6.21)Netlink仅支持四种类型的控制消息，如下： a) NLMSG_NOOP-空消息，什么也不做； b) NLMSG_ERROR-指明该消息中包含一个错误； c) NLMSG_DONE-如果内核通过Netlink队列返回了多个消息，那么队列的最后一条消息的类型为NLMSG_DONE，其余所有消息的nlmsg_flags属性都被设置NLM_F_MULTI位有效。 d) NLMSG_OVERRUN-暂时没用到。 (3) nlmsg_flags：附加在消息上的额外说明信息，如上面提到的NLM_F_MULTI。 3，用户空间Netlink socket API1.创建socket int socket(int domain, int type, int protocol) domain指代地址族,即AF_NETLINK; 套接字类型为SOCK_RAW或SOCK_DGRAM,因为netlink是一个面向数据报的服务; protocol选择该套接字使用哪种netlink特征。 以下是几种预定义的协议类型: NETLINK_ROUTE, NETLINK_FIREWALL, NETLINK_APRD, NETLINK_ROUTE6_FW。 可以非常容易的添加自己的netlink协议。为每一个协议类型最多可以定义32个多播组。每一个多播组用一个bitmask来表示,1&lt;&lt;i(0&lt;&#x3D;i&lt;&#x3D; 31),这在一组进程和内核进程协同完成一项任务时非常有用。发送多播netlink消息可以减少系统调用的数量,同时减少用来维护多播组成员信息的负担。 2.地址绑定bind() bind(fd, (struct sockaddr*)&amp;, nladdr, sizeof(nladdr)); 3.发送netlink消息 为了发送一条netlink消息到内核或者其他的用户空间进程,另外一个struct sockaddr_nl nladdr需要作为目的地址,这和使用sendmsg()发送一个UDP包是一样的。 如果该消息是发送至内核的,那么nl_pid和nl_groups都置为0. 如果消息是发送给另一个进程的单播消息,nl_pid是另外一个进程的pid值而nl_groups为零。 如果消息是发送给一个或多个多播组的多播消息,所有的目的多播组必须bitmask必须or起来从而形成nl_groups域。sendmsg(fd, &amp;, msg, 0); 4.接收netlink消息 一个接收程序必须分配一个足够大的内存用于保存netlink消息头和消息负载。然后其填充struct msghdr msg,再使用标准的recvmsg()函数来接收netlink消息。 当消息被正确的接收之后,nlh应该指向刚刚接收到的netlink消息的头。nladdr应该包含接收消息的目的地址,其中包括了消息发送者的pid和多播组。同时,宏NLMSG_DATA(nlh),定义在netlink.h中,返回一个指向netlink消息负载的指针。调用close(fd)关闭fd描述符所标识的socket；recvmsg(fd, &amp;, msg, 0); 4、内核空间Netlink socket API1.创建 netlink socket 1234struct sock *netlink_kernel_create(struct net *net, int unit,unsigned int groups, void (*input)(struct sk_buff *skb), struct mutex *cb_mutex,struct module *module); 参数说明： (1) net：是一个网络名字空间namespace，在不同的名字空间里面可以有自己的转发信息库，有自己的一套net_device等等。默认情况下都是使用 init_net这个全局变量。 (2) unit：表示netlink协议类型，如NETLINK_TEST、NETLINK_SELINUX。 (3) groups：多播地址。 (4) input：为内核模块定义的netlink消息处理函数，当有消 息到达这个netlink socket时，该input函数指针就会被引用，且只有此函数返回时，调用者的sendmsg才能返回。 (5) cb_mutex：为访问数据时的互斥信号量。 (6) module： 一般为THIS_MODULE。 2.发送单播消息 netlink_unicast 1int netlink_unicast(struct sock *ssk, struct sk_buff *skb, u32 pid, int nonblock) 参数说明： (1) ssk：为函数 netlink_kernel_create()返回的socket。 (2) skb：存放消息，它的data字段指向要发送的netlink消息结构，而 skb的控制块保存了消息的地址信息，宏NETLINK_CB(skb)就用于方便设置该控制块。 (3) pid：为接收此消息进程的pid，即目标地址，如果目标为组或内核，它设置为 0。 (4) nonblock：表示该函数是否为非阻塞，如果为1，该函数将在没有接收缓存可利用时立即返回；而如果为0，该函数在没有接收缓存可利用定时睡眠。 3.发送广播消息 netlink_broadcast 1int netlink_broadcast(struct sock *ssk, struct sk_buff *skb, u32 pid, u32 group, gfp_t allocation) 前面的三个参数与 netlink_unicast相同，参数group为接收消息的多播组，该参数的每一个位代表一个多播组，因此如果发送给多个多播组，就把该参数设置为多个多播组组ID的位或。参数allocation为内核内存分配类型，一般地为GFP_ATOMIC或GFP_KERNEL，GFP_ATOMIC用于原子的上下文（即不可以睡眠），而GFP_KERNEL用于非原子上下文。 4.释放 netlink socket 1int netlink_broadcast(struct sock *ssk, struct sk_buff *skb, u32 pid, u32 group, gfp_t allocation) 5、用户态范例一 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;string.h&gt;#include &lt;asm/types.h&gt;#include &lt;linux/netlink.h&gt;#include &lt;linux/socket.h&gt;#include &lt;errno.h&gt;#define MAX_PAYLOAD 1024 // maximum payload size#define NETLINK_TEST 25 //自定义的协议int main(int argc, char* argv[])&#123; int state; struct sockaddr_nl src_addr, dest_addr; struct nlmsghdr *nlh = NULL; //Netlink数据包头 struct iovec iov; struct msghdr msg; int sock_fd, retval; int state_smg = 0; // Create a socket sock_fd = socket(AF_NETLINK, SOCK_RAW, NETLINK_TEST); if(sock_fd == -1)&#123; printf(&quot;error getting socket: %s&quot;, strerror(errno)); return -1; &#125; // To prepare binding memset(&amp;src_addr, 0, sizeof(src_addr)); src_addr.nl_family = AF_NETLINK; src_addr.nl_pid = 100; //A：设置源端端口号 src_addr.nl_groups = 0; //Bind retval = bind(sock_fd, (struct sockaddr*)&amp;src_addr, sizeof(src_addr)); if(retval &lt; 0)&#123; printf(&quot;bind failed: %s&quot;, strerror(errno)); close(sock_fd); return -1; &#125; // To orepare create mssage nlh = (struct nlmsghdr *)malloc(NLMSG_SPACE(MAX_PAYLOAD)); if(!nlh)&#123; printf(&quot;malloc nlmsghdr error! &quot;); close(sock_fd); return -1;&#125; memset(&amp;dest_addr,0,sizeof(dest_addr)); dest_addr.nl_family = AF_NETLINK; dest_addr.nl_pid = 0; //B：设置目的端口号 dest_addr.nl_groups = 0; nlh-&gt;nlmsg_len = NLMSG_SPACE(MAX_PAYLOAD); nlh-&gt;nlmsg_pid = 100; //C：设置源端口 nlh-&gt;nlmsg_flags = 0; strcpy(NLMSG_DATA(nlh),&quot;Hello you!&quot;); //设置消息体 iov.iov_base = (void *)nlh; iov.iov_len = NLMSG_SPACE(MAX_PAYLOAD); //Create mssage memset(&amp;msg, 0, sizeof(msg)); msg.msg_name = (void *)&amp;dest_addr; msg.msg_namelen = sizeof(dest_addr); msg.msg_iov = &amp;iov; msg.msg_iovlen = 1; //send message printf(&quot;state_smg &quot;); state_smg = sendmsg(sock_fd,&amp;msg,0); if(state_smg == -1) &#123; printf(&quot;get error sendmsg = %s &quot;,strerror(errno)); &#125; memset(nlh,0,NLMSG_SPACE(MAX_PAYLOAD)); //receive message printf(&quot;waiting received! &quot;); while(1)&#123; printf(&quot;In while recvmsg &quot;); state = recvmsg(sock_fd, &amp;msg, 0); if(state&lt;0) &#123; printf(&quot;state&lt;1&quot;); &#125; printf(&quot;Received message: %s &quot;,(char *) NLMSG_DATA(nlh)); &#125; close(sock_fd); return 0;&#125; 上面程序首先向内核发送一条消息；“Hello you”，然后进入循环一直等待读取内核的回复，并将收到的回复打印出来。如果看上面程序感觉很吃力，那么应该首先复习一下UDP中使用sendmsg的用法，特别时struct msghdr的结构要清楚，这里再赘述。 下面主要分析与UDP发送数据包的不同点： \\1. socket地址结构不同，UDP为sockaddr_in，Netlink为struct sockaddr_nl； \\2. 与UDP发送数据相比，Netlink多了一个消息头结构struct nlmsghdr需要我们构造。 注意代码注释中的A、B、C三处分别设置了pid。首先解释一下什么是pid，网上很多文章把这个字段说成是进程的pid，其实这完全是望文生义。这里的pid和进程pid没有什么关系，仅仅相当于UDP的port。对于UDP来说port和ip标示一个地址，那对我们的NETLINK_TEST协议（注意Netlink本身不是一个协议）来说，pid就唯一标示了一个地址。所以你如果用进程pid做为标示当然也是可以的。当然同样的pid对于NETLINK_TEST协议和内核定义的其他使用Netlink的协议是不冲突的（就像TCP的80端口和UDP的80端口）。 下面分析这三处设置pid分别有什么作用，首先A和B位置的比较好理解，这是在地址（sockaddr_nl）上进行的设置，就是相当于设置源地址和目的地址（其实是端口），只是注意B处设置pid为0，0就代表是内核，可以理解为内核专用的pid，那么用户进程就不能用0做为自己的pid吗？这个只能说如果你非要用也是可以的，只是会产生一些问题，后面在分析。 接下来看为什么C处的消息头仍然需要设置pid呢？这里首先要知道一个前提：内核不会像UDP一样根据我们设置的原、目的地址为我们构造消息头，所以我们不在包头写入我们自己的地址（pid），那内核怎么知道是谁发来的报文呢？当然如果内核只是处理消息不需要回复进程的话舍不设置这个消息头pid都可以。 所以每个pid的设置功能不同：A处的设置是要设置发送者的源地址，有人会说既然源地址又不会自动填充到报文中，我们为什么还要设置这个，因为你还可能要接收回复啊。就像寄信，你连“门牌号”都没有，即使你在写信时候写上你的地址是100号，对方回信目的地址也是100号，但是邮局发现根本没有这个地址怎么可能把信送到你手里呢？所以A的主要作用是注册源地址，保证可以收到回复，如果不需要回复当然可以简单将pid设置为0；B处自然就是收信人的地址，pid为0代表内核的地址，假如有一个进程在101号上注册了地址，并调用了recvmsg，如果你将B处的pid设置为101，那数据包就发给了另一个进程，这就实现了使用Netlink进行进程间通信；C相当于你在信封上写的源地址，通常情况下这个应该和你的真实地址（A）处注册的源地址相同，当然你要是不想收到回信，又想恶搞一下或者有特殊需求，你可以写成其他进程注册的pid（比如101）。这和我们现实中寄信是一样的，你给你朋友写封情书，把写信人写成你的另一个好基友，然后后果你懂得…… 好了，有了这个例子我们就大概知道用户态怎么使用Netlink了。 6、内核态程序范例一 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#include &lt;linux/init.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/timer.h&gt;#include &lt;linux/time.h&gt;#include &lt;linux/types.h&gt;#include &lt;net/sock.h&gt;#include &lt;net/netlink.h&gt;#define NETLINK_TEST 25#define MAX_MSGSIZE 1024int stringlength(char *s);int err;struct sock *nl_sk = NULL;int flag = 0;//向用户态进程回发消息void sendnlmsg(char *message, int pid)&#123; struct sk_buff *skb_1; struct nlmsghdr *nlh; int len = NLMSG_SPACE(MAX_MSGSIZE); int slen = 0; if(!message || !nl_sk) &#123; return ; &#125; printk(KERN_ERR &quot;pid:%d &quot;,pid); skb_1 = alloc_skb(len,GFP_KERNEL); if(!skb_1) &#123; printk(KERN_ERR &quot;my_net_link:alloc_skb error &quot;); &#125; slen = stringlength(message); nlh = nlmsg_put(skb_1,0,0,0,MAX_MSGSIZE,0); NETLINK_CB(skb_1).pid = 0; NETLINK_CB(skb_1).dst_group = 0; message[slen]= &#x27;\\0&#x27;; memcpy(NLMSG_DATA(nlh),message,slen+1); printk(&quot;my_net_link:send message &#x27;%s&#x27;. &quot;,(char *)NLMSG_DATA(nlh)); netlink_unicast(nl_sk,skb_1,pid,MSG_DONTWAIT);&#125;int stringlength(char *s)&#123; int slen = 0; for(; *s; s++) &#123; slen++; &#125; return slen;&#125;//接收用户态发来的消息void nl_data_ready(struct sk_buff *__skb) &#123; struct sk_buff *skb; struct nlmsghdr *nlh; char str[100]; struct completion cmpl; printk(&quot;begin data_ready &quot;); int i=10; int pid; skb = skb_get (__skb); if(skb-&gt;len &gt;= NLMSG_SPACE(0)) &#123; nlh = nlmsg_hdr(skb); memcpy(str, NLMSG_DATA(nlh), sizeof(str)); printk(&quot;Message received:%s &quot;,str) ; pid = nlh-&gt;nlmsg_pid; while(i--) &#123;//我们使用completion做延时，每3秒钟向用户态回发一个消息 init_completion(&amp;cmpl); wait_for_completion_timeout(&amp;cmpl,3 * HZ); sendnlmsg(&quot;I am from kernel!&quot;,pid); &#125; flag = 1; kfree_skb(skb); &#125; &#125;// Initialize netlinkint netlink_init(void)&#123; nl_sk = netlink_kernel_create(&amp;init_net, NETLINK_TEST, 1, nl_data_ready, NULL, THIS_MODULE); if(!nl_sk)&#123; printk(KERN_ERR &quot;my_net_link: create netlink socket error. &quot;); return 1; &#125; printk(&quot;my_net_link_4: create netlink socket ok. &quot;); return 0;&#125;static void netlink_exit(void)&#123; if(nl_sk != NULL)&#123; sock_release(nl_sk-&gt;sk_socket); &#125; printk(&quot;my_net_link: self module exited &quot;);&#125;module_init(netlink_init);module_exit(netlink_exit);MODULE_AUTHOR(&quot;zhao_h&quot;);MODULE_LICENSE(&quot;GPL&quot;); 附上内核代码的Makefile文件： 12345678ifneq ($(KERNELRELEASE),)obj-m :=netl.oelseKERNELDIR ?=/lib/modules/$(shell uname -r)/buildPWD :=$(shell pwd)default:$(MAKE) -C $(KERNELDIR) M=$(PWD) modulesendif 我们将内核模块insmod后，运行用户态程序，结果如下： 这个结果复合我们的预期，但是运行过程中打印出“state_smg”卡了好久才输出了后面的结果。这时候查看客户进程是处于D状态的（不了解D状态的同学可以google一下）。这是为什么呢？因为进程使用Netlink向内核发数据是同步，内核向进程发数据是异步。什么意思呢？也就是用户进程调用sendmsg发送消息后，内核会调用相应的接收函数，但是一定到这个接收函数执行完用户态的sendmsg才能够返回。我们在内核态的接收函数中调用了10次回发函数，每次都等待3秒钟，所以内核接收函数30秒后才返回，所以我们用户态程序的sendmsg也要等30秒后才返回。相反，内核回发的数据不用等待用户程序接收，这是因为内核所发的数据会暂时存放在一个队列中。 再来回到之前的一个问题，用户态程序的源地址（pid）可以用0吗？我把上面的用户程序的A和C处pid都改为了0，结果一运行就死机了。为什么呢？我们看一下内核代码的逻辑，收到用户消息后，根据消息中的pid发送回去，而pid为0，内核并不认为这是用户程序，认为是自身，所有又将回发的10个消息发给了自己（内核），这样就陷入了一个死循环，而用户态这时候进程一直处于D。 另外一个问题，如果同时启动两个用户进程会是什么情况？答案是再调用bind时出错：“Address already in use”，这个同UDP一样，同一个地址同一个port如果没有设置SO_REUSEADDR两次bind就会出错，之后我用同样的方式再Netlink的socket上设置了SO_REUSEADDR，但是并没有什么效果。 7、用户态范例二 之前我们说过UDP可以使用sendmsg／recvmsg也可以使用sendto／recvfrom，那么Netlink同样也可以使用sendto／recvfrom。具体实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;string.h&gt;#include &lt;asm/types.h&gt;#include &lt;linux/netlink.h&gt;#include &lt;linux/socket.h&gt;#include &lt;errno.h&gt;#define MAX_PAYLOAD 1024 // maximum payload size#define NETLINK_TEST 25int main(int argc, char* argv[])&#123; struct sockaddr_nl src_addr, dest_addr; struct nlmsghdr *nlh = NULL; int sock_fd, retval; int state,state_smg = 0; // Create a socket sock_fd = socket(AF_NETLINK, SOCK_RAW, NETLINK_TEST); if(sock_fd == -1)&#123; printf(&quot;error getting socket: %s&quot;, strerror(errno)); return -1; &#125; // To prepare binding memset(&amp;src_addr, 0, sizeof(src_addr)); src_addr.nl_family = AF_NETLINK; src_addr.nl_pid = 100; src_addr.nl_groups = 0; //Bind retval = bind(sock_fd, (struct sockaddr*)&amp;src_addr, sizeof(src_addr)); if(retval &lt; 0)&#123; printf(&quot;bind failed: %s&quot;, strerror(errno)); close(sock_fd); return -1; &#125; // To orepare create mssage head nlh = (struct nlmsghdr *)malloc(NLMSG_SPACE(MAX_PAYLOAD)); if(!nlh)&#123; printf(&quot;malloc nlmsghdr error! &quot;); close(sock_fd); return -1; &#125; memset(&amp;dest_addr,0,sizeof(dest_addr)); dest_addr.nl_family = AF_NETLINK; dest_addr.nl_pid = 0; dest_addr.nl_groups = 0; nlh-&gt;nlmsg_len = NLMSG_SPACE(MAX_PAYLOAD); nlh-&gt;nlmsg_pid = 100; nlh-&gt;nlmsg_flags = 0; strcpy(NLMSG_DATA(nlh),&quot;Hello you!&quot;); //send message printf(&quot;state_smg &quot;); sendto(sock_fd,nlh,NLMSG_LENGTH(MAX_PAYLOAD),0,(struct sockaddr*)(&amp;dest_addr),sizeof(dest_addr)); if(state_smg == -1) &#123; printf(&quot;get error sendmsg = %s &quot;,strerror(errno)); &#125; memset(nlh,0,NLMSG_SPACE(MAX_PAYLOAD)); //receive message printf(&quot;waiting received! &quot;);while(1)&#123; printf(&quot;In while recvmsg &quot;);state=recvfrom(sock_fd,nlh,NLMSG_LENGTH(MAX_PAYLOAD),0,NULL,NULL); if(state&lt;0) &#123; printf(&quot;state&lt;1&quot;); &#125; printf(&quot;Received message: %s &quot;,(char *) NLMSG_DATA(nlh)); memset(nlh,0,NLMSG_SPACE(MAX_PAYLOAD)); &#125; close(sock_fd); return 0;&#125; 熟悉UDP编程的同学看到这个程序一定很熟悉，除了多了一个Netlink消息头的设置。但是我们发现程序中调用了bind函数，这个函数再UDP编程中的客户端不是必须的，因为我们不需要把UDP socket与某个地址关联，同时再发送UDP数据包时内核会为我们分配一个随即的端口。但是对于Netlink必须要有这一步bind，因为Netlink内核可不会为我们分配一个pid。再强调一遍消息头（nlmsghdr）中的pid是告诉内核接收端要回复的地址，但是这个地址存不存在内核并不关心，这个地址只有用户端调用了bind后才存在。 我们看到这两个例子都是用户态首先发起的，那Netlink是否支持内核态主动发起的情况呢？ 当然是可以的，只是内核一般需要事件触发，这里，只要和用户态约定号一个地址（pid），内核直接调用netlink_unicast就可以了。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/458996875","tags":["Linux开发"],"categories":["linux"]},{"title":"QEMU调试Linux内核环境搭建","path":"/2023/01/15/linux-docs/QEMU调试Linux内核环境搭建/","content":"一个最小可运行Linux操作系统需要内核镜像bzImage和rootfs，本文整理了其制作、安装过程，调试命令，以及如何添加共享磁盘。 1、编译内核源码从 The Linux Kernel Archives 网站下载内核源码，本文下载的版本为4.14.191，4.14.191源码下载。 使用wget获取源码。 wget https://mirrors.edge.kernel.org/pub/linux/kernel/v4.x/linux-4.14.191.tar.gz 解压源码： tar -xvf linux-4.14.191.tar.gz 解压后进入源码根目录linux-4.14.191，指定编译的架构，依次执行下面的命令，打开配置菜单。 12345671cd linux-4.14.19123export ARCH=x8645make x86_64_defconfig67make menuconfig 在配置菜单中，启用内核debug，关闭地址随机化，不然断点处无法停止。 1234567891Kernel hacking ---&gt; 2 [*] Kernel debugging3 Compile-time checks and compiler options ---&gt;4 [*] Compile the kernel with debug info5 [*] Provide GDB scripts for kernel debuggin678Processor type and features ----&gt;9 [] Randomize the address of the kernel image (KASLR) 开始编译内核，-j 指定并行编译作业数。最终生成linux-4.14.191&#x2F;arch&#x2F;x86_64&#x2F;boot&#x2F;bzImage文件。 11make -j 20 内核编译完成。 2、配置Busybox启动内核还需要一个具有根文件系统的磁盘镜像文件，根文件系统中提供可供交互的shell程序以及一些常用工具命令。 我们借助busybox工具来制作根文件系统。 本文使用1.32.0版本，下载busybox。 解压： tar -jxvf busybox-1.32.0.tar.bz2 进入busybox根目录，配置编译选项。 121cd busybox-1.32.02make menuconfig 把busybox配置为静态编译。 121 Settings ---&gt;2 [*] Build BusyBox as a static binary (no shared libs) 配置如下图所示。 3、制作rootfs接下来制作rootfs镜像文件，并把busybox安装到其中。 使用dd命令创建文件，并格式化为ext4文件系统。 1231szp@r420-PowerEdge-R420:~/busybox-1.32.0$ dd if=/dev/zero of=rootfs.img bs=1M count=1023szp@r420-PowerEdge-R420:~/busybox-1.32.0$ mkfs.ext4 rootfs.img 创建用于挂载该镜像文件的目录fs，挂载后才能往里面写入busybox。使用mount命令将rootfs.img挂载到fs目录，编译busybox并写入fs目录中。 123451szp@r420-PowerEdge-R420:~$ mkdir fs23szp@r420-PowerEdge-R420:~/busybox-1.32.0$ sudo mount -t ext4 -o loop rootfs.img ./fs45szp@r420-PowerEdge-R420:~/busybox-1.32.0$ sudo make install CONFIG_PREFIX=./fs 接下来对写入的busybox进行补充配置。 123451szp@r420-PowerEdge-R420:~/busybox-1.32.0/fs$ sudo mkdir proc dev etc home mnt23szp@r420-PowerEdge-R420:~/busybox-1.32.0/fs$ sudo cp -r ../examples/bootfloppy/etc/* etc/45szp@r420-PowerEdge-R420:~/busybox-1.32.0$ sudo chmod -R 777 fs/ 制作完成的rootfs目录如下： 最后，卸载rootfs.img 11szp@r420-PowerEdge-R420:~/busybox-1.32.0$ sudo umount fs 至此，一个带有rootfs的磁盘镜像制作完成。 4、启动qemu使用如下命令启动无GUI的qemu，参数含义如下： -kernel # 指定编译好的内核镜像-hda # 指定硬盘-append “root&#x3D;&#x2F;dev&#x2F;sda” 指示根文件系统 console&#x3D;ttyS0 把QEMU的输入输出定向到当前终端上-nographic 不使用图形输出窗口-s 是-gdb tcp::1234缩写，监听1234端口，在GDB中可以通过target remote localhost:1234连接 11qemu-system-x86_64 -kernel ./linux-4.14.191/arch/x86_64/boot/bzImage -hda ./busybox-1.32.0/rootfs.img -append &quot;root=/dev/sda console=ttyS0&quot; -nographic 启动后如下图： Ctrl+A 松开后按C退出qemu。 5、内核函数调试启动命令中添加-s参数与-S参数启动qemu。 11szp@r420-PowerEdge-R420:~$ qemu-system-x86_64 -kernel ~/linux-4.14.191/arch/x86_64/boot/bzImage -hda ~/busybox-1.32.0/rootfs.img -append &quot;root=/dev/sda console=ttyS0&quot; -s -S -smp 1 -nographic 启动gdb远程调试。vmlinux文件在编译后的内核源码根目录下。 1231szp@r420-PowerEdge-R420:~$ gdb ./linux-4.14.191/vmlinux 23(gdb) target remote localhost:1234 在new_sync_read函数添加断点，continue。 在系统中执行ls命令，触发new_sync_read函数， 至此，完成了qemu环境下使用gdb进行内核函数的调试。 6、添加共享磁盘有时候需要在宿主机和qemu虚拟机之间共享文件，添加一个共享磁盘将有助于该项工作。 创建64MB磁盘镜像文件，并格式化为ext4，作为共享磁盘备用。 1231szp@r420-PowerEdge-R420:~/shadisk$ dd if=/dev/zero of=ext4.img bs=512 count=13107223szp@r420-PowerEdge-R420:~/shadisk$ mkfs.ext4 ext4.img 修改qemu启动命令，使用-hdb增加一个磁盘。 11qemu-system-x86_64 -kernel ~/linux-4.14.191/arch/x86_64/boot/bzImage -hda ~/busybox-1.32.0/rootfs.img -append &quot;root=/dev/sda console=ttyS0&quot; -s -smp 1 -nographic -hdb ~/shadisk/ext4.img 进入qemu系统后使用mount命令挂载sdb到mnt目录。 在原系统中挂载ext4.img，实现qemu与原系统的文件共享。 11szp@r420-PowerEdge-R420:~/shadisk$ sudo mount -t ext4 -o loop ext4.img ./share 至此，可以在宿主机器share目录下，与qemu中的虚拟机器进行文件共享。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/499637419","tags":["Linux开发"],"categories":["linux"]},{"title":"任务空间管理","path":"/2023/01/15/linux-docs/任务空间管理/","content":"一. 前言 从本文开始，我们进入内存部分的学习。首先会接着前面的任务task_struct讲解任务空间管理结构体mm_struct，并简单介绍物理内存和虚拟内存的相关知识，关于详细的基础知识和概念可以参照CSAPP一书，这里不会做过多的赘述，而是默认在已了解其映射关系的基础上进行的学习。在后文中，会继续介绍物理内存的管理以及用户态和内核态的内存映射。 二. 基本概念梳理 CPU、缓存、内存、主存的架构是源于越快的设备越贵，因此出于节约（qiong)考虑设计了多层架构，CPU中有了MMU 物理内存有限，多进程共享物理内存存在安全问题，因此出现了虚拟内存的设计 虚拟内存根据ELF的结构进行了相应的设计，存在堆、映射区、栈、数据段等部分 考虑到虚拟内存的结构，出现了堆的申请即动态内存 虚拟内存为每个进程分配单独的地址空间，映射到物理内存上执行，因此有了物理内存和虚拟内存的映射方法：页 为了管理虚拟内存，出现了页表和多级页表 为了加速映射，出现了CPU中的TLB 为了满足共享的需求，出现了内存映射中的共享内存 由于内存碎片的存在，出现了碎片管理的设计以及垃圾回收器 三. 进程内存管理 对于一个进程来说，需要考虑用户态和内核态两部分需要存储在内核内的各个结构 用户态包括 代码段 全局变量 常量字符串 函数栈，包括函数调用，局部变量，函数参数等 堆：malloc 分配的内存等 内存映射，如 glibc 的调用， glibc 的代码是以 so 文件的形式存在的，也需要放在内存里面。 内核态包括 内核部分的代码 内核中全局变量 task_struct 内核栈 在内核里面也有动态分配的内存 虚拟地址到物理地址的映射表 进程在内核态中通过task_struct管理，而task_struct中关于内存有如下成员变量 1234struct mm_struct *mm;struct mm_struct *active_mm;/* Per-thread vma caching: */struct vmacache vmacache; 其中mm_struct结构体也较为复杂，我们将分步介绍。首先我们来看看内核态和用户态的地址划分。这里highest_vm_end存储当前虚拟内存地址的最大地址，而task_size则是用户态的大小。 123456struct mm_struct &#123;...... unsigned long task_size;\t/* size of task vm space */ unsigned long highest_vm_end;\t/* highest vma end address */......&#125; task_size定义如下，从注释可见用户态分配了4G虚拟内存中的3G空间，而64位因为空间巨大因此在内核态和用户态中间还保留了空闲区域进行隔离，用户态仅使用47位，即128TB。内核态同样分配128TB，位于最高位。 1234567891011121314151617181920#ifdef CONFIG_X86_32/* * User space process size: 3GB (default). */#define TASK_SIZE PAGE_OFFSET#define TASK_SIZE_MAX TASK_SIZE/*config PAGE_OFFSET hex default 0xC0000000 depends on X86_32*/#else/* * User space process size. 47bits minus one guard page.*/#define TASK_SIZE_MAX ((1UL &lt;&lt; 47) - PAGE_SIZE)#define TASK_SIZE (test_thread_flag(TIF_ADDR32) ? \\ IA32_PAGE_OFFSET : TASK_SIZE_MAX)...... 3.1 用户态内存结构 在用户态，mm_struct有着以下成员变量 mmap_base：内存映射的起始地址 mmap_legacy_base：表示映射的基址，在32位中为固定的TASK_UNMAPPED_BASE，而在64位中，存在一个虚拟地址随机映射机制，因此为TASK_UNMAPPED_BASE + mmap_rnd() hiwater_rss：RSS的高水位使用情况 hiwater_vm：高水位虚拟内存使用情况 total_vm：映射的总页数 locked_vm：被锁定不能换出的页数 pinned_vm：不能换出也不能移动的页数 data_vm：存放数据的页数 exec_vm：存放可执行文件的页数 stack_vm：存放栈的页数 arg_lock：引入spin_lock用于保护对下面区域变量们的并行访问 start_code 和 end_code： 可执行代码的开始和结束位置 start_data 和 end_data ：已初始化数据的开始位置和结束位置 start_brk ：堆的起始位置 brk ：堆当前的结束位置 start_stack ：栈的起始位置，栈的结束位置在寄存器的栈顶指针中 arg_start 和 arg_end ：参数列表的位置，位于栈中最高地址的地方。 env_start 和 env_end ：环境变量的位置，位于栈中最高地址的地方。 1234567891011121314151617181920struct mm_struct &#123;...... unsigned long mmap_base;\t/* base of mmap area */ unsigned long mmap_legacy_base;\t/* base of mmap area in bottom-up allocations */ ...... unsigned long hiwater_rss; /* High-watermark of RSS usage */ unsigned long hiwater_vm; /* High-water virtual memory usage */ unsigned long total_vm; /* Total pages mapped */ unsigned long locked_vm; /* Pages that have PG_mlocked set */ atomic64_t pinned_vm; /* Refcount permanently increased */ unsigned long data_vm; /* VM_WRITE &amp; ~VM_SHARED &amp; ~VM_STACK */ unsigned long exec_vm; /* VM_EXEC &amp; ~VM_WRITE &amp; ~VM_STACK */ unsigned long stack_vm; /* VM_STACK */ spinlock_t arg_lock; /* protect the below fields */ unsigned long start_code, end_code, start_data, end_data; unsigned long start_brk, brk, start_stack; unsigned long arg_start, arg_end, env_start, env_end; unsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */ ......&#125; 根据这些成员变量，我们可以规划出用户态中各个部分的位置，但是我们还需要一个结构体描述这些区域的属性，即vm_area_struct 123456struct mm_struct &#123;...... struct vm_area_struct *mmap; /* list of VMAs */ struct rb_root mm_rb; ...... &#125; vm_area_struct的具体结构体定义如下所示，实际是通过vm_next和vm_prev组合而成的双向链表，即通过一系列的vm_area_struct来表述一个进程在用户态分配的各个区域的内容。 vm_start和vm_end表述该块区域的开始和结束为止 vm_rb对应一颗红黑树，这颗红黑树将所有vm_area_struct组合起来，便于增删查找。 rb_subtree_gap存储当前区域和上个区域之间的间隔，用于后续分配使用。 vm_mm指向该结构体所属的vm_struct vm_page_prot管理该页的接入权限，vm_flags为标记位 rb和rb_subtree_last：有空余位置的区间树结构 ano_vma 和 ano_vma_chain：匿名映射。虚拟内存区域可以映射到物理内存，也可以映射到文件，映射到物理内存的时候称为匿名映射，映射到文件需要vm_file指定被映射文件，vm_pgoff存储偏移量。 vm_opts：指向该结构体的函数指针，用于处理该结构体 vm_private_data：私有数据存储 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/* * This struct defines a memory VMM memory area. There is one of these * per VM-area/task. A VM area is any part of the process virtual memory * space that has a special rule for the page-fault handlers (ie a shared * library, the executable area etc). */struct vm_area_struct &#123; /* The first cache line has the info for VMA tree walking. */ unsigned long vm_start; /* Our start address within vm_mm. */ unsigned long vm_end; /* The first byte after our end address within vm_mm. */ /* linked list of VM areas per task, sorted by address */ struct vm_area_struct *vm_next, *vm_prev; struct rb_node vm_rb; /* * Largest free memory gap in bytes to the left of this VMA. * Either between this VMA and vma-&gt;vm_prev, or between one of the * VMAs below us in the VMA rbtree and its -&gt;vm_prev. This helps * get_unmapped_area find a free area of the right size. */ unsigned long rb_subtree_gap; /* Second cache line starts here. */ struct mm_struct *vm_mm;\t/* The address space we belong to. */ pgprot_t vm_page_prot; /* Access permissions of this VMA. */ unsigned long vm_flags; /* Flags, see mm.h. */ /* * For areas with an address space and backing store, * linkage into the address_space-&gt;i_mmap interval tree. */ struct &#123; struct rb_node rb; unsigned long rb_subtree_last; &#125; shared; /* * A file&#x27;s MAP_PRIVATE vma can be in both i_mmap tree and anon_vma * list, after a COW of one of the file pages.\tA MAP_SHARED vma * can only be in the i_mmap tree. An anonymous MAP_PRIVATE, stack * or brk vma (with NULL file) can only be in an anon_vma list. */ struct list_head anon_vma_chain; /* Serialized by mmap_sem &amp; page_table_lock */ struct anon_vma *anon_vma;\t/* Serialized by page_table_lock */ /* Function pointers to deal with this struct. */ const struct vm_operations_struct *vm_ops; /* Information about our backing store: */ unsigned long vm_pgoff; /* Offset (within vm_file) in PAGE_SIZE units */ struct file * vm_file; /* File we map to (can be NULL). */ void * vm_private_data; /* was vm_pte (shared mem) */ atomic_long_t swap_readahead_info;#ifndef CONFIG_MMU struct vm_region *vm_region;\t/* NOMMU mapping region */#endif#ifdef CONFIG_NUMA struct mempolicy *vm_policy;\t/* NUMA policy for the VMA */#endif struct vm_userfaultfd_ctx vm_userfaultfd_ctx;&#125; __randomize_layout; 对一个mm_struct来说，其众多的vm_area_struct会在ELF文件加载，即load_elf_binary()时构造。该函数在解析ELF文件格式后，就会进行内存映射的建立，主要包括 调用 setup_new_exec，设置内存映射区 mmap_base 调用 setup_arg_pages，设置栈的 vm_area_struct，这里面设置了 mm-&gt;arg_start 是指向栈底的，current-&gt;mm-&gt;start_stack 就是栈底 elf_map 会将 ELF 文件中的代码部分映射到内存中来 set_brk 设置了堆的 vm_area_struct，这里面设置了 current-&gt;mm-&gt;start_brk &#x3D; current-&gt;mm-&gt;brk，也即堆里面还是空的 load_elf_interp 将依赖的 so 映射到内存中的内存映射区域 1234567891011121314151617181920212223242526272829303132static int load_elf_binary(struct linux_binprm *bprm)&#123;...... setup_new_exec(bprm);...... /* Do this so that we can load the interpreter, if need be. We will change some of these later */ retval = setup_arg_pages(bprm, randomize_stack_top(STACK_TOP), executable_stack);...... error = elf_map(bprm-&gt;file, load_bias + vaddr, elf_ppnt, elf_prot, elf_flags, total_size);...... /* Calling set_brk effectively mmaps the pages that we need * for the bss and break sections. We must do this before * mapping in the interpreter, to make sure it doesn&#x27;t wind * up getting placed where the bss needs to go. */ retval = set_brk(elf_bss, elf_brk, bss_prot);...... elf_entry = load_elf_interp(&amp;loc-&gt;interp_elf_ex, interpreter, &amp;interp_map_addr, load_bias, interp_elf_phdata);...... current-&gt;mm-&gt;end_code = end_code; current-&gt;mm-&gt;start_code = start_code; current-&gt;mm-&gt;start_data = start_data; current-&gt;mm-&gt;end_data = end_data; current-&gt;mm-&gt;start_stack = bprm-&gt;p;......&#125; 3.2 内核态结构 由于32位和64位系统空间大小差距过大，因此结构上也有一些区别。我们这里分别讨论二者的结构。 3.2.1 32位内核态结构 内核态的虚拟空间和进程是无关的，即所有进程通过系统调用进入内核后，看到的虚拟地址空间是一样的。如下图所示为32位内核态虚拟空间分布图。 1、直接映射区 前896M为直接映射区，该区域用于和物理内存进行直接映射。虚拟内存地址减去 3G，就得到对应的物理内存的位置。在内核里面，有两个宏： __pa(vaddr) 返回与虚拟地址 vaddr 相关的物理地址； __va(paddr) 则计算出对应于物理地址 paddr 的虚拟地址。 对于该部分虚拟地址的访问，同样采取分页的方式进行，但是页表地址比较简单，直接一一对应即可。 在系统启动的时候，物理内存的前 1M 已经被占用了，从 1M 开始加载内核代码段，然后就是内核的全局变量、BSS 等，也是 ELF 里面涵盖的。这样内核的代码段，全局变量，BSS 也就会被映射到 3G 后的虚拟地址空间里面。具体的物理内存布局可以查看 &#x2F;proc&#x2F;iomem，具体会因为每个人的系统、配置等产生区别。 2、high_memory 高端内存的名字来源于x86架构中将物理地址空间划分三部分：ZONE_DMA、ZONE_NORMAL和ZONE_HIGHMEM。ZONE_HIGHMEM即为高端内存。 高端内存是内存管理模块看待物理内存的称谓，指的也即896M直接映射区上面的区域。内核中除了内存管理模块外，其余均操作虚拟地址。而内存管理模块会直接操作物理地址，进行虚拟地址的分配和映射。其存在的意义是以32位系统有限的内核空间去访问无限的物理内存空间：借用这段逻辑地址空间，建立映射到想访问的那段物理内存（即填充内核页表），临时用一会，用完后归还。 3、内核动态映射空间（noncontiguous memory allocation） 在VMALLOC_START和VMALLOC_END之间的区域称之为内核动态映射空间，对应于用户态进程malloc申请内存一样，在内核态可以通过vmalloc来申请。内核态有单独的页表管理，和用户态分开。 4、持久内核映射区（permanent kernel mapping） PKMAP_BASE 到 FIXADDR_START 的空间称为持久内核映射，这个地址范围是 4G-8M 到 4G-4M 之间。使用 alloc_pages() 函数的时候，在物理内存的高端内存得到 struct page 结构，可以调用 kmap() 将其映射到这个区域。因为允许永久映射的数量有限，当不再需要高端内存时，应该解除映射，这可以通过kunmap()函数来完成。 5、固定映射区 FIXADDR_START 到 FIXADDR_TOP(0xFFFF F000) 的空间，称为固定映射区域，主要用于满足特殊需求。 6、临时映射区（temporary kernel mapping） 临时内核映射通过kmap_atomic和kunmap_atomic实现，主要用于当需要写入物理内存或主存时的操作，如写入文件时使用。 3.2.2 64位内核态结构 64位内核态因为空间巨大，所以不需要像32位一样精打细算，直接分出很多的空闲区域做保护，结构如下图所示 从 0xffff800000000000 开始就是内核的部分，只不过一开始有 8T 的空档区域。 从 __PAGE_OFFSET_BASE(0xffff880000000000) 开始的 64T 的虚拟地址空间是直接映射区域，也就是减去 PAGE_OFFSET 就是物理地址。虚拟地址和物理地址之间的映射在大部分情况下还是会通过建立页表的方式进行映射。 从 VMALLOC_START（0xffffc90000000000）开始到 VMALLOC_END（0xffffe90000000000）的 32T 的空间是给 vmalloc 的。 从 VMEMMAP_START（0xffffea0000000000）开始的 1T 空间用于存放物理页面的描述结构 struct page 的。 从 __START_KERNEL_map（0xffffffff80000000）开始的 512M 用于存放内核代码段、全局变量、BSS 等。这里对应到物理内存开始的位置，减去 __START_KERNEL_map 就能得到物理内存的地址。这里和直接映射区有点像，但是不矛盾，因为直接映射区之前有 8T 的空当区域，早就过了内核代码在物理内存中加载的位置。 总结 本文比较详细的分析了内存在用户态和内核态的结构，以此为基础，后文可以开始分析内存的管理、映射了。","tags":["Linux开发"],"categories":["linux"]},{"title":"你真的理解Linux中断机制嘛","path":"/2023/01/15/linux-docs/你真的理解Linux中断机制嘛/","content":"Linux中断是指在CPU正常运行期间，由于内外部事件或由程序预先安排的事件引起的CPU暂时停止正在运行的程序，转而为该内部或外部事件或预先安排的事件服务的程序中去，服务完毕后再返回去继续运行被暂时中断的程序。 进程的不可中断状态是系统的一种保护机制，可以保证硬件的交互过程不被意外打断。所以，短时间的不可中断状态是很正常的。但是，当进程长时间都处于不可中断状态时，你就需要提起注意力确认下是不是磁盘I&#x2F;O存在问题，相关的进程和磁盘设备是否工作正常。 今天我们详细了解一下中断的机制，进而对其中的软中断进行一个剖析。 概念解释（1）中断：是一种异步的事件处理机制，可以提高系统的并发处理能力。 （2）如何解决中断处理程序执行过长和中断丢失的问题：Linux 将中断处理过程分成了两个阶段，也就是上半部和下半部。上半部用来快速处理中断，它在中断禁止模式下运行，主要处理跟硬件紧密相关的或时间敏感的工作。也就是我们常说的硬中断，特点是快速执行。下半部用来延迟处理上半部未完成的工作，通常以内核线程的方式运行。也就是我们常说的软中断，特点是延迟执行。 （3）proc 文件系统：是一种内核空间和用户空间进行通信的机制，可以用来查看内核的数据结构，或者用来动态修改内核的配置。&#x2F;proc&#x2F;softirqs 提供了软中断的运行情况；&#x2F;proc&#x2F;interrupts 提供了硬中断的运行情况。 （4）硬中断：硬中断是由硬件产生的，比如，像磁盘，网卡，键盘，时钟等。每个设备或设备集都有它自己的IRQ（中断请求）。基于IRQ，CPU可以将相应的请求分发到对应的硬件驱动上。硬中断可以直接中断CPU，引起内核中相关的代码被触发。 （5）软中断：软中断仅与内核相关，由当前正在运行的进程所产生。 通常，软中断是一些对I&#x2F;O的请求，这些请求会调用内核中可以调度I&#x2F;O发生的程序。 软中断并不会直接中断CPU，也只有当前正在运行的代码（或进程）才会产生软中断。这种中断是一种需要内核为正在运行的进程去做一些事情（通常为I&#x2F;O）的请求。除了iowait(等待I&#x2F;O的CPU使用率)升高，软中断(softirq)CPU使用率升高也是最常见的一种性能问题。 查看软中断和内核线程小伙伴们肯定好奇该怎么查看系统里有哪些软中断？接下来将教给大家方法。前面有提到过proc文件系统，它是一种内核空间和用户空间进行通信的机制， 可以用来查看内核的数据结构，或者用来动态修改内核的配置。 &#x2F;proc&#x2F;softirqs 提供了软中断的运行情况； &#x2F;proc&#x2F;interrupts 提供了硬中断的运行情况。 （1）如何查看各种类型软中断在不同 CPU上的累积运行次数： 12345678910111213$ cat /proc/softirqs CPU0 CPU1 CPU2 CPU3 HI: 276180 286764 2509097 254357 TIMER: 1550133 1285854 1440533 1812909 NET_TX: 102895 16 15 57 NET_RX: 155 178 115 1619192 BLOCK: 1713 15048 251826 1082 IRQ_POLL: 0 0 0 0 TASKLET: 9 63 6 2830 SCHED: 1484942 1207449 1310735 1724911 HRTIMER: 0 0 0 0 RCU: 690954 685825 787447 878963 软中断的类型：对应第1列，包含了10个类别，分别对应不同的工作类型。比如说NET_RX 表示网络接收中断，而 NET_TX 表示网络发送中 断。 同一种软中断类型在不同CPU上的分布情况：对应每一行，正常情况下，同一种中断类型在不同CPU上的累计次数基本在同一个数量级。但是也有例外，比如TASKLET 拓展：什么是TASKLET？ TASKLET是最常用的软中断实现机制，每个TASKLET只会运行一次就会结束，并且只在调用它的函数所在的CPU上运行，不能并行而只能串行执行。 多个不同类型的TASKLET可以并行在多个CPU上。 软中断是静态，只能支持有限的几种软中断类型，一旦内核编译好之后就不能改变；而TASKLET灵活很多，可以通过添加内核模块的方式在运行时修改。 （2）如何查看软中断内核线程的运行状况？软中断是以内核线程的方式运行的，每个CPU都会对应一个软中断内核线程，查看的方式如下： 12345$ ps -ef|grep softirqroot 7 2 0 Nov04 ? 00:00:53 [ksoftirqd/0]root 16 2 0 Nov04 ? 00:00:51 [ksoftirqd/1]root 22 2 0 Nov04 ? 00:00:53 [ksoftirqd/2]root 28 2 0 Nov04 ? 00:00:53 [ksoftirqd/3] 这些线程的名字外面都有中括号，这说明 ps 无法获取它们的命令行参数 （cmline）。 一般来说，ps 的输出中，名字括在中括号里的，一般都是内核线程。 （3）如何查看硬中断运行情况 1234567891011121314151617181920212223242526272829303132$ cat /proc/interrupts CPU0 CPU1 CPU2 CPU3 0: 33 0 0 0 IO-APIC-edge timer 1: 10 0 0 0 IO-APIC-edge i8042 4: 325 0 0 0 IO-APIC-edge serial 8: 1 0 0 0 IO-APIC-edge rtc0 9: 0 0 0 0 IO-APIC-fasteoi acpi 10: 0 0 0 0 IO-APIC-fasteoi virtio3 40: 0 0 0 0 PCI-MSI-edge virtio1-config 41: 16669006 0 0 0 PCI-MSI-edge virtio1-requests 42: 0 0 0 0 PCI-MSI-edge virtio2-config 43: 59166530 0 0 0 PCI-MSI-edge virtio2-requests 44: 0 0 0 0 PCI-MSI-edge virtio0-config 45: 6689988 0 0 0 PCI-MSI-edge virtio0-input.0 46: 0 0 0 0 PCI-MSI-edge virtio0-output.0 47: 2093616484 0 0 0 PCI-MSI-edge peth1-TxRx-0 48: 5 2045859720 0 0 PCI-MSI-edge peth1-TxRx-1 49: 81 0 0 0 PCI-MSI-edge peth1NMI: 0 0 0 0 Non-maskable interruptsLOC: 2936184495 965056330 1641503935 1442909354 Local timer interruptsSPU: 0 0 0 0 Spurious interruptsPMI: 0 0 0 0 Performance monitoring interruptsIWI: 53775871 47387196 47737572 44243915 IRQ work interruptsRTR: 0 0 0 0 APIC ICR read retriesRES: 1198594562 964481221 966552350 902484234 Rescheduling interruptsCAL: 4294967071 4438 430547422 419910155 Function call interruptsTLB: 1206563963 65932469 1378887038 1028081848 TLB shootdownsTRM: 0 0 0 0 Thermal event interruptsTHR: 0 0 0 0 Threshold APIC interruptsMCE: 0 0 0 0 Machine check exceptionsMCP: 65623 65623 65623 65623 Machine check pollsERR: 0 工具与技巧 （1）sar 是一个系统活动报告工具，既可以实时查看系统的当前活动，又可以配置保存和报告历史统计数据。命令：sar -n DEV 1含义：-n DEV 1 表示显示网络收发的报告，间隔1秒输出一组数据 123456$ sar -n DEV 116:01:21 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil16:01:22 eth0 12605.00 6304.00 664.86 358.11 0.00 0.00 0.00 0.0116:01:22 docker0 6302.00 12604.00 270.79 664.66 0.00 0.00 0.00 0.0016:01:22 lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0016:01:22 veth9f6bbcd 6302.00 12604.00 356.95 664.66 0.00 0.00 0.00 0.05 第1列：表示报告的时间 第2列：IFACE 表示网卡 第3，4列：rxpck&#x2F;s 和 txpck&#x2F;s 分别表示每秒接收、发送的网络帧数，也就是 PPS 第5，6列：rxkB&#x2F;s 和 txkB&#x2F;s 分别表示每秒接收、发送的千字节数，也就是 BPS。 （2）tcpdump 是一个常用的网络抓包工具，常用来分析各种网络问题。 命令：tcpdump -i eth0 -n tcp port 80含义：-i eth0 只抓取eth0网卡，-n不解析协议名和主机名 tcp port 80表示只抓取tcp协议并且端口号为80的网络帧","tags":["Linux开发"],"categories":["linux"]},{"title":"浅谈Linux内核之CPU缓存","path":"/2023/01/15/linux-docs/浅谈Linux内核之CPU缓存/","content":"一、什么是CPU缓存1. CPU缓存的来历众所周知,CPU是计算机的大脑，它负责执行程序的指令，而内存负责存数据, 包括程序自身的数据。在很多年前，CPU的频率与内存总线的频率在同一层面上。内存的访问速度仅比寄存器慢一些。但是，这一局面在上世纪90年代被打破了。CPU的频率大大提升，但内存总线的频率与内存芯片的性能却没有得到成比例的提升。并不是因为造不出更快的内存，只是因为太贵了。内存如果要达到目前CPU那样的速度，那么它的造价恐怕要贵上好几个数量级。所以，CPU的运算速度要比内存读写速度快很多，这样会使CPU花费很长的时间等待数据的到来或把数据写入到内存中。所以，为了解决CPU运算速度与内存读写速度不匹配的矛盾，就出现了CPU缓存。 2. CPU缓存的概念CPU缓存是位于CPU与内存之间的临时数据交换器，它的容量比内存小的多但是交换速度却比内存要快得多。CPU缓存一般直接跟CPU芯片集成或位于主板总线互连的独立芯片上。 为了简化与内存之间的通信，高速缓存控制器是针对数据块，而不是字节进行操作的。高速缓存其实就是一组称之为缓存行(Cache Line)的固定大小的数据块组成的，典型的一行是64字节。 3. CPU缓存的意义CPU往往需要重复处理相同的数据、重复执行相同的指令，如果这部分数据、指令CPU能在CPU缓存中找到，CPU就不需要从内存或硬盘中再读取数据、指令，从而减少了整机的响应时间。所以，缓存的意义满足以下两种局部性原理： 时间局部性（Temporal Locality）：如果一个信息项正在被访问，那么在近期它很可能还会被再次访问。 空间局部性（Spatial Locality）：如果一个存储器的位置被引用，那么将来他附近的位置也会被引用。 二、CPU的三级缓存1. CPU的三级缓存随着多核CPU的发展，CPU缓存通常分成了三个级别：L1，L2，L3。级别越小越接近CPU，所以速度也更快，同时也代表着容量越小。L1 是最接近CPU的, 它容量最小（例如：32K），速度最快，每个核上都有一个 L1 缓存，L1 缓存每个核上其实有两个 L1 缓存, 一个用于存数据的 L1d Cache（Data Cache），一个用于存指令的 L1i Cache（Instruction Cache）。L2 缓存 更大一些（例如：256K），速度要慢一些, 一般情况下每个核上都有一个独立的L2 缓存; L3 缓存是三级缓存中最大的一级（例如3MB），同时也是最慢的一级, 在同一个CPU插槽之间的核共享一个 L3 缓存。下面是三级缓存的处理速度参考表： 下图是Intel Core i5-4285U的CPU三级缓存示意图： 就像数据库缓存一样，获取数据时首先会在最快的缓存中找数据，如果缓存没有命中(Cache miss) 则往下一级找, 直到三级缓存都找不到时，那只有向内存要数据了。一次次地未命中，代表取数据消耗的时间越长。 2. 带有高速缓存CPU执行计算的流程1、程序以及数据被加载到主内存2、指令和数据被加载到CPU的高速缓存3、CPU执行指令，把结果写到高速缓存4、高速缓存中的数据写回主内存 目前流行的多级缓存结构如下图： 三、CPU缓存一致性协议(MESI)MESI（Modified Exclusive Shared Or Invalid）(也称为伊利诺斯协议，是因为该协议由伊利诺斯州立大学提出的）是一种广泛使用的支持写回策略的缓存一致性协议。为了保证多个CPU缓存中共享数据的一致性，定义了缓存行(Cache Line)的四种状态，而CPU对缓存行的四种操作可能会产生不一致的状态，因此缓存控制器监听到本地操作和远程操作的时候，需要对地址一致的缓存行的状态进行一致性修改，从而保证数据在多个缓存之间保持一致性。 1. MESI协议中的状态CPU中每个缓存行（Cache line)使用4种状态进行标记，使用2bit来表示: 注意：对于M和E状态而言总是精确的，他们在和该缓存行的真正状态是一致的，而S状态可能是非一致的。如果一个缓存将处于S状态的缓存行作废了，而另一个缓存实际上可能已经独享了该缓存行，但是该缓存却不会将该缓存行升迁为E状态，这是因为其它缓存不会广播他们作废掉该缓存行的通知，同样由于缓存并没有保存该缓存行的copy的数量，因此（即使有这种通知）也没有办法确定自己是否已经独享了该缓存行。 从上面的意义看来E状态是一种投机性的优化：如果一个CPU想修改一个处于S状态的缓存行，总线事务需要将所有该缓存行的copy变成invalid状态，而修改E状态的缓存不需要使用总线事务。 MESI状态转换图： 下图表示了当一个缓存行(Cache line)的调整的状态的时候，另外一个缓存行(Cache line)需要调整的状态。 举个示例： 假设cache 1 中有一个变量x &#x3D; 0的 Cache line 处于S状态(共享)。那么其他拥有x变量的 cache 2、cache 3 等x的 Cache line调整为S状态（共享）或者调整为I状态（无效）。 2. 多核缓存协同操作(1) 内存变量假设有三个CPU A、B、C，对应三个缓存分别是cache a、b、c。在主内存中定义了x的引用值为0。 (2) 单核读取执行流程是： CPU A发出了一条指令，从主内存中读取x。 从主内存通过 bus 读取到 CPU A 的缓存中（远端读取 Remote read）,这时该 Cache line 修改为 E 状态（独享）。 (3) 双核读取执行流程是： CPU A发出了一条指令，从主内存中读取x。 CPU A从主内存通过bus读取到 cache a 中并将该 Cache line 设置为E状态。 CPU B发出了一条指令，从主内存中读取x。 CPU B试图从主内存中读取x时，CPU A检测到了地址冲突。这时CPU A对相关数据做出响应。此时x存储于 cache a 和 cache b 中，x在 chche a 和 cache b 中都被设置为S状态(共享)。 (4) 修改数据执行流程是： CPU A 计算完成后发指令需要修改x. CPU A 将x设置为M状态（修改）并通知缓存了x的 CPU B, CPU B 将本地 cache b 中的x设置为I状态(无效) CPU A 对x进行赋值。 (5) 同步数据那么执行流程是： CPU B 发出了要读取x的指令。 CPU B 通知CPU A,CPU A将修改后的数据同步到主内存时cache a 修改为E（独享） CPU A同步CPU B的x,将cache a和同步后cache b中的x设置为S状态（共享）。 3. CPU 存储模型简介MESI协议为了保证多个 CPU cache 中共享数据的一致性，定义了 Cache line 的四种状态，而 CPU 对 cache 的4种操作可能会产生不一致状态，因此 cache 控制器监听到本地操作和远程操作的时候，需要对地址一致的 Cache line 状态做出一定的修改，从而保证数据在多个cache之间流转的一致性。 但是，缓存的一致性消息传递是要时间的，这就使得状态切换会有更多的延迟。某些状态的切换需要特殊的处理，可能会阻塞处理器。这些都将会导致各种各样的稳定性和性能问题。比如你需要修改本地缓存中的一条信息，那么你必须将I（无效）状态通知到其他拥有该缓存数据的CPU缓存中，并且等待确认。等待确认的过程会阻塞处理器，这会降低处理器的性能。因为这个等待远远比一个指令的执行时间长的多。所以，为了为了避免这种阻塞导致时间的浪费，引入了存储缓存(Store Buffer)和无效队列(Invalidate Queue)。 (1) 存储缓存在没有存储缓存时，CPU 要写入一个量，有以下情况： 量不在该 CPU 缓存中，则需要发送 Read Invalidate 信号，再等待此信号返回，之后再写入量到缓存中。 量在该 CPU 缓存中，如果该量的状态是 Exclusive 则直接更改。而如果是 Shared 则需要发送 Invalidate 消息让其它 CPU 感知到这一更改后再更改。 这些情况中，很有可能会触发该 CPU 与其它 CPU 进行通讯，接着需要等待它们回复。这会浪费大量的时钟周期！为了提高效率，可以使用异步的方式去处理：先将值写入到一个 Buffer 中，再发送通讯的信号，等到信号被响应，再应用到 cache 中。并且此 Buffer 能够接受该 CPU 读值。这个 Buffer 就是 Store Buffer。而不须要等待对某个量的赋值指令的完成才继续执行下一条指令，直接去 Store Buffer 中读该量的值，这种优化叫Store Forwarding。 (2) 无效队列同理，解决了主动发送信号端的效率问题，那么，接受端 CPU 接受到 Invalidate 信号后如果立即采取相应行动(去其它 CPU 同步值)，再返回响应信号，则时钟周期也太长了，此处也可优化。接受端 CPU 接受到信号后不是立即采取行动，而是将 Invalidate 信号插入到一个队列 Queue 中，立即作出响应。等到合适的时机，再去处理这个 Queue 中的 Invalidate 信号，并作相应处理。这个 Queue 就是Invalidate Queue。 四、乱序执行乱序执行（out-of-orderexecution）：是指CPU允许将多条指令不按程序规定的顺序分开发送给各相应电路单元处理的技术。这样将根据各电路单元的状态和各指令能否提前执行的具体情况分析后，将能提前执行的指令立即发送给相应电路。 这好比请A、B、C三个名人为晚会题写横幅“春节联欢晚会”六个大字，每人各写两个字。如果这时在一张大纸上按顺序由A写好”春节”后再交给B写”联欢”，然后再由C写”晚会”，那么这样在A写的时候，B和C必须等待，而在B写的时候C仍然要等待而A已经没事了。 但如果采用三个人分别用三张纸同时写的做法， 那么B和C都不必须等待就可以同时各写各的了，甚至C和B还可以比A先写好也没关系（就象乱序执行），但当他们都写完后就必须重新在横幅上（自然可以由别人做，就象CPU中乱序执行后的重新排列单元）按”春节联欢晚会”的顺序排好才能挂出去。 所以，CPU 为什么会有乱序执行优化？本质原因是CPU为了效率，将长费时的操作“异步”执行，排在后面的指令不等前面的指令执行完毕就开始执行后面的指令。而且允许排在前面的长费时指令后于排在后面的指令执行完。 CPU 执行乱序主要有以下几种： **写写乱序(store store)**：a&#x3D;1;b&#x3D;2; -&gt; b&#x3D;2;a&#x3D;1; **写读乱序(store load)**：a&#x3D;1;load(b); -&gt; load(b);a&#x3D;1; **读读乱序(load load)**：load(a);load(b); -&gt; load(b);load(a); **读写乱序(load store)**：load(a);b&#x3D;2; -&gt; b&#x3D;2;load(a); 总而言之，CPU的乱序执行优化指的是处理器为提高运算速度而做出违背代码原有顺序的优化。","tags":["Linux开发"],"categories":["linux"]},{"title":"系统调用","path":"/2023/01/15/linux-docs/系统调用/","content":"一. 前言 通过前面几篇文章，我们分析了从按下电源键到内核启动、完成初始化的整个过程。在后面的文章中我们将分别深入剖析Linux内核各个重要部分的源码。考虑到后面的部分我们会从用户态的代码开始入手一步一步深入，因此在分析这些之前，我们需要仔细看一看如何实现一个从用户态到内核态再回到用户态的系统调用的全过程，即系统调用的实现。 本文的说明顺序如下： 首先从一个简单的例子开始分析glibc中对应的调用 针对32位和64位中调用的结构不同会分开两部分单独介绍，会介绍整个调用至完成的过程。即用户态-&gt;内核态-&gt;用户态 在整个调用过程中最重要的一步是中间访问系统调用表，该部分为了描述清楚单独拉出来最后介绍 二. GLIBC标准库的调用 让我们从一个简单的程序开始 1234567891011121314#include &lt;stdio.h&gt;int main(int argc, char **argv)&#123; FILE *fp; char buff[255]; fp = fopen(&quot;test.txt&quot;, &quot;r&quot;); fgets(buff, 255, fp); printf(&quot;%s &quot;, buff); fclose(fp); return 0;&#125; 如上所示的程序主要调用了glibc中的函数，然后在其上进行了封装而成。比如fopen实际使用的是open，这里我们就以该函数为例来说明整个调用过程。首先open函数的系统调用在syscalls.list表中定义 12# File name Caller Syscall name Args Strong name Weak namesopen - open Ci:siv __libc_open __open open 根据此配置文件，glibc会调用脚本make_syscall.sh将其封装为宏，如SYSCALL_NAME open的形式。这些宏会通过T_PSEUDO来调用（位于syscall-template.S），而实际上使用的则是DO_CALL(syscall_name, args) 123456789101112T_PSEUDO (SYSCALL_SYMBOL, SYSCALL_NAME, SYSCALL_NARGS) retT_PSEUDO_END (SYSCALL_SYMBOL)#define T_PSEUDO(SYMBOL, NAME, N) PSEUDO (SYMBOL, NAME, N)#define PSEUDO(name, syscall_name, args) \\ .text; \\ ENTRY (name) \\ DO_CALL (syscall_name, args); \\ cmpl $-4095, %eax; \\ jae SYSCALL_ERROR_LABEL 2.1 32位系统调用过程 考虑到32位和64位代码结构有一些区别，因此这里需要分开讨论。在32位系统中，DO_CALL()位于i386 目录下的sysdep.h文件中 12345678910111213141516/* Linux takes system call arguments in registers: syscall number %eax call-clobbered arg 1 %ebx call-saved arg 2 %ecx call-clobbered arg 3 %edx call-clobbered arg 4 %esi call-saved arg 5 %edi call-saved arg 6 %ebp call-saved......*/#define DO_CALL(syscall_name, args) \\ PUSHARGS_##args \\ DOARGS_##args \\ movl $SYS_ify (syscall_name), %eax; \\ ENTER_KERNEL \\ POPARGS_##args 这里，我们将请求参数放在寄存器里面，根据系统调用的名称，得到系统调用号，放在寄存器 eax 里面，然后执行ENTER_KERNEL。 1# define ENTER_KERNEL int $0x80 ENTER_KERNEL实际调用的是80软中断，以此陷入内核。这些中断在trap_init()中被定义并初始化。在前文中对trap_init()已有一些简单的叙述，后面在中断部分会再详细介绍。 初始化好的中断表会等待到中断触发，触发的时候则调用对应的回调函数，这里的话就是entry_INT80_32。该中断首先通过push和SAVE_ALL保存所有的寄存器，存储在pt_regs中，然后调用do_syscall_32_irqs_on()函数。该函数将系统调用号从eax里面取出来，然后根据系统调用号，在系统调用表中找到相应的函数进行调用，并将寄存器中保存的参数取出来，作为函数参数。最后调用INTERRUPT_RETURN，实际使用的是iret指令将原来用户保存的现场包含代码段、指令指针寄存器等恢复，并返回至用户态执行。 1234567891011121314151617181920212223242526ENTRY(entry_INT80_32) ASM_CLAC pushl %eax /* pt_regs-&gt;orig_ax */ SAVE_ALL pt_regs_ax=$-ENOSYS /* save rest */ movl %esp, %eax call do_syscall_32_irqs_on.Lsyscall_32_done:.......Lirq_return: INTERRUPT_RETURN ...... static __always_inline void do_syscall_32_irqs_on(struct pt_regs *regs)&#123; struct thread_info *ti = current_thread_info(); unsigned int nr = (unsigned int)regs-&gt;orig_ax;...... if (likely(nr &lt; IA32_NR_syscalls)) &#123; regs-&gt;ax = ia32_sys_call_table[nr]( (unsigned int)regs-&gt;bx, (unsigned int)regs-&gt;cx, (unsigned int)regs-&gt;dx, (unsigned int)regs-&gt;si, (unsigned int)regs-&gt;di, (unsigned int)regs-&gt;bp); &#125; syscall_return_slowpath(regs);&#125; 2.2 64位系统调用过程 对于64位系统来说，DO_CALL位于x86_64 目录下的 sysdep.h 文件中 1234567891011121314/* The Linux/x86-64 kernel expects the system call parameters in registers according to the following table: syscall number rax arg 1 rdi arg 2 rsi arg 3 rdx arg 4 r10 arg 5 r8 arg 6 r9......*/#define DO_CALL(syscall_name, args) \\ lea SYS_ify (syscall_name), %rax; \\ syscall 和之前一样，还是将系统调用名称转换为系统调用号，放到寄存器rax。这里是真正进行调用，不是用中断了，而是改用syscall指令了。并且，通过注释我们也可以知道，传递参数的寄存器也变了。syscall指令还使用了一种特殊的寄存器，我们叫特殊模块寄存器（Model Specific Registers，简称 MSR）。这种寄存器是 CPU 为了完成某些特殊控制功能为目的的寄存器，其中就有系统调用。 在系统初始化的时候，trap_init() 除了初始化上面的中断模式，这里面还会调用 cpu_init-&gt;syscall_init()。这里面有这样的代码： 1wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64); rdmsr() 和 wrmsr() 是用来读写特殊模块寄存器的。MSR_LSTAR 就是这样一个特殊的寄存器，当 syscall 指令调用的时候，会从这个寄存器里面拿出函数地址来调用，也就是调用 entry_SYSCALL_64。在 arch&#x2F;x86&#x2F;entry&#x2F;entry_64.S中定义了 entry_SYSCALL_64函数。 该函数开始于一条宏：SWAPGS_UNSAFE_STACK，其定义如下，主要是交换当前GS基寄存器中的值和特殊模块寄存器中包含的值，即进入内核栈。 1#define SWAPGS_UNSAFE_STACK swapgs 对于旧的栈，我们会将其存于rsp_scratch，并将栈指针移至当前进程的栈顶。 12movq %rsp, PER_CPU_VAR(rsp_scratch)movq PER_CPU_VAR(cpu_current_top_of_stack), %rsp 下一步，我们将栈段和旧的栈指针压入栈中 12pushq $__USER_DSpushq PER_CPU_VAR(rsp_scratch) 接下来，我们需要打开中断并保存很多寄存器到 pt_regs 结构里面，例如用户态的代码段、数据段、保存参数的寄存器，并校验当前线程的信息_TIF_WORK_SYSCALL_ENTRY，这里涉及到Linux的debugging和tracing技术，会单独在后文中详细分析。该部分代码具体如下所示。 123456789101112131415161718192021ENTRY(entry_SYSCALL_64) /* Construct struct pt_regs on stack */ pushq $__USER_DS /* pt_regs-&gt;ss */ pushq PER_CPU_VAR(rsp_scratch) /* pt_regs-&gt;sp */ pushq %r11 /* pt_regs-&gt;flags */ pushq $__USER_CS /* pt_regs-&gt;cs */ pushq %rcx /* pt_regs-&gt;ip */ pushq %rax /* pt_regs-&gt;orig_ax */ pushq %rdi /* pt_regs-&gt;di */ pushq %rsi /* pt_regs-&gt;si */ pushq %rdx /* pt_regs-&gt;dx */ pushq %rcx /* pt_regs-&gt;cx */ pushq $-ENOSYS /* pt_regs-&gt;ax */ pushq %r8 /* pt_regs-&gt;r8 */ pushq %r9 /* pt_regs-&gt;r9 */ pushq %r10 /* pt_regs-&gt;r10 */ pushq %r11 /* pt_regs-&gt;r11 */ sub $(6*8), %rsp /* pt_regs-&gt;bp, bx, r12-15 not saved */ movq PER_CPU_VAR(current_task), %r11 testl $_TIF_WORK_SYSCALL_ENTRY|_TIF_ALLWORK_MASK, TASK_TI_flags(%r11) jnz entry_SYSCALL64_slow_path 各寄存器的作用如下所示： rax：系统调用数目 rcx：函数返回的用户空间地址 r11：寄存器标记 rdi：系统调用回调函数的第一个参数 rsi：系统调用回调函数的第二个参数 rdx：系统调用回调函数的第三个参数 r10：系统调用回调函数的第四个参数 r8 ：系统调用回调函数的第五个参数 r9 ：系统调用回调函数的第六个参数 rbp,rbx,r12-r15：通用的callee-preserved寄存器 在此之后，其实存在着两个处理分支：entry_SYSCALL64_slow_path 和 entry_SYSCALL64_fast_path，这里是根据_TIF_WORK_SYSCALL_ENTRY判断的结果进行选择，这里涉及到ptrace部分的知识，暂时先不介绍了，会在后面单独开一文详细研究。如果设置了_TIF_ALLWORK_MASK或者_TIF_WORK_SYSCALL_ENTRY，则跳转至slow_path，否则继续运行fast_path。 1234#define _TIF_WORK_SYSCALL_ENTRY \\ (_TIF_SYSCALL_TRACE | _TIF_SYSCALL_EMU | _TIF_SYSCALL_AUDIT | \\ _TIF_SECCOMP | _TIF_SINGLESTEP | _TIF_SYSCALL_TRACEPOINT | \\ _TIF_NOHZ) 2.2.1 fastpath分支 该分支主要分为以下部分内容 再次检测TRACE部分，如果有标记则跳转至slow_path 检测__SYSCALL_MASK，如果CONFIG_X86_X32_ABI未设置我们就比较rax寄存器的值和最大系统调用数__NR_syscall_max，否则则标记eax寄存器为__x32_SYSCALL_BIT，再进行比较 ja指令会在CF和ZF设置为0时进行跳转，即如果不满足条件则会跳转至-ENOSYS，否则继续执行 将第四个参数从r10放入rcx以保持x86_64 C ABI编译 执行sys_call_table，去系统调用表中查找系统调用 12345678910111213141516171819202122232425262728293031323334entry_SYSCALL_64_fastpath: /* * Easy case: enable interrupts and issue the syscall. If the syscall * needs pt_regs, we&#x27;ll call a stub that disables interrupts again * and jumps to the slow path. */ TRACE_IRQS_ON ENABLE_INTERRUPTS(CLBR_NONE) #if __SYSCALL_MASK == ~0 cmpq\t$__NR_syscall_max, %rax#else andl\t$__SYSCALL_MASK, %eax cmpl\t$__NR_syscall_max, %eax#endif ja\t1f /* return -ENOSYS (already in pt_regs-&gt;ax) */ movq\t%r10, %rcx /* * This call instruction is handled specially in stub_ptregs_64. * It might end up jumping to the slow path. If it jumps, RAX * and all argument registers are clobbered. */ call\t*sys_call_table(, %rax, 8) ......# ifdef CONFIG_X86_X32_ABI# define __SYSCALL_MASK (~(__X32_SYSCALL_BIT))# else# define __SYSCALL_MASK (~0)# endif#define __X32_SYSCALL_BIT 0x40000000 2.2.2 slow_path分支 slow_path部分的源码如下 12345entry_SYSCALL64_slow_path: /* IRQs are off. */ SAVE_EXTRA_REGS movq %rsp, %rdi call do_syscall_64 /* returns with IRQs disabled */ slow_path会调用entry_SYSCALL64_slow_pat-&gt;do_syscall_64()，执行完毕后恢复寄存器，最后调用USERGS_SYSRET64，实际使用sysretq指令返回。 123456789101112return_from_SYSCALL_64: RESTORE_EXTRA_REGS TRACE_IRQS_IRETQ movq RCX(%rsp), %rcx movq RIP(%rsp), %r11 movq R11(%rsp), %r11......syscall_return_via_sysret: /* rcx and r11 are already restored (see code above) */ RESTORE_C_REGS_EXCEPT_RCX_R11 movq RSP(%rsp), %rsp USERGS_SYSRET64 在 do_syscall_64 里面，从 rax里面拿出系统调用号，然后根据系统调用号，在系统调用表 sys_call_table 中找到相应的函数进行调用，并将寄存器中保存的参数取出来，作为函数参数。 123456789101112__visible void do_syscall_64(struct pt_regs *regs)&#123; struct thread_info *ti = current_thread_info(); unsigned long nr = regs-&gt;orig_ax;...... if (likely((nr &amp; __SYSCALL_MASK) &lt; NR_syscalls)) &#123; regs-&gt;ax = sys_call_table[nr &amp; __SYSCALL_MASK]( regs-&gt;di, regs-&gt;si, regs-&gt;dx, regs-&gt;r10, regs-&gt;r8, regs-&gt;r9); &#125; syscall_return_slowpath(regs);&#125; 至此，32位和64位又回到了同样的位置：查找系统调用表sys_call_table。 三. 系统调用表的生成 32位和64位的sys_call_table均位于arch&#x2F;x86&#x2F;entry&#x2F;syscalls&#x2F;目录下，分别为syscall_32.tbl和syscall_64.tbl。如下所示为32位和64位中open函数的定义 1235 i386 open sys_open compat_sys_open2 common open sys_open 第一列的数字是系统调用号。可以看出，32 位和 64 位的系统调用号是不一样的。第三列是系统调用的名字，第四列是系统调用在内核的实现函数。不过，它们都是以 sys_ 开头。系统调用在内核中的实现函数要有一个声明。声明往往在 include&#x2F;linux&#x2F;syscalls.h文件中。例如 sys_open 是这样声明的： 12asmlinkage long sys_open(const char __user *filename, int flags, umode_t mode); 真正的实现这个系统调用，一般在一个.c 文件里面，例如 sys_open 的实现在 fs&#x2F;open.c 里面。其中采用了宏的方式对函数名进行了封装，实际拆开是一样的。 1234567891011121314151617181920SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode)&#123; if (force_o_largefile()) flags |= O_LARGEFILE; return do_sys_open(AT_FDCWD, filename, flags, mode);&#125;......asmlinkage long sys_open(const char __user * filename, int flags, int mode)&#123; long ret; if (force_o_largefile()) flags |= O_LARGEFILE; ret = do_sys_open(AT_FDCWD, filename, flags, mode); asmlinkage_protect(3, ret, filename, flags, mode); return ret;&#125; 其中SYSCALL_DEFINE3 是一个宏系统调用最多六个参数，根据参数的数目选择宏。具体是这样定义如下所示，首先使用SYSCALL_METADATA()宏解决syscall_metada结构体的初始化，该结构体包括了不同的有用区域包括系统调用的名字、系统调用表中对应的序号、系统调用的参数、参数类型链表等。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#define SYSCALL_DEFINE1(name, ...) SYSCALL_DEFINEx(1, _##name, __VA_ARGS__)#define SYSCALL_DEFINE2(name, ...) SYSCALL_DEFINEx(2, _##name, __VA_ARGS__)#define SYSCALL_DEFINE3(name, ...) SYSCALL_DEFINEx(3, _##name, __VA_ARGS__)#define SYSCALL_DEFINE4(name, ...) SYSCALL_DEFINEx(4, _##name, __VA_ARGS__)#define SYSCALL_DEFINE5(name, ...) SYSCALL_DEFINEx(5, _##name, __VA_ARGS__)#define SYSCALL_DEFINE6(name, ...) SYSCALL_DEFINEx(6, _##name, __VA_ARGS__)#define SYSCALL_DEFINEx(x, sname, ...) \\ SYSCALL_METADATA(sname, x, __VA_ARGS__) \\ __SYSCALL_DEFINEx(x, sname, __VA_ARGS__)#define __PROTECT(...) asmlinkage_protect(__VA_ARGS__)#define __SYSCALL_DEFINEx(x, name, ...) \\ asmlinkage long sys##name(__MAP(x,__SC_DECL,__VA_ARGS__)) \\ __attribute__((alias(__stringify(SyS##name)))); \\ static inline long SYSC##name(__MAP(x,__SC_DECL,__VA_ARGS__)); \\ asmlinkage long SyS##name(__MAP(x,__SC_LONG,__VA_ARGS__)); \\ asmlinkage long SyS##name(__MAP(x,__SC_LONG,__VA_ARGS__)) \\ &#123; \\ long ret = SYSC##name(__MAP(x,__SC_CAST,__VA_ARGS__)); \\ __MAP(x,__SC_TEST,__VA_ARGS__); \\ __PROTECT(x, ret,__MAP(x,__SC_ARGS,__VA_ARGS__)); \\ return ret; \\ &#125; \\ static inline long SYSC##name(__MAP(x,__SC_DECL,__VA_ARGS__).......... #define SYSCALL_METADATA(sname, nb, ...) \\ ... \\ ... \\ ... \\ struct syscall_metadata __used \\ __syscall_meta_##sname = &#123; \\ .name = &quot;sys&quot;#sname, \\ .syscall_nr = -1, \\ .nb_args = nb, \\ .types = nb ? types_##sname : NULL, \\ .args = nb ? args_##sname : NULL, \\ .enter_event = &amp;event_enter_##sname, \\ .exit_event = &amp;event_exit_##sname, \\ .enter_fields = LIST_HEAD_INIT(__syscall_meta_##sname.enter_fields), \\ &#125;; \\ static struct syscall_metadata __used \\ __attribute__((section(&quot;__syscalls_metadata&quot;))) \\ *__p_syscall_meta_##sname = &amp;__syscall_meta_##sname; 在编译的过程中，需要根据 syscall_32.tbl 和 syscall_64.tbl 生成自己的syscalls_32.h 和 syscalls_64.h。生成方式在 arch&#x2F;x86&#x2F;entry&#x2F;syscalls&#x2F;Makefile 中。这里面会使用两个脚本 第一个脚本arch&#x2F;x86&#x2F;entry&#x2F;syscalls&#x2F;syscallhdr.sh，会在文件中生成 #define __NR_open； 第二个脚本 arch&#x2F;x86&#x2F;entry&#x2F;syscalls&#x2F;syscalltbl.sh，会在文件中生成__SYSCALL(__NR_open, sys_open)。 这样最终生成syscalls_32.h 和 syscalls_64.h 就保存了系统调用号和系统调用实现函数之间的对应关系，如下所示 12345678__SYSCALL_COMMON(0, sys_read, sys_read)__SYSCALL_COMMON(1, sys_write, sys_write)__SYSCALL_COMMON(2, sys_open, sys_open)__SYSCALL_COMMON(3, sys_close, sys_close)__SYSCALL_COMMON(5, sys_newfstat, sys_newfstat)......... 其中__SYSCALL_COMMON宏定义如下，主要是将对应的数字序号和系统调用名对应 12#define __SYSCALL_COMMON(nr, sym, compat) __SYSCALL_64(nr, sym, compat)#define __SYSCALL_64(nr, sym, compat) [nr] = sym, 最终形成的表如下 123456789asmlinkage const sys_call_ptr_t sys_call_table[__NR_syscall_max+1] = &#123; [0 ... __NR_syscall_max] = &amp;sys_ni_syscall, [0] = sys_read, [1] = sys_write, [2] = sys_open, ... ... ...&#125;; 最后，所有的系统调用会存储在arch&#x2F;x86&#x2F;entry&#x2F;目录下的syscall_32.c和syscall_64.c中，里面包含了syscalls_32.h 和 syscalls_64.h 头文件，其形式如下： 123456789101112131415161718__visible const sys_call_ptr_t ia32_sys_call_table[__NR_syscall_compat_max+1] = &#123; /* * Smells like a compiler bug -- it doesn&#x27;t work * when the &amp; below is removed. */ [0 ... __NR_syscall_compat_max] = &amp;sys_ni_syscall,#include &lt;asm/syscalls_32.h&gt;&#125;;/* System call table for x86-64. */asmlinkage const sys_call_ptr_t sys_call_table[__NR_syscall_max+1] = &#123; /* * Smells like a compiler bug -- it doesn&#x27;t work * when the &amp; below is removed. */ [0 ... __NR_syscall_max] = &amp;sys_ni_syscall,#include &lt;asm/syscalls_64.h&gt;&#125;; 其中__NR_syscall_max宏定义规定了最大系统调用数量，该数量取决于操作系统的架构，在X86下定义如下 1#define __NR_syscall_max 547 这里还需要注意sys_call_ptr_t表示指向系统调用表的指针，定义为函数指针 1typedef void (*sys_call_ptr_t)(void); 系统调用表数组中的每一个系统调用均会指向sys_ni_syscall，该函数表示一个未实现的系统调用（not-implement），从而系统调用表的初始化。 123456asmlinkage long sys_ni_syscall(void)&#123; return -ENOSYS;&#125;ENOSYS Function not implemented (POSIX.1) 由此，整个系统调用表的生成过程就全部说明完了，而在实际产生系统调用的时候，过程则刚好相反： 用户态调用syscall syscall导致中断，程序由用户态陷入内核态 内核C函数执行syscalls_32&#x2F;64.c，并由此获得对应关系最终在对应的源码中找到函数实现 针对对应的sys_syscall_name函数，做好调用准备工作，如初始化系统调用入口、保存寄存器、切换新的栈、构造新的task以备中断回调等。 调用函数实现 切换寄存器、栈，返回用户态 四. 总结 本文较为深入的分析了系统调用的整个过程，并着重分析了系统调用表的形成和使用原理，如有遗漏错误还请多多指正。","tags":["Linux开发"],"categories":["linux"]},{"title":"计算机Intel CPU体系结构分析","path":"/2023/01/15/linux-docs/计算机Intel CPU体系结构分析/","content":"前段meldown漏洞事件的影响，那段时间也正好在读Paul的论文关于内存屏障的知识，其中有诸多细节想不通，便陷入无尽的煎熬和冥想中，看了《计算机系统结构》、《深入理解计算机系统》、《大话处理器》等经典书籍，也在google上搜了一大堆资料，前前后后、断断续续地折腾了一个多月，终于想通了，现在把自己的思想心得记录下来，希望对有这方面困惑的朋友有些帮助。 本文主要关注以下几个问题。 什么是CPU的流水线？为什么需要流水线？ 为什么需要内存屏障？在只有单个Core的CPU中是否还需要内存屏障？ 什么是乱序执行？分为几种？ MOB和ROB是干什么的？ load buffer和store buffer的功能是什么？ x86和arm、power中的memory model有什么区别？ MESI主要是做什么的？ meldown漏洞的原理是什么？ 1、CPU指令的执行过程几乎所有的冯·诺伊曼型计算机的CPU，其工作都可以分为 5 个阶段：取指令、指令译码、执行指令、访存取数、结果写回。 1.1取指令阶段取指令（Instruction Fetch，IF）阶段是将一条指令从主存中取到指令寄存器的过程。 程序计数器 PC 中的数值，用来指示当前指令在主存中的位置。当一条指令被取出后，PC 中的数值将根据指令字长度而自动递增：若为单字长指令，则(PC)+1-&gt;PC；若为双字长指令，则(PC)+2-&gt;PC，依此类推。 1.2指令译码阶段取出指令后，计算机立即进入指令译码（Instruction Decode，ID）阶段。 在指令译码阶段，指令译码器按照预定的指令格式，对取回的指令进行拆分和解释，识别区分出不同的指令类别以及各种获取操作数的方法。在组合逻辑控制的计算机中，指令译码器对不同的指令操作码产生不同的控制电位，以形成不同的微操作序列；在微程序控制的计算机中，指令译码器用指令操作码来找到执行该指令的微程序的入口，并从此入口开始执行。 在传统的设计里，CPU中负责指令译码的部分是无法改变的。不过，在众多运用微程序控制技术的新型 CPU 中，微程序有时是可重写的。 1.3执行指令阶段在取指令和指令译码阶段之后，接着进入执行指令（Execute，EX）阶段。 此阶段的任务是完成指令所规定的各种操作，具体实现指令的功能。为此，CPU 的不同部分被连接起来，以执行所需的操作。 例如，如果要求完成一个加法运算，算术逻辑单元 ALU 将被连接到一组输入和一组输出，输入端提供需要相加的数值，输出端将含有最后的运算结果。 1.4访存取数阶段根据指令需要，有可能要访问主存，读取操作数，这样就进入了访存取数（Memory，MEM）阶段。 此阶段的任务是：根据指令地址码，得到操作数在主存中的地址，并从主存中读取该操作数用于运算。 1.5结果写回阶段作为最后一个阶段，结果写回（Writeback，WB）阶段把执行指令阶段的运行结果数据“写回”到某种存储形式：结果数据经常被写到CPU内部寄存器中，以便被后续的指令快速地存取；在有些情况下， 结果数据也可被写入相对较慢、但较廉价且容量较大的主存。许多指令还会改变程序状态字寄存器中标志位 的状态，这些标志位标识着不同的操作结果，可被用来影响程序的动作。 在指令执行完毕、结果数据写回之后，若无意外事件（如结果溢出等）发生，计算机就接着从程序计数器PC中取得下一条指令地址，开始新一轮的循环，下一个指令周期将顺序取出下一条指令。 许多新型 CPU 可以同时取出、译码和执行多条指令，体现并行处理的特性。 2、CPU指令流水线在任一条指令的执行过程中，各个功能部件都会随着指令执行的进程而呈现出时忙时闲的现象。要加快计算机的工作速度，就应使各个功能部件并行工作，即以各自可能的高速度同时、不停地工作，使得各部件的操作在时间上重叠进行，实现流水式作业。 从原理上说，计算机的流水线（Pipeline）工作方式就是将一个计算任务细分成若干个子任务，每个子任务都由专门的功能部件进行处理，一个计算任务的各个子任务由流水线上各个功能部件轮流进行处理 （即各子任务在流水线的各个功能阶段并发执行），最终完成工作。这样，不必等到上一个计算任务完成， 就可以开始下一个计算任务的执行。 流水线的硬件基本结构如图2所示。流水线由一系列串联的功能部件（Si）组成，各个功能部件之间设有高速缓冲寄存器（L），以暂时保存上一功能部件对子任务处理的结果，同时又能够接受新的处理任务。在一个统一的时钟（C）控制下，计算任务从功能部件的一个功能段流向下一个功能段。在流水线中， 所有功能段同时对不同的数据进行不同的处理，各个处理步骤并行地操作。 当任务连续不断地输入流水线时，在流水线的输出端便连续不断地输出执行结果，流水线达到不间断流水的稳定状态，从而实现了子任务级的并行。 当指令流不能顺序执行时，流水过程会中断（即断流）。为了保证流水过程的工作效率，流水过程不应经常断流。在一个流水过程中，实现各个流水过程的各个功能段所需要的时间应该尽可能保持相等，以避免产生瓶颈，导致流水线断流。 流水线技术本质上是将一个重复的时序过程分解成若干个子过程，而每一个子过程都可有效地在其专用功能段上与其他子过程同时执行。采用流水线技术通过硬件实现并行操作后，就某一条指令而言，其执行速度并没有加快，但就程序执行过程的整体而言，程序执行速度大大加快。 流水线技术适合于大量的重复性的处理。 前面我提到过CPU 中一个指令周期的任务分解。假设指令周期包含取指令（IF）、指令译码（ID）、 指令执行（EX）、访存取数（MEM）、结果写回（WB）5 个子过程（过程段），流水线由这 5个串联的过程段 组成，各个过程段之间设有高速缓冲寄存器，以暂时保存上一个过程段的任务处理的结果，在统一的时钟信号控制下，数据从一个过程段流向相邻的过程段。 非流水计算机的时空图如下: 2.1标量流水计算机工作方式标量（Scalar）流水计算机是只有一条指令流水线的计算机。图4表示标量流水计算机的时空图。 当流水线满载时，每一个时钟周期可以执行 2 条以上的指令。因此，图5中仅用了 9 个时钟周期就完成了10条指令，每条指令平均用时 0.9 个时钟周期。 超标量流水计算机是时间并行技术和空间并行技术的综合应用。 3、指令的相关性指令流水线的一个特点是流水线中的各条指令之间存在一些相关性，使得指令的执行受到影响。要使流水线发挥高效率，就要使流水线连续不断地流动，尽量不出现断流的情况。然而，由于流水过程中存在的相关性冲突，断流现象是不可避免的。 3.1数据相关在流水计算机中，指令的处理是重叠进行的，前一条指令还没有结束，第二、三条指令就陆续开始工 作。由于多条指令的重叠处理，当后继指令所需的操作数刚好是前一条指令的运算结果时，便发生数据相关冲突。由于这两条指令的执行顺序直接影响到操作系统读取的内容，必须等前一条指令执行完毕后才能执行后一条指令。在这种情况下，这两条指令就是与数据相关的。因此，数据相关是由于指令之间存在数据依赖性而引起的。根据指令间对同一寄存器读和写操作的先后次序关系，可将数据相关性分为写后读（Read-AfterWrite，RAW）相关、读后写（Write-After-Read，WAR）相关、写后写（Write-After-Write，WAW）相关三种类型。 解决数据相关冲突的办法如下： ⑴采用编译的方法 编译程序通过在两条相关指令之间插入其他不相关的指令（或空操作指令）而推迟指令的执行，使数据相关消失，从而产生没有相关性的程序代码。这种方式简单，但降低了运行效率。 ⑵由硬件监测相关性的存在，采用数据旁路技术设法解决数据相关 当前一条指令要写入寄存器而下一条指令要读取同一个寄存器时，在前一条指令执行完毕、结果数据还未写入寄存器前，由内部数据通路把该结果数据直接传递给下一条指令，也就是说，下一条指令所需的 操作数不再通过读取寄存器获得，而是直接获取。这种方式效率较高，但控制也较为复杂。 3.2资源相关所谓资源相关，是指多条指令进入流水线后在同一机器周期内争用同一个功能部件所发生的冲突。 例如，在图 4所示的标量流水计算机中，在第 4 个时钟周期时，第 1 条指令处于访存取数（MEM） 阶段，而第 4 条指令处于取指令（IF）阶段。如果数据和指令存放在同一存储器中，且存储器只有一个端口，这样便会发生这两条指令争用存储器的资源相关冲突。 因为每一条指令都可能需要 2 次访问存储器（读指令和读写数据），在指令流水过程中，可能会有 2 条件同时需要访问存储器，导致资源相关冲突解决资源相关冲突的一般办法是增加资源，例如增设一个存储器，将指令和数据分别放在两个存储器中。 3.3控制相关控制相关冲突是由转移指令引起的。当执行转移指令时，依据转移条件的产生结果，可能顺序取下一 条指令，也可能转移到新的目标地址取指令。若转移到新的目标地址取指令，则指令流水线将被排空，并等待转移指令形成下一条指令的地址，以便读取新的指令，这就使得流水线发生断流。 为了减小转移指令对流水线性能的影响，通常采用以下两种转移处理技术： ⑴延迟转移法 由编译程序重排指令序列来实现。其基本思想是“先执行再转移”，即发生转移时并不排空指令流水线，而是继续完成下几条指令。如果这些后继指令是与该转移指令结果无关的有用指令，那么延迟损失时间片正好得到了有效的利用。 ⑵转移预测法 用硬件的方法来实现。依据指令过去的行为来预测将来的行为，即选择出现概率较高的分支进行预取。通过使用转移取和顺序取两路指令选取队列以及目标指令 Cache，可将转移预测提前到取指令阶段进行，以获得良好的效果。 4、指令的动态执行技术4.1指令调度为了减少指令相关性对执行速度的影响，可以在保证程序正确性的前提下，调整指令的顺序，即进行指令调度。 指令调度可以由编译程序进行，也可以由硬件在执行的时候进行，分别称为静态指令调度和动态指令调度。静态指令调度是指编译程序通过调整指令的顺序来减少流水线的停顿，提高程序的执行速度；动态 指令调度用硬件方法调度指令的执行以减少流水线停顿。 流水线中一直采用的有序（in-order）指令启动是限制流水线性能的主要因素之一。如果有一条指令在流水线中停顿了，则其后的指令就都不能向前流动了，这样，如果相邻的两条指令存在相关性，流水线就将发生停顿，如果有多个功能部件，这些部件就有可能被闲置。消除这种限制流水线性能的因素从而提高指令执行速度，其基本思想就是允许指令的执行是无序的（out-of-order，也称乱序），也就是说，在保持指令间、数据间的依赖关系的前提下，允许不相关的指令的执行顺序与程序的原有顺序有所不同，这一思想是实行动态指令调度的前提。 4.2乱序执行技术乱序执行（Out-of-order Execution）是以乱序方式执行指令，即 CPU 允许将多条指令不按程序规定的顺序而分开发送给各相应电路单元进行处理。这样，根据各个电路单元的状态和各指令能否提前执行的具体情况分析，将能够提前执行的指令立即发送给相应电路单元予以执行，在这期间不按规定顺序执行指令；然后由重新排列单元将各执行单元的结果按指令顺序重新排列。乱序执行的目的，就是为了使 CPU 内部电路满负荷运转，并相应提高 CPU 运行程序的速度。 实现乱序执行的关键在于取消传统的“取指”和“执行”两个阶段之间指令需要线性排列的限制，而使用一个指令缓冲池来开辟一个较长的指令窗口，允许执行单元在一个较大的范围内调遣和执行已译码的程序指令流。 4.3分支预测分支预测（Branch Prediction）是对程序的流程进行预测，然后读取其中一个分支的指令。采用分支预测的主要目的是为了提高 CPU的运算速度。 分支预测的方法有静态预测和动态预测两类：静态预测方法比较简单，如预测永远不转移、预测永远转移、预测后向转移等等，它并不根据执行时的条件和历史信息来进行预测，因此预测的准确性不可能很高；动态预测方法则根据同一条转移指令过去的转移情况来预测未来的转移情况。 由于程序中的条件分支是根据程序指令在流水线处理后的结果来执行的，所以当 CPU 等待指令结果时， 流水线的前级电路也处于等待分支指令的空闲状态，这样必然出现时钟周期的浪费。如果 CPU 能在前条指令结果出来之前就预测到分支是否转移，那么就可以提前执行相应的指令，这样就避免了流水线的空闲等待，也就相应提高了 CPU 的运算速度。但另一方面，一旦前条指令结果出来后证明分支预测是错误的，那么就必须将已经装入流水线执行的指令和结果全部清除，然后再装入正确的指令重新处理，这样就比不进行分支预测而是等待结果再执行新指令还要慢了。 因此，分支预测的错误并不会导致结果的错误，而只是导致流水线的停顿，如果能够保持较高的预测 准确率，分支预测就能提高流水线的性能。 5、实例分析前面的知识只是一个理论基础铺垫，下面我们就结合一款真实的CPU架构进行对应分析，图6和图7分别是x86和ARM体系结构的内核架构图（都是具有OoOE特性的CPU架构），可以看到他们基本的组成都是一样的（虽然x86是CISC而ARM是RISC，但是现代x86内部也是先把CISC翻译成RISC的），因此我在这里就只分析x86结构。 5.1取指令阶段（IF）处理器在执行指令之前，必须先装载指令。指令会先保存在 L1 缓存的 I-cache (Instruction-cache)指令缓存当中，Nehalem指令拾取单元使用 128bit 带宽的通道从 I-cache 中读取指令。这个 I-cache 的大小为 32KB，采用了 4 路组相连，在后面的存取单元介绍中我们可以得知这种比 Core 更少的集合关联数量是为了降低延迟。 为了适应超线程技术，RIP(Relative Instruction Point，相对指令指针)的数量也从一个增加到了两个，每个线程单独使用一个。 指令拾取单元包含了分支预测器(Branch Predictor)，分支预测是在 Pentium Pro 处理器开始加入的功能，预测如 if then 这样的语句的将来走向，提前读取相关的指令并执行的技术，可以明显地提升性能。指令拾取单元也包含了 Hardware Prefetcher，根据历史操作预先加载以后会用到的指令来提高性能，这会在后面得到详细的介绍。 当分支预测器决定了走向一个分支之后，它使用 BTB(Branch Target Buffer，分支目标缓冲区)来保存预测指令的地址。Nehalem 从以前的一级 BTB 升级到了两个级别，这是为了适应很大体积的程序(数据库以及 ERP 等应用，跳转分支将会跨过很大的区域并具有很多的分支)。Intel 并没有提及 BTB 详细的结构。与BTB 相对的 RSB(Return Stack Buffer，返回堆栈缓冲区)也得到了提升，RSB 用来保存一个函数或功能调用结束之后的返回地址，通过重命名的 RSB 来避免多次推测路径导致的入口&#x2F;出口破坏。RSB 每个线程都有一个，一个核心就拥有两个，以适应超线程技术的存在。 指令拾取单元使用预测指令的地址来拾取指令，它通过访问 L1 ITLB 里的索引来继续访问 L1 ICache，128 条目的小页面 L1 ITLB 按照两个线程静态分区，每个线程可以获得 64 个条目，这个数目比 Core 2 地少。当关闭超线程时，单独的线程将可以获得全部的 TLB 资 源。除了小页面 TLB 之外，Nehalem 还每个线程拥有 7 个条目的全关联(Full Associativity) 大页面 ITLB，这些 TLB 用于访问 2M&#x2F;4M 的大容量页面，每个线程独立，因此关闭超线程不会让你得到 14 个大页面 ITLB 条目。 指令拾取单元通过 128bit 的总线将指令从 L1 ICache 拾取到一个 16Bytes(刚好就是 128bit)的预解码拾取缓冲区。128 位的带宽让人有些迷惑不解，Opteron 一早就已经使用 了 256bit 的指令拾取带宽。最重要的是，L1D 和 L1I 都是通过 256bit 的带宽连接到 L2 Cache 的。 由于一般的CISC x86指令都小于4Bytes（32位x86指令;x86指令的特点就是不等长)， 因此一次可以拾取 4 条以上的指令，而预解码拾取缓冲区的输出带宽是 6 指令每时钟周期， 因此可以看出指令拾取带宽确实有些不协调，特别是考虑到 64 位应用下指令会长一些的情 况下(解码器的输入输出能力是 4 指令每时钟周期，因此 32 位下问题不大)。 指令拾取结束后会送到 18 个条目的指令队列，在 Core 架构，送到的是 LSD 循环流缓冲区，在后面可以看到，Nehalem 通过将 LSD 移动后更靠后的位置来提高性能。 5.2指令译码阶段（ID）再将指令充填到可容纳 18 条目的指令队列之后，就可以进行解码工作了。解码是类 RISC (精简指令集或简单指令集)处理器导致的一项设计，从 Pentium Pro 开始在 IA 架构出现。 处理器接受的是 x86 指令(CISC 指令，复杂指令集)，而在执行引擎内部执行的却不是x86 指令，而是一条一条的类 RISC 指令，Intel 称之为 Micro Operation——micro-op，或者写 为 μ-op，一般用比较方便的写法来替代掉希腊字母:u-op 或者 uop。相对地，一条一条地 x86 指令就称之为 Macro Operation或 macro-op。 RISC 架构的特点就是指令长度相等，执行时间恒定(通常为一个时钟周期)，因此处理器设计起来就很简单，可以通过深长的流水线达到很高的频率，IBM 的 Power6 就可以轻松地达到 4.7GHz 的起步频率。和 RISC 相反，CISC 指令的长度不固定，执行时间也不固定，因此 Intel 的 RISC&#x2F;CISC 混合处理器架构就要通过解码器 将 x86 指令翻译为 uop，从而获得 RISC 架构的长处，提升内部执行效率。 和 Core 一样，Nehalem 的解码器也是 4 个（3 个简单解码器加 1 个复杂解码器）。简单解码器可以将一条 x86 指令(包括大部分 SSE 指令在内)翻译为一条 uop，而复杂解码器则将一些特别的(单条)x86 指令翻译为 1~4 条 uops——在极少数的情况下，某些指令需要通过 额外的可编程 microcode 解码器解码为更多的 uops(有些时候甚至可以达到几百个，因为 一些 IA 指令很复杂，并且可以带有很多的前缀&#x2F;修改量，当然这种情况很少见)，下图 Complex Decoder 左方的 ucode 方块就是这个解码器，这个解码器可以通过一些途径进行升级或者扩展，实际上就是通过主板 Firmware 里面的 Microcode ROM 部分。 之所以具有两种解码器，是因为仍然是关于 RISC&#x2F;CISC 的一个事实: 大部分情况下(90%) 的时间内处理器都在运行少数的指令，其余的时间则运行各式各样的复杂指令(不幸的是， 复杂就意味着较长的运行时间)，RISC 就是将这些复杂的指令剔除掉，只留下最经常运行的指令(所谓的精简指令集)，然而被剔除掉的那些指令虽然实现起来比较麻烦，却在某些领域确实有其价值，RISC 的做法就是将这些麻烦都交给软件，CISC 的做法则是像现在这样: 由硬件设计完成。因此 RISC 指令集对编译器要求很高，而 CISC 则很简单。对编程人员的要求也类似。 5.3循环流检测在解码为 uop 之后 Nehalem 会将它们都存放在一个叫做 uop LSD Buffer 的缓存区。在Core 2 上，这个 LSD Buffer 是出现在解码器前方的，Nehalem 将其移动到解码器后方，并相对加大了缓冲区的条目。Core 2 的 LSD 缓存区可以保存 18 个 x86 指令而 Nehalem 可以保 存 28 个 uop，从前文可以知道，大部分 x86 指令都可以解码为一个 uop，少部分可以解码 为 1~4 个 uop，因此 Nehalem 的 LSD 缓冲区基本上可以相当于保存 21~23 条x86 指令，比 Core 2 要大上一些。 LSD 循环流监测器也包含在解码部分，它的作用是: 假如程序使用的循环段(如 for..do&#x2F;do..while 等)少于 28 个 uops，那么 Nehalem 就可以将这个循环保存起来，不再需要重新通过取指单元、分支预测操作，以及解码器，Core 2 的 LSD 放在解码器前方，因此无法省下解码器的工作。 Nehalem LSD 的工作比较像 NetBurst 架构的 Trace Cache，其也是保存 uops，作用也是部分地去掉一些严重的循环，不过由于 Trace Cache 与此同时担当着类似于 Core&#x2F;Nehalem 架构的 Reorder Buffer 乱序缓冲区的作用，容量比较大(可以保存 12k uops，准确的大小 是 20KB)，因此在 cache miss 的时候后果严重(特别是在 SMT 同步多线程之后，miss 增加 倍的情况下)，LSD 的小数目设计显然会好得多。不过笔者认为 28 个 uop 条目有些少，特 别是考虑到 SMT 技术带来的两条线程都同时使用这个 LSD 的时候。 在 LSD 之后，Nehalem 将会进行 Micro-ops Fusion，这也是前端(The Front-End)的最后一个功能，在这些工作都做完之后，uops 就可以准备进入执行引擎了。 5.4乱序执行指令阶段（OoOE）OoOE— Out-of-Order Execution 乱序执行也是在 Pentium Pro 开始引入的，它有些类似于多线程的概念。乱序执行是为了直接提升 ILP(Instruction Level Parallelism)指令级并行化的设计，在多个执行单元的超标量设计当中，一系列的执行单元可以同时运行一些没有数据关联性的若干指令，只有需要等待其他指令运算结果的数据会按照顺序执行，从而总体提升了运行效率。乱序执行引擎是一个很重要的部分，需要进行复杂的调度管理。 首先，在乱序执行架构中，不同的指令可能都会需要用到相同的通用寄存器(GPR，General Purpose Registers)，特别是在指令需要改写该通用寄存器的情况下——为了让这些指令们能并行工作，处理器需要准备解决方法。一般的 RISC 架构准备了大量的GPR， 而x86 架构天生就缺乏 GPR(x86具有8个GPR，x86-64 具有 16 个，一般 RISC 具有 32 个，IA64 则具有 128 个)，为此 Intel 开始引入**重命名寄存器(Rename Register)**，不同的指令可以通过具有名字相同但实际不同的寄存器来解决。 此外，为了 SMT 同步多线程，这些寄存器还要准备双份，每个线程具有独立的一份。 乱序执行从Allocator定位器开始，Allocator 管理着RAT(Register Alias Table，寄存器别名表)、ROB(Re-Order Buffer，重排序缓冲区)和 RRF(Retirement Register File，退回寄存器文件)。在 Allocator 之前，流水线都是顺序执行的，在 Allocator 之后，就可以进入乱序执行阶段了。在每一个线程方面，Nehalem 和 Core 2 架构相似，RAT 将重命名的、虚拟的寄存器(称为 Architectural Register 或 Logical Register)指向ROB 或者RRF。RAT 是一式两份,每个线程独立，每个 RAT 包含了 128 各重命名寄存器。RAT 指向在 ROB 里面的最近的执行寄存器状态，或者指向RRF保存的最终的提交状态。 ROB(Re-Order Buffer，重排序缓冲区)是一个非常重要的部件，它是将乱序执行完毕的指令们按照程序编程的原始顺序重新排序的一个队列，以保证所有的指令都能够逻辑上实现正确的因果关系。打乱了次序的指令们(分支预测、硬件预取)依次插入这个队列，当一条指令通过 RAT 发往下一个阶段确实执行的时候这条指令(包括寄存器状态在内)将被加入 ROB 队列的一端，执行完毕的指令(包括寄存器状态)将从 ROB 队列的另一端移除(期间这些指令的数据可以被一些中间计算结果刷新)，因为调度器是 In-Order 顺序的，这个队列（ROB）也就是顺序的。从 ROB 中移出一条指令就意味着指令执行完毕了，这个阶段叫做 Retire 回退，相应地 ROB 往往也叫做 Retirement Unit(回退单元)，并将其画为流水线的最后一部分。 在一些超标量设计中，Retire 阶段会将 ROB 的数据写入 L1D 缓存（这是将MOB集成到ROB的情况），而在另一些设计里， 写入 L1D 缓存由另外的队列完成。例如，Core&#x2F;Nehalem 的这个操作就由 **MOB(Memory Order Buffer，内存重排序缓冲区)**来完成。 ROB 是乱序执行引擎架构中都存在的一个缓冲区，重新排序指令的目的是将指令们的寄存器状态依次提交到RRF退回寄存器文件当中，以确保具有因果关系的指令们在乱序执行中可以得到正确的数据。从执行单元返回的数据会将先前由调度器加入ROB 的指令刷新数据部分并标志为结束(Finished)，再经过其他检查通过后才能标志为完毕(Complete)，一旦标志为完毕，它就可以提交数据并删除重命名项目并退出ROB 了。提交状态的工作由 Retirement Unit(回退单元)完成，它将确实完毕的指令包含的数据写入RRF(“确实” 的意思是，非猜测执性、具备正确因果关系，程序可以见到的最终的寄存器状态)。和 RAT 一样，RRF 也同时具有两个，每个线程独立。Core&#x2F;Nehalem 的 Retirement Unit 回退单元每时钟周期可以执行 4 个 uops 寄存器文件写入，和 RAT 每时钟 4 个 uops 的重命名一致。 由于 ROB 里面保存的指令数目是如此之大(128 条目)，因此一些人认为它的作用是用来从中挑选出不相关的指令来进入执行单元，这多少是受到一些文档中的 Out-of-Order Window 乱序窗口这个词的影响(后面会看到ROB 会和 MOB 一起被计入乱序窗口资源中)。 ROB 确实具有 RS 的一部分相似的作用，不过，ROB 里面的指令是调度器（dispacher）通过 RAT发往 RS 的同时发往ROB的（里面包含着正常顺序的指令和猜测执行的指令，但是乱序执行并不是从ROB中乱序挑选的），也就是说，在“乱序”之前，ROB 的指令就已经确定了。指令并不是在 ROB 当中乱序挑选的(这是在RS当中进行)，ROB 担当的是流水线的最终阶段: 一个指令的 Retire回退单元;以及担当中间计算结果的缓冲区。RS(Reservation Station，中继站): 等待源数据到来以进行OoOE乱序执行(没有数据的指令将在 RS 等待)， ROB(ReOrder Buffer，重排序缓冲区): 等待结果到达以进行 Retire 指令回退 (没有结果的指令将在 ROB等待)。 Nehalem 的 128 条目的 ROB 担当中间计算结果的缓冲区，它保存着猜测执行的指令及其数据，猜测执行允许预先执行方向未定的分支指令。在大部分情况下，猜测执行工作良好——分支猜对了，因此其在 ROB 里产生的结果被标志为已结束，可以立即地被后继指令使用而不需要进行 L1 Data Cache 的 Load 操作(这也是 ROB 还有另一个重要用处，典型的 x86 应用中 Load 操作是如此频繁，达到了几乎占 1&#x2F;3 的地步，因此 ROB 可以避免大量的Cache Load 操作，作用巨大)。在剩下的不幸的情况下，分支未能按照如期的情况进行，这时猜测的分支指令段将被清除，相应指令们的流水线阶段清空，对应的寄存器状态也就全都无效了，这种无效的寄存器状态不会也不能出现在 RRF 里面。 重命名技术并不是没有代价的，在获得前面所说的众多的优点之后，它令指令在发射的时候需要扫描额外的地方来寻找到正确的寄存器状态，不过总体来说这种代价是非常值得的。RAT可以在每一个时钟周期重命名 4 个 uops 的寄存器，经过重命名的指令在读取到正确的操作数并发射到统一的RS(Reservation Station，中继站，Intel 文档翻译为保留站点) 上。RS 中继站保存了所有等待执行的指令。 和 Core 2 相比，Nehalem 的 ROB 大小和 RS 大小都得到了提升，ROB 重排序缓冲区从 96 条目提升到 128 条目(鼻祖 Pentium Pro 具有 40 条)，RS 中继站从 32 提升到 36(Pentium Pro 为 20)，它们都在两个线程(超线程中的线程)内共享，不过采用了不同的策略:ROB 是采用了静态的分区方法，而 RS 则采用了动态共享，因为有时候会有一条线程内的指令因 等待数据停滞，这时另一个线程就可以获得更多的 RS 资源。停滞的指令不会发现 RS，但是仍然会占用 ROB 条目。由于 ROB 是静态分区，因此在开启 HTT 在这情况下，每一个线程只能 分到 64 条，不算多，在一些极少数的应用上，我们应该可以观察到一些应用开启 HTT 后悔 速度降低，尽管可能非常微小。 5.5执行单元在为 SMT 做好准备工作并打乱指令的执行顺序之后（指的是分支预测、硬件预取），uops 通过每时钟周期 4 条的速度进入 Reservation Station 中继站(保留站)，总共 36 条目的中继站 uops 就开始等待超标量(Superscaler)执行引擎乱序执行了。自从 Pentium 开始，Intel 就开始在处理器里面采用了超标量设计(Pentium 是两路超标量处理器)，超标量的意思就是多个执行单元，它可以同时执行多条没有相互依赖性的指令，从而达到提升 ILP 指令级别并行化的目的。Nehalem 具备 6 个执行端口，每个执行端口具有多个不同的单元以执行不同的任务，然而同一时间只能有一条指令(uop)进入执行端口，因此也可以认为 Nehalem 有 6 个“执行单元”，在每个时钟周期内可以执行最多 6 个操作(或者说，6 条指令)，和 Core 一样。 36 条目的中继站指令在分发器的管理下，挑选出尽量多的可以同时执行的指令(也就是乱序执行的意思)——最多 6 条——发送到执行端口。 这些执行端口并不都是用于计算，实际上，有三个执行端口是专门用来执行内存相关的操作的，只有剩下的三个是计算端口，因此，在这一点上 Nehalem 实际上是跟 Core 架构一 样的，这也可以解释为什么有些情况下，Nehalem 和 Core 相比没有什么性能提升。 计算操作分为两种: 使用 ALU(Arithmetic Logic Unit，算术逻辑单元)的整数(Integer) 运算和使用 FPU(Floating Point Unit，浮点运算单元)的浮点(Floating Point)运算。SSE 指令(包括 SSE1 到 SSE4)是一种特例，它虽然有整数也有浮点，然而它们使用的都是 128bit 浮点寄存器，使用的也大部分是 FPU 电路。在 Nehalem 中，三个计算机端口都可以做整数运算(包括 MMX)或者SSE 运算(浮点运算不太一样，只有两个端口可以进行浮点 ADD 和 MUL&#x2F;DIV 运算，因此每个时钟周期最多进行 2 个浮点计算，这也是目前 Intel 处理器浮点性能不如整数性能突出的原因)，不过每一个执行端口都不是完全一致:只有端口 0 有浮点乘和除功能，只有端口 5 有分支能力(这个执行单元将会与分支预测单元连接)，其他 FP&#x2F;SSE 能力也不尽相同，这些不对称之处都由统一的分发器来理解，并进行指令的调度管理。没有采用完全对称的设计可能是基于统计学上的考虑。和 Core 一样，Nehalem 的也没有采用 Pentium 4 那样的 2 倍频的 ALU 设计(在 Pentium 4，ALU 的运算频率是 CPU 主频的两倍， 因此整数性能明显要比浮点性能突出)。 不幸的是，虽然可以同时执行的指令很多，然而在流水线架构当中运行速度并不是最 “宽”的单元来决定的，而是由最“窄”的单元来决定的。这就是木桶原理，Opteron的解码器后端只能每时钟周期输出 3 条 uops，而 Nehalem&#x2F;Core2 则能输出 4 条，因此它们的实际最大每时钟运行指令数是 3&#x2F;4，而不是 6。同样地，多少路超标量在这些乱序架构处理器中也不再按照运算单元来划分，Core Duo 及之前(到 Pentium Pro 为止)均为三路超标量处理器，Core 2&#x2F;Nehalem 则为四路超标量处理器。可见在微架构上，Nehalem&#x2F;Core 显然是 要比其他处理器快一些。顺便说一下，这也是 Intel 在超线程示意图中，使用 4 个宽度的方 块来表示而不是 6 个方块的原因。 5.6存取单元运算需要用到数据，也会生成数据，这些数据存取操作就是存取单元所做的事情，实际 上，Nehalem 和 Core 的存取单元没什么变化，仍然是 3 个。 这三个存取单元中，一个用于所有的 Load 操作(地址和数据)，一个用于 Store 地址，一个用于 Store 数据，前两个与数据相关的单元带有 AGU(Address Generation Unit，地址生成单元)功能(NetBurst架构使用快速 ALU 来进行地址生成)。 在乱序架构中，存取操作也可以打乱进行。类似于指令预取一样，Load&#x2F;Store 操作也可以提前进行以降低延迟的影响，提高性能。然而，由于Store操作会修改数据影响后继的Load 操作，而指令却不会有这种问题(寄存器依赖性问题通过ROB解决)，因此数据的乱序操作更为复杂。 如上图所示，第一条 ALU 指令的运算结果要 Store 在地址 Y(第二条指令)，而第九条 指令是从地址 Y Load 数据，显然在第二条指令执行完毕之前，无法移动第九条指令，否则将会产生错误的结果。同样，如果CPU也不知道第五条指令会使用什么地址，所以它也无法确定是否可以把第九条指令移动到第五条指令附近。 内存数据相依性预测功能(Memory Disambiguation)可以预测哪些指令是具有依赖性的或者使用相关的地址(地址混淆，Alias)，从而决定哪些 Load&#x2F;Store 指令是可以提前的， 哪些是不可以提前的。可以提前的指令在其后继指令需要数据之前就开始执行、读取数据到ROB当中，这样后继指令就可以直接从中使用数据，从而避免访问了无法提前 Load&#x2F;Store 时访问 L1 缓存带来的延迟(3~4 个时钟周期)。 不过，为了要判断一个 Load 指令所操作的地址没有问题，缓存系统需要检查处于 in-flight 状态(处理器流水线中所有未执行的指令)的 Store 操作，这是一个耗费资源的过程。在 NetBurst 微架构中，通过把一条 Store 指令分解为两个 uops——一个用于计算地址、一个用于真正的存储数据，这种方式可以提前预知 Store 指令所操作的地址，初步地解决了数据相关性问题。在 NetBurst 微架构中，Load&#x2F;Store 乱序操作的算法遵循以下几条 原则: 如果一个对于未知地址进行操作的 Store 指令处于 in-flight 状态，那么所有的人 Load 指令都要被延迟 在操作相同地址的 Store 指令之前 Load 指令不能继续执行 一个 Store 指令不能移动到另外一个地方 Store 指令之前（指的是在RS中不能先挑选执行后面的一条store指令，注意这只是说某一种架构不允许重排store，其实还是有很多架构如Alpha等是松散内存模型，允许不相关的store重排序的，这一块就牵扯到memory models知道相关知识了，建议参考这里） 这种原则下的问题也很明显，比如第一条原则会在一条处于等待状态的 Store 指令所操作的地址未确定之前，就延迟所有的时间 Load 操作，显然过于保守了。实际上，地址冲突问题是极少发生的。根据某些机构的研究，在一个Alpha EV6 处理器中最多可以允许 512 条指令处于 in-flight 状态，但是其中的 97%以上的 Load 和 Store 指令都不会存在地址冲突问题。基于这种理念，Core 微架构采用了大胆的做法，它令 Load 指令总是提前进行，除非新加入的动态混淆预测器(Dynamic Alias Predictor)预测到了该 Load 指令不能被移动到 Store 指令附近。这个预测是根据历史行为来进行的，据说准确率超过 90%。在执行了预 Load 之后，一个冲突监测器会扫描出来 MOB 的 Store 队列，检查该是否有Store操作与该 Load 冲突。在很不幸的情况下(1%2%)，发现了冲突，那么该 Load 操作作废、 流水线清除并重新进行 Load 操作。这样大约会损失 20 个时钟周期的时间，然而从整体上看， Core 微架构的激进 Load&#x2F;Store 乱序策略确实很有效地提升了性能，因为Load 操作占据了通常程序的 1&#x2F;3 左右，并且 Load 操作可能会导致巨大的延迟(在命中的情况下，Core 的 L1D Cache 延迟为 3 个时钟周期，Nehalem 则为 4 个。L1 未命中时则会访问 L2 缓存，一般为 1012 个时钟周期。访问 L3 通常需要 30~40 个时钟周期，访问主内存则可以达到最多约 100 个时钟周期)。Store 操作并不重要，什么时候写入到 L1 乃至主内存并不会影响到执行性能。 如上图所示，我们需要载入地址 X 的数据，加 1 之后保存结果;载入地址 Y 的数据，加1 之后保存结果;载入地址 Z 的数据，加 1 之后保存结果。如果根据 Netburst 的基本准则， 在第三条指令未决定要存储在什么地址之前，处理器是不能移动第四条指令和第七条指令的。实际上，它们之间并没有依赖性。因此，Core 微架构中则“大胆”的将第四条指令和第七条指令分别移动到第二和第三指令的并行位置，这种行为是基于一定的猜测的基础上的“投机”行为，如果猜测的对的话(几率在 90%以上)，完成所有的运算只要5个周期，相比之前的9个周期几乎快了一倍。 和为了顺序提交到寄存器而需要 ROB 重排序缓冲区的存在一样，在乱序架构中，多个打乱了顺序的 Load 操作和Store操作也需要按顺序提交到内存，MOB(Memory Reorder Buffer， 内存在重排序缓冲区)就是起到这样一个作用的重排序缓冲区(介于 Load&#x2F;Store 单元 与 L1D Cache 之间的部件，有时候也称之为LSQ)，MOB 通过一个 128bit 位宽的 Load 通道与一个 128bit 位宽的 Store 通道与双口 L1D Cache 通信。和 ROB 一样，MOB的内容按照 Load&#x2F;Store 指令实际的顺序加入队列的一端，按照提交到 L1 DCache 的顺序从队列的另一端移除。ROB 和 MOB 实际上形成了一个分布式的 Order Buffer 结构，有些处理器上只存在 ROB，兼备了 MOB 的功能（把MOB看做ROB的一部分可能更好理解）。 和ROB 一样，Load&#x2F;Store 单元的乱序存取操作会在 MOB 中间按照原始程序顺序排列，以提供正确的数据，内存数据依赖性检测功能也在里面实现(内存数据依赖性的检测比指令寄存器间的依赖性检测要复杂的多)。MOB 的 Load&#x2F;Store 操作结果也会直接反映到 ROB当中（中间结果）。 MOB还附带了数据预取(Data Prefetch)功能，它会猜测未来指令会使用到的数据，并预先从L1D Cache 缓存 Load入MOB 中(Data Prefetcher 也会对 L2 至系统内存的数据进行这样的操作)， 这样 MOB 当中的数据有些在 ROB 中是不存在的(这有些像 ROB 当中的 Speculative Execution 猜测执行，MOB 当中也存在着“Speculative Load Execution 猜测载入”，只不过失败的猜测执行会导致管线停顿，而失败的猜测载入仅仅会影响到性能，然而前端时间发生的Meltdown漏洞却造成了严重的安全问题)。MOB包括了Load Buffers和Store Buffers。 在秩序执行中我们可以看到很多缓冲区性质的东西: RAT 寄存器别名表、ROB 重排序缓冲 区、RS 中继站、MOB 内存重排序缓冲区(包括 load buffer 载入缓冲和 store buffer 存储缓冲)。在超线程的作 用下，RAT是一式两份，包含了 128 重命名寄存器; 128 条目的 ROB、48 条目的 LB 和 32 条目的 SB 都 每个线程 64 个 ROB、24 个 LB 和 16 个 SB; RS 则是在两个线程中动态共享。可见，虽然整体数量增加了，然而就单个线程而言，获得的资源并没有 提升。这会影响到 HTT 下单线程下的性能。 6、缓存（cache）通常缓存具有两种设计:非独占和独占，Nehalem 处理器的 L3 采用了非独占高速缓存 设计(或者说“包含式”，L3 包含了 L1&#x2F;L2 的内容)，这种方式在 Cache Miss 的时候比独 占式具有更好的性能，而在缓存命中的时候需要检查不同的核心的缓存一致性。Nehalem 并 采用了“内核有效”数据位的额外设计，降低了这种检查带来的性能影响。随着核心数目的 逐渐增多(多线程的加入也会增加 Cache Miss 率)，对缓存的压力也会继续增大，因此这 种种方式会比较符合未来的趋势。在后面可以看到，这种设计也是考虑到了多处理器协作的情况(此时 Miss 率会很容易地增加)。这可以看作是 Nehalem 与以往架构的基础不同:之前的架构都是来源于移动处理设计，而 Nehalem 则同时为企业、桌面和移动考虑而设计。 在 L3 缓存命中的时候(单处理器上是最通常的情况，多处理器下则不然)，处理器检查内核有效位看看是否其他内核也有请求的缓存页面内容，决定是否需要对内核进行侦听。 在NUMA架构中，多个处理器中的同一个缓存页面必定在其中一个处理器中属于 F 状态(可以修改的状态)，这个页面在这个处理器中没有理由不可以多核心共享(可以多核心共享就意味着这个能进入修改状态的页面的多个有效位被设置为一)。MESIF协议应该是工作在核心(L1+L2)层面而不是处理器(L3)层面，这样同一处理器里多个核心共享的页面，只有其中一个是出于 F 状态(可以修改的状态)。见后面对 NUMA 和 MESIF 的解析。(L1&#x2F;L2&#x2F;L3 同步应该是不需要 MESIF 的同步机制) 在 L3 缓存未命中的时候(多数处理器下会频繁发生)，处理器决定进行内存存取，按照 页面的物理位置，它分为近端内存存取(本地内存空间)和远端内存存取(地址在其他处理 器的内存的空间): 关于缓存Cache架构原理和Cache一致性MESI的原理不是本文的重点，此处不再赘述。 7、总结之前一直存在疑惑是因为看了Paul的论文，有两点，一个是CPU在猜测执行时，如果猜测乱序执行的是一条store指令，并且store指令要操作的地址和之前正处于in-flight状态的store指令没有相关关系，那么意味着这条猜测执行的store会直接更新DCache，如果后期发现猜测执行错误，那么此时是没有办法回滚的（因为数据已经写入DCache），而对于猜测执行的load就不会造成这么严重的问题，顶多就是提前把主内存的内容读到DCache中（但是前段时间发生的Meltdown漏洞就是利用了该特性）；第二个疑问就是，类似于x86手册中说了store指令（参见memory model）是不允许重排执行的，那还会还会存在Paul的论文所描述的问题现象（讲store store barrier那里）吗，理论上store指令都是顺序提交结果到内存的。 对于第一个疑惑，其答案就是，处于猜测执行阶段的store指令是不允许提交的（commit）,因为猜测执行之前的代码还没有提交（时刻记住 按顺序执行、按顺序提交 贯穿全文），而一旦猜测执行之前的代码提交，也就可以验证猜测执行是否成功，此时如果猜测成功就执行commit，store数据到DCache（但是还是允许提前load数据到DCache），否则就直接丢弃猜测执行的结果（直接丢弃load buffer里面的数据）。也有些CPU架构中，对于store类型的指令是不允许猜测执行的，因此也不会有问题。 对于第二个疑惑，对于允许store重排的CPU架构来说，虽然内存读写指令在RS、MOB中都是按照编程顺序存放的，但是对于前后两条不相关的store指令而言，store指令就会乱序执行，但是虽然可以乱序执行（执行的结果反映在MOB中），但是依然必须顺序提交MOB中的结果。而对于x86这种不允许store重排的CPU而言，store只能按顺序执行，并且在MOB中被标记，并等待顺序提交。进入提交阶段后，MOB中store buffer中的store直连会按照编程顺序一条一条进行提交（即写数据到DCache），但是如果前一条store指令操作的数据不在本地cahce中，此时该store指令就无法被立即写入DCache，需要等待cache层MESI协议把数据同步过来，这是相当耗时的。而如果恰巧后一条store指令（前提是与前一条store不存在地址冲突）操作的数据就在本地cache中，此时如果允许后一条store指令先提交（这样可以大大的降低CPU的等待时间），则就会出现store乱序的问题（Paul的论文描述的现象，注意，这并不是真正的store乱序，store结果依然还是按照编程顺序进行提交的）。因此，如果这个结果正好违背业务逻辑，解决方式就是显示的在两条store之间添加store store内存屏障，这样后一条store在作用到cache之前，会先等待store buffer被排空，这样就不会存在store“乱序”执行的现象，说白了，这个问题导致的原因，就是这个store buffer在提交时是否是FIFO的（即队列必须严格的先进先出）。如果store buffer是FIFO类型的（如x86 CPU），那么就不会存在该现象。x86体系结构中store指令的确是不会被乱序执行的，所以的确不会发生Paul的论文描述的问题。 由于load指令一般是允许乱序执行的，也就是load指令会在RS中被乱序dispatch到执行引擎（计算地址等），因此Memory Subsystem中的load buffer就用来暂存这些提前执行的load指令结果，如果乱序执行出现错误、或者分支预测错误，直接丢弃load buffer中的内容即可，但是load操作也是带有副作用的，就是它会导致数据被load到cache上，很容易被Meltdown漏洞 利用。 最后概括下乱序执行的含义 现代处理器采用指令并行技术,在不存在数据依赖性的前提下,处理器可以改变语句对应的机器指令的执行顺序来提高处理器执行速度 现代处理器采用内部缓存技术,导致数据的变化不能及时反映在主存所带来的乱序. 现代编译器为优化而重新安排语句的执行顺序 整个乱序执行的过程为：经过取值、译码、寄存器重命名之后的指令，被顺序送到保留站（如果是一般的指令还会顺序发到ROB，即重排序缓冲区，如果是内存store&#x2F;load指令，会按顺序放入MOB，即内存重排序缓冲区，有时候MOB就包含在ROB中），在保留站中的指令，一旦源操作数已经ready，并且当前有空闲的硬件单元，那么这个指令就可以执行，而不需要等待前面的指令完成，这就造成了乱序执行的效果。乱序执行中的指令，产生的寄存器修改不会修改最终的物理寄存器，而只会修改自己重命名的私有寄存器（中间检结果反应在ROB中）。同样，如果是内存存取操作（store&#x2F;load）乱序执行，也只会把结果暂存到MOB中（MOB包含store buffer和load buffer）。 因此乱序执行的结果还是未生效的，乱序执行后，需要按照顺序提交ROB和MOB中的结果（其实就是对这些缓冲区中的entry做个commit的标记，表示这些entry可以被应用到寄存区或者被写入内存）。ROB和MOB除了可以暂存中间结果、保证按序提交，还会应用于指令间的依赖分析、分支预测、硬件预取等功能。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/436340295","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核IO基础知识与概念","path":"/2023/01/15/linux-docs/进程管理/Linux内核IO基础知识与概念/","content":"什么是 IO在计算机操作系统中，所谓的I&#x2F;O就是 输入（Input）和输出（Output），也可以理解为读（Read）和写（Write)，针对不同的对象，I&#x2F;O模式可以划分为磁盘IO模型和网络IO模型。 IO操作会涉及到用户空间和内核空间的转换，先来理解以下规则： 内存空间分为用户空间和内核空间，也称为用户缓冲区和内核缓冲区； 用户的应用程序不能直接操作内核空间，需要将数据从内核空间拷贝到用户空间才能使用； 无论是read操作，还是write操作，都只能在内核空间里执行； 磁盘IO和网络IO请求加载到内存的数据都是先放在内核空间的； 再来看看所谓的读（Read）和写（Write)操作： 读操作：操作系统检查内核缓冲区有没有需要的数据，如果内核缓冲区已经有需要的数据了，那么就直接把内核空间的数据copy到用户空间，供用户的应用程序使用。如果内核缓冲区没有需要的数据，对于磁盘IO，直接从磁盘中读取到内核缓冲区（这个过程可以不需要cpu参与）。而对于网络IO，应用程序需要等待客户端发送数据，如果客户端还没有发送数据，对应的应用程序将会被阻塞，直到客户端发送了数据，该应用程序才会被唤醒，从Socket协议找中读取客户端发送的数据到内核空间，然后把内核空间的数据copy到用户空间，供应用程序使用。 写操作：用户的应用程序将数据从用户空间copy到内核空间的缓冲区中（如果用户空间没有相应的数据，则需要从磁盘—&gt;内核缓冲区—&gt;用户缓冲区依次读取），这时对用户程序来说写操作就已经完成，至于什么时候再写到磁盘或通过网络发送出去，由操作系统决定。除非应用程序显示地调用了sync 命令，立即把数据写入磁盘，或执行flush()方法，通过网络把数据发送出去。 绝大多数磁盘IO和网络IO的读写操作都是上述过程，除了后面要讲到的零拷贝IO。 用户空间&amp;内核空间野生程序员对于这个概念可能比较陌生，这其实是 Linux 操作系统中的概念。虚拟内存（操作系统中的概念，和物理内存是对应的）被操作系统划分成两块：User Space（用户空间 和 Kernel Space（内核空间），本质上电脑的物理内存是不划分这些的，只是操作系统开机启动后在逻辑上虚拟划分了地址和空间范围。 操作系统会给每个进程分配一个独立的、连续的虚拟内存地址空间（物理上可能不连续），以32位操作系统为例，该大小一般是4G，即232 。其中将高地址值的内存空间分配给系统内核占用（网上查资料得知：Linux下占1G，Windows下占2G），其余的内存地址空间分配给用户进程使用。 因为我们不是要深入学习操作系统，所以这里以32位系统举例旨在帮助你理解原理。32 位的 LInux 操作系统下，03G为用户空间，34G为内核空间： 那为什么要这样划分出空间范围呢？ 也很好理解，毕竟操作系统身份高贵，太重要了，不能和用户应用程序在一起玩耍，各自的数据都要分开存储并且严格控制权限不能越界。这样才能保证操作系统的稳定运行，用户应用程序太不可控了，不同公司或者个人都可以开发，碰到坑爹的误操作或者恶意破坏系统数据直接宕机玩完了。隔离后应用程序要挂你就挂，操作系统可以正常运行。 简单说，内核空间 是操作系统 内核代码运行的地方，用户空间 是 用户程序代码运行的地方。当应用进程执行系统调用陷入内核代码中执行时就处于内核态，当应用进程在运行用户代码时就处于用户态。 同时内核空间可以执行任意的命令，而用户空间只能执行简单的运算，不能直接调用系统资源和数据。必须通过操作系统提供接口，向系统内核发送指令。 一旦调用系统接口，应用进程就从用户空间切换到内核空间了，因为开始运行内核代码了。 简单看几行代码，分析下是应用程序在用户空间和内核空间之间的切换过程： 1234str = &quot;i am qige&quot; // 用户空间x = x + 2file.write(str) // 切换到内核空间y = x + 4 // 切换回用户空间 上面代码中，第一行和第二行都是简单的赋值运算，在用户空间执行。第三行需要写入文件，就要切换到内核空间，因为用户不能直接写文件，必须通过内核安排。第四行又是赋值运算，就切换回用户空间。 用户态切换到内核态的3种方式： 系统调用。也称为 System Call，是说用户态进程主动要求切换到内核态的一种方式，用户态进程使用操作系统提供的服务程序完成工作，比如上面示例代码中的写文件调用，还有像 fork() 函数实际上就是执行了一个创建新进程的系统调用。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现。 异常。当CPU在用户空间执行程序代码时发生了不可预期的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，切换到内核态，比如缺页异常。 外围设备的中断。 当外围设备完成用户请求的某些操作后，会向CPU发送相应的中断信号，这时CPU会暂停执行下一条即将执行的指令转而去执行与中断信号对应的处理程序，如果当前正在运行用户态下的程序指令，自然就发了由用户态到内核态的切换。比如硬盘数据读写完成，系统会切换到中断处理程序中执行后续操作等。 以上3种方式，除了系统调用是进程主动发起切换，异常和外围设备中断是被动切换的。 查看 CPU 时间在 User Space 与 Kernel Space 之间的分配情况，可以使用top命令。它的第三行输出就是 CPU 时间分配统计。 我们来看看图中圈出来的 CPU 使用率的三个指标： 其中，第一项 7.57% user 就是 CPU 消耗在 User Space 的时间百分比，第二项 7.0% sys是消耗在 Kernel Space 的时间百分比。第三项 85.4% idle 是 CPU 消耗在闲置进程的时间百分比，这个值越低，表示 CPU 越忙。 PIO&amp;DMA大家都知道一般我们的数据是存储在磁盘上的，应用程序想要读写这些数据肯定就需要加载到内存中。接下来给大家介绍下 PIO 和 DMA 这两种 IO 设备和内存之间的数据传输方式。 PIO 工作原理 用户进程通过read等系统调用接口向操作系统（即CPU）发出IO请求，请求读取数据到自己的用户内存缓冲区中，然后该进程进入阻塞状态。 操作系统收到用户进程的请求后，进一步将IO请求发送给磁盘。 磁盘驱动器收到内核的IO请求后，把数据读取到自己的缓冲区中，此时不占用CPU。当磁盘的缓冲区被读满之后，向内核发起中断信号告知自己缓冲区已满。 内核收到磁盘发来的中断信号，使用CPU将磁盘缓冲区中的数据copy到内核缓冲区中。 如果内核缓冲区的数据少于用户申请读的数据，则重复步骤2、3、4，直到内核缓冲区的数据符合用户的要求为止。 内核缓冲区的数据已经符合用户的要求，CPU停止向磁盘IO请求。 CPU将数据从内核缓冲区拷贝到用户缓冲区，同时从系统调用中返回。 用户进程读取到数据后继续执行原来的任务。 PIO缺点：每次IO请求都需要CPU多次参与，效率很低。 DMA 工作原理DMA（直接内存访问，Direct Memory Access）。 用户进程通过read等系统调用接口向操作系统（即CPU）发出IO请求，请求读取数据到自己的用户内存缓冲区中，然后该进程进入阻塞状态。 操作系统收到用户进程的请求后，进一步将IO请求发送给DMA，然后CPU就可以去干别的事了。 DMA将IO请求转发给磁盘。 磁盘驱动器收到内核的IO请求后，把数据读取到自己的缓冲区中，当磁盘的缓冲区被读满后，向DMA发起中断信号告知自己缓冲区已满。 DMA收到磁盘驱动器的信号，将磁盘缓冲区中的数据copy到内核缓冲区中，此时不占用CPU（ PIO 这里是占用CPU的）。 如果内核缓冲区的数据少于用户申请读的数据，则重复步骤3、4、5，直到内核缓冲区的数据符合用户的要求为止。 内核缓冲区的数据已经符合用户的要求，DMA停止向磁盘发IO请求。 DMA发送中断信号给CPU。 CPU收到DMA的信号，知道数据已经准备好，于是将数据从内核空间copy到用户空间，系统调用返回。 用户进程读取到数据后继续执行原来的任务。 跟PIO模式相比，DMA就是CPU的一个代理，它负责了一部分的拷贝工作，从而减轻了CPU的负担。 需要注意的是，DMA承担的工作是从磁盘的缓冲区到内核缓冲区或网卡设备到内核的 soket buffer的拷贝工作，以及内核缓冲区到磁盘缓冲区或内核的 soket buffer 到网卡设备的拷贝工作，而内核缓冲区到用户缓冲区之间的拷贝工作仍然由CPU负责。 可以肯定的是，PIO模式的计算机我们现在已经很少见到了。 缓冲IO和直接IO学习用户空间和内核空间的时候我们也说了，用户空间是不能直接访问内核空间的数据的，如果需要访问怎么办？很简单，就需要将数据从内核空间拷贝的用户空间。 缓冲 IO：其实就是磁盘中的数据通过 DMA 先拷贝到内核空间，然后再从内核空间拷贝到用户空间。 直接 IO：磁盘中的数据直接通过 DMA 拷贝到用户空间。 缓冲 IO缓冲 IO 也被成为标准 IO，大多数的文件系统系统默认都是以缓冲 IO 的方式来工作的。在Linux的缓冲I&#x2F;O机制中，数据先从磁盘复制到内核空间的缓冲区，然后从内核空间缓冲区复制到应用程序的地址空间。 接下来我们看看缓冲 IO 下读写操作是如何进行？ 读操作： 操作系统检查内核的缓冲区有没有需要的数据，如果已经缓冲了，那么就直接从缓冲中返回；否则从磁盘中读取到内核缓冲中，然后再复制到用户空间缓冲中。 写操作： 将数据从用户空间复制到内核空间的缓冲中。这时对用户程序来说写操作就已经完成，至于什么时候再写到磁盘中由操作系统决定，除非显示地调用了sync同步命令。 缓冲I&#x2F;O的优点： 在一定程度上分离了内核空间和用户空间，保护系统本身的运行安全； 因为内核中有缓冲，可以减少读盘的次数，从而提高性能。 缓冲I&#x2F;O的缺点： 在缓冲 I&#x2F;O 机制中，DMA 方式可以将数据直接从磁盘读到内核空间页缓冲中，或者将数据从内核空间页缓冲直接写回到磁盘上，而不能直接在用户地址空间和磁盘之间进行数据传输，这样数据在传输过程中需要在应用程序地址空间（用户空间）和内核缓冲（内核空间）之间进行多次数据拷贝操作，这些数据拷贝操作所带来的CPU以及内存开销是非常大的。 直接IO顾名思义，直接IO就是应用程序直接访问磁盘数据，而不经过内核缓冲区，也就是绕过内核缓冲区,自己管理I&#x2F;O缓冲区，这样做的目的是减少一次从内核缓冲区到用户程序缓冲的数据复制。 引入内核缓冲区这个主要是为了提升从磁盘读写数据文件的性能，这也是很多系统优化中常见的手段，多一层缓存可以有效减少很多磁盘 IO 操作；而当用户程序需要向磁盘文件中写入数据时，实际上只需要写入到内核缓冲区便可以返回了，而真正的落盘是有一定的延迟策略的，但这无疑提升了应用程序写入文件的响应速度。 在数据库管理系统这类应用中，它们更倾向于选择自己实现的缓存机制，因为数据库管理系统往往比操作系统更了解数据库中存放的数据，数据库管理系统可以提供一种更加有效的缓存机制来提高数据库中数据的存取性能。 直接I&#x2F;O的优点： 应用程序直接访问磁盘数据，不经过操作系统内核数据缓冲区，这样做的最直观目的是减少一次从内核缓冲区到用户程序缓冲的数据复制。这种方式通常用在数据库、消息中间件中，由应用程序来实现数据的缓存管理。 直接I&#x2F;O的缺点： 如果访问的数据不在应用程序缓冲中，那么每次数据都会直接从磁盘进行加载，这种直接加载会非常缓慢。通常 直接I&#x2F;O 跟 异步I&#x2F;O 结合使用会得到较好的性能。（异步IO：当访问数据的线程发出请求之后，线程会接着去处理其他事，而不是阻塞等待） IO 访问方式我们常说的 IO 操作，不仅仅是磁盘 IO，还有常见的网络数据传输即网络 IO。 磁盘 IO 读操作： 当应用程序调用read()方法时，操作系统检查内核高速缓冲区中是否存在需要的数据，如果存在，那么就直接把内核空间的数据copy到用户空间，供用户的应用程序使用。如果内核缓冲区没有需要的数据，通过通过DMA方式从磁盘中读取数据到内核缓冲区，然后由CPU控制，把内核空间的数据copy到用户空间。 这个过程会涉及到两次缓冲区copy，第一次是从磁盘到内核缓冲区，第二次是从内核缓冲区到用户缓冲区，第一次是DMA的copy，第二次是CPU的copy。 写操作： 当应用程序调用write()方法时，应用程序将数据从用户空间copy到内核空间的缓冲区中（如果用户空间没有相应的数据，则需要从磁盘—&gt;内核缓冲区—&gt;用户缓冲区），这时对用户程序来说写操作就已经完成，至于什么时候把数据再写到磁盘（从内核缓冲区到磁盘的写操作也由DMA控制，不需要cpu参与），由操作系统决定。除非应用程序显示地调用了sync命令，立即把数据写入磁盘。 如果应用程序没准备好写的数据，则必须先从磁盘读取数据才能执行写操作，这时会涉及到四次缓冲区的copy，第一次是从磁盘的缓冲区到内核缓冲区，第二次是从内核缓冲区到用户缓冲区，第三次是从用户缓冲区到内核缓冲区，第四次是从内核缓冲区写回到磁盘。前两次是为了读，后两次是为了写。这其中有两次 CPU 拷贝，两次DMA拷贝。 磁盘IO的延时： 为了读或写，磁头必须能移动到所指定的磁道上，并等待所指定的扇区的开始位置旋转到磁头下，然后再开始读或写数据。磁盘IO的延时分成以下三部分： 寻道时间：把磁头移动到指定磁道上所经历的时间； 旋转延迟时间 ：指定扇区移动到磁头下面所经历的时间； 传输时间 ：数据的传输时间（数据读出或写入的时间）； 网络 IO 读操作： 网络 IO 既可以从物理磁盘中读数据，也可以从Socket中读数据（从网卡中获取）。当从物理磁盘中读数据的时候，其流程和磁盘IO的读操作一样。当从Socket中读数据，应用程序需要等待客户端发送数据，如果客户端还没有发送数据，对应的应用程序将会被阻塞，直到客户端发送了数据，该应用程序才会被唤醒，从Socket协议栈（网卡）中读取客户端发送的数据到内核空间（这个过程也由DMA控制），然后把内核空间的数据 copy 到用户空间，供应用程序使用。 写操作： 为了简化描述，我们假设网络IO的数据从磁盘中获取，读写操作的流程如下： 当应用程序调用read()方法时，通过DMA方式将数据从磁盘拷贝到内核缓冲区； 由cpu控制，将内核缓冲区的数据拷贝到用户空间的缓冲区中，供应用程序使用； 当应用程序调用 write() 方法时，CPU 会把用户缓冲区中的数据 copy 到内核缓冲区的 Socket Buffer 中； 最后通过DMA方式将内核空间中的Socket Buffer拷贝到Socket协议栈（即网卡设备）中传输； 网络IO 的写操作也有四次缓冲区的copy，第一次是从磁盘缓冲区到内核缓冲区（由DMA控制），第二次是内核缓冲区到用户缓冲区（CPU控制），第三次是用户缓冲区到内核缓冲区的 Socket Buffer（由CPU控制），第四次是从内核缓冲区的 Socket Buffer 到网卡设备（由DMA控制）。四次缓冲区的copy工作两次由CPU控制，两次由DMA控制。 网络IO的延时: 网络IO主要延时是由：服务器响应延时+带宽限制+网络延时+跳转路由延时+本地接收延时 决定。一般为几十到几千毫秒，受环境影响较大。所以，一般来说，网络IO延时要大于磁盘IO延时(不过同数据中心的交互除外，会比磁盘 IO 更快)。 零拷贝 IO在上述IO中，一次读写操作要经过四次缓冲区的拷贝，并经历了四次内核态和用户态的切换。 零拷贝（zero copy）IO 技术减少不必要的内核缓冲区跟用户缓冲区之间的拷贝，从而减少CPU的开销和状态切换带来的开销，达到性能的提升。 我们还是对比上面不使用零拷贝时的网络 IO 传输过程来对比分析下： 和上图普通的网络 IO 传输过程对比，零拷贝的传输过程：硬盘 -&gt; kernel buffer (快速拷贝到kernel socket buffer) -&gt; Socket协议栈（网卡设备中）。 当应用程序调用read()方法时，通过DMA方式将数据从磁盘拷贝到内核缓冲区； 由CPU控制，将内核缓冲区的数据直接拷贝到另外一个与 Socket 相关的内核缓冲区，即kernel socket buffer； 然后由 DMA 把数据从 kernel socket buffer 直接拷贝给 Socket 协议栈（网卡设备中）； 这里，只经历了三次缓冲区的拷贝，第一次是从磁盘缓冲区到内核缓冲区，第二次是从内核缓冲区到 kernel socket buffer，第三次是从 kernel socket buffer 到Socket 协议栈（网卡设备中）。只发生两次内核态和用户态的切换，第一次是当应用程序调用read方法时，用户态切换到内核态执行read系统调用，第二次是将数据从网络中发送出去后系统调用返回，从内核态切换到用户态。 零拷贝（zero copy）的应用： Linux 下提供了zero copy的接口：sendfile和splice，用户可通过这两个接口实现零拷贝传输； Nginx 可以通过sendfile配置开启零拷贝； 在 Linux 系统中，Java NIO中 FileChannel.transferTo 的实现依赖于 sendfile()调用； Apache 使用了 sendfile64() 来传送文件，sendfile64() 是 sendfile() 的扩展实现； Kafka也用到了零拷贝的功能； 注意：零拷贝要求输入的fd必须是文件句柄，不能是socket，输出的fd必须是socket，也就是说，数据的来源必须是从本地的磁盘，而不能是从网络中，如果数据来源于socket，就不能使用零拷贝功能了。我们看一下sendfile接口就知道了： 12#include &lt;sys/sendfile.h&gt;ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count) out_fd：待写入文件描述符; in_fd：待读出文件描述符; offset：从读入文件流的哪个位置开始读，如果为空，则默认从起始位置开始； count：指定在文件描述符in_fd 和out_fd之间传输的字节数； 返回值：成功时，返回出传输的字节数，失败返回-1； in_fd 必须指向真实的文件，不能是socket和管道；而out_fd则必须是一个socket。由此可见，sendfile 几乎是专门为在网络上传输文件而设计的。 在Linxu系统中，一切皆文件，因此socket也是一个文件，也有文件句柄（或文件描述符）。 同步&amp;异步、阻塞&amp;非阻塞这两组概念，我接触编程以来，经过听到别人说服务端是 同步非阻塞模型 或者 异步阻塞的 IO 模型，也前后了解过几次，但是理解都不够透彻，特别是这个非阻塞和异步、同步和阻塞的概念很容易懵逼，每个人的说法都不一样，最近我耐心看了几篇文章，这次我感觉我是顿悟了，这里分享下我的理解： 同步和异步是针对应用程序向内核发起任务后的状态而言的：如果发起调用后，在没有得到结果之前，当前调用就不返回，不能接着做后面的事情，一直等待就是同步。异步就是发出调用后，虽然不能立即得到结果，但是可以继续执行后面的事情，等调用结果出来时，会通过状态、通知和回调来通知调用者。 举个例子加深下理解： 在互联网普及之前，我们去医院看病都是排队的模式，想看病就得排队等，直到轮到你，在此之前你必须一直排队等待，这个就是同步； 现在互联网普及了，都是直接大屏叫号，我们预约登记后，就可以去休息厅坐着，打游戏啥的都可以干，到自己看病的时候会有通知的，这就是异步； 阻塞blocking、非阻塞non-blocking，则聚焦的是CPU在等待结果的过程中的状态。 阻塞调用是指调用结果返回之前，当前线程会被挂起，只有在得到结果之后才会返回。你可能会把阻塞调用和同步调用等同起来，实际上它们是不同的，同步只是说必须等到出结果才可以返回，但是等的过程中线程可以是激活的，阻塞是说线程被挂起了。 非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。 比如前面的例子，排队的过程中什么也不能做就是阻塞，CPU 执行权是交出去的；一边排队，一边看手机就是非阻塞，CPU 执行权还在自己手里，但是没看完病之前依旧是在排队死等，所以还是同步的。 总结通过今天的学习，我们掌握了什么是 IO、常见的 IO 操作类型以及对应操作的原理，还有非常重要但是却很容易搞混的同步&amp;异步、阻塞&amp;非阻塞之间的区别，讲解的应该还是比较清楚的。 本文内容还是比较简单的，是一些基础知识，但是如果想深入学习网络编程这些基础是绕不开的，了解了操作系统对于 IO 操作的优化，才能搞明白各种高性能网络服务器的原理。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/473639031","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核Socket通信原理和实例讲解","path":"/2023/01/15/linux-docs/进程管理/Linux内核Socket通信原理和实例讲解/","content":"关于对 Socket 的认识，大致分为下面几个主题，Socket 是什么，Socket 是如何创建的，Socket 是如何连接并收发数据的，Socket 套接字的删除等。 Socket 是什么以及创建过程一个数据包经由应用程序产生，进入到协议栈中进行各种报文头的包装，然后操作系统调用网卡驱动程序指挥硬件，把数据发送到对端主机。整个过程的大体的图示如下。 我们大家知道，协议栈其实是位于操作系统中的一些协议的堆叠，这些协议包括 TCP、UDP、ARP、ICMP、IP等。 通常某个协议的设计都是为了解决某些问题，比如 TCP 的设计就负责安全可靠的传输数据，UDP 设计就是报文小，传输效率高，ARP 的设计是能够通过 IP 地址查询物理（Mac）地址，ICMP 的设计目的是返回错误报文给主机，IP 设计的目的是为了实现大规模主机的互联互通。 应用程序比如浏览器、电子邮件、文件传输服务器等产生的数据，会通过传输层协议进行传输，而应用程序是不会和传输层直接建立联系的，而是有一个能够连接应用层和传输层之间的套件，这个套件就是 Socket。 在上面这幅图中，应用程序包含 Socket 和解析器，解析器的作用就是向 DNS 服务器发起查询，查询目标 IP 地址。 应用程序的下面就是操作系统内部，操作系统内部包括协议栈，协议栈是一系列协议的堆叠。操作系统下面就是网卡驱动程序，网卡驱动程序负责控制网卡硬件，驱动程序驱动网卡硬件完成收发工作。 在操作系统内部有一块用于存放控制信息的存储空间，这块存储空间记录了用于控制通信的控制信息。其实这些控制信息就是 Socket 的实体，或者说存放控制信息的内存空间就是套接字的实体。 这里大家有可能不太清楚所以然，所以我用了一下 netstat 命令来给大伙看一下套接字是啥玩意。 我们在 Windows 的命令提示符中输入 123456netstat -ano# netstat 用于显示套接字内容 , -ano 是可选选项# a 不仅显示正在通信的套接字，还显示包括尚未开始通信等状态的所有套接字# n 显示 IP 地址和端口号# o 显示套接字的程序 PID 我的计算机会出现下面结果。 图中的每一行都相当于一个套接字，每一列也被称为一个元组，所以一个套接字就是五元组（协议、本地地址、外部地址、状态、PID）。有的时候也被叫做四元组，四元组不包括协议。 比如图中的第一行，它的协议就是 TCP，本地地址和远程地址都是 0.0.0.0，这表示通信还没有开始，IP 地址暂时还未确定，而本地端口已知是 135，但是远程端口还未知，此时的状态是 LISTENING，LISTENING 表示应用程序已经打开，正在等待与远程主机建立连接最后一个元组是 PID，即进程标识符，PID 就像我们的身份证号码，能够精确定位唯一的进程。 现在你可能对 Socket 有了一个基本的认识，现在喝口水，休息一下，让我们继续探究 Socket。 现在我有个问题，Socket 是如何创建的呢？ Socket 是和应用程序一起创建的。应用程序中有一个 socket 组件，在应用程序启动时，会调用 socket 申请创建套接字，协议栈会根据应用程序的申请创建套接字：首先分配一个套接字所需的内存空间，这一步相当于是为控制信息准备一个容器，但只有容器并没有实际作用，所以你还需要向容器中放入控制信息；如果你不申请创建套接字所需要的内存空间，你创建的控制信息也没有地方存放，所以分配内存空间，放入控制信息缺一不可。至此套接字的创建就已经完成了。 套接字创建完成后，会返回一个套接字描述符给应用程序，这个描述符相当于是区分不同套接字的号码牌。根据这个描述符，应用程序在委托协议栈收发数据时就需要提供这个描述符。 套接字连接套接字创建完成后，最终还是为数据收发服务的，在数据收发之前，还需要进行一步 connect，也就是建立连接的过程。这个连接并不是真实的连接：用一根水管插在两个电脑之间。 而是应用程序通过 TCP&#x2F;IP 协议标准从一个主机通过网络介质传输到另一个主机的过程。 套接字刚刚创建完成后，还没有数据，也不知道通信对象。在这种状态下，即使你让客户端应用程序委托协议栈发送数据，它也不知道发送到哪里。 所以浏览器需要根据网址来查询服务器的 IP 地址，做这项工作的协议是 DNS，查询到目标主机后，再把目标主机的 IP 告诉协议栈，至此，客户端这边就准备好了。 在服务器上，与客户端一样也需要创建套接字，但是同样的它也不知道通信对象是谁，所以我们需要让客户端向服务器告知客户端的必要信息：IP 地址和端口号。 现在通信双方建立连接的必要信息已经具备，只欠一股东南风了。通信双方收到数据之后，还需要一块位置来存放，这个位置就是缓冲区，它是内存的一部分，有了缓冲区，就能够进行数据的收发操作了。 OK，现在客户端想要给服务器发送一条数据，该进行哪些操作呢？ 首先，客户端应用程序需要调用 Socket 库中的 connect 方法，提供 socket 描述符和服务器 IP 地址、端口号。 1connect(&lt;描述符&gt;、&lt;服务器IP地址和端口号&gt;) 这些信息会传递给协议栈中的 TCP 模块，TCP 模块会对请求报文进行封装，再传递给 IP 模块，进行 IP 报文头的封装，然后传递给物理层，进行帧头封装，之后通过网络介质传递给服务器，服务器上会对帧头、IP 模块、TCP 模块的报文头进行解析，从而找到对应的套接字，套接字收到请求后，会写入相应的信息，并且把状态改为正在连接。请求过程完成后，服务器的 TCP 模块会返回响应，这个过程和客户端是一样的。 在一个完整的请求和响应过程中，控制信息起到非常关键的作用（具体的作用我们后面会说）。 SYN 就是同步的缩写，客户端会首先发送 SYN 数据包，请求服务端建立连接。 ACK 就是相应的意思，它是对发送 SYN 数据包的响应。 FIN 是终止的意思，它表示客户端&#x2F;服务器想要终止连接。 由于网络环境的复杂多变，经常会存在数据包丢失的情况，所以双方通信时需要相互确认对方的数据包是否已经到达，而判断的标准就是 ACK 的值。 当所有建立连接的报文都能够正常收发之后，此时套接字就已经进入可收发状态了，此时可以认为用一根管理把两个套接字连接了起来。 当然，实际上并不存在这个管子。建立连接之后，协议栈的连接操作就结束了，也就是说 connect 已经执行完毕，控制流程被交回给应用程序。 收发数据当控制流程从 connect 回到应用程序之后，接下来就会直接进入数据收发阶段，数据收发操作是从应用程序调用 write 将要发送的数据交给协议栈开始的，协议栈收到数据之后执行发送操作。 协议栈不会关心应用程序传输过来的是什么数据，因为这些数据最终都会转换为二进制序列，协议栈在收到数据之后并不会马上把数据发送出去，而是会将数据放在发送缓冲区，再等待应用程序发送下一条数据。 为什么收到数据包不会直接发送出去，而是放在缓冲区中呢？ 因为只要一旦收到数据就会发送，就有可能发送大量的小数据包，导致网络效率下降。所以协议栈需要将数据积攒到一定数量才能将其发送出去。 至于协议栈会向缓冲区放多少数据，这个不同版本和种类的操作系统有不同的说法，不过，所有的操作系统和种类都会遵循下面这几个标准： 第一个判断要素是每个网络包能够容纳的数据长度，判断的标准是 MTU，它表示的是一个网络包的最大长度。最大长度包含头部，所以如果单论数据区的话，就会用 MTU - 包头长度，由此的出来的最大数据长度被称为 MSS。 另一个判断标准是时间，当应用程序产生的数据比较少，协议栈向缓冲区放置数据效率不高时，如果每次都等到 MSS 再发送的话，可能因为等待时间太长造成延迟，在这种情况下，即使数据长度没有到达 MSS，也应该把数据发送出去。 协议栈并没有告诉我们怎样平衡这两个因素，如果数据长度优先，那么效率有可能比较低；如果时间优先，那又会降低网络的效率。 经过了一段时间。。。。。。 假设我们使用的是长度有限法则，此时缓冲区已满，协议栈要发送数据了，协议栈刚要把数据发送出去，却发现无法一次性传输这么大数据量（相对的）的数据，那怎么办呢？ 在这种情况下，发送缓冲区中的数据就会超过 MSS 的长度，发送缓冲区中的数据会以 MSS 大小为一个数据包进行拆分，拆分出来的每块数据都会加上 TCP，IP，以太网头部，然后被放进单独的网络包中。 到现在，网络包已经准备好发往服务器了，但是数据发送操作还没有结束，因为服务器还未确认是否已经收到网络包。因此在客户端发送数据包之后，还需要服务器进行确认。 TCP 模块在拆分数据时，会计算出网络包偏移量，这个偏移量就是相对于数据从头开始计算的第几个字节，并将算好的字节数写在 TCP 头部，TCP 模块还会生成一个网络包的序号（SYN），这个序号是唯一的，这个序号就是用来让服务器进行确认的。 服务器会对客户端发送过来的数据包进行确认，确认无误之后，服务器会生成一个序号和确认号（ACK）并一起发送给客户端，客户端确认之后再发送确认号给服务器。 我们来看一下实际的工作过程。 首先，客户端在连接时需要计算出序号初始值，并将这个值发送给服务器。接下来，服务器通过这个初始值计算出 确认号并返回给客户端。初始值在通信过程中有可能会丢弃，因此当服务器收到初始值后需要返回确认号用于确认。 同时，服务器也需要计算出从服务器到客户端方向的序号初始值，并将这个值发送给客户端。然后，客户端也需要根据服务器发来的初始值计算出确认号发送给服务器，至此，连接建立完成，接下来就可以进入数据收发阶段了。 数据收发阶段中，通信双方可以同时发送请求和响应，双方也可以同时对请求进行确认。 请求 - 确认机制非常强大，通过这一机制，我们可以确认接收方有没有收到某个包，如果没有收到则重新发送，这样一来，但凡网络中出现的任何错误，我们都可以即使发现并补救。 网卡、集线器、路由器都没有错误补救机制，一旦检测到错误就会直接丢弃数据包，应用程序也没有这种机制，起作用的只是 TCP&#x2F;IP 模块。 由于网络环境复杂多变，所以数据包会存在丢失情况，因此发送序号和确认号也存在一定规则，TCP 会通过窗口管理确认号。 断开连接当通信双方不再需要收发数据时，需要断开连接。不同的应用程序断开连接的时机不同。以 Web 为例，浏览器向 Web 服务器发送请求消息，Web 服务器再返回响应消息，这时收发数据就全部结束了，服务器可能会首先发起断开响应，当然客户端也有可能会首先发起（谁先断开连接是应用程序做出的判断），与协议栈无关。 无论哪一方发起断开连接的请求，都会调用 Socket 库的 close 程序。我们以服务器断开连接为例，服务器发起断开连接请求，协议栈会生成断开连接的 TCP 头部，其实就是设置 FIN 位，然后委托 IP 模块向客户端发送数据，与此同时，服务器的套接字会记录下断开连接的相关信息。 收到服务器发来 FIN 请求后，客户端协议栈会将套接字标记为断开连接状态，然后，客户端会向服务器返回一个确认号，这是断开连接的第一步，在这一步之后，应用程序还会调用 read 来读取数据。等到服务器数据发送完成后，协议栈会通知客户端应用程序数据已经接收完毕。 只要收到服务器返回的所有数据，客户端就会调用 close 程序来结束收发操作，这时客户端会生成一个 FIN 发送给服务器，一段时间后服务器返回 ACK 号，至此，客户端和服务器的通信就结束了。 删除套接字通信完成后，用来通信的套接字就不再会使用了，此时我们就可以删除这个套接字了。不过，这时候套接字不会马上删除，而是等过一段时间再删除。 等待这段时间是为了防止误操作，最常见的误操作就是客户端返回的确认号丢失，至于等待多长时间，和数据包重传的方式有关。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/456631465","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核crash分析内核死锁实践","path":"/2023/01/15/linux-docs/进程管理/Linux内核crash分析内核死锁实践/","content":"上文讲了内核死锁的debug方法通过lockdep的方式可以debug出死锁的信息，但是如果出问题的系统没有lockdep的配置，或者没有相关的日志该怎么办？这里分享通过crash工具来动态检测死锁时的问题 1、用crash初步分析一般卡死时可能是因为核心线程处在UNINTERRUPTIBLE状态，所以先在crash环境下用ps命令查看系统中UNINTERRUPTIBLE状态的线程，参数-u可过滤掉内核线程： bt命令可查看某个线程的调用栈，我们看一下上面UN状态的最关键的watchdog线程： 从调用栈中可以看到proc_pid_cmdline_read()函数中被阻塞的，对应的代码为： 12345678910111213static ssize_t proc_pid_cmdline_read(struct file *file, char __user *buf, size_t _count, loff_t *pos)&#123; ...... tsk = get_proc_task(file_inode(file)); if (!tsk) return -ESRCH; mm = get_task_mm(tsk); put_task_struct(tsk); ...... down_read(&amp;mm-&gt;mmap_sem); ......&#125; 这里是要获取被某个线程mm的mmap_sem锁，而这个锁又被另外一个线程持有。 2、推导读写锁要想知道哪个线程持有了这把锁，我们得先用汇编推导出这个锁的具体值。可用dis命令看一下proc_pid_cmdline_read()的汇编代码： 0xffffff99a680aaa0处就是调用down_read()的地方，它的第一个参数x0就是sem锁，如： 1void __sched down_read(struct rw_semaphore *sem) x0和x28寄存器存放的就是sem的值，那x21自然就是mm_struct的地址了，因为mm_struct的mmap_sem成员的offset就是104（0x68），用whatis命令可以查看结构体的声明，如： 因此我们只需要知道x21或者x28就知道mm和mmap_sem锁的值。 函数调用时被调用函数会在自己的栈帧中保存即将被修改到的寄存器，所以我们可以在down_read()及它之后的函数调用中找到这两个寄存器： 也就是说下面几个函数中，只要找到用到x21或x28，必然会在它的栈帧中保存这些寄存器。 先从最底部的down_read()开始找： 显然它没有用到x21或x28，继续看rwsem_down_read_failed()的汇编代码： 在这个函数中找到x21，它保存在rwsem_down_read_failed栈帧的偏移32字节的位置。rwsem_down_read_failed()的sp是0xffffffd6d9e4bcb0 sp + 32 &#x3D;0xffffffd6d9e4bcd0，用rd命令查看地址0xffffffd6d9e4bcd0中存放的x21的值为： 用struct命令查看这个mm_struct： 这里的owner是mm_struct所属线程的task_struct： sem锁的地址为0xffffffd76e349a00+0x68 &#x3D; 0xffffffd76e349a68，所以： 分析到这里我们知道watchdog线程是在读取1651线程的proc节点时被阻塞了，原因是这个进程的mm，它的mmap_sem锁被其他线程给拿住了，那到底是谁持了这把锁呢？ 3、持读写锁的线程带着问题我们继续分析，通过search命令加-t参数从系统中所有的线程的栈空间里查找当前锁： 一般锁的值都会保存在寄存器中，而寄存器又会在子函数调用过程中保存在栈中。所以只要在栈空间中找到当前锁的值（0xffffffd76e349a68），那这个线程很可能就是持锁或者等锁线程 这里搜出的20个线程中19个就是前面提到的等锁线程，剩下的1个很可能就是持锁线程了： 查看这个线程的调用栈： 由于2124线程中存放锁的地址是0xffffffd6d396b8b0，这个是在handle_mm_fault()的栈帧范围内，因此可以推断持锁的函数应该是在handle_mm_fault()之前。 我们先看一下do_page_fault函数： 代码中确实是存在持mmap_sem的地方，并且是读者，因此可以确定是2124持有的读写锁阻塞了watchdog在内的19个线程。 接下来我们需要看一下2124线程为什么会持锁后迟迟不释放就可以了。 4、死锁可以看出2124线程是等待fuse的处理结果，而我们知道fuse的请求是sdcard来处理的。 在log中我们看到的确有sdcard相关的UNINTERRUPTIBLE状态的线程2767： 得2767线程等待的mutex锁是0xffffffd6948f4090， 它的owner的task和pid为： 先通过bt命令查找2124的栈范围为0xffffffd6d396b4b0～0xffffffd6d396be70： 从栈里面可以找到mutex： mutex值在ffffffd6d396bc40这个地址上找到了，它是在__generic_file_write_iter的栈帧里。 那可以肯定是在__generic_file_write_iter之前就持锁了，并且很可能是ext4_file_write_iter中，查看其源码： 这下清楚了，原来2124在等待2767处理fuse请求，而2767又被2124线程持有的mutex锁给锁住了，也就是说两个线程互锁了。 版权声明：本文为知乎博主「Linux内核库」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文 出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/530352450","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核之进程和线程的创建和派生","path":"/2023/01/15/linux-docs/进程管理/Linux内核之进程和线程的创建和派生/","content":"1、前言在前文中，我们分析了内核中进程和线程的统一结构体task_struct，本文将继续分析进程、线程的创建和派生的过程。首先介绍如何将一个程序编辑为执行文件最后成为进程执行，然后会介绍线程的执行，最后会分析如何通过已有的进程、线程实现多进程、多线程。因为进程和线程有诸多相似之处，也有一些不同之处，因此本文会对比进程和线程来加深理解和记忆。 2、 进程的创建以C语言为例，我们在Linux下编写C语言代码，然后通过gcc编译和链接生成可执行文件后直接执行即可完成一个进程的创建和工作。下面将详细介绍这个创建进程的过程。在 Linux 下面，二进制的程序也要有严格的格式，这个格式我们称为 ELF（Executable and Linkable Format，可执行与可链接格式）。这个格式可以根据编译的结果不同，分为不同的格式。主要包括 1、可重定位的对象文件(Relocatable file) 由汇编器汇编生成的 .o 文件 2、可执行的对象文件(Executable file) 可执行应用程序 3、可被共享的对象文件(Shared object file) 动态库文件，也就是 .so 文件 下面在进程创建过程中会详细说明三种文件。 2. 1 编译写完C程序后第一步就是程序编译（其实还有IDE的预编译，那些属于编辑器操作这里不表）。编译指令如下所示 1gcc -c -fPIC xxxx.c -c表示编译、汇编指定的源文件，不进行链接。-fPIC表示生成与位置无关（Position-Independent Code）代码，即采用相对地址而非绝对地址，从而满足共享库加载需求。在编译的时候，先做预处理工作，例如将头文件嵌入到正文中，将定义的宏展开，然后就是真正的编译过程，最终编译成为.o 文件，这就是 ELF 的第一种类型，可重定位文件（Relocatable File）。之所以叫做可重定位文件，是因为对于编译好的代码和变量，将来加载到内存里面的时候，都是要加载到一定位置的。比如说，调用一个函数，其实就是跳到这个函数所在的代码位置执行；再比如修改一个全局变量，也是要到变量的位置那里去修改。但是现在这个时候，还是.o 文件，不是一个可以直接运行的程序，这里面只是部分代码片段。因此.o 里面的位置是不确定的，但是必须要重新定位以适应需求。 ELF文件的开头是用于描述整个文件的。这个文件格式在内核中有定义，分别为 struct elf32_hdr 和struct elf64_hdr。 其他各个section作用如下所示： .text：放编译好的二进制可执行代码 .rodata：只读数据，例如字符串常量、const 的变量 .data：已经初始化好的全局变量 .bss：未初始化全局变量，运行时会置 0 .symtab：符号表，记录的则是函数和变量 .rel.text： .text部分的重定位表 .rel.data：.data部分的重定位表 .strtab：字符串表、字符串常量和变量名 这些节的元数据信息也需要有一个地方保存，就是最后的节头部表（Section Header Table）。在这个表里面，每一个 section 都有一项，在代码里面也有定义 struct elf32_shdr和struct elf64_shdr。在 ELF 的头里面，有描述这个文件的接头部表的位置，有多少个表项等等信息。 2.2 链接链接分为静态链接和动态链接。静态链接库会和目标文件通过链接生成一个可执行文件，而动态链接则会通过链接形成动态连接器，在可执行文件执行的时候动态的选择并加载其中的部分或全部函数。 二者的各自优缺点如下所示： 静态链接库的优点 (1) 代码装载速度快，执行速度略比动态链接库快； (2) 只需保证在开发者的计算机中有正确的.LIB文件，在以二进制形式发布程序时不需考虑在用户的计算机上.LIB文件是否存在及版本问题，可避免DLL地狱等问题。 静态链接库的缺点 使用静态链接生成的可执行文件体积较大，包含相同的公共代码，造成浪费 动态链接库的优点 (1) 更加节省内存并减少页面交换； (2) DLL文件与EXE文件独立，只要输出接口不变（即名称、参数、返回值类型和调用约定不变），更换DLL文件不会对EXE文件造成任何影响，因而极大地提高了可维护性和可扩展性； (3) 不同编程语言编写的程序只要按照函数调用约定就可以调用同一个DLL函数； (4)适用于大规模的软件开发，使开发过程独立、耦合度小，便于不同开发者和开发组织之间进行开发和测试。 动态链接库的缺点 使用动态链接库的应用程序不是自完备的，它依赖的DLL模块也要存在，如果使用载入时动态链接，程序启动时发现DLL不存在，系统将终止程序并给出错误信息。而使用运行时动态链接，系统不会终止，但由于DLL中的导出函数不可用，程序会加载失败；速度比静态连接慢。当某个模块更新后，如果新的模块与旧的模块不兼容，那么那些需要该模块才能运行的软件均无法执行。这在早期Windows中很常见。 下面分别介绍静态链接和动态链接： 2.2.1 静态链接静态链接库.a文件（Archives）的执行指令如下 1ar cr libXXX.a XXX.o XXXX.o 当需要使用该静态库的时候，会将.o文件从.a文件中依次抽取并链接到程序中，指令如下 1gcc -o XXXX XXX.O -L. -lsXXX -L表示在当前目录下找.a 文件，-lsXXXX会自动补全文件名，比如加前缀 lib，后缀.a，变成libXXX.a，找到这个.a文件后，将里面的 XXXX.o 取出来，和 XXX.o 做一个链接，形成二进制执行文件XXXX。在这里，重定位会从.o中抽取函数并和.a中的文件抽取的函数进行合并，找到实际的调用位置，形成最终的可执行文件(Executable file)，即ELF的第二种格式文件。 对比ELF第一种格式可重定位文件，这里可执行文件略去了重定位表相关段落。此处将ELF文件分为了代码段、数据段和不加载到内存中的部分，并加上了段头表（Segment Header Table）用以记录管理，在代码中定义为struct elf32_phdr和 struct elf64_phdr，这里面除了有对于段的描述之外，最重要的是 p_vaddr，这个是这个段加载到内存的虚拟地址。这部分会在内存篇章详细介绍。 2.2.2 动态链接动态链接库（Shared Libraries)的作用主要是为了解决静态链接大量使用会造成空间浪费的问题，因此这里设计成了可以被多个程序共享的形式，其执行命令如下 1gcc -shared -fPIC -o libXXX.so XXX.o 当一个动态链接库被链接到一个程序文件中的时候，最后的程序文件并不包括动态链接库中的代码，而仅仅包括对动态链接库的引用，并且不保存动态链接库的全路径，仅仅保存动态链接库的名称。 1gcc -o XXX XXX.O -L. -lXXX 当运行这个程序的时候，首先寻找动态链接库，然后加载它。默认情况下，系统在 &#x2F;lib 和&#x2F;usr&#x2F;lib 文件夹下寻找动态链接库。如果找不到就会报错，我们可以设定 LD_LIBRARY_PATH环境变量，程序运行时会在此环境变量指定的文件夹下寻找动态链接库。动态链接库，就是 ELF 的第三种类型，共享对象文件（Shared Object）。 动态链接的ELF相对于静态链接主要多了以下部分： .interp段，里面是ld-linux.so，负责运行时的链接动作 .plt（Procedure Linkage Table），过程链接表 .got.plt（Global Offset Table），全局偏移量表 当程序编译时，会对每个函数在PLT中建立新的项，如PLT[n]，而动态库中则存有该函数的实际地址，记为GOT[m]。 整体寻址过程如下所示： PLT[n]向GOT[m]寻求地址 GOT[m]初始并无地址，需要采取以下方式获取地址 回调PLT[0] PLT[0]调用GOT[2]，即ld-linux.so ld-linux.so查找所需函数实际地址并存放在GOT[m]中 由此，我们建立了PLT[n]到GOT[m]的对应关系，从而实现了动态链接。 2.3 加载运行完成了上述的编译、汇编、链接，我们最终形成了可执行文件，并加载运行。在内核中，有这样一个数据结构，用来定义加载二进制文件的方法。 12345678struct linux_binfmt &#123; struct list_head lh; struct module *module; int (*load_binary)(struct linux_binprm *); int (*load_shlib)(struct file *); int (*core_dump)(struct coredump_params *cprm); unsigned long min_coredump; /* minimal dump size */&#125; __randomize_layout; 对于ELF文件格式，其对应实现为： 1234567static struct linux_binfmt elf_format = &#123; .module = THIS_MODULE, .load_binary = load_elf_binary, .load_shlib = load_elf_library, .core_dump = elf_core_dump, .min_coredump = ELF_EXEC_PAGESIZE,&#125;; 其中加载的函数指针指向的函数和内核镜像加载是同一份函数，实际上通过exec函数完成调用。exec 比较特殊，它是一组函数： 包含 p 的函数（execvp, execlp）会在 PATH 路径下面寻找程序；不包含 p 的函数需要输入程序的全路径； 包含 v 的函数（execv, execvp, execve）以数组的形式接收参数； 包含 l 的函数（execl, execlp, execle）以列表的形式接收参数； 包含 e 的函数（execve, execle）以数组的形式接收环境变量。 当我们通过shell运行可执行文件或者通过fork派生子类，均是通过该类函数实现加载。 3、线程的创建之用户态线程的创建对应的函数是pthread_create()，线程不是一个完全由内核实现的机制，它是由内核态和用户态合作完成的。pthread_create()不是一个系统调用，是 Glibc 库的一个函数，所以我们还要从 Glibc 说起。但是在开始之前，我们先要提一下，线程的创建到了内核态和进程的派生会使用同一个函数：__do_fork()，这也很容易理解，因为对内核态来说，线程和进程是同样的task_struct结构体。本节介绍线程在用户态的创建，而内核态的创建则会和进程的派生放在一起说明。 在Glibc的ntpl&#x2F;pthread_create.c中定义了__pthread_create_2_1()函数，该函数主要进行了以下操作 处理线程的属性参数。例如前面写程序的时候，我们设置的线程栈大小。如果没有传入线程属性，就取默认值。 123456789const struct pthread_attr *iattr = (struct pthread_attr *) attr;struct pthread_attr default_attr;//c11 thrd_createbool c11 = (attr == ATTR_C11_THREAD);if (iattr == NULL || c11)&#123; ...... iattr = &amp;default_attr;&#125; 就像在内核里每一个进程或者线程都有一个 task_struct 结构，在用户态也有一个用于维护线程的结构，就是这个 pthread 结构。 1struct pthread *pd = NULL; 凡是涉及函数的调用，都要使用到栈。每个线程也有自己的栈，接下来就是创建线程栈了。 1int err = ALLOCATE_STACK (iattr, &amp;pd); ALLOCATE_STACK 是一个宏，对应的函数allocate_stack()主要做了以下这些事情： 如果在线程属性里面设置过栈的大小，则取出属性值； 为了防止栈的访问越界在栈的末尾添加一块空间 guardsize，一旦访问到这里就会报错； 线程栈是在进程的堆里面创建的。如果一个进程不断地创建和删除线程，我们不可能不断地去申请和清除线程栈使用的内存块，这样就需要有一个缓存。get_cached_stack 就是根据计算出来的 size 大小，看一看已经有的缓存中，有没有已经能够满足条件的。如果缓存里面没有，就需要调用__mmap创建一块新的缓存，系统调用那一节我们讲过，如果要在堆里面 malloc 一块内存，比较大的话，用__mmap； 线程栈也是自顶向下生长的，每个线程要有一个pthread 结构，这个结构也是放在栈的空间里面的。在栈底的位置，其实是地址最高位； 计算出guard内存的位置，调用 setup_stack_prot 设置这块内存的是受保护的； 填充pthread 这个结构里面的成员变量 stackblock、stackblock_size、guardsize、specific。这里的 specific 是用于存放Thread Specific Data 的，也即属于线程的全局变量； 将这个线程栈放到 stack_used 链表中，其实管理线程栈总共有两个链表，一个是 stack_used，也就是这个栈正被使用；另一个是stack_cache，就是上面说的，一旦线程结束，先缓存起来，不释放，等有其他的线程创建的时候，给其他的线程用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# define ALLOCATE_STACK(attr, pd) allocate_stack (attr, pd, &amp;stackaddr)static intallocate_stack (const struct pthread_attr *attr, struct pthread **pdp, ALLOCATE_STACK_PARMS)&#123; struct pthread *pd; size_t size; size_t pagesize_m1 = __getpagesize () - 1;...... /* Get the stack size from the attribute if it is set. Otherwise we use the default we determined at start time. */ if (attr-&gt;stacksize != 0) size = attr-&gt;stacksize; else &#123; lll_lock (__default_pthread_attr_lock, LLL_PRIVATE); size = __default_pthread_attr.stacksize; lll_unlock (__default_pthread_attr_lock, LLL_PRIVATE); &#125;...... /* Allocate some anonymous memory. If possible use the cache. */ size_t guardsize; void *mem; const int prot = (PROT_READ | PROT_WRITE | ((GL(dl_stack_flags) &amp; PF_X) ? PROT_EXEC : 0)); /* Adjust the stack size for alignment. */ size &amp;= ~__static_tls_align_m1; /* Make sure the size of the stack is enough for the guard and eventually the thread descriptor. */ guardsize = (attr-&gt;guardsize + pagesize_m1) &amp; ~pagesize_m1; size += guardsize;...... /* Try to get a stack from the cache. */ pd = get_cached_stack (&amp;size, &amp;mem); if (pd == NULL) &#123; /* If a guard page is required, avoid committing memory by first allocate with PROT_NONE and then reserve with required permission excluding the guard page. */ mem = __mmap (NULL, size, (guardsize == 0) ? prot : PROT_NONE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_STACK, -1, 0); /* Place the thread descriptor at the end of the stack. */#if TLS_TCB_AT_TP pd = (struct pthread *) ((char *) mem + size) - 1;#elif TLS_DTV_AT_TP pd = (struct pthread *) ((((uintptr_t) mem + size - __static_tls_size) &amp; ~__static_tls_align_m1) - TLS_PRE_TCB_SIZE);#endif /* Now mprotect the required region excluding the guard area. */ char *guard = guard_position (mem, size, guardsize, pd, pagesize_m1); setup_stack_prot (mem, size, guard, guardsize, prot); pd-&gt;stackblock = mem; pd-&gt;stackblock_size = size; pd-&gt;guardsize = guardsize; pd-&gt;specific[0] = pd-&gt;specific_1stblock; /* And add to the list of stacks in use. */ stack_list_add (&amp;pd-&gt;list, &amp;stack_used); &#125; *pdp = pd; void *stacktop;# if TLS_TCB_AT_TP /* The stack begins before the TCB and the static TLS block. */ stacktop = ((char *) (pd + 1) - __static_tls_size);# elif TLS_DTV_AT_TP stacktop = (char *) (pd - 1);# endif *stack = stacktop;...... &#125; 4、线程的内核态创建及进程的派生多进程是一种常见的程序实现方式，采用的系统调用为fork()函数。前文中已经详细叙述了系统调用的整个过程，对于fork()来说，最终会在系统调用表中查找到对应的系统调用sys_fork完成子进程的生成，而sys_fork 会调用 _do_fork()。 12345SYSCALL_DEFINE0(fork)&#123;...... return _do_fork(SIGCHLD, 0, 0, NULL, NULL, 0);&#125; 关于__do_fork()先按下不表，再接着看看线程。我们接着pthread_create ()看。其实有了用户态的栈，接着需要解决的就是用户态的程序从哪里开始运行的问题。start_routine() 就是给线程的函数，start_routine()， 参数 arg，以及调度策略都要赋值给 pthread。接下来 __nptl_nthreads 加一，说明又多了一个线程。 12345678pd-&gt;start_routine = start_routine;pd-&gt;arg = arg;pd-&gt;schedpolicy = self-&gt;schedpolicy;pd-&gt;schedparam = self-&gt;schedparam;/* Pass the descriptor to the caller. */*newthread = (pthread_t) pd;atomic_increment (&amp;__nptl_nthreads);retval = create_thread (pd, iattr, &amp;stopped_start, STACK_VARIABLES_ARGS, &amp;thread_ran); 真正创建线程的是调用 create_thread() 函数，这个函数定义如下。同时，这里还规定了当完成了内核态线程创建后回调的位置：start_thread()。 12345678910static intcreate_thread (struct pthread *pd, const struct pthread_attr *attr,bool *stopped_start, STACK_VARIABLES_PARMS, bool *thread_ran)&#123; const int clone_flags = (CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SYSVSEM | CLONE_SIGHAND | CLONE_THREAD | CLONE_SETTLS | CLONE_PARENT_SETTID | CLONE_CHILD_CLEARTID | 0); ARCH_CLONE (&amp;start_thread, STACK_VARIABLES_ARGS, clone_flags, pd, &amp;pd-&gt;tid, tp, &amp;pd-&gt;tid)； /* It&#x27;s started now, so if we fail below, we&#x27;ll have to cancel it and let it clean itself up. */ *thread_ran = true;&#125; 在 start_thread() 入口函数中，才真正的调用用户提供的函数，在用户的函数执行完毕之后，会释放这个线程相关的数据。例如，线程本地数据 thread_local variables，线程数目也减一。如果这是最后一个线程了，就直接退出进程，另外 __free_tcb() 用于释放 pthread。 1234567891011121314151617#define START_THREAD_DEFN \\ static int __attribute__ ((noreturn)) start_thread (void *arg)START_THREAD_DEFN&#123; struct pthread *pd = START_THREAD_SELF; /* Run the code the user provided. */ THREAD_SETMEM (pd, result, pd-&gt;start_routine (pd-&gt;arg)); /* Call destructors for the thread_local TLS variables. */ /* Run the destructor for the thread-local data. */ __nptl_deallocate_tsd (); if (__glibc_unlikely (atomic_decrement_and_test (&amp;__nptl_nthreads))) /* This was the last thread. */ exit (0); __free_tcb (pd); __exit_thread ();&#125; __free_tcb ()会调用 __deallocate_stack()来释放整个线程栈，这个线程栈要从当前使用线程栈的列表 stack_used 中拿下来，放到缓存的线程栈列表 stack_cache中，从而结束了线程的生命周期。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869voidinternal_function__free_tcb (struct pthread *pd)&#123; ...... __deallocate_stack (pd);&#125;voidinternal_function__deallocate_stack (struct pthread *pd)&#123; /* Remove the thread from the list of threads with user defined stacks. */ stack_list_del (&amp;pd-&gt;list); /* Not much to do. Just free the mmap()ed memory. Note that we do not reset the &#x27;used&#x27; flag in the &#x27;tid&#x27; field. This is done by the kernel. If no thread has been created yet this field is still zero. */ if (__glibc_likely (! pd-&gt;user_stack)) (void) queue_stack (pd);&#125; ARCH_CLONE其实调用的是 __clone()。# define ARCH_CLONE __clone/* The userland implementation is: int clone (int (*fn)(void *arg), void *child_stack, int flags, void *arg), the kernel entry is: int clone (long flags, void *child_stack). The parameters are passed in register and on the stack from userland: rdi: fn rsi: child_stack rdx: flags rcx: arg r8d: TID field in parent r9d: thread pointer%esp+8: TID field in child The kernel expects: rax: system call number rdi: flags rsi: child_stack rdx: TID field in parent r10: TID field in child r8: thread pointer */ .textENTRY (__clone) movq $-EINVAL,%rax...... /* Insert the argument onto the new stack. */ subq $16,%rsi movq %rcx,8(%rsi) /* Save the function pointer. It will be popped off in the child in the ebx frobbing below. */ movq %rdi,0(%rsi) /* Do the system call. */ movq %rdx, %rdi movq %r8, %rdx movq %r9, %r8 mov 8(%rsp), %R10_LP movl $SYS_ify(clone),%eax...... syscall......PSEUDO_END (__clone) 内核中的clone()定义如下。如果在进程的主线程里面调用其他系统调用，当前用户态的栈是指向整个进程的栈，栈顶指针也是指向进程的栈，指令指针也是指向进程的主线程的代码。此时此刻执行到这里，调用 clone的时候，用户态的栈、栈顶指针、指令指针和其他系统调用一样，都是指向主线程的。但是对于线程来说，这些都要变。因为我们希望当 clone 这个系统调用成功的时候，除了内核里面有这个线程对应的 task_struct，当系统调用返回到用户态的时候，用户态的栈应该是线程的栈，栈顶指针应该指向线程的栈，指令指针应该指向线程将要执行的那个函数。所以这些都需要我们自己做，将线程要执行的函数的参数和指令的位置都压到栈里面，当从内核返回，从栈里弹出来的时候，就从这个函数开始，带着这些参数执行下去。 1234567SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp, int __user *, parent_tidptr, int __user *, child_tidptr, unsigned long, tls)&#123; return _do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr, tls);&#125; 线程和进程到了这里殊途同归，进入了同一个函数__do_fork()工作。其源码如下所示，主要工作包括复制结构copy_process()和唤醒新进程wak_up_new()两部分。其中线程会根据create_thread()函数中的clone_flags完成上文所述的栈顶指针和指令指针的切换，以及一些线程和进程的微妙区别。 123456789101112131415161718192021222324252627282930313233long _do_fork(unsigned long clone_flags, unsigned long stack_start, unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr, unsigned long tls)&#123; struct task_struct *p; int trace = 0; long nr;...... p = copy_process(clone_flags, stack_start, stack_size, child_tidptr, NULL, trace, tls, NUMA_NO_NODE);...... if (IS_ERR(p)) return PTR_ERR(p); struct pid *pid; pid = get_task_pid(p, PIDTYPE_PID); nr = pid_vnr(pid); if (clone_flags &amp; CLONE_PARENT_SETTID) put_user(nr, parent_tidptr); if (clone_flags &amp; CLONE_VFORK) &#123; p-&gt;vfork_done = &amp;vfork; init_completion(&amp;vfork); get_task_struct(p); &#125; wake_up_new_task(p);...... put_pid(pid); return nr;&#125;; 4.1 任务结构体复制如下所示为copy_process()函数源码精简版，task_struct结构复杂也注定了复制过程的复杂性，因此此处省略了很多，仅保留了各个部分的主要调用函数 1234567891011121314151617181920212223242526272829303132333435363738394041static __latent_entropy struct task_struct *copy_process( unsigned long clone_flags, unsigned long stack_start, unsigned long stack_size, int __user *child_tidptr, struct pid *pid, int trace, unsigned long tls, int node)&#123; int retval; struct task_struct *p;...... //分配task_struct结构 p = dup_task_struct(current, node); ...... //权限处理 retval = copy_creds(p, clone_flags);...... //设置调度相关变量 retval = sched_fork(clone_flags, p); ...... //初始化文件和文件系统相关变量 retval = copy_files(clone_flags, p); retval = copy_fs(clone_flags, p); ...... //初始化信号相关变量 init_sigpending(&amp;p-&gt;pending); retval = copy_sighand(clone_flags, p); retval = copy_signal(clone_flags, p); ...... //拷贝进程内存空间 retval = copy_mm(clone_flags, p);...... //初始化亲缘关系变量 INIT_LIST_HEAD(&amp;p-&gt;children); INIT_LIST_HEAD(&amp;p-&gt;sibling);...... //建立亲缘关系 //源码放在后面说明 &#125;; 1、copy_process()首先调用了dup_task_struct()分配task_struct结构，dup_task_struct() 主要做了下面几件事情： 调用 alloc_task_struct_node 分配一个 task_struct结构； 调用 alloc_thread_stack_node 来创建内核栈，这里面调用 __vmalloc_node_range 分配一个连续的 THREAD_SIZE 的内存空间，赋值给 task_struct 的 void *stack成员变量； 调用 arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)，将 task_struct 进行复制，其实就是调用 memcpy； 调用setup_thread_stack设置 thread_info。 12345678910111213141516171819202122static struct task_struct *dup_task_struct(struct task_struct *orig, int node)&#123; struct task_struct *tsk; unsigned long *stack;...... tsk = alloc_task_struct_node(node); if (!tsk) return NULL; stack = alloc_thread_stack_node(tsk, node); if (!stack) goto free_tsk; if (memcg_charge_kernel_stack(tsk)) goto free_stack; stack_vm_area = task_stack_vm_area(tsk); err = arch_dup_task_struct(tsk, orig);...... setup_thread_stack(tsk, orig);...... &#125;; 2、接着，调用copy_creds处理权限相关内容 调用prepare_creds，准备一个新的 struct cred *new。如何准备呢？其实还是从内存中分配一个新的 struct cred结构，然后调用 memcpy 复制一份父进程的 cred； 接着 p-&gt;cred &#x3D; p-&gt;real_cred &#x3D; get_cred(new)，将新进程的“我能操作谁”和“谁能操作我”两个权限都指向新的 cred。 123456789101112131415161718192021222324/* * Copy credentials for the new process created by fork() * * We share if we can, but under some circumstances we have to generate a new * set. * * The new process gets the current process&#x27;s subjective credentials as its * objective and subjective credentials */int copy_creds(struct task_struct *p, unsigned long clone_flags)&#123; struct cred *new; int ret;...... new = prepare_creds(); if (!new) return -ENOMEM;...... atomic_inc(&amp;new-&gt;user-&gt;processes); p-&gt;cred = p-&gt;real_cred = get_cred(new); alter_cred_subscribers(new, 2); validate_creds(new); return 0;&#125; 3、设置调度相关的变量。该部分源码先不展示，会在进程调度中详细介绍。sched_fork主要做了下面几件事情： 调用__sched_fork，在这里面将on_rq设为 0，初始化sched_entity，将里面的 exec_start、sum_exec_runtime、prev_sum_exec_runtime、vruntime 都设为 0。这几个变量涉及进程的实际运行时间和虚拟运行时间。是否到时间应该被调度了，就靠它们几个； 设置进程的状态 p-&gt;state &#x3D; TASK_NEW； 初始化优先级 prio、normal_prio、static_prio； 设置调度类，如果是普通进程，就设置为 p-&gt;sched_class &#x3D; &amp;fair_sched_class； 调用调度类的 task_fork 函数，对于 CFS 来讲，就是调用 task_fork_fair。在这个函数里，先调用 update_curr，对于当前的进程进行统计量更新，然后把子进程和父进程的 vruntime 设成一样，最后调用 place_entity，初始化 sched_entity。这里有一个变量 sysctl_sched_child_runs_first，可以设置父进程和子进程谁先运行。如果设置了子进程先运行，即便两个进程的 vruntime 一样，也要把子进程的 sched_entity 放在前面，然后调用 resched_curr，标记当前运行的进程 TIF_NEED_RESCHED，也就是说，把父进程设置为应该被调度，这样下次调度的时候，父进程会被子进程抢占。 4、初始化文件和文件系统相关变量 copy_files 主要用于复制一个任务打开的文件信息。 对于进程来说，这些信息用一个结构 files_struct 来维护，每个打开的文件都有一个文件描述符。在 copy_files 函数里面调用 dup_fd，在这里面会创建一个新的 files_struct，然后将所有的文件描述符数组 fdtable 拷贝一份。 对于线程来说，由于设置了CLONE_FILES 标识位变成将原来的files_struct 引用计数加一，并不会拷贝文件。 123456789101112131415161718192021222324static int copy_files(unsigned long clone_flags, struct task_struct *tsk)&#123; struct files_struct *oldf, *newf; int error = 0; /* * A background process may not have any files ... */ oldf = current-&gt;files; if (!oldf) goto out; if (clone_flags &amp; CLONE_FILES) &#123; atomic_inc(&amp;oldf-&gt;count); goto out; &#125; newf = dup_fd(oldf, &amp;error); if (!newf) goto out; tsk-&gt;files = newf; error = 0;out: return error;&#125; copy_fs 主要用于复制一个任务的目录信息。 对于进程来说，这些信息用一个结构 fs_struct 来维护。一个进程有自己的根目录和根文件系统 root，也有当前目录 pwd 和当前目录的文件系统，都在 fs_struct 里面维护。copy_fs 函数里面调用 copy_fs_struct，创建一个新的 fs_struct，并复制原来进程的 fs_struct。 对于线程来说，由于设置了CLONE_FS 标识位变成将原来的fs_struct 的用户数加一，并不会拷贝文件系统结构。 12345678910111213141516171819static int copy_fs(unsigned long clone_flags, struct task_struct *tsk)&#123; struct fs_struct *fs = current-&gt;fs; if (clone_flags &amp; CLONE_FS) &#123; /* tsk-&gt;fs is already what we want */ spin_lock(&amp;fs-&gt;lock); if (fs-&gt;in_exec) &#123; spin_unlock(&amp;fs-&gt;lock); return -EAGAIN; &#125; fs-&gt;users++; spin_unlock(&amp;fs-&gt;lock); return 0; &#125; tsk-&gt;fs = copy_fs_struct(fs); if (!tsk-&gt;fs) return -ENOMEM; return 0;&#125; 5、初始化信号相关变量 整个进程里的所有线程共享一个shared_pending，这也是一个信号列表，是发给整个进程的，哪个线程处理都一样。由此我们可以做到发给进程的信号虽然可以被一个线程处理，但是影响范围应该是整个进程的。例如，kill 一个进程，则所有线程都要被干掉。如果一个信号是发给一个线程的 pthread_kill，则应该只有线程能够收到。 copy_sighand 对于进程来说，会分配一个新的 sighand_struct。这里最主要的是维护信号处理函数，在 copy_sighand 里面会调用 memcpy，将信号处理函数 sighand-&gt;action 从父进程复制到子进程。 对于线程来说，由于设计了CLONE_SIGHAND标记位，会对引用计数加一并退出，没有分配新的信号变量。 1234567891011121314151617static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)&#123; struct sighand_struct *sig; if (clone_flags &amp; CLONE_SIGHAND) &#123; refcount_inc(&amp;current-&gt;sighand-&gt;count); return 0; &#125; sig = kmem_cache_alloc(sighand_cachep, GFP_KERNEL); rcu_assign_pointer(tsk-&gt;sighand, sig); if (!sig) return -ENOMEM; refcount_set(&amp;sig-&gt;count, 1); spin_lock_irq(&amp;current-&gt;sighand-&gt;siglock); memcpy(sig-&gt;action, current-&gt;sighand-&gt;action, sizeof(sig-&gt;action)); spin_unlock_irq(&amp;current-&gt;sighand-&gt;siglock); return 0;&#125; init_sigpending 和 copy_signal 用于初始化信号结构体，并且复制用于维护发给这个进程的信号的数据结构。copy_signal 函数会分配一个新的 signal_struct，并进行初始化。对于线程来说也是直接退出并未复制。 123456789101112131415161718static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)&#123; struct signal_struct *sig; if (clone_flags &amp; CLONE_THREAD) return 0; sig = kmem_cache_zalloc(signal_cachep, GFP_KERNEL);...... /* list_add(thread_node, thread_head) without INIT_LIST_HEAD() */ sig-&gt;thread_head = (struct list_head)LIST_HEAD_INIT(tsk-&gt;thread_node); tsk-&gt;thread_node = (struct list_head)LIST_HEAD_INIT(sig-&gt;thread_head); init_waitqueue_head(&amp;sig-&gt;wait_chldexit); sig-&gt;curr_target = tsk; init_sigpending(&amp;sig-&gt;shared_pending); INIT_HLIST_HEAD(&amp;sig-&gt;multiprocess); seqlock_init(&amp;sig-&gt;stats_lock); prev_cputime_init(&amp;sig-&gt;prev_cputime);......&#125;; 6、复制进程内存空间 进程都有自己的内存空间，用 mm_struct 结构来表示。copy_mm() 函数中调用 dup_mm()，分配一个新的 mm_struct 结构，调用 memcpy 复制这个结构。dup_mmap() 用于复制内存空间中内存映射的部分。前面讲系统调用的时候，我们说过，mmap 可以分配大块的内存，其实 mmap 也可以将一个文件映射到内存中，方便可以像读写内存一样读写文件，这个在内存管理那节我们讲。 线程不会复制内存空间，因此因为CLONE_VM标识位而直接指向了原来的mm_struct。 123456789101112131415161718192021222324252627282930static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)&#123; struct mm_struct *mm, *oldmm; int retval;...... /* * Are we cloning a kernel thread? * We need to steal a active VM for that.. */ oldmm = current-&gt;mm; if (!oldmm) return 0; /* initialize the new vmacache entries */ vmacache_flush(tsk); if (clone_flags &amp; CLONE_VM) &#123; mmget(oldmm); mm = oldmm; goto good_mm; &#125; retval = -ENOMEM; mm = dup_mm(tsk); if (!mm) goto fail_nomem;good_mm: tsk-&gt;mm = mm; tsk-&gt;active_mm = mm; return 0;fail_nomem: return retval;&#125; 7、分配 pid，设置 tid，group_leader，并且建立任务之间的亲缘关系。 group_leader：进程的话 group_leader就是它自己，和旧进程分开。线程的话则设置为当前进程的group_leader。 tgid: 对进程来说是自己的pid，对线程来说是当前进程的pid real_parent : 对进程来说即当前进程，对线程来说则是当前进程的real_parent 12345678910111213141516171819202122232425static __latent_entropy struct task_struct *copy_process(......) &#123;...... p-&gt;pid = pid_nr(pid); if (clone_flags &amp; CLONE_THREAD) &#123; p-&gt;exit_signal = -1; p-&gt;group_leader = current-&gt;group_leader; p-&gt;tgid = current-&gt;tgid; &#125; else &#123; if (clone_flags &amp; CLONE_PARENT) p-&gt;exit_signal = current-&gt;group_leader-&gt;exit_signal; else p-&gt;exit_signal = (clone_flags &amp; CSIGNAL); p-&gt;group_leader = p; p-&gt;tgid = p-&gt;pid; &#125;...... if (clone_flags &amp; (CLONE_PARENT|CLONE_THREAD)) &#123; p-&gt;real_parent = current-&gt;real_parent; p-&gt;parent_exec_id = current-&gt;parent_exec_id; &#125; else &#123; p-&gt;real_parent = current; p-&gt;parent_exec_id = current-&gt;self_exec_id; &#125; ...... &#125;; 4.2 新进程的唤醒1234567891011121314_do_fork 做的第二件大事是通过调用 wake_up_new_task()唤醒进程。void wake_up_new_task(struct task_struct *p)&#123; struct rq_flags rf; struct rq *rq;...... p-&gt;state = TASK_RUNNING;...... activate_task(rq, p, ENQUEUE_NOCLOCK); trace_sched_wakeup_new(p); check_preempt_curr(rq, p, WF_FORK);......&#125; 首先，我们需要将进程的状态设置为 TASK_RUNNING。activate_task() 函数中会调用 enqueue_task()。 123456789101112131415void activate_task(struct rq *rq, struct task_struct *p, int flags)&#123; if (task_contributes_to_load(p)) rq-&gt;nr_uninterruptible--; enqueue_task(rq, p, flags); p-&gt;on_rq = TASK_ON_RQ_QUEUED;&#125;static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags)&#123;..... p-&gt;sched_class-&gt;enqueue_task(rq, p, flags);&#125; 如果是 CFS 的调度类，则执行相应的 enqueue_task_fair()。在 enqueue_task_fair() 中取出的队列就是 cfs_rq，然后调用 enqueue_entity()。在 enqueue_entity() 函数里面，会调用 update_curr()，更新运行的统计量，然后调用 __enqueue_entity，将 sched_entity 加入到红黑树里面，然后将 se-&gt;on_rq &#x3D; 1 设置在队列上。回到 enqueue_task_fair 后，将这个队列上运行的进程数目加一。然后，wake_up_new_task 会调用 check_preempt_curr，看是否能够抢占当前进程。 1234567891011121314151617181920212223static voidenqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)&#123; struct cfs_rq *cfs_rq; struct sched_entity *se = &amp;p-&gt;se;...... for_each_sched_entity(se) &#123; if (se-&gt;on_rq) break; cfs_rq = cfs_rq_of(se); enqueue_entity(cfs_rq, se, flags); cfs_rq-&gt;h_nr_running++; cfs_rq-&gt;idle_h_nr_running += idle_h_nr_running; /* end evaluation on encountering a throttled cfs_rq */ if (cfs_rq_throttled(cfs_rq)) goto enqueue_throttle; flags = ENQUEUE_WAKEUP; &#125;......&#125; 在 check_preempt_curr 中，会调用相应的调度类的 rq-&gt;curr-&gt;sched_class-&gt;check_preempt_curr(rq, p, flags)。对于CFS调度类来讲，调用的是 check_preempt_wakeup。在 check_preempt_wakeup函数中，前面调用 task_fork_fair的时候，设置 sysctl_sched_child_runs_first 了，已经将当前父进程的 TIF_NEED_RESCHED 设置了，则直接返回。否则，check_preempt_wakeup 还是会调用 update_curr 更新一次统计量，然后 wakeup_preempt_entity 将父进程和子进程 PK 一次，看是不是要抢占，如果要则调用 resched_curr 标记父进程为 TIF_NEED_RESCHED。如果新创建的进程应该抢占父进程，在什么时间抢占呢？别忘了 fork 是一个系统调用，从系统调用返回的时候，是抢占的一个好时机，如果父进程判断自己已经被设置为 TIF_NEED_RESCHED，就让子进程先跑，抢占自己。 12345678910111213141516171819static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)&#123; struct task_struct *curr = rq-&gt;curr; struct sched_entity *se = &amp;curr-&gt;se, *pse = &amp;p-&gt;se; struct cfs_rq *cfs_rq = task_cfs_rq(curr);...... if (test_tsk_need_resched(curr)) return;...... find_matching_se(&amp;se, &amp;pse); update_curr(cfs_rq_of(se)); if (wakeup_preempt_entity(se, pse) == 1) &#123; goto preempt; &#125; return;preempt: resched_curr(rq);......&#125; 至此，我们就完成了任务的整个创建过程，并根据情况唤醒任务开始执行。 5、总结本文十分之长，因为内容极多，源码复杂，本来想拆分为两篇文章，但是又因为过于紧密的联系因此合在了一起。本文介绍了进程的创建和线程的创建，而多进程的派生因为使用和线程内核态创建一样的函数因此放在了一起边对比边说明。由此，进程、线程的结构体以及创建过程就全部分析完了，下文将继续分析进程、线程的调度。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/442916346","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核调度和内核同步","path":"/2023/01/15/linux-docs/进程管理/Linux内核调度和内核同步/","content":"Linux2.6版本中的内核引入了一个全新的调度程序，称为O(1)调度程序，进程在被初始化并放到运行队列后，在某个时刻应该获得对CPU的访问，它负责把CPU的控制权传递到不同进程的两个函数schedule()和schedule_tick()中。下图是随着时间推移，CPU是如何在不同进程之间传递的，至于细节这里不多阐释，大家看待就可以理解的啦~ 下面开始介绍一下上下文切换，在操作系统中，CPU切换到另一个进程需要保存当前进程的状态并恢复另一个进程的状态：当前运行任务转为就绪（或者挂起、删除）状态，另一个被选定的就绪任务成为当前任务。上下文切换包括保存当前任务的运行环境，恢复将要运行任务的运行环境。 1、如何获得上下文切换的次数？vmstat直接运行即可，在最后几列，有CPU的context switch次数。 这个是系统层面的，加入想看特定进程的情况，可以使用pidstat。 1234567$ vmstat 1 100procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------ r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 88 233484 288756 1784744 0 0 0 23 0 0 4 1 94 0 0 4 0 88 233236 288756 1784752 0 0 0 0 6202 7880 4 1 96 0 0 2 0 88 233360 288756 1784800 0 0 0 112 6277 7612 4 1 95 0 0 0 0 88 232864 288756 1784804 0 0 0 644 5747 6593 6 0 92 2 0 ****执行pidstat，将输出系统启动后所有活动进程的cpu统计信息： 123456789101112131415linux:~ # pidstatLinux 2.6.32.12-0.7-default (linux) 06/18/12 _x86_64_11:37:19 PID %usr %system %guest %CPU CPU Command……11:37:19 11452 0.00 0.00 0.00 0.00 2 bash11:37:19 11509 0.00 0.00 0.00 0.00 3 dd11:37:19: pidstat获取信息时间点PID: 进程pid%usr: 进程在用户态运行所占cpu时间比率%system: 进程在内核态运行所占cpu时间比率%CPU: 进程运行所占cpu时间比率CPU: 指示进程在哪个核运行Command: 拉起进程对应的命令备注:执行pidstat默认输出信息为系统启动后到执行时间点的统计信息，因而即使当前某进程的cpu占用率很高 2、上下文切换的性能消耗在哪里呢？context switch过高，会导致CPU像个搬运工，频繁在寄存器和运行队列直接奔波 ，更多的时间花在了线程切换，而不是真正工作的线程上。直接的消耗包括CPU寄存器需要保存和加载，系统调度器的代码需要执行。间接消耗在于多核cache之间的共享数据。 **** 3、引起上下文切换的原因有哪些？对于抢占式操作系统而言， 大体有几种： 当前任务的时间片用完之后，系统CPU正常调度下一个任务； 当前任务碰到IO阻塞，调度线程将挂起此任务，继续下一个任务； 多个任务抢占锁资源，当前任务没有抢到，被调度器挂起，继续下一个任务； 用户代码挂起当前任务，让出CPU时间； 硬件中断； 4、如何测试上下文切换的时间消耗？这里我再网上查找到了一个程序，代码不长，切换一个 差不多就是 20个微秒吧，这个程序望大神指教 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/time.h&gt;#include&lt;pthread.h&gt;int pipes[20][3];char buffer[10];int running = 1;void inti()&#123; int i =20; while(i--) &#123; if(pipe(pipes[i])&lt;0) exit(1); pipes[i][2] = i; &#125;&#125;void distroy()&#123; int i =20; while(i--) &#123; close(pipes[i][0]); close(pipes[i][1]); &#125;&#125;double self_test()&#123; int i =20000; struct timeval start, end; gettimeofday(&amp;start, NULL); while(i--) &#123; if(write(pipes[0][1],buffer,10)==-1) exit(1); read(pipes[0][0],buffer,10); &#125; gettimeofday(&amp;end, NULL); return (double)(1000000*(end.tv_sec-start.tv_sec)+ end.tv_usec-start.tv_usec)/20000;&#125;void *_test(void *arg)&#123; int pos = ((int *)arg)[2]; int in = pipes[pos][0]; int to = pipes[(pos + 1)%20][1]; while(running) &#123; read(in,buffer,10); if(write(to,buffer,10)==-1) exit(1); &#125;&#125;double threading_test()&#123; int i = 20; struct timeval start, end; pthread_t tid; while(--i) &#123; pthread_create(&amp;tid,NULL,_test,(void *)pipes[i]); &#125; i = 10000; gettimeofday(&amp;start, NULL); while(i--) &#123; if(write(pipes[1][1],buffer,10)==-1) exit(1); read(pipes[0][0],buffer,10); &#125; gettimeofday(&amp;end, NULL); running = 0; if(write(pipes[1][1],buffer,10)==-1) exit(1); return (double)(1000000*(end.tv_sec-start.tv_sec)+ end.tv_usec-start.tv_usec)/10000/20;&#125;int main()&#123; inti(); printf(&quot;%6.6f &quot;,self_test()); printf(&quot;%6.6f &quot;,threading_test()); distroy(); exit(0);&#125; 总而言之，我们可以认为，这最多只能是依赖于底层操作系统的近似计算。 一个近似的解法是记录一个进程结束时的时间戳，另一个进程开始的时间戳及排除等待时间。如果所有进程总共用时为T，那么总的上下文切换时间为： T – (所有进程的等待时间和执行时间) 接下来来述说抢占，抢占是一个进程到另一个进程的切换，那么Linux是如何决定在何时进行切换的呢？下面我们一次来介绍这三种抢占方式。 显式内核抢占： 最容易的就是这个抢占啦，它发生在内核代码调用schedule(可以直接调用或者阻塞调用)时候的内核空间中。当这种方式抢占时，例如在wait_queue等待队列中设备驱动程序在等候时，控制权被简单地传递到调度程序，从而新的进程被选中执行。 隐式用户抢占： 当内核处理完内核空间的进程并准备把控制权传递到用户空间的进程时，它首先查看应该把控制权传递到哪一个用户空间的进程上，这个进程也行不是传递其控制权到内核的那个用户空间进程。系统中中的每一个进程有一个“必须重新调度”，在进程应该被重新调度的任何时候设置它。代码可以在include&#x2F;linux&#x2F;sched.h中查看~ 1234567891011121314static inline void set_tsk_need_resched(struct task_struct *tsk)&#123; set_tsk_thread_flag(tsk,TIF_NEED_RESCHED);&#125;static inline void clear_tsk_need_resched(struct task_struct *tsk)&#123; clear_tsk_thread_flag(tsk,TIF_NEED_RESCHED);&#125;//set_tsk_need_resched和clear_tsk_need_resched是两个接口，用于设置体系结构特有的TIF_NEED_RESCHED标志stactic inline int need_resched(void)&#123; return unlikely(test_thread_flag(TIF_NEED_RESCHED));&#125;//need_resched测试当前线程的标志，看看TIF_NEED_RESCHED是否被设置 隐式内核抢占： 隐式内核抢占有两种可能性：内核代码出自使抢占禁止的代码块，或者处理正在从中断返回到内核代码时，如果控制权正在从一个中断返回到内核空间，该中断调用schedule()，一个新的 进程以刚才描述的同一种方式被选中，如果内核代码出自禁止抢占的代码块，激活抢占 的操作可能引起当前进程被抢占(代码在include&#x2F;linux&#x2F;preempt.h中查看到): 12345#define preempt_enable() \\do &#123; \\ preempt_enable_no_resched(); \\ preempt_check_resched(); \\&#125; while(0) preempt_enable()调用preempt_enable_no_resched()，它把与当前进程相关的preempt_count减1，然后调用preempt_check_resched(): 12345#define preempt_check_resched() \\do &#123; \\ if(unlikely(test_thread_flag(TIF_NEED_RESCHED))); \\ preempt_schedule(); \\&#125; preempt_check_resched()判断当前进程是否被标记为重新调度，如果是，它调用preempt_schedule()。当两个或者两个以上的进程请求对共享资源独自访问时候，它们需要具有这样一种条件，即它们是在给代码段中操作的唯一进程，在Linux内核锁的基本形式是自旋锁。自旋锁会因为连续循环等待或者试图两次获得锁这种方式的操作而导致死锁，所以在此之前必须初始化spin_lock_t，这个可以通过调用spin_lock_init()来完成(代码在include&#x2F;linux&#x2F;spinlock.h中查看)： 123456789#define spin_lock_init(x) \\ do &#123; \\ (x) -&gt; magic = SPINLOCK_MAGIC; \\ (x) -&gt; lock = 0; \\ //设置自旋锁为&quot;开锁&quot; (x) -&gt; babble = 5; \\ (x) -&gt; module = __FILE__; \\ (x) -&gt; owner = NULL; \\ (x) -&gt; oline = 0; \\ &#125; while(0) 自旋锁被初始化后，可以通过调用spin_lock()或者spin_lock_irqsave()来获取，如果你使用spin_lock()那么进程可能在上锁的代码中被中断，为了在代码的临界区执行后释放，必须调用spin_unlock()或者spin_unlock_irqrestroe(),spin_unlock_irqrestroe()把中断寄存器的状态恢复成调用spin_lock_irq()时寄存器所处的状态。 自旋锁的缺点是它们频繁地循环直到等待锁的释放，那么对于等待时间长的代码区，最好是使用Linux kernel的另一个上锁工具：信号量。它的主要优势之一是：持有信号量的进程可以安全的阻塞，它们在SMP和中断中是保险的(代码在include&#x2F;asm-i386&#x2F;semaphore.h，include&#x2F;asm-ppc&#x2F;semaphore.h中可以查看)： 12345678struct semaphore&#123; atomic_t count; int sleepers; wait_queue_head_t wait;#ifdef WAITQUEUE_DEBUG long __magic;#endif&#125;; 1234567struct semaphore&#123; atomic_t count; wait_queue_head_t wait;#ifdef WAITQUEUE_DEBUG long __magic;#endif&#125;; 两种体系结构的实现都提供了指向wait_queue的一个指针和一个计数，有了信号量我们能够让多于一个的进程同时进去代码的临界区，如果计数初始化为1，则表示只有一个进程能够进去代码的临界区，信号量用sema_init()来初始化，分别调用down()和up()来上锁和解锁，down()和up()函数的使用以及一些情况就不多说了看信号量的调用问题，很容易理解的。 版权声明：本文为知乎博主「极致Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/548630794","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核进程栈内存底层原理到Segmentation fault报错","path":"/2023/01/15/linux-docs/进程管理/Linux内核进程栈内存底层原理到Segmentation fault报错/","content":"栈是编程中使用内存最简单的方式。例如，下面的简单代码中的局部变量 n 就是在堆栈中分配内存的。 123456#include &lt;stdio.h&gt;void main()&#123; int n = 0; printf(&quot;0x%x &quot;,&amp;v); &#125; 那么我有几个问题想问问大家，看看大家对于堆栈内存是否真的了解。 堆栈的物理内存是什么时候分配的？ 堆栈的大小限制是多大？这个限制可以调整吗？ 当堆栈发生溢出后应用程序会发生什么？ 如果你对以上问题还理解不是特别深刻，今天来带你好好修炼进程堆栈内存这块的内功！ 1、进程堆栈的初始化进程启动调用 exec 加载可执行文件过程的时候，会给进程栈申请一个 4 KB 的初始内存。我们今天来专门抽取并看一下这段逻辑。 加载系统调用 execve 依次调用 do_execve、do_execve_common 来完成实际的可执行程序加载。 123456//file:fs/exec.cstatic int do_execve_common(const char *filename, ...)&#123; bprm_mm_init(bprm); ...&#125; 在 bprm_mm_init 中会申请一个全新的地址空间 mm_struct 对象，准备留着给新进程使用。 1234567//file:fs/exec.cstatic int bprm_mm_init(struct linux_binprm *bprm)&#123; //申请个全新的地址空间 mm_struct 对象 bprm-&gt;mm = mm = mm_alloc(); __bprm_mm_init(bprm);&#125; 还会给新进程的栈申请一页大小的虚拟内存空间，作为给新进程准备的栈内存。申请完后把栈的指针保存到 bprm-&gt;p 中记录起来。 12345678910//file:fs/exec.cstatic int __bprm_mm_init(struct linux_binprm *bprm)&#123; bprm-&gt;vma = vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL); vma-&gt;vm_end = STACK_TOP_MAX; vma-&gt;vm_start = vma-&gt;vm_end - PAGE_SIZE; ... bprm-&gt;p = vma-&gt;vm_end - sizeof(void *);&#125; 我们平时所说的进程虚拟地址空间在 Linux 是通过一个个的 vm_area_struct 对象来表示的。 每一个 vm_area_struct（就是上面 __bprm_mm_init 函数中的 vma）对象表示进程虚拟地址空间里的一段范围，其 vm_start 和 vm_end 表示启用的虚拟地址范围的开始和结束。 123456//file:include/linux/mm_types.hstruct vm_area_struct &#123; unsigned long vm_start; unsigned long vm_end; ...&#125; 要注意的是这只是地址范围，而不是真正的物理内存分配 。 在上面 __bprm_mm_init 函数中通过 kmem_cache_zalloc 申请了一个 vma 内核对象。vm_end 指向了 STACK_TOP_MAX（地址空间的顶部附近的位置），vm_start 和 vm_end 之间留了一个 Page 大小。也就是说默认给栈准备了 4KB 的大小 。最后把栈的指针记录到 bprm-&gt;p 中。 接下来进程加载过程会使用 load_elf_binary 真正开始加载可执行二进制程序。在加载时，会把前面准备的进程栈的地址空间指针设置到了新进程 mm 对象上。 12345678910//file:fs/binfmt_elf.cstatic int load_elf_binary(struct linux_binprm *bprm)&#123; //ELF 文件头解析 //Program Header 读取 //清空父进程继承来的资源 retval = flush_old_exec(bprm); ... current-&gt;mm-&gt;start_stack = bprm-&gt;p;&#125; 这样新进程将来就可以使用栈进行函数调用，以及局部变量的申请了。 前面我们说了，这里只是给栈申请了地址空间对象，并没有真正申请物理内存。我们接着再来看一下，物理内存页究竟是什么时候分配的。 2、物理页的申请当进程在运行的过程中在栈上开始分配和访问变量的时候，如果物理页还没有分配，会触发缺页中断。在缺页中断种来真正地分配物理内存。 为了避免篇幅过长，触发缺页中断的过程就先不展开了。我们直接看一下缺页中断的核心处理入口 __do_page_fault，它位于 arch&#x2F;x86&#x2F;mm&#x2F;fault.c 文件下。 12345678910111213141516171819202122//file:arch/x86/mm/fault.cstatic void __kprobes__do_page_fault(struct pt_regs *regs, unsigned long error_code)&#123; ... //根据新的 address 查找对应的 vma vma = find_vma(mm, address); //如果找到的 vma 的开始地址比 address 小 //那么就不调用expand_stack了，直接调用 if (likely(vma-&gt;vm_start &lt;= address)) goto good_area; ... if (unlikely(expand_stack(vma, address))) &#123; bad_area(regs, error_code, address); return; &#125;good_area: //调用handle_mm_fault来完成真正的内存申请 fault = handle_mm_fault(mm, vma, address, flags);&#125; 当访问栈上变量的内存的时候，首先会调用 find_vma 根据变量地址 address 找到其所在的 vma 对象。接下来调用的 if (vma-&gt;vm_start &lt;&#x3D; address) 是在判断地址空间还够不够用。 如果栈内存 vma 的 start 比要访问的 address 小，则证明地址空间够用，只需要分配物理内存页就行了。如果栈内存 vma 的 start 比要访问的 address 大，则需要调用 expand_stack 先扩展一下栈的虚拟地址空间 vma。扩展虚拟地址空间的具体细节我们在第三节再讲。 这里先假设要访问的变量地址 address 处于栈内存 vma 对象的 vm_start 和 vm_end 之间。那么缺页中断处理就会跳转到 good_area 处运行。在这里调用 handle_mm_fault 来完成真正物理内存的申请 。 1234567891011121314//file:mm/memory.cint handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma, unsigned long address, unsigned int flags)&#123; ... //依次查看每一级页表项 pgd = pgd_offset(mm, address); pud = pud_alloc(mm, pgd, address); pmd = pmd_alloc(mm, pud, address); pte = pte_offset_map(pmd, address); return handle_pte_fault(mm, vma, address, pte, pmd, flags);&#125; Linux 是用四级页表来管理虚拟地址空间到物理内存之间的映射管理的。所以在实际申请物理页面之前，需要先 check 一遍需要的每一级页表项是否存在，不存在的话需要申请。 为了好区分，Linux 还给每一级页表都起了一个名字。 一级页表：Page Global Dir，简称 pgd 二级页表：Page Upper Dir，简称 pud 三级页表：Page Mid Dir，简称 pmd 四级页表：Page Table，简称 pte 看一下下面这个图就比较好理解了 1234567891011//file:mm/memory.cint handle_pte_fault(struct mm_struct *mm, struct vm_area_struct *vma, unsigned long address, pte_t *pte, pmd_t *pmd, unsigned int flags)&#123; ... //匿名映射页处理 return do_anonymous_page(mm, vma, address, pte, pmd, flags);&#125; 在 handle_pte_fault 会处理很多种的内存缺页处理，比如文件映射缺页处理、swap缺页处理、写时复制缺页处理、匿名映射页处理等等几种情况。我们今天讨论的主题是栈内存，这个对应的是匿名映射页处理，会进入到 do_anonymous_page 函数中。 123456789//file:mm/memory.cstatic int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma, unsigned long address, pte_t *page_table, pmd_t *pmd, unsigned int flags)&#123; // 分配可移动的匿名页面，底层通过 alloc_page page = alloc_zeroed_user_highpage_movable(vma, address); ...&#125; 在 do_anonymous_page 调用 alloc_zeroed_user_highpage_movable 分配一个可移动的匿名物理页出来。在底层会调用到伙伴系统的 alloc_pages 进行实际物理页面的分配。 内核是用伙伴系统来管理所有的物理内存页的。其它模块需要物理页的时候都会调用伙伴系统对外提供的函数来申请物理内存。 到了这里，开篇的问题一就有答案了，堆栈的物理内存是什么时候分配的？进程在加载的时候只是会给新进程的栈内存分配一段地址空间范围。而真正的物理内存是等到访问的时候触发缺页中断，再从伙伴系统中申请的。 3、栈的自动增长前面我们看到了，进程在被加载启动的时候，栈内存默认只分配了 4 KB 的空间。那么随着程序的运行，当栈中保存的调用链，局部变量越来越多的时候，必然会超过 4 KB。 我回头看下缺页处理函数 __do_page_fault。如果栈内存 vma 的 start 比要访问的 address 大，则需要调用 expand_stack 先扩展一下栈的虚拟地址空间 vma。 回顾 __do_page_fault 源码，看到扩充栈空间的是由 expand_stack 函数来完成的。 12345678910111213141516//file:arch/x86/mm/fault.cstatic void __kprobes__do_page_fault(struct pt_regs *regs, unsigned long error_code)&#123; ... if (likely(vma-&gt;vm_start &lt;= address)) goto good_area; //如果栈 vma 的开始地址比 address 大，需要扩大栈 if (unlikely(expand_stack(vma, address))) &#123; bad_area(regs, error_code, address); return; &#125;good_area: ...&#125; 我们来看下 expand_stack 的内部细节。 其实在 Linux 栈地址空间增长是分两种方向的，一种是从高地址往低地址增长，一种是反过来。大部分情况都是由高往低增长的。本文只以向下增长为例。 123456789101112131415161718192021222324//file:mm/mmap.cint expand_stack(struct vm_area_struct *vma, unsigned long address)&#123; ... return expand_downwards(vma, address);&#125;int expand_downwards(struct vm_area_struct *vma, unsigned long address)&#123; ... //计算栈扩大后的最后大小 size = vma-&gt;vm_end - address; //计算需要扩充几个页面 grow = (vma-&gt;vm_start - address) &gt;&gt; PAGE_SHIFT; //判断是否允许扩充 acct_stack_growth(vma, size, grow); //如果允许则开始扩充 vma-&gt;vm_start = address; return ...&#125; 在 expand_downwards 中先进行了几个计算。 计算出新的堆栈大小。计算公式是 size &#x3D; vma-&gt;vm_end - address; 计算需要增长的页数。计算公式是 grow &#x3D; (vma-&gt;vm_start - address) &gt;&gt; PAGE_SHIFT; 然后会判断此次栈空间是否被允许扩充， 判断是在 acct_stack_growth 中完成的。如果允许扩展，则简单修改一下 vma-&gt;vm_start 就可以了！ 我们再来看 acct_stack_growth 都进行了哪些限制判断。 1234567891011121314//file:mm/mmap.cstatic int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, unsigned long grow)&#123; ... //检查地址空间是否超出限制 if (!may_expand_vm(mm, grow)) return -ENOMEM; //检查是否超出栈的大小限制 if (size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur)) return -ENOMEM; ... return 0;&#125; 在 acct_stack_growth 中只是进行一系列的判断。may_expand_vm 判断的是增长完这几个页后是否超出整体虚拟地址空间大小的限制。rlim[RLIMIT_STACK].rlim_cur 中记录的是栈空间大小的限制。这些限制都可以通过 ulimit 命令查看到。 12345# ulimit -a......max memory size (kbytes, -m) unlimitedstack size (kbytes, -s) 8192virtual memory (kbytes, -v) unlimited 上面的这个输出表示虚拟地址空间大小没有限制，栈空间的限制是 8 MB。如果进程栈大小超过了这个限制，会返回 -ENOMEM。如果觉得系统默认的大小不合适可以通过 ulimit 命令修改。 123# ulimit -s 10240# ulimit -astack size (kbytes, -s) 10240 到这里开篇的第二个问题也有答案了，堆栈的大小限制是多大？这个限制可以调整吗？进程堆栈大小的限制在每个机器上都是不一样的，可以通过 ulimit 命令来查看，也同样可以使用该命令修改。 至于开篇的问题3，当堆栈发生溢出后应用程序会发生什么？写个简单的无限递归调用就知道了，估计你也遇到过。报错结果就是 1&#x27;Segmentation fault (core dumped) 4、本文总结来总结下本文的内容，本文讨论了进程栈内存的工作原理。 第一，进程在加载的时候给进程栈申请了一块虚拟地址空间 vma 内核对象。vm_start 和 vm_end 之间留了一个 Page ，也就是说默认给栈准备了 4KB 的空间。第二，当进程在运行的过程中在栈上开始分配和访问变量的时候，如果物理页还没有分配，会触发缺页中断。在缺页中断中调用内核的伙伴系统真正地分配物理内存。第三，当栈中的存储超过 4KB 的时候会自动进行扩大。不过大小要受到限制，其大小限制可以通过 ulimit -s来查看和设置。 注意，今天我们讨论的都是进程栈。线程栈和进程栈有些不一样。 在回顾和总结下开篇我们抛出的三个问题： 问题一：堆栈的物理内存是什么时候分配的？进程在加载的时候只是会给新进程的栈内存分配一段地址空间范围。而真正的物理内存是等到访问的时候触发缺页中断，再从伙伴系统中申请的。 问题二：堆栈的大小限制是多大？这个限制可以调整吗？进程堆栈大小的限制在每个机器上都是不一样的，可以通过 ulimit 命令来查看，也同样可以使用该命令修改。 问题3：当堆栈发生溢出后应用程序会发生什么？当堆栈溢出的时候，我们会收到报错 “Segmentation fault (core dumped)” 最后，抛个问题大家一起思考吧。你觉得内核为什么要对进程栈的地址空间进行限制呢？ 版权声明：本文为知乎博主「Linux内核库」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文 出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/606284748","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核进程的管理与调度","path":"/2023/01/15/linux-docs/进程管理/Linux内核进程的管理与调度/","content":"一，前戏1.1 进程调度内存中保存了对每个进程的唯一描述, 并通过若干结构与其他进程连接起来. 调度器面对的情形就是这样, 其任务是在程序之间共享CPU时间, 创造并行执行的错觉, 该任务分为两个不同的部分, 其中一个涉及调度策略, 另外一个涉及上下文切换. 1.2 进程的分类linux把进程区分为实时进程和非实时进程, 其中非实时进程进一步划分为交互式进程和批处理进程 类型描述示例交互式进程(interactive process)此类进程经常与用户进行交互, 因此需要花费很多时间等待键盘和鼠标操作. 当接受了用户的输入后, 进程必须很快被唤醒, 否则用户会感觉系统反应迟钝shell, 文本编辑程序和图形应用程序批处理进程(batch process)此类进程不必与用户交互, 因此经常在后台运行. 因为这样的进程不必很快相应, 因此常受到调度程序的怠慢程序语言的编译程序, 数据库搜索引擎以及科学计算实时进程(real-time process)这些进程由很强的调度需要, 这样的进程绝不会被低优先级的进程阻塞. 并且他们的响应时间要尽可能的短视频音频应用程序, 机器人控制程序以及从物理传感器上收集数据的程序 在linux中, 调度算法可以明确的确认所有实时进程的身份, 但是没办法区分交互式程序和批处理程序, linux2.6的调度程序实现了基于进程过去行为的启发式算法, 以确定进程应该被当做交互式进程还是批处理进程. 当然与批处理进程相比, 调度程序有偏爱交互式进程的倾向 1.3 不同进程采用不同的调度策略根据进程的不同分类Linux采用不同的调度策略. 对于实时进程，采用FIFO或者Round Robin的调度策略. 对于普通进程，则需要区分交互式和批处理式的不同。传统Linux调度器提高交互式应用的优先级，使得它们能更快地被调度。而CFS和RSDL等新的调度器的核心思想是”完全公平”。这个设计理念不仅大大简化了调度器的代码复杂度，还对各种调度需求的提供了更完美的支持. 注意Linux通过将进程和线程调度视为一个，同时包含二者。进程可以看做是单个线程，但是进程可以包含共享一定资源（代码和&#x2F;或数据）的多个线程。因此进程调度也包含了线程调度的功能. 目前非实时进程的调度策略比较简单, 因为实时进程值只要求尽可能快的被响应, 基于优先级, 每个进程根据它重要程度的不同被赋予不同的优先级，调度器在每次调度时, 总选择优先级最高的进程开始执行. 低优先级不可能抢占高优先级, 因此FIFO或者Round Robin的调度策略即可满足实时进程调度的需求. 但是普通进程的调度策略就比较麻烦了, 因为普通进程不能简单的只看优先级, 必须公平的占有CPU, 否则很容易出现进程饥饿, 这种情况下用户会感觉操作系统很卡, 响应总是很慢，因此在linux调度器的发展历程中经过了多次重大变动, linux总是希望寻找一个最接近于完美的调度策略来公平快速的调度进程. 1.4 linux调度器的演变一开始的调度器是复杂度为O(n) O(n)的始调度算法(实际上每次会遍历所有任务，所以复杂度为O(n)), 这个算法的缺点是当内核中有很多任务时，调度器本身就会耗费不少时间，所以，从linux2.5开始引入赫赫有名的O(1) O(1)调度器 然而，linux是集全球很多程序员的聪明才智而发展起来的超级内核，没有最好，只有更好，在O(1) O(1)调度器风光了没几天就又被另一个更优秀的调度器取代了，它就是CFS调度器Completely Fair Scheduler. 这个也是在2.6内核中引入的，具体为2.6.23，即从此版本开始，内核使用CFS作为它的默认调度器，O(1) O(1)调度器被抛弃了。 所以完全有理由相信，后续如果再会出现一个更优秀的调度器，CFS也不会幸免。因为linux只要最好的那个。 二，O(n)的始调度算法2.1 Linux2.4之前的内核调度器早期的Linux进程调度器使用了最低的设计，它显然不关注具有很多处理器的大型架构，更不用说是超线程了。 Linux调度器使用了环形队列用于可运行的任务管理, 使用循环调度策略. 此调度器添加和删除进程效率很高（具有保护结构的锁）。简而言之，该调度器并不复杂但是简单快捷. Linux版本2.2引入了调度类的概念，允许针对实时任务、非抢占式任务、非实时任务的调度策略。调度器还包括对称多处理 (SMP) 支持。 2.2 Linux2.4的调度器2.2.1 概述 在Linux2.4.18中(linux-2.5)之前的内核, 当很多任务都处于活动状态时, 调度器有很明显的限制. 这是由于调度器是使用一个复杂度为O(n) O(n)的算法实现的调度器采用基于优先级的设计，这个调度器和Linus在1992年发布的调度器没有大的区别。该调度器的pick next算法非常简单：对runqueue中所有进程的优先级进行依次进行比较，选择最高优先级的进程作为下一个被调度的进程。(Runqueue是Linux 内核中保存所有就绪进程的队列). pick next用来指从所有候选进程中挑选下一个要被调度的进程的过程。 这种调度算法非常简单易懂: 在每次进程切换时, 内核扫描可运行进程的链表, 计算优先级,然胡选择”最佳”进程来运行. 在这种调度器中, 调度任务所花费的时间是一个系统中任务个数的函数. 换而言之, 活动的任务越多, 调度任务所花费的时间越长. 在任务负载非常重时, 处理器会因调度消耗掉大量的时间, 用于任务本身的时间就非常少了。因此，这个算法缺乏可伸缩性 2.2.2 详情 每个进程被创建时都被赋予一个时间片。时钟中断递减当前运行进程的时间片，当进程的时间片被用完时，它必须等待重新赋予时间片才能有机会运行。Linux2.4调度器保证只有当所有RUNNING进程的时间片都被用完之后，才对所有进程重新分配时间片。这段时间被称为一个epoch。这种设计保证了每个进程都有机会得到执行。每个epoch中，每个进程允许执行到其时间切片用完。如果某个进程没有使用其所有的时间切片，那么剩余时间切片的一半将被添加到新时间切片使其在下个epoch中可以执行更长时间。调度器只是迭代进程，应用goodness函数（指标）决定下面执行哪个进程。当然，各种进程对调度的需求并不相同，Linux 2.4调度器主要依靠改变进程的优先级，来满足不同进程的调度需求。事实上，所有后来的调度器都主要依赖修改进程优先级来满足不同的调度需求。 实时进程：实时进程的优先级是静态设定的，而且始终大于普通进程的优先级。因此只有当runqueue中没有实时进程的情况下，普通进程才能够获得调度。 实时进程采用两种调度策略，SCHED_FIFO 和 SCHED_RRFIFO 采用先进先出的策略，对于所有相同优先级的进程，最先进入 runqueue 的进程总能优先获得调度；Round Robin采用更加公平的轮转策略，使得相同优先级的实时进程能够轮流获得调度。 普通进程：对于普通进程，调度器倾向于提高交互式进程的优先级，因为它们需要快速的用户响应。普通进程的优先级主要由进程描述符中的Counter字段决定 (还要加上 nice 设定的静态优先级) 。进程被创建时子进程的 counter值为父进程counter值的一半，这样保证了任何进程不能依靠不断地 fork() 子进程从而获得更多的执行机会。 Linux2.4调度器是如何提高交互式进程的优先级的呢？如前所述，当所有 RUNNING 进程的时间片被用完之后，调度器将重新计算所有进程的 counter 值，所有进程不仅包括 RUNNING 进程，也包括处于睡眠状态的进程。处于睡眠状态的进程的 counter 本来就没有用完，在重新计算时，他们的 counter 值会加上这些原来未用完的部分，从而提高了它们的优先级。交互式进程经常因等待用户输入而处于睡眠状态，当它们重新被唤醒并进入 runqueue 时，就会优先于其它进程而获得 CPU。从用户角度来看，交互式进程的响应速度就提高了。 该调度器的主要缺点： 可扩展性不好 调度器选择进程时需要遍历整个 runqueue 从中选出最佳人选，因此该算法的执行时间与进程数成正比。另外每次重新计算 counter 所花费的时间也会随着系统中进程数的增加而线性增长，当进程数很大时，更新 counter 操作的代价会非常高，导致系统整体的性能下降。 高负载系统上的调度性能比较低 2.4的调度器预分配给每个进程的时间片比较大，因此在高负载的服务器上，该调度器的效率比较低，因为平均每个进程的等待时间于该时间片的大小成正比。 交互式进程的优化并不完善 Linux2.4识别交互式进程的原理基于以下假设，即交互式进程比批处理进程更频繁地处于SUSPENDED状态。然而现实情况往往并非如此，有些批处理进程虽然没有用户交互，但是也会频繁地进行IO操作，比如一个数据库引擎在处理查询时会经常地进行磁盘IO，虽然它们并不需要快速地用户响应，还是被提高了优先级。当系统中这类进程的负载较重时，会影响真正的交互式进程的响应时间。 对实时进程的支持不够 Linux2.4内核是非抢占的，当进程处于内核态时不会发生抢占，这对于真正的实时应用是不能接受的。 为了解决这些问题，Ingo Molnar开发了新的$O(1)调度器，在CFS和RSDL之前，这个调度器不仅被Linux2.6采用，还被backport到Linux2.4中，很多商业的发行版本都采用了这个调度器 三，O(1)的调度算法3.1 概述由于进程优先级的最大值为139，因此MAX_PRIO的最大值取140(具体的是，普通进程使用100到139的优先级，实时进程使用0到99的优先级). 因此，该调度算法为每个优先级都设置一个可运行队列, 即包含140个可运行状态的进程链表，每一条优先级链表上的进程都具有相同的优先级，而不同进程链表上的进程都拥有不同的优先级。 除此之外, 还包括一个优先级位图bitmap。该位图使用一个位(bit)来代表一个优先级，而140个优先级最少需要5个32位来表示， 因此只需要一个int[5]就可以表示位图，该位图中的所有位都被置0，当某个优先级的进程处于可运行状态时，该优先级所对应的位就被置1。 如果确定了优先级，那么选取下一个进程就简单了，只需在queue数组中对应的链表上选取一个进程即可。 最后，在早期的内核中，抢占是不可能的；这意味着如果有一个低优先级的任务在执行，高优先级的任务只能等待它完成。 3.2 详情从名字就可以看出O(1)调度器主要解决了以前版本中的扩展性问题。 O(1)调度算法所花费的时间为常数，与当前系统中的进程个数无关。 此外Linux 2.6内核支持内核态抢占，因此更好地支持了实时进程。 相对于前任，O(1)调度器还更好地区分了交互式进程和批处理式进程。 Linux 2.6内核也支持三种调度策略。其中SCHED_FIFO和SCHED_RR用于实时进程，而SCHED_NORMAL用于普通进程。 O(1)调度器在两个方面修改了Linux 2.4调度器，一是进程优先级的计算方法；二是pick next算法。 O(1)调度器跟踪运行队列中可运行的任务（实际上，每个优先级水平有两个运行队列，一个用于活动任务，一个用于过期任务）， 这意味着要确定接下来执行的任务，调度器只需按优先级将下一个任务从特定活动的运行队列中取出即可。 3.2.1 普通进程的优先级计算 不同类型的进程应该有不同的优先级。每个进程与生俱来（即从父进程那里继承而来）都有一个优先级，我们将其称为静态优先级。普通进程的静态优先级范围从100到139，100为最高优先级，139 为最低优先级，0－99保留给实时进程。当进程用完了时间片后，系统就会为该进程分配新的时间片（即基本时间片），静态优先级本质上决定了时间片分配的大小。 静态优先级和基本时间片的关系如下： 静态优先级&lt;120，基本时间片&#x3D;max((140-静态优先级)*20, MIN_TIMESLICE)静态优先级&gt;&#x3D;120，基本时间片&#x3D;max((140-静态优先级)*5, MIN_TIMESLICE) 其中MIN_TIMESLICE为系统规定的最小时间片。从该计算公式可以看出，静态优先级越高（值越低），进程得到的时间片越长。其结果是，优先级高的进程会获得更长的时间片，而优先级低的进程得到的时间片则较短。进程除了拥有静态优先级外，还有动态优先级，其取值范围是100到139。当调度程序选择新进程运行时就会使用进程的动态优先级，动态优先级和静态优先级的关系可参考下面的公式： 动态优先级&#x3D;max(100 , min(静态优先级 – bonus + 5) , 139) 从上面看出，动态优先级的生成是以静态优先级为基础，再加上相应的惩罚或奖励(bonus)。这个bonus并不是随机的产生，而是根据进程过去的平均睡眠时间做相应的惩罚或奖励。 所谓平均睡眠时间（sleep_avg，位于task_struct结构中）就是进程在睡眠状态所消耗的总时间数，这里的平均并不是直接对时间求平均数。平均睡眠时间随着进程的睡眠而增长，随着进程的运行而减少。因此，平均睡眠时间记录了进程睡眠和执行的时间，它是用来判断进程交互性强弱的关键数据。如果一个进程的平均睡眠时间很大，那么它很可能是一个交互性很强的进程。反之，如果一个进程的平均睡眠时间很小，那么它很可能一直在执行。另外，平均睡眠时间也记录着进程当前的交互状态，有很快的反应速度。比如一个进程在某一小段时间交互性很强，那么sleep_avg就有可能暴涨（当然它不能超过 MAX_SLEEP_AVG），但如果之后都一直处于执行状态，那么sleep_avg就又可能一直递减。理解了平均睡眠时间，那么bonus的含义也就显而易见了。交互性强的进程会得到调度程序的奖励（bonus为正），而那些一直霸占CPU的进程会得到相应的惩罚（bonus为负）。其实bonus相当于平均睡眠时间的缩影，此时只是将sleep_avg调整成bonus数值范围内的大小。可见平均睡眠时间可以用来衡量进程是否是一个交互式进程。如果满足下面的公式，进程就被认为是一个交互式进程： 动态优先级≤3*静态优先级&#x2F;4 + 28 平均睡眠时间是进程处于等待睡眠状态下的时间，该值在进程进入睡眠状态时增加，而进入RUNNING状态后则减少。该值的更新时机分布在很多内核函数内：时钟中断scheduler_tick()；进程创建；进程从TASK_INTERRUPTIBLE状态唤醒；负载平衡等。 3.2.2 实时进程的优先级计算实时进程的优先级由sys_sched_setschedule()设置。该值不会动态修改，而且总是比普通进程的优先级高。在进程描述符中用rt_priority域表示。 3.2.3 pick next算法普通进程的调度选择算法基于进程的优先级，拥有最高优先级的进程被调度器选中。 2.4中，时间片counter同时也表示了一个进程的优先级。2.6中时间片用任务描述符中的time_slice域表示，而优先级用prio（普通进程）或者rt_priority（实时进程）表示。调度器为每一个CPU维护了两个进程队列数组：指向活动运行队列的active数组和指向过期运行队列的expire数组。数组中的元素着保存某一优先级的进程队列指针。系统一共有140个不同的优先级，因此这两个数组大小都是140。它们是按照先进先出的顺序进行服务的。被调度执行的任务都会被添加到各自运行队列优先级列表的末尾。每个任务都有一个时间片，这取决于系统允许执行这个任务多长时间。运行队列的前100个优先级列表保留给实时任务使用，后40个用于用户任务，参见下图： 当需要选择当前最高优先级的进程时，2.6调度器不用遍历整个runqueue，而是直接从active数组中选择当前最高优先级队列中的第一个进程。假设当前所有进程中最高优先级为50（换句话说，系统中没有任何进程的优先级小于50）。则调度器直接读取 active[49]，得到优先级为50的进程队列指针。该队列头上的第一个进程就是被选中的进程。这种算法的复杂度为O(1)，从而解决了2.4调度器的扩展性问题。为了实现O(1)算法active数组维护了一个由5个32位的字（140个优先级）组成的bitmap，当某个优先级别上有进程被插入列表时，相应的比特位就被置位。 sched_find_first_bit()函数查询该bitmap，返回当前被置位的最高优先级的数组下标。在上例中sched_find_first_bit函数将返回49。在IA处理器上可以通过bsfl等指令实现。可见查找一个任务来执行所需要的时间并不依赖于活动任务的个数，而是依赖于优先级的数量。这使得 2.6 版本的调度器成为一个复杂度为 O(1) 的过程，因为调度时间既是固定的，而且也不会受到活动任务个数的影响。 为了提高交互式进程的响应时间，O(1)调度器不仅动态地提高该类进程的优先级，还采用以下方法：每次时钟tick中断时，进程的时间片(time_slice)被减一。当time_slice为0时，表示当前进程的时间片用完，调度器判断当前进程的类型，如果是交互式进程或者实时进程，则重置其时间片并重新插入active数组。如果不是交互式进程则从active数组中移到expired数组，并根据上述公式重新计算时间片。这样实时进程和交互式进程就总能优先获得CPU。然而这些进程不能始终留在active数组中，否则进入expire数组的进程就会产生饥饿现象。当进程已经占用CPU时间超过一个固定值后，即使它是实时进程或者交互式进程也会被移到expire数组中。当active数组中的所有进程都被移到expire数组中后，调度器交换active数组和expire数组。因此新的active数组又恢复了初始情况，而expire数组为空，从而开始新的一轮调度。 Linux 2.6调度器改进了前任调度器的可扩展性问题，schedule()函数的时间复杂度为O(1)。这取决于两个改进： pick next算法借助于active数组，无需遍历runqueue； 消了定期更新所有进程counter的操作，动态优先级的修改分布在进程切换，时钟tick中断以及其它一些内核函数中进行。 O(1)调度器区分交互式进程和批处理进程的算法与以前虽大有改进，但仍然在很多情况下会失效。有一些著名的程序总能让该调度器性能下降，导致交互式进程反应缓慢。例如fiftyp.c, thud.c, chew.c, ring-test.c, massive_intr.c等。而且O(1)调度器对NUMA支持也不完善。为了解决这些问题，大量难以维护和阅读的复杂代码被加入Linux2.6.0的调度器模块，虽然很多性能问题因此得到了解决，可是另外一个严重问题始终困扰着许多内核开发者，那就是代码的复杂度问题。很多复杂的代码难以管理并且对于纯粹主义者而言未能体现算法的本质。 为了解决O(1)调度器面临的问题以及应对其他外部压力, 需要改变某些东西。这种改变来自Con Kolivas的内核补丁staircase scheduler（楼梯调度算法），以及改进的RSDL（Rotating Staircase Deadline Scheduler）。它为调度器设计提供了一个新的思路。Ingo Molnar在RSDL之后开发了CFS，并最终被2.6.23内核采用。接下来我们开始介绍这些新一代调度器。 四，Linux 2.6的新一代调度器CFS4.1 楼梯调度算法staircase scheduler楼梯算法(SD)在思路上和O(1)算法有很大不同，它抛弃了动态优先级的概念。而采用了一种完全公平的思路。前任算法的主要复杂性来自动态优先级的计算，调度器根据平均睡眠时间和一些很难理解的经验公式来修正进程的优先级以及区分交互式进程。这样的代码很难阅读和维护。楼梯算法思路简单，但是实验证明它对应交互式进程的响应比其前任更好，而且极大地简化了代码。 和O(1)算法一样，楼梯算法也同样为每一个优先级维护一个进程列表，并将这些列表组织在active数组中。当选取下一个被调度进程时，SD算法也同样从active数组中直接读取。与O(1)算法不同在于，当进程用完了自己的时间片后，并不是被移到expire数组中。而是被加入active数组的低一优先级列表中，即将其降低一个级别。不过请注意这里只是将该任务插入低一级优先级任务列表中，任务本身的优先级并没有改变。当时间片再次用完，任务被再次放入更低一级优先级任务队列中。就象一部楼梯，任务每次用完了自己的时间片之后就下一级楼梯。任务下到最低一级楼梯时，如果时间片再次用完，它会回到初始优先级的下一级任务队列中。比如某进程的优先级为1，当它到达最后一级台阶140后，再次用完时间片时将回到优先级为2的任务队列中，即第二级台阶。不过此时分配给该任务的time_slice将变成原来的2倍。比如原来该任务的时间片time_slice为10ms，则现在变成了20ms。基本的原则是，当任务下到楼梯底部时，再次用完时间片就回到上次下楼梯的起点的下一级台阶。并给予该任务相同于其最初分配的时间片。总结如下：设任务本身优先级为P，当它从第N级台阶开始下楼梯并到达底部后，将回到第N+1级台阶。并且赋予该任务N+1倍的时间片。 以上描述的是普通进程的调度算法，实时进程还是采用原来的调度策略，即FIFO或者Round Robin。 楼梯算法能避免进程饥饿现象，高优先级的进程会最终和低优先级的进程竞争，使得低优先级进程最终获得执行机会。对于交互式应用，当进入睡眠状态时，与它同等优先级的其他进程将一步一步地走下楼梯，进入低优先级进程队列。当该交互式进程再次唤醒后，它还留在高处的楼梯台阶上，从而能更快地被调度器选中，加速了响应时间。 楼梯算法的优点：从实现角度看，SD基本上还是沿用了O(1)的整体框架，只是删除了O(1)调度器中动态修改优先级的复杂代码；还淘汰了expire数组，从而简化了代码。它最重要的意义在于证明了完全公平这个思想的可行性。 4.2 RSDL(Rotating Staircase Deadline Scheduler)RSDL也是由Con Kolivas开发的，它是对SD算法的改进。核心的思想还是”完全公平”。没有复杂的动态优先级调整策略。RSDL重新引入了expire数组。它为每一个优先级都分配了一个 “组时间配额”，记为Tg；同一优先级的每个进程都拥有同样的”优先级时间配额”，用Tp表示。当进程用完了自身的Tp时，就下降到下一优先级进程组中。这个过程和SD相同，在RSDL中这个过程叫做minor rotation（次轮询）。请注意Tp不等于进程的时间片，而是小于进程的时间片。下图表示了minor rotation。进程从priority1的队列中一步一步下到priority140之后回到priority2的队列中，这个过程如下图左边所示，然后从priority 2开始再次一步一步下楼，到底后再次反弹到priority3队列中，如下图所示： 在SD算法中，处于楼梯底部的低优先级进程必须等待所有的高优先级进程执行完才能获得CPU。因此低优先级进程的等待时间无法确定。RSDL中，当高优先级进程组用完了它们的Tg(即组时间配额)时，无论该组中是否还有进程Tp尚未用完，所有属于该组的进程都被强制降低到下一优先级进程组中。这样低优先级任务就可以在一个可以预计的未来得到调度。从而改善了调度的公平性。这就是RSDL中Deadline代表的含义。 进程用完了自己的时间片time_slice时（下图中T2），将放入expire数组指向的对应初始优先级队列中(priority 1)。 当active数组为空，或者所有的进程都降低到最低优先级时就会触发主轮询major rotation。Major rotation交换active数组和expire数组，所有进程都恢复到初始状态，再一次从新开始minor rotation的过程。 RSDL对交互式进程的支持：和SD同样的道理，交互式进程在睡眠时间时，它所有的竞争者都因为minor rotation而降到了低优先级进程队列中。当它重新进入RUNNING状态时，就获得了相对较高的优先级，从而能被迅速响应。 4.3 完全公平的调度器CFSCFS是最终被内核采纳的调度器。它从RSDL&#x2F;SD中吸取了完全公平的思想，不再跟踪进程的睡眠时间，也不再企图区分交互式进程。它将所有的进程都统一对待，这就是公平的含义。CFS的算法和实现都相当简单，众多的测试表明其性能也非常优越。 按照作者Ingo Molnar的说法（参考Documentation&#x2F;scheduler&#x2F;sched-design-CFS.txt），CFS百分之八十的工作可以用一句话概括：CFS在真实的硬件上模拟了完全理想的多任务处理器。在真空的硬件上，同一时刻我们只能运行单个进程，因此当一个进程占用CPU时，其它进程就必须等待，这就产生了不公平。但是在“完全理想的多任务处理器 “下，每个进程都能同时获得CPU的执行时间，即并行地每个进程占1&#x2F;nr_running的时间。例如当系统中有两个进程时，CPU的计算时间被分成两份，每个进程获得50%。假设runqueue中有n个进程，当前进程运行了10ms。在“完全理想的多任务处理器”中，10ms应该平分给n个进程(不考虑各个进程的nice值)，因此当前进程应得的时间是(10&#x2F;n)ms，但是它却运行了10ms。所以CFS将惩罚当前进程，使其它进程能够在下次调度时尽可能取代当前进程。最终实现所有进程的公平调度。 与之前的Linux调度器不同，CFS没有将任务维护在链表式的运行队列中，它抛弃了active&#x2F;expire数组，而是对每个CPU维护一个以时间为顺序的红黑树。 该树方法能够良好运行的原因在于： 红黑树可以始终保持平衡，这意味着树上没有路径比任何其他路径长两倍以上。 由于红黑树是二叉树，查找操作的时间复杂度为O(log n)。但是除了最左侧查找以外，很难执行其他查找，并且最左侧的节点指针始终被缓存。 对于大多数操作（插入、删除、查找等），红黑树的执行时间为O(log n)，而以前的调度程序通过具有固定优先级的优先级数组使用 O(1)。O(log n) 行为具有可测量的延迟，但是对于较大的任务数无关紧要。Molnar在尝试这种树方法时，首先对这一点进行了测试。 红黑树可通过内部存储实现，即不需要使用外部分配即可对数据结构进行维护。 要实现平衡，CFS使用”虚拟运行时”表示某个任务的时间量。任务的虚拟运行时越小，意味着任务被允许访问服务器的时间越短，其对处理器的需求越高。CFS还包含睡眠公平概念以便确保那些目前没有运行的任务（例如，等待 I&#x2F;O）在其最终需要时获得相当份额的处理器。 4.3.1 CFS如何实现pick next 下图是一个红黑树的例子： 所有可运行的任务通过不断地插入操作最终都存储在以时间为顺序的红黑树中（由 sched_entity 对象表示），对处理器需求最多的任务（最低虚拟运行时）存储在树的左侧，处理器需求最少的任务（最高虚拟运行时）存储在树的右侧。 为了公平，CFS调度器会选择红黑树最左边的叶子节点作为下一个将获得cpu的任务。这样，树左侧的进程就被给予时间运行了。 4.3.2 tick中断 在CFS中，tick中断首先更新调度信息。然后调整当前进程在红黑树中的位置。调整完成后如果发现当前进程不再是最左边的叶子，就标记need_resched标志，中断返回时就会调用scheduler()完成进程切换。否则当前进程继续占用CPU。从这里可以看到 CFS抛弃了传统的时间片概念。Tick中断只需更新红黑树，以前的所有调度器都在tick中断中递减时间片，当时间片或者配额被用完时才触发优先级调整并重新调度。 4.3.3 红黑树键值计算 理解CFS的关键就是了解红黑树键值的计算方法。该键值由三个因子计算而得：一是进程已经占用的CPU时间；二是当前进程的nice值；三是当前的cpu负载。进程已经占用的CPU时间对键值的影响最大，其实很大程度上我们在理解CFS时可以简单地认为键值就等于进程已占用的 CPU时间。因此该值越大，键值越大，从而使得当前进程向红黑树的右侧移动。另外CFS规定，nice值为1的进程比nice值为0的进程多获得10%的 CPU时间。在计算键值时也考虑到这个因素，因此nice值越大，键值也越大。 CFS为每个进程都维护两个重要变量：fair_clock和wait_runtime。这里我们将为每个进程维护的变量称为进程级变量，为每个CPU维护的称作CPU级变量，为每个runqueue维护的称为runqueue级变量。进程插入红黑树的键值即为fair_clock – wait_runtime。其中fair_clock从其字面含义上讲就是一个进程应获得的CPU时间，即等于进程已占用的CPU时间除以当前 runqueue中的进程总数；wait_runtime是进程的等待时间。它们的差值代表了一个进程的公平程度。该值越大，代表当前进程相对于其它进程越不公平。对于交互式任务，wait_runtime长时间得不到更新，因此它能拥有更高的红黑树键值，更靠近红黑树的左边。从而得到快速响应。 红黑树是平衡树，调度器每次总最左边读出一个叶子节点，该读取操作的时间复杂度是O(LogN) O(LogN) 4.3.4 调度器管理器 为了支持实时进程，CFS提供了调度器模块管理器。各种不同的调度器算法都可以作为一个模块注册到该管理器中。不同的进程可以选择使用不同的调度器模块。2.6.23中，CFS实现了两个调度算法，CFS算法模块和实时调度模块。对应实时进程，将使用实时调度模块。对应普通进程则使用CFS算法。CFS 调度模块（在 kernel&#x2F;sched_fair.c 中实现）用于以下调度策略：SCHED_NORMAL、SCHED_BATCH 和 SCHED_IDLE。对于 SCHED_RR 和 SCHED_FIFO 策略，将使用实时调度模块（该模块在 kernel&#x2F;sched_rt.c 中实现）。 4.3.5 CFS组调度 CFS组调度（在 2.6.24 内核中引入）是另一种为调度带来公平性的方式，尤其是在处理产生很多其他任务的任务时。 假设一个产生了很多任务的服务器要并行化进入的连接（HTTP 服务器的典型架构）。不是所有任务都会被统一公平对待， CFS 引入了组来处理这种行为。产生任务的服务器进程在整个组中（在一个层次结构中）共享它们的虚拟运行时，而单个任务维持其自己独立的虚拟运行时。这样单个任务会收到与组大致相同的调度时间。您会发现 &#x2F;proc 接口用于管理进程层次结构，让您对组的形成方式有完全的控制。使用此配置，您可以跨用户、跨进程或其变体分配公平性。 考虑一个两用户示例，用户 A 和用户 B 在一台机器上运行作业。用户 A 只有两个作业正在运行，而用户 B 正在运行 48 个作业。组调度使 CFS 能够对用户 A 和用户 B 进行公平调度，而不是对系统中运行的 50 个作业进行公平调度。每个用户各拥有 50% 的 CPU 使用。用户 B 使用自己 50% 的 CPU 分配运行他的 48 个作业，而不会占用属于用户 A 的另外 50% 的 CPU 分配。 更多CFS的信息, 请参照http://www.ibm.com/developerworks/cn/linux/l-completely-fair-scheduler/index.html?ca=drs-cn-0125另外内核文档sched-design-CFS.txt中也有介绍。 五，返璞归真的Linux BFS调度器BFS 是一个进程调度器，可以解释为“脑残调度器”。这古怪的名字有多重含义，比较容易被接受的一个说法为：它如此简单，却如此出色，这会让人对自己的思维能力产生怀疑。 BFS 不会被合并进入 Linus 维护的 Linux mainline，BFS 本身也不打算这么做。但 BFS 拥有众多的拥趸，这只有一个原因：BFS 非常出色，它让用户的桌面环境达到了前所未有的流畅。在硬件越来越先进，系统却依然常显得迟钝的时代，这实在让人兴奋。 进入 2010 年，Android 开发一个分支使用 BFS 作为其操作系统的标准调度器，这也证明了 BFS 的价值。后来放弃。 5.1 BFS的引入前些天突然在网上看到了下面的图片： 后来发现该图片是BFS调度器的引子, 太具有讽刺意义了。 5.2 可配置型调度器的需求为了避免小手段，那就要彻底抛弃“鱼与熊掌可兼得”的思想，采用“一种调度器只适用于一种场景”的新思路. 如此我们可以设计多种调度器, 在安装操作系统的时候可以由管理员进行配置, 比如我们将其用于桌面，那么就使用”交互调度器”, 如果用于路由器, 那就使用”大吞吐调度器”, …消除了兼顾的要求，调度器设计起来就更佳简单和纯粹了. 面对需要大吞吐量的网络操作系统, 我们有传统的UNIX调度器, 然而面对日益桌面化的操作系统比如Android手机, 我们是否能摒弃那种大而全的调度策略呢? Con Kolivas老大设计出的BFS调度器就是为桌面交互式应用量身打造的. 5.3 问题在哪?Linux 2.6内核实现了那么多的调度器，然而其效果总是有美中不足的地方，到底问题出在哪里？事实上，Linux 2.6的各种调度器的实现都不是完全按照理论完成的，其中都添加了一些小手段. 比如虽然CFS号称支持大于2048的CPU个数，然而实际应用中，效果未必好，因为CFS调度器继承了O(1)调度器的load_balance特性，因此在那么多处理器之间进行基于调度域的load_balance，锁定以及独占的代价将会十分大，从而抵消了每CPU队列带来的消除锁定的优势. 总之，这些调度器太复杂了，而且越来越复杂，将80%的精力消耗在了20%的场景中. 实际上，做设计不要联想，完全依照我们目前所知道的和所遇到的来，在可用性和效率上被证明是明智的，当然不考虑太多的可扩展性。 5.4 回到O(n)调度器BFS调度器用一句话来总结就是”回到了O(n)调度器”，它在O(n)调度器的基础上进行了优化，而没有引入看起来很好的O(1)调度器, 这就是其实质. O(n)调度器有什么不好么?有的, 大不了就是遍历的时间太长，BFS根据实际的测试数据忽略之；每个处理器都要锁定整个队列，BFS改之，做到这些既可，这才叫基于O(n)调度器的优化而不是彻底颠覆O(n)调度器而引入O(1)调度器-当然前提是桌面环境。如果说能回到原始的O(n)调度器进行修改使之重新发挥其作用而不是彻底抛弃它，这才是最佳的做法，反之，如果我们把问题的解决方案搞的越来越复杂，最终就是陷入一个泥潭而不可自拔。要知道方案复杂性的积累是一个笛卡儿积式的积累，你必须考虑到每一种排列组合才能，当你做不到这一点的时候，你就需要返璞归真。 5.5 BFS调度器的原理BFS的原理十分简单，其实质正是使用了O(1)调度器中的位图的概念，所有进程被安排到103个queue中，各个进程不是按照优先级而是按照优先级区间被排列到各自所在的区间，每一个区间拥有一个queue，如下图所示： 内核在pick-next的时候，按照O(1)调度器的方式首先查找位图中不为0的那个queue，然后在该queue中执行O(n)查找，查找到virtual deadline(如下所述)最小的那个进程投入执行。过程很简单，就像流水一样。之所以规划103个队列而不是一个完全是为了进程按照其性质而分类，这个和每CPU没有任何关系，将进程按照其性质(RT?优先级?)分类而不是按照CPU分类是明智之举。内核中只有一个“103队列”，m个CPU和“103队列”完全是一个“消费者-生产者”的关系。O(1)调度器，内核中拥有m(CPU个数)个“消费者-生产者”的关系，每一个CPU附带一个“生产者(140队列组)”。 只有统一的，单一的“消费者-生产者”的关系才能做到调度的公平，避免了多个关系之间踢皮球现象，这是事实。在结构单一，功能确定且硬件简单的系统中，正确的调度器架构如下图所示： 在结构单一，功能确定且硬件简单的系统中，不正确的调度器架构如下图所示： 虚拟 Deadline ( Virtual Deadline ) 当一个进程被创建时，它被赋予一个固定的时间片，和一个虚拟 Deadline。该虚拟 deadline 的计算公式非常简单： Virtual Deadline &#x3D; jiffies + (user_priority * rr_interval) 1 其中 jiffies 是当前时间 , user_priority 是进程的优先级，rr_interval 代表 round-robin interval，近似于一个进程必须被调度的最后期限，所谓 Deadline 么。不过在这个 Deadline 之前还有一个形容词为 Virtual，因此这个 Deadline 只是表达一种愿望而已，并非很多领导们常说的那种 deadline。 虚拟 Deadline 将用于调度器的 picknext 决策 进程队列的表示方法和调度策略 在操作系统内部，所有的 Ready 进程都被存放在进程队列中，调度器从进程队列中选取下一个被调度的进程。因此如何设计进程队列是我们研究调度器的一个重要话题。BFS 采用了非常传统的进程队列表示方法，即 bitmap 加 queue。 BFS 将所有进程分成 4 类，分别表示不同的调度策略 : Realtime，实时进程 SCHED_ISO，isochronous 进程，用于交互式任务 SCHED_NORMAL，普通进程 SCHED_IDELPRO，低优先级任务 实时进程总能获得 CPU，采用 Round Robin 或者 FIFO 的方法来选择同样优先级的实时进程。他们需要 superuser 的权限，通常限于那些占用 CPU 时间不多却非常在乎 Latency 的进程。 SCHED_ISO 在主流内核中至今仍未实现，Con 早在 2003 年就提出了这个 patch，但一直无法进入主流内核，这种调度策略是为了那些 near-realtime 的进程设计的。如前所述，实时进程需要用户有 superuser 的权限，这类进程能够独占 CPU，因此只有很少的进程可以被配置为实时进程。对于那些对交互性要求比较高的，又无法成为实时进程的进程，BFS 将采用 SCHED_ISO，这些进程能够抢占 SCHED_NORMAL 进程。他们的优先级比 SCHED_NORMAL 高，但又低于实时进程。此外当 SCHED_ISO 进程占用 CPU 时间达到一定限度后，会被降级为 SCHED_NORMAL，防止其独占整个系统资源。 SCHED_NORMAL 类似于主流调度器 CFS 中的 SCHED_OTHER，是基本的分时调度策略。 SCHED_IDELPRO 类似于 CFS 中的 SCHED_IDLE，即只有当 CPU 即将处于 IDLE 状态时才被调度的进程。 在这些不同的调度策略中，实时进程分成 100 个不同的优先级，加上其他三个调度策略，一共有 103 个不 同的进程类型。对于每个进程类型，系统中都有可能有多个进程同时 Ready，比如很可能有两个优先级为 10 的 RT 进程同时 Ready，所以对于每个类型，还需要一个队列来存储属于该类型的 ready 进程。 BFS 用 103 个 bitmap 来表示是否有相应类型的进程准备进行调度。如图所示： 当任何一种类型的进程队列非空时，即存在 Ready 进程时，相应的 bitmap 位被设置为 1。 调度器如何在这样一个 bitmap 加 queue 的复杂结构中选择下一个被调度的进程的问题被称为 Task Selection 或者 pick next。 Task Selection i.e. Pick Next 当调度器决定进行进程调度的时候，BFS 将按照下面的原则来进行任务的选择： 首先查看 bitmap 是否有置位的比特。比如上图，对应于 SCHED_NORMAL 的 bit 被置位，表明有类型为 SCHED_NORMAL 的进程 ready。如果有 SCHED_ISO 或者 RT task 的比特被置位，则优先处理他们。 选定了相应的 bit 位之后，便需要遍历其相应的子队列。假如是一个 RT 进程的子队列，则选取其中的第一个进程。如果是其他的队列，那么就采用 EEVDF 算法来选取合适的进程。 EEVDF，即 earliest eligible virtual deadline first。BFS 将遍历该子队列，一个双向列表，比较队列中的每一个进程的 Virtual Deadline 值，找到最小的那个。最坏情况下，这是一个 O(n) 的算法，即需要遍历整个双向列表，假如其中有 n 个进程，就需要进行 n 此读取和比较。 但实际上，往往不需要遍历整个 n 个进程，这是因为 BFS 还有这样一个搜索条件： 当某个进程的 Virtual Deadline 小于当前的 jiffies 值时，直接返回该进程。并将其从就绪队列中删除，下次再 insert 时会放到队列的尾部，从而保证每个进程都有可能被选中，而不会出现饥饿现象。 这条规则对应于这样一种情况，即进程已经睡眠了比较长的时间，以至于已经睡过了它的 Virtual Deadline， 5.6 BFS调度器初始版本的链表的非O(n)遍历BFS调度器的发展历程中也经历了一个为了优化性能而引入“小手段”的时期，该“小手段”是如此合理，以至于每一个细节都值得品味，现表述如下： 大家都知道，遍历一个链表的时间复杂度是O(n)，然而这只是遍历的开销，在BFS调度器中，遍历的目的其实就是pick-next，如果该链表某种意义上是预排序的，那么pick-next的开销可以减少到接近O(1)。BFS如何做到的呢？ 我们首先看一下virtual deadline的概念 virtual deadline(VD) VD&#x3D;jiffies + (prio_ratio * rr_interval) 1 2 其中prio_ratio为进程优先级，rr_interval为一个Deadline，表示该进程在最多多久内被调度，链表中的每一个entry代表一个进程，都有一个VD与之相关。VD的存在使得entry在链表的位置得以预排序，这里的预排序指的是vitrual deadline expire的影响下的预排序，BFS和O(n)的差别就在于这个expire，由于这个expire在，一般都会在遍历的途中遇到VD expire，进而不需要O(n)。基于VD的O(n)和基于优先级的O(n)是不同的，其区别在于根据上述的计算公式，VD是单调向前的，而优先级几乎是不怎么变化的，因此基于VD的O(n)调度器某种程度上和基于红黑树的CFS是一样的，VD也正类似于CFS中的虚拟时钟，只是数据结构不同而已，BFS用链表实现，CFS用红黑树实现。 其实，O(n)并没有那么可怕，特别是在桌面环境中，你倒是有多少进程需要调度呢？理论上O(n)会随着进程数量的增加而效率降低，然而桌面环境下实际上没有太多的进程需要被调度，所以采用了BFS而抛弃了诸多小手段的调度器效果会更好些。理论上，CFS或者O(1)可以支持SMP下的诸多进程调度的高效性，然而，桌面环境下，第一，SMP也只是2到4个处理器，进程数也大多不超过1000个，进程在CPU之间蹦来蹦去，很累，何必杀鸡用牛刀呢？瓶颈不是鸡，而是杀鸡的刀，是吧！ 5.7 pick-next算法 BFS的pick-next算法对于SCHED_ISO进程依照以下的原则进行： 依照FIFO原则进行，不再遍历链表 BFS的pick-next算法对于SCHED_NORMAL或者SCHED_IDLEPRIO进程依照以下的原则进行： 遍历运行链表，比较每一个entry的VD，找出最小的entry，从链表中删除，投入运行 如果发现有entry的VD小于当前的jiffers，则停止遍历，取出该entry，投入运行–小手段 以上的原则可以总结为“最小最负最优先”原则。作者一席话如下： BFS has 103 priority queues. 100 of these are dedicated to the static priority of realtime tasks, and the remaining 3 are, in order of best to worst priority, SCHED_ISO (isochronous), SCHED_NORMAL, and SCHED_IDLEPRIO (idle priority scheduling). When a task of these priorities is queued, a bitmap of running priorities is set showing which of these priorities has tasks waiting for CPU time. When a CPU is made to reschedule, the lookup for the next task to get CPU time is performed in the following way:First the bitmap is checked to see what static priority tasks are queued. If any realtime priorities are found, the corresponding queue is checked and the first task listed there is taken (provided CPU affinity is suitable) and lookup is complete. If the priority corresponds to a SCHED_ISO task, they are also taken in FIFO order (as they behave like SCHED_RR). If the priority corresponds to either SCHED_NORMAL or SCHED_IDLEPRIO, then the lookup becomes O(n). At this stage, every task in the runlist that corresponds to that priority is checkedto see which has the earliest set deadline, and (provided it has suitable CPU affinity) it is taken off the runqueue and given the CPU. If a task has an expired deadline, it is taken and the rest of the lookup aborted (as they arechosen in FIFO order).Thus, the lookup is O(n) in the worst case only, where n is as described earlier, as tasks may be chosen before the whole task list is looked over. 使用virtual deadline，类似于CFS的virtual runtime的概念，然而不要红黑树，而采用了双向链表来实现，因为红黑树的插入效率不如链表插入效率，在pick-next算法上虽然红黑树占优势，然而由于VD expire的存在也使得pick-next不再是O(n)了 BFS初始版本的小手段的意义在于减少O(n)遍历比较时间复杂度带来的恐惧。 5.8 去除了小手段的BFS调度器最终将小手段去除是重要的，否则BFS最终还是会陷入类似O(1)，CFS等复杂化的泥潭里面不可自拔，因此在后续的patch中，BFS去除了上述的小手段，用统一的O(n)复杂度来pick-next，毕竟前面已经说了O(n)在特定环境下并不是问题的关键，该patch在2.6.31.14-bfs318-330test.patch中体现。 5.9 队列外部执行BFS调度器和CFS是一样的，都是队列外执行进程的，这样可以减少锁争用带来的性能问题。再列出作者的一席话： BFS has one single lock protecting the process local data of every task in the global queue. Thus every insertion, removal and modification of task data in the global runqueue needs to grab the global lock. However, once a task is taken by a CPU, the CPU has its own local data copy of the running process’ accounting information which only that CPU accesses and modifies (such as during a timer tick) thus allowing the accounting data to be updated lockless. Once a CPU has taken a task to run, it removes it from the global queue. Thus theglobal queue only ever has, at most,(number of tasks requesting cpu time) - (number of logical CPUs) + 1tasks in the global queue. This value is relevant for the time taken to look up tasks during scheduling. This will increase if many tasks with CPU affinity set in their policy to limit which CPUs they’re allowed to run on if they outnumber the number of CPUs. The +1 is because when rescheduling a task, the CPU’s currently running task is put back on the queue. Lookup will be described after the virtual deadline mechanism is explained. 在schedule核心函数中，使用return_task来把prev进程重新入队，在earliest_deadline_task这个pick-next中，使用take_task将选中的next从队列取出，从而实现队列外执行。 5.10 结论从上面的论述，我们丝毫没有看到有任何的诸如“SMP负载均衡”，“CPU亲和力”，“补偿”，“惩罚”之类的字眼，是的，这些字眼在BFS中完全不需要，BFS也正是摒弃了这些字眼才获得成功的，毕竟在一个一般人使用的桌面操作系统中，没有这么多的套套，大多数人使用的就是一个只有一个到两个处理器核心的系统，难道有必要搞什么调度域么？难道有必要搞什么NUMA么？需求决定一切，面对大型服务器，有UNIX的机制站在那里，而如果我们想把Linux推广到每一个掌上设备，那就没必要复制UNIX的那套了，BFS完全可以完美的搞定一切。小手段的去除，说明BFS调度器的发展方向起码是正确的。 BFS对SMP的支持如何呢？答案是它仅仅支持少量CPU的SMP体系，别忘了BFS的应用场合。因为在调度过程中需要一个遍历所有CPU的O(m)复杂度的计算，这就明确告诉人们，别指望BFS使用在拥有4096个CPU的系统上，正如没人用这种系统看视频一样，那样的话，还是乖乖使用CFS吧。 BFS调度器思想很简单：集中精力做好一件事，适应一种场景，代码同样十分简单，因此即使贴上代码整个文章也不会显得过于冗长，你再也看不到诸如load_balance或者for_each_domain之类的东西了，至于CPU cache的亲和力智能判断，如果你非要做，那么就自己调用sched_setaffinity系统调用设置吧，把一个线程或者一组相关的进程设置到一个或者一组共享Cache的CPU上，让内核这些，在进程不那么多，CPU个数不那么多，没有NUMA的系统上，真的太累了。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/472955572","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核进程管理之进程ID","path":"/2023/01/15/linux-docs/进程管理/Linux内核进程管理之进程ID/","content":"Linux 内核使用 task_struct 数据结构来关联所有与进程有关的数据和结构，Linux 内核所有涉及到进程和程序的所有算法都是围绕该数据结构建立的，是内核中最重要的数据结构之一。该数据结构在内核文件 include/linux/sched.h 中定义，在Linux 3.8 的内核中，该数据结构足足有 380 行之多，在这里我不可能逐项去描述其表示的含义，本篇文章只关注该数据结构如何来组织和管理进程ID的。 1、进程ID类型要想了解内核如何来组织和管理进程ID，先要知道进程ID的类型： PID：这是 Linux 中在其命名空间中唯一标识进程而分配给它的一个号码，称做进程ID号，简称PID。在使用 fork 或 clone 系统调用时产生的进程均会由内核分配一个新的唯一的PID值。 TGID：在一个进程中，如果以CLONE_THREAD标志来调用clone建立的进程就是该进程的一个线程，它们处于一个线程组，该线程组的ID叫做TGID。处于相同的线程组中的所有进程都有相同的TGID；线程组组长的TGID与其PID相同；一个进程没有使用线程，则其TGID与PID也相同。 PGID：另外，独立的进程可以组成进程组（使用setpgrp系统调用），进程组可以简化向所有组内进程发送信号的操作，例如用管道连接的进程处在同一进程组内。进程组ID叫做PGID，进程组内的所有进程都有相同的PGID，等于该组组长的PID。 SID：几个进程组可以合并成一个会话组（使用setsid系统调用），可以用于终端程序设计。会话组中所有进程都有相同的SID。 2、PID 命名空间命名空间是为操作系统层面的虚拟化机制提供支撑，目前实现的有六种不同的命名空间，分别为mount命名空间、UTS命名空间、IPC命名空间、用户命名空间、PID命名空间、网络命名空间。命名空间简单来说提供的是对全局资源的一种抽象，将资源放到不同的容器中（不同的命名空间），各容器彼此隔离。命名空间有的还有层次关系，如PID命名空间，图1 为命名空间的层次关系图。 在上图有四个命名空间，一个父命名空间衍生了两个子命名空间，其中的一个子命名空间又衍生了一个子命名空间。以PID命名空间为例，由于各个命名空间彼此隔离，所以每个命名空间都可以有 PID 号为 1 的进程；但又由于命名空间的层次性，父命名空间是知道子命名空间的存在，因此子命名空间要映射到父命名空间中去，因此上图中 level 1 中两个子命名空间的六个进程分别映射到其父命名空间的PID 号5~10。 命名空间增大了 PID 管理的复杂性，对于某些进程可能有多个PID——在其自身命名空间的PID以及其父命名空间的PID，凡能看到该进程的命名空间都会为其分配一个PID。因此就有： 全局ID：在内核本身和初始命名空间中唯一的ID，在系统启动期间开始的 init 进程即属于该初始命名空间。系统中每个进程都对应了该命名空间的一个PID，叫全局ID，保证在整个系统中唯一。 局部ID：对于属于某个特定的命名空间，它在其命名空间内分配的ID为局部ID，该ID也可以出现在其他的命名空间中。 3、进程ID管理数据结构Linux 内核在设计管理ID的数据结构时，要充分考虑以下因素： 如何快速地根据进程的 task_struct、ID类型、命名空间找到局部ID 如何快速地根据局部ID、命名空间、ID类型找到对应进程的 task_struct 如何快速地给新进程在可见的命名空间内分配一个唯一的 PID 如果将所有因素考虑到一起，将会很复杂，下面将会由简到繁设计该结构。 3.1一个PID对应一个task_struct如果先不考虑进程之间的关系，不考虑命名空间，仅仅是一个PID号对应一个task_struct，那么我们可以设计这样的数据结构： 12345678910111213141516struct task_struct &#123; //... struct pid_link pids; //...&#125;;struct pid_link &#123; struct hlist_node node; struct pid *pid; &#125;;struct pid &#123; struct hlist_head tasks; //指回 pid_link 的 node int nr; //PID struct hlist_node pid_chain; //pid hash 散列表结点&#125;; 每个进程的 task_struct 结构体中有一个指向 pid 结构体的指针，pid 结构体包含了 PID 号。 pid_hash[]: 这是一个hash表的结构，根据 pid 的 nr 值哈希到其某个表项，若有多个 pid 结构对应到同一个表项，这里解决冲突使用的是散列表法。这样，就能解决开始提出的第2个问题了，根据PID值怎样快速地找到task_struct结构体： 首先通过 PID 计算 pid 挂接到哈希表 pid_hash[] 的表项 遍历该表项，找到 pid 结构体中 nr 值与 PID 值相同的那个 pid 再通过该 pid 结构体的 tasks 指针找到 node 最后根据内核的 container_of 机制就能找到 task_struct 结构体 pid_map：这是一个位图，用来唯一分配PID值的结构，图中灰色表示已经分配过的值，在新建一个进程时，只需在其中找到一个为分配过的值赋给 pid 结构体的 nr，再将pid_map 中该值设为已分配标志。这也就解决了上面的第3个问题——如何快速地分配一个全局的PID。 至于上面的第1个问题就更加简单，已知 task_struct 结构体，根据其 pid_link 的 pid 指针找到 pid 结构体，取出其 nr 即为 PID 号。 3.2进程ID有类型之分如果考虑进程之间有复杂的关系，如线程组、进程组、会话组，这些组均有组ID，分别为 TGID、PGID、SID，所以原来的 task_struct 中pid_link 指向一个 pid 结构体需要增加几项，用来指向到其组长的 pid 结构体，相应的 struct pid 原本只需要指回其 PID 所属进程的task_struct，现在要增加几项，用来链接那些以该 pid 为组长的所有进程组内进程。数据结构如下： 123456789101112131415161718192021222324252627282930enum pid_type&#123; PIDTYPE_PID, PIDTYPE_PGID, PIDTYPE_SID, PIDTYPE_MAX&#125;;struct task_struct &#123; //... pid_t pid; //PID pid_t tgid; //thread group id struct task_struct *group_leader; // threadgroup leader struct pid_link pids[PIDTYPE_MAX]; //...&#125;;struct pid_link &#123; struct hlist_node node; struct pid *pid; &#125;;struct pid &#123; struct hlist_head tasks[PIDTYPE_MAX]; int nr; //PID struct hlist_node pid_chain; // pid hash 散列表结点&#125;; 上面 ID 的类型 PIDTYPE_MAX 表示 ID 类型数目。之所以不包括线程组ID，是因为内核中已经有指向到线程组的 task_struct 指针 group_leader，线程组 ID 无非就是 group_leader 的PID。 假如现在有三个进程A、B、C为同一个进程组，进程组长为A 3.3增加进程PID命名空间若在第二种情形下再增加PID命名空间，一个进程就可能有多个PID值了，因为在每一个可见的命名空间内都会分配一个PID，这样就需要改变 pid 的结构了，如下： 12345678910111213struct pid&#123; unsigned int level; /* lists of tasks that use this pid */ struct hlist_head tasks[PIDTYPE_MAX]; struct upid numbers[1];&#125;;struct upid &#123; int nr; struct pid_namespace *ns; struct hlist_node pid_chain;&#125;; 在 pid 结构体中增加了一个表示该进程所处的命名空间的层次level，以及一个可扩展的 upid 结构体。对于struct upid，表示在该命名空间所分配的进程的ID，ns指向是该ID所属的命名空间，pid_chain 表示在该命名空间的散列表。 4、进程ID管理函数有了上面的复杂的数据结构，再加上散列表等数据结构的操作，就可以写出我们前面所提到的三个问题的函数了： 4.1获得局部ID根据进程的 task_struct、ID类型、命名空间，可以很容易获得其在命名空间内的局部ID： 获得与task_struct 关联的pid结构体。辅助函数有 task_pid、task_tgid、task_pgrp和task_session，分别用来获取不同类型的ID的pid 实例，如获取 PID 的实例：staticinlinestruct pid task_pid(struct task_struct *task) { return task-&gt;*pids[PIDTYPE_PID].pid; }获取线程组的ID，前面也说过，TGID不过是线程组组长的PID而已，所以：static*inline*struct** pid task_tgid(struct task_struct *task) { return task-&gt;group_leader-&gt;*pids[PIDTYPE_PID].pid; }而获得PGID和SID，首先需要找到该线程组组长的task_struct，再获得其相应的 pid：static*inline*struct** pid task_pgrp(struct task_struct *task) { return task-&gt;group_leader-&gt;*pids[PIDTYPE_PGID].pid; } static*inline*struct** pid *task_session(struct task_struct ***task) { return task-&gt;group_leader-&gt;**pids[PIDTYPE_SID].pid; } 获得 pid 实例之后，再根据 pid 中的numbers 数组中 uid 信息，获得局部PID。pid_t****pid_nr_ns(struct pid *****pid, struct pid_namespace *****ns) { struct upid ***upid; pid_t nr &#x3D; 0; if (pid &amp;&amp; ns-&gt;**level **&lt;&#x3D;** pid**-&gt;*level) { upid &#x3D;*&amp;pid-&gt;numbers[ns-&gt;level]; if (upid-&gt;ns &#x3D;&#x3D; ns) nr &#x3D; upid-&gt;**nr; } return nr; }这里值得注意的是，由于PID命名空间的层次性，父命名空间能看到子命名空间的内容，反之则不能，因此，函数中需要确保当前命名空间的level 小于等于产生局部PID的命名空间的level。除了这个函数之外，内核还封装了其他函数用来从 pid 实例获得 PID 值，如 pid_nr、pid_vnr 等。在此不介绍了。 结合这两步，内核提供了更进一步的封装，提供以下函数： 1234pid_t task_pid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);pid_t task_pigd_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);pid_t task_session_nr_ns(struct task_struct *tsk, struct pid_namespace *ns); 从函数名上就能推断函数的功能，其实不外于封装了上面的两步。 4.2查找进程task_struct根据局部ID、以及命名空间，怎样获得进程的task_struct结构体呢？也是分两步： 获得 pid 实体。根据局部PID以及命名空间计算在 pid_hash 数组中的索引，然后遍历散列表找到所要的 upid， 再根据内核的 container_of 机制找到 pid 实例。代码如下：struct pid *find_pid_ns(int nr, struct pid_namespace *****ns) { struct hlist_node ****elem; struct upid pnr; &#x2F;&#x2F;遍历散列表 hlist_for_each_entry_rcu(pnr, elem, &amp;pid_hash[pid_hashfn(nr, ns)], pid_chain) &#x2F;&#x2F;pid_hashfn() 获得hash的索引if (pnr-&gt;nr &#x3D;&#x3D; nr &amp;&amp; pnr-&gt;ns &#x3D;&#x3D; ns) &#x2F;&#x2F;比较 nr 与 ns 是否都相同return container_of(pnr, struct pid, &#x2F;&#x2F;根据container_of机制取得pid 实体 numbers[ns-&gt;**level]); return NULL; } 根据ID类型取得task_struct 结构体。struct task_struct *pid_task(struct pid *****pid, enum pid_type type) { struct task_struct *****result &#x3D; NULL; if (pid) { struct hlist_node ***first; first &#x3D; rcu_dereference_check(hlist_first_rcu(&amp;pid-&gt;**tasks[type]), lockdep_tasklist_lock_is_held()); if (first) result &#x3D; hlist_entry(first, struct task_struct, pids[(type)].node); } return result; } 内核还提供其它函数用来实现上面两步： 123struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);struct task_struct *find_task_by_vpid(pid_t vnr);struct task_struct *find_task_by_pid(pid_t vnr); 具体函数实现的功能也比较简单。 4.3生成唯一的PID内核中使用下面两个函数来实现分配和回收PID的： 12static int alloc_pidmap(struct pid_namespace *pid_ns);static void free_pidmap(struct upid *upid); 在这里我们不关注这两个函数的实现，反而应该关注分配的 PID 如何在多个命名空间中可见，这样需要在每个命名空间生成一个局部ID，函数 alloc_pid 为新建的进程分配PID，简化版如下： 1234567891011121314151617181920212223242526272829303132struct pid *alloc_pid(struct pid_namespace *ns)&#123; struct pid *pid; enum pid_type type; int i, nr; struct pid_namespace *tmp; struct upid *upid; tmp = ns; pid-&gt;level = ns-&gt;level; // 初始化 pid-&gt;numbers[] 结构体 for (i = ns-&gt;level; i &gt;= 0; i--) &#123; nr = alloc_pidmap(tmp); //分配一个局部ID pid-&gt;numbers[i].nr = nr; pid-&gt;numbers[i].ns = tmp; tmp = tmp-&gt;parent; &#125; // 初始化 pid-&gt;task[] 结构体 for (type = 0; type &lt; PIDTYPE_MAX; ++type) INIT_HLIST_HEAD(&amp;pid-&gt;tasks[type]); // 将每个命名空间经过哈希之后加入到散列表中 upid = pid-&gt;numbers + ns-&gt;level; for ( ; upid &gt;= pid-&gt;numbers; --upid) &#123; hlist_add_head_rcu(&amp;upid-&gt;pid_chain, &amp;pid_hash[pid_hashfn(upid-&gt;nr, upid-&gt;ns)]); upid-&gt;ns-&gt;nr_hashed++; &#125; return pid;&#125; 版权声明：本文为知乎博主「极致Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/546814252","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核进程管理几种CPU调度策略","path":"/2023/01/15/linux-docs/进程管理/Linux内核进程管理几种CPU调度策略/","content":"CPU调度我们知道，程序需要获得CPU的资源才能被调度和执行，那么当一个进程由于某种原因放弃CPU然后进入阻塞状态，下一个获得CPU资源去被调度执行的进程会是谁呢？下图中，进程1因为阻塞放弃CPU资源，此时，进程2刚IO操作结束，可以获得CPU资源去被调度，进程3的时间片轮转结束，也同样可以获得CPU资源去被调度，那么，此时的操作系统应该安排哪个进程去获得CPU资源呢？这就涉及到我们操作系统的CPU调度策略了。 根据生活中的例子，我们很容易想到以下两种策略 CPU调度的直观想法1.FIFO 谁先进入，先调度谁，这是一种非常简单有效的方法，就好比我们去饭堂打饭，谁先到就给谁先打饭。但是这种策略会遇到一个问题：如果遇到一个很小的任务，但是它是最后进入的，那么必须得前面一大堆任务结束完后才能执行这个小小的任务，这样就感觉很不划算呀！因为我只是简简单单的一个小任务，但是从打开这个任务到结束这个任务要很久。这显然不符合我们的需求，因而我们会想到第2种策略，就是先调度小任务，后调度大任务。 2.Priority 很简单，就是任务短的优先执行，但是此时又有问题了，任务虽然短，但是它的执行时间不一定短，就好比在一个银行业务中，客户填写一个表，这是一个非常短的任务吧——就单单填个表，但是这个表很长很长，那么这个短任务它的执行时间就很长了，我们怎么知道这个短的任务将来会执行多长的时间呢？所以，这样的策略还是依然有问题。 那么，面对诸多的场景，如何设计调度算法呢？ 首先，我们要明白我们的算法应该让什么更好呢？面对客户：银行调度算法的设计目标应该是让用户满意；而面对进程：CPU调度的目标应该是进程满意。 那怎么才能让进程满意呢？那就是时间了。 进程希望尽早地结束任务，这就是周转时间(从任务到达到任务结束)要短，而且希望用户的操作能够尽快地被响应，这就是响应时间(从操作发生到响应)要短。而且系统内耗时间要少，吞吐量(任务的完成量)要大，系统需要把更多的时间用在任务的执行上，而不能老是去做无关紧要的事情，例如：频繁切换任务，切换栈，分配资源等事情。同时，系统还要合理地调配任务。 那么，CPU的调度策略如何做到合理呢？ 首先得明白系统中有以下的几种矛盾。 1.吞吐量和响应时间之间有矛盾 响应时间小&#x3D;&gt;切换次数多&#x3D;&gt;系统内耗大&#x3D;&gt;吞吐量小 由于需要较短的响应时间，那么就得频繁地切换任务，这样系统的很多时间都花在切换任务上面了，系统的内耗大了，吞吐量就小了。 2.前台任务和后台任务的关注点不同 前台任务关注响应时间，后台任务关注周转时间。 前台任务例如我们的word文档，我们打一个字，需要立马显示在文档中，这就是word文档这个任务关注的是响应时间；而后台任务中，例如我们的javac编译java代码，它的周转时间要小，即该任务从进入到结束所花的时间要小，即编译完成的时间要小。 http://3.IO约束型任务和CPU约束型任务各有各的特点 IO约束型任务就是使用CPU的时间较少，进行IO操作的时间较长，CPU约束型的任务就是使用CPU的时间较长。 因此，要做到合理，需要折中、综合考虑以上的几种矛盾；由此，产生了以下一些CPU的调度算法。 各种CPU调度算法1.First Come，First Served(FCFS) 就是先来先服务的调度算法，哪个任务先进来，就为哪个任务先服务。 我们上面说过，周转时间&#x3D;任务结束的时间-任务到达的时间，因此，我们算一算以上四个任务的平均周转时间。进程A到达时间为0时刻，进程B到达时间为1时刻，进程C到达时间为2时刻，进程D到达时间为3时刻，因此，按照FCFS调度算法，我们一词调度A、B、C、D. 四个任务的平均周转时间&#x3D;(5-0)+(65-1)+(165-2)+(175-3) &#x2F; 4 &#x3D; 101. 那么，因为一个系统中，可能短任务占的比重较多，那些后来进入的短任务，就得等前面一大堆的任务执行完后，CPU才为这些短任务服务，这样的话，很多短任务即使服务时间短，但是它们的周转时间都比较长。我们可以尝试着把短任务提前，短任务提前了，减少以短任务为主的系统的平均周转时间，由此我们产生了短作业优先的CPU调度算法。 2.SJF(Short Job First，短作业优先) 也很简单，就是哪个任务的服务时间短就先调度哪个。还是上面那四个进程。 进程A的服务时间为5，进程B的服务时间为60，进程C的服务时间为100，进程D的服务时间为10，因此，按照短作业优先的CPU调度算法，我们依次调度A、D、B、C. 因此，这四个任务的平均周转时间&#x3D;(5-0)+(15-3)+(75-1)+(175-2) &#x2F; 4 &#x3D; 66. 很明显看到，在以短作业为主的系统中，短作业优先的调度算法的平均周转时间比先来先服务的调度算法平均周转时间要低. 现在问题又来了，如果任务C这个任务是急需要响应的，比如是word文档任务，那么它就要快速响应用户的按键输入请求，就是要求其响应时间要小。很明显，上面的SJF调度策略没有考虑到响应时间这个问题，使得任务C仅仅是周转时间短，而下响应时间较长(必须等A、D、B任务结束后才会响应C)。 由此，我们想到了按时间轮转的方式去调度。 3.RR算法(按时间片来轮转调度) 还是以上面的那四个进程为例。 那按时间片轮转的调度算法是设置一个时间片，比如为10的CPU时间，然后不停地在A、B、C、D四个进程中切换，每个进程执行时间10，时间到了就切换到下一个进程执行时间10，直到全部执行完毕。 为每个进程分配10的CPU时间，轮转调度执行，这样每个进程的响应时间就变小了。 如果时间片设置过大，那响应的时间就会太长，如果时间片设置过小，那整个系统都在不停地切换进程，系统很多时间都浪费在切换进程上面了，造成系统的吞吐量小，折中考虑后，时间片设置为10100ms，切换的时间为0.11ms. 说到这里，SJF算法是关注系统的平均周转时间，而RR算法是关注系统的响应时间，但是如果一个系统需要响应时间小和周转时间小同时存在，那该怎么办？ 比如word很关心响应时间，而javac编译java程序更关心周转时间，两类任务同时存在该怎么办？ 前台的任务更关心响应时间，因为前台任务是与用户直接进行交互的，需要快速响应用户的请求，后台任务更关心周转时间，需要快速的结束任务的。 一个很直观的想法，定义前台任务和后台任务两条队列，前台使用RR算法，后台使用SJF算法，只有前台任务没有时才调度后台任务。 但是这样又会产生问题，如果一直有前台任务怎么办，那这样后台任务就永远得不到调度了。在这里有一个有趣的小故事想跟大家讲：1973年有位工作人员去关闭MIT的IBM7094计算机时，发现有一个进程在1967年提交但一直未运行。 这时候我们可以让后台的任务优先级动态升高，但后台任务(用SJF调度)一旦执行，那前台任务的响应时间又得变大了。 如果我们前后台任务都用时间片，那又退化为了RR算法。 所以，问题还有很多等着我们去发现去想办法解决。 如我们怎么知道哪些是前台任务哪些是后台任务呢，前台任务难道就没有后台任务的工作？后台任务难道没有前台任务的工作？SJF中的短作业优先如何体现？如何判断作业的长度？ 等等这些问题到现在都在疯狂地探讨和研究当中，有兴趣向这方面进行深入了解的可以阅读相关文献，或者阅读以下linux的CPU调度算法源码。单单一个CPU的调度算法就要考虑这么多东西了，可以看到，我们的操作系统真的是人类的一项很伟大的发明。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/455680633","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核进程管理几种进程状态","path":"/2023/01/15/linux-docs/进程管理/Linux内核进程管理几种进程状态/","content":"进程生命周期在Linux内核里，无论是进程还是线程，统一使用 task_struct{} 结构体来表示，也就是统一抽象为任务（task）。task_struct{} 定义在 include&#x2F;linux&#x2F;sched.h 文件中，十分复杂，这里简单了解下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// include/linux/sched.h// ... 省略struct task_struct &#123;#ifdef CONFIG_THREAD_INFO_IN_TASK\t/* * For reasons of header soup (see current_thread_info()), this * must be the first element of task_struct. */\tstruct thread_info thread_info;#endif\t/* -1 unrunnable, 0 runnable, &gt;0 stopped: */\tvolatile long state;\tint exit_state;\tint exit_code;\tint exit_signal;\t/* * This begins the randomizable portion of task_struct. Only * scheduling-critical items should be added above here. */\trandomized_struct_fields_start\tvoid *stack;\trefcount_t usage;\t/* Per task flags (PF_*), defined further below: */\tunsigned int flags;\tunsigned int ptrace;#ifdef CONFIG_SMP\tint on_cpu;\tstruct __call_single_node\twake_entry;#ifdef CONFIG_THREAD_INFO_IN_TASK\t/* Current CPU: */\tunsigned int cpu;#endif\tunsigned int wakee_flips;\tunsigned long wakee_flip_decay_ts;\tstruct task_struct *last_wakee; // ...省略 struct sched_info sched_info;\tstruct list_head tasks; // 链表，将所有task_struct串起来\tpid_t pid; // process id，指的是线程id\tpid_t tgid; // thread group ID，指的是进程的主线程id struct task_struct *group_leader; // 指向的是进程的主线程\t/* Signal handlers: */\tstruct signal_struct *signal;\tstruct sighand_struct __rcu *sighand;\tsigset_t blocked;\tsigset_t real_blocked;\t/* Restored if set_restore_sigmask() was used: */\tsigset_t saved_sigmask;\tstruct sigpending pending;\tunsigned long sas_ss_sp;\tsize_t sas_ss_size;\tunsigned int sas_ss_flags; // ... 省略&#125; 查阅相关资料后，对Linux中进程的生命周期总结如下： 从图上可以看出，进程的睡眠状态是最多的，那进程一般在何时进入睡眠状态呢？答案是I&#x2F;O操作时，因为I&#x2F;O操作的速度与CPU运行速度相比，相差太大，所以此时进程会释放CPU，进入睡眠状态。 进程状态相关的定义同样在 include&#x2F;linux&#x2F;sched.h 文件的开头部分，以 #define TASK_KILLABLE (TASK_WAKEKILL | TASK_UNINTERRUPTIBLE) 为例，TASK_WAKEKILL表示用于在接收到致命信号时唤醒进程，将它与 TASK_UNINTERRUPTIBLE 按位或，就得到了 TASK_KILLABLE。代码注释中提到了 fs&#x2F;proc&#x2F;array.c，所以也将其代码贴出，作为补充。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788// include/linux/sched.h// ... 省略/* * Task state bitmask. NOTE! These bits are also * encoded in fs/proc/array.c: get_task_state(). * * We have two separate sets of flags: task-&gt;state * is about runnability, while task-&gt;exit_state are * about the task exiting. Confusing, but this way * modifying one set can&#x27;t modify the other one by * mistake. *//* Used in tsk-&gt;state: */#define TASK_RUNNING 0x0000#define TASK_INTERRUPTIBLE 0x0001#define TASK_UNINTERRUPTIBLE 0x0002#define __TASK_STOPPED 0x0004#define __TASK_TRACED 0x0008/* Used in tsk-&gt;exit_state: */#define EXIT_DEAD 0x0010#define EXIT_ZOMBIE 0x0020#define EXIT_TRACE (EXIT_ZOMBIE | EXIT_DEAD)/* Used in tsk-&gt;state again: */#define TASK_PARKED 0x0040#define TASK_DEAD 0x0080#define TASK_WAKEKILL 0x0100#define TASK_WAKING 0x0200#define TASK_NOLOAD 0x0400#define TASK_NEW 0x0800#define TASK_STATE_MAX 0x1000/* Convenience macros for the sake of set_current_state: */#define TASK_KILLABLE (TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)#define TASK_STOPPED (TASK_WAKEKILL | __TASK_STOPPED)#define TASK_TRACED (TASK_WAKEKILL | __TASK_TRACED)#define TASK_IDLE (TASK_UNINTERRUPTIBLE | TASK_NOLOAD)/* Convenience macros for the sake of wake_up(): */#define TASK_NORMAL (TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)/* get_task_state(): */#define TASK_REPORT (TASK_RUNNING | TASK_INTERRUPTIBLE | \\ TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \\ __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \\ TASK_PARKED)#define task_is_traced(task) ((task-&gt;state &amp; __TASK_TRACED) != 0)#define task_is_stopped(task) ((task-&gt;state &amp; __TASK_STOPPED) != 0)#define task_is_stopped_or_traced(task)\t((task-&gt;state &amp; (__TASK_STOPPED | __TASK_TRACED)) != 0)// ... 省略// fs/proc/array.c// ... 省略/* * The task state array is a strange &quot;bitmap&quot; of * reasons to sleep. Thus &quot;running&quot; is zero, and * you can test for combinations of others with * simple bit tests. */static const char * const task_state_array[] = &#123;\t/* states in TASK_REPORT: */\t&quot;R (running)&quot;, /* 0x00 */\t&quot;S (sleeping)&quot;, /* 0x01 */\t&quot;D (disk sleep)&quot;,\t/* 0x02 */\t&quot;T (stopped)&quot;, /* 0x04 */\t&quot;t (tracing stop)&quot;,\t/* 0x08 */\t&quot;X (dead)&quot;, /* 0x10 */\t&quot;Z (zombie)&quot;, /* 0x20 */\t&quot;P (parked)&quot;, /* 0x40 */\t/* states beyond TASK_REPORT: */\t&quot;I (idle)&quot;, /* 0x80 */&#125;;static inline const char *get_task_state(struct task_struct *tsk)&#123;\tBUILD_BUG_ON(1 + ilog2(TASK_REPORT_MAX) != ARRAY_SIZE(task_state_array));\treturn task_state_array[task_state_index(tsk)];&#125;// ... 省略 在单核的CPU上，同一时刻只有一个task会被调度，所以即使看到了 R 状态，也不代表进程就被分配到了CPU时间片。但了解了进程状态之后，我们再通过 top, ps aux 等命令查看进程，分析问题起来效率就更高了: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647top - 17:24:07 up 10:20, 1 user, load average: 0.15, 0.08, 0.02Tasks: 216 total, 1 running, 215 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.4 us, 0.3 sy, 0.0 ni, 99.2 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 5945.2 total, 2655.4 free, 1580.8 used, 1709.1 buff/cacheMiB Swap: 2048.0 total, 2048.0 free, 0.0 used. 4084.6 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1914 demonlee 20 0 5252100 395940 132816 S 1.0 6.5 1:33.09 gnome-shell 824 root 20 0 2495652 89512 48936 S 0.7 1.5 0:05.59 dockerd 1687 demonlee 20 0 1153156 81436 50128 S 0.3 1.3 0:15.90 Xorg 1957 demonlee 20 0 206556 28348 18504 S 0.3 0.5 0:00.26 ibus-x11 2897 demonlee 20 0 874684 61296 44532 S 0.3 1.0 0:06.22 gnome-terminal- 19984 demonlee 20 0 20632 4036 3376 R 0.3 0.1 0:00.02 top 1 root 20 0 169076 12952 8288 S 0.0 0.2 0:04.61 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd 3 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 rcu_gp 4 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 rcu_par_gp 6 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/0:0H-kblockd 9 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 mm_percpu_wq 10 root 20 0 0 0 0 S 0.0 0.0 0:00.08 ksoftirqd/0 11 root 20 0 0 0 0 I 0.0 0.0 0:03.60 rcu_sched 12 root rt 0 0 0 0 S 0.0 0.0 0:00.51 migration/0 13 root -51 0 0 0 0 S 0.0 0.0 0:00.00 idle_inject/0 14 root 20 0 0 0 0 S 0.0 0.0 0:00.00 cpuhp/0 15 root 20 0 0 0 0 S 0.0 0.0 0:00.00 cpuhp/1 16 root -51 0 0 0 0 S 0.0 0.0 0:00.00 idle_inject/1 17 root rt 0 0 0 0 S 0.0 0.0 0:00.89 migration/1 18 root 20 0 0 0 0 S 0.0 0.0 0:00.09 ksoftirqd/1 20 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/1:0H 21 root 20 0 0 0 0 S 0.0 0.0 0:00.00 cpuhp/2 22 root -51 0 0 0 0 S 0.0 0.0 0:00.00 idle_inject/2 23 root rt 0 0 0 0 S 0.0 0.0 0:00.83 migration/2 24 root 20 0 0 0 0 S 0.0 0.0 0:00.07 ksoftirqd/2 26 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/2:0H-kblockd 27 root 20 0 0 0 0 S 0.0 0.0 0:00.00 cpuhp/3 28 root -51 0 0 0 0 S 0.0 0.0 0:00.00 idle_inject/3 29 root rt 0 0 0 0 S 0.0 0.0 0:00.77 migration/3 30 root 20 0 0 0 0 S 0.0 0.0 0:00.18 ksoftirqd/3 32 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/3:0H-kblockd 33 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kdevtmpfs 34 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 netns 35 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_tasks_kthre 36 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_tasks_rude_ 37 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_tasks_trace 38 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kauditd 39 root 20 0 0 0 0 S 0.0 0.0 0:00.03 khungtaskd demonlee@demonlee-ubuntu:~$ 最后，再补充一个知识点：使用ps命令查看进程时，会发现状态上面有其他符号，比如 S+、Z+ 等，如下所示， 123456789demonlee@demonlee-ubuntu:~$ demonlee 1704 0.0 0.6 557904 37256 ? Sl 05:01 0:00 /usr/libexec/goa-daemondemonlee 1707 0.0 0.1 172652 6936 tty2 Ssl+ 05:01 0:00 /usr/lib/gdm3/gdm-x-session --run-script env GNOME_SHELL_SESSION_MODE=ubuntu /usr/bin/gdemonlee 1714 0.0 0.1 323388 9068 ? Sl 05:01 0:00 /usr/libexec/goa-identity-servicedemonlee 1720 0.1 1.3 1151880 80576 tty2 Sl+ 05:01 1:05 /usr/lib/xorg/Xorg vt2 -displayfd 3 -auth /run/user/1000/gdm/Xauthority -background nondemonlee 1723 0.0 0.1 325356 9016 ? Ssl 05:01 0:02 /usr/libexec/gvfs-afc-volume-monitordemonlee 1728 0.0 0.1 244336 6532 ? Ssl 05:01 0:00 /usr/libexec/gvfs-mtp-volume-monitordemonlee 1759 0.0 0.2 197052 14276 tty2 Sl+ 05:01 0:00 /usr/libexec/gnome-session-binary --systemd --systemd --session=ubuntu... 这个 + 是啥意思呢，其实manps中就有描述，只是我们从来都没认真看说明文档： 12345678910111213141516171819202122PROCESS STATE CODES Here are the different values that the s, stat and state output specifiers (header &quot;STAT&quot; or &quot;S&quot;) will display to describe the state of a process: D uninterruptible sleep (usually IO) I Idle kernel thread R running or runnable (on run queue) S interruptible sleep (waiting for an event to complete) T stopped by job control signal t stopped by debugger during the tracing W paging (not valid since the 2.6.xx kernel) X dead (should never be seen) Z defunct (&quot;zombie&quot;) process, terminated but not reaped by its parent For BSD formats and when the stat keyword is used, additional characters may be displayed: &lt; high-priority (not nice to other users) N low-priority (nice to other users) L has pages locked into memory (for real-time and custom IO) s is a session leader l is multi-threaded (using CLONE_THREAD, like NPTL pthreads do) + is in the foreground process group 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/442905446","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核进程管理原理详解(代码演示)","path":"/2023/01/15/linux-docs/进程管理/Linux内核进程管理原理详解(代码演示)/","content":"前言：Linux内核里大部分都是C语言。建议先看《Linux内核设计与实现(Linux Kernel Development)》,Robert Love，也就是LKD。 Linux是一种动态系统，能够适应不断变化的计算需求。Linux计算需求的表现是以进程的通用抽象为中心的。进程可以是短期的（从命令行执行的一个命令），也可以是长期的（一种网络服务）。因此，对进程及其调度进行一般管理就显得极为重要。 在用户空间，进程是由进程标识符（PID）表示的。从用户的角度来看，一个 PID 是一个数字值，可惟一标识一个进程。一个 PID 在进程的整个生命期间不会更改，但 PID 可以在进程销毁后被重新使用，所以对它们进行缓存并不见得总是理想的。在用户空间，创建进程可以采用几种方式。可以 执行一个程序（这会导致新进程的创建），也可以 在程序内，调用一个 fork或 exec 系统调用。fork调用会导致创建一个子进程，而exec调用则会用新程序代替当前进程上下文。这里将对这几种方法进行讨论以便您能很好地理解它们的工作原理。 这里将按照下面的顺序展开对进程的介绍，首先展示进程的内核表示以及它们是如何在内核内被管理的，然后来看看进程创建和调度的各种方式（在一个或多个处理器上），最后介绍进程的销毁。内核的版本为2.6.32.45。 1，进程描述符在Linux内核内，进程是由相当大的一个称为 task_struct 的结构表示的。此结构包含所有表示此进程所必需的数据，此外，还包含了大量的其他数据用来统计（accounting）和维护与其他进程的关系（如父和子）。task_struct 位于 .&#x2F;linux&#x2F;include&#x2F;linux&#x2F;sched.h（注意.&#x2F;linux&#x2F;指向内核源代码树）。 下面是task_struct结构： 1234567891011121314151617181920struct task_struct &#123; volatile long state; /* -1 不可运行, 0 可运行, &gt;0 已停止 */ void *stack; /* 堆栈 */ atomic_t usage; unsigned int flags; /* 一组标志 */ unsigned int ptrace; /* ... */ int prio, static_prio, normal_prio; /* 优先级 */ /* ... */ struct list_head tasks; /* 执行的线程（可以有很多） */ struct plist_node pushable_tasks; struct mm_struct *mm, *active_mm; /* 内存页（进程地址空间） */ /* 进行状态 */ int exit_state; int exit_code, exit_signal; int pdeath_signal; /* 当父进程死亡时要发送的信号 */ &#x2F;* … *&#x2F; 123456789101112131415161718192021222324 pid_t pid; /* 进程号 */ pid_t tgid; /* ... */ struct task_struct *real_parent; /* 实际父进程real parent process */ struct task_struct *parent; /* SIGCHLD的接受者，由wait4()报告 */ struct list_head children; /* 子进程列表 */ struct list_head sibling; /* 兄弟进程列表 */ struct task_struct *group_leader; /* 线程组的leader */ /* ... */ char comm[TASK_COMM_LEN]; /* 可执行程序的名称（不包含路径） */ /* 文件系统信息 */ int link_count, total_link_count; /* ... */ /* 特定CPU架构的状态 */ struct thread_struct thread; /* 进程当前所在的目录描述 */ struct fs_struct *fs; /* 打开的文件描述信息 */ struct files_struct *files; /* ... */ &#125;; 在task_struct结构中，可以看到几个预料之中的项，比如执行的状态、堆栈、一组标志、父进程、执行的线程（可以有很多）以及开放文件。state 变量是一些表明任务状态的比特位。最常见的状态有：TASK_RUNNING 表示进程正在运行，或是排在运行队列中正要运行；TASK_INTERRUPTIBLE 表示进程正在休眠、TASK_UNINTERRUPTIBLE表示进程正在休眠但不能叫醒；TASK_STOPPED 表示进程停止等等。这些标志的完整列表可以在 .&#x2F;linux&#x2F;include&#x2F;linux&#x2F;sched.h 内找到。 flags 定义了很多指示符，表明进程是否正在被创建（PF_STARTING）或退出（PF_EXITING），或是进程当前是否在分配内存（PF_MEMALLOC）。可执行程序的名称（不包含路径）占用 comm（命令）字段。每个进程都会被赋予优先级（称为 static_prio），但进程的实际优先级是基于加载以及其他几个因素动态决定的。优先级值越低，实际的优先级越高。tasks字段提供了链接列表的能力。它包含一个 prev 指针（指向前一个任务）和一个 next 指针（指向下一个任务）。 进程的地址空间由mm 和 active_mm 字段表示。mm代表的是进程的内存描述符，而 active_mm 则是前一个进程的内存描述符（为改进上下文切换时间的一种优化）。thread_struct thread结构则用来标识进程的存储状态，此元素依赖于Linux在其上运行的特定架构。例如对于x86架构，在.&#x2F;linux&#x2F;arch&#x2F;x86&#x2F;include&#x2F;asm&#x2F;processor.h的thread_struct结构中可以找到该进程自执行上下文切换后的存储（硬件注册表、程序计数器等）。 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738struct thread_struct &#123;\t/* Cached TLS descriptors: */\tstruct desc_struct\ttls_array[GDT_ENTRY_TLS_ENTRIES];\tunsigned long sp0;\tunsigned long sp;#ifdef CONFIG_X86_32\tunsigned long sysenter_cs;#else\tunsigned long usersp;\t/* Copy from PDA */\tunsigned short es;\tunsigned short ds;\tunsigned short fsindex;\tunsigned short gsindex;#endif#ifdef CONFIG_X86_32\tunsigned long ip;#endif\t/* ... */#ifdef CONFIG_X86_32\t/* Virtual 86 mode info */\tstruct vm86_struct __user *vm86_info;\tunsigned long screen_bitmap;\tunsigned long v86flags;\tunsigned long v86mask;\tunsigned long saved_sp0;\tunsigned int saved_fs;\tunsigned int saved_gs;#endif\t/* IO permissions: */\tunsigned long *io_bitmap_ptr;\tunsigned long iopl;\t/* Max allowed port in the bitmap, in bytes: */\tunsigned io_bitmap_max;/* MSR_IA32_DEBUGCTLMSR value to switch in if TIF_DEBUGCTLMSR is set. */\tunsigned long\tdebugctlmsr;\t/* Debug Store context; see asm/ds.h */\tstruct ds_context\t*ds_ctx;&#125;; 2，进程管理在很多情况下，进程都是动态创建并由一个动态分配的 task_struct 表示。一个例外是init 进程本身，它总是存在并由一个静态分配的task_struct表示，参看.&#x2F;linux&#x2F;arch&#x2F;x86&#x2F;kernel&#x2F;init_task.c。 代码如下： 12345678910111213141516171819static struct signal_struct init_signals = INIT_SIGNALS(init_signals);static struct sighand_struct init_sighand = INIT_SIGHAND(init_sighand); /* * 初始化线程结构 */union thread_union init_thread_union __init_task_data =\t&#123; INIT_THREAD_INFO(init_task) &#125;; /* * 初始化init进程的结构。所有其他进程的结构将由fork.c中的slabs来分配 */struct task_struct init_task = INIT_TASK(init_task);EXPORT_SYMBOL(init_task); /* * per-CPU TSS segments. */DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss) = INIT_TSS; 注意进程虽然都是动态分配的，但还是需要考虑最大进程数。在内核内最大进程数是由一个称为max_threads的符号表示的，它可以在 .&#x2F;linux&#x2F;kernel&#x2F;fork.c 内找到。可以通过&#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;threads-max 的 proc 文件系统从用户空间更改此值。 Linux 内所有进程的分配有两种方式。第一种方式是通过一个哈希表，由PID 值进行哈希计算得到；第二种方式是通过双链循环表。循环表非常适合于对任务列表进行迭代。由于列表是循环的，没有头或尾；但是由于 init_task 总是存在，所以可以将其用作继续向前迭代的一个锚点。让我们来看一个遍历当前任务集的例子。任务列表无法从用户空间访问，但该问题很容易解决，方法是以模块形式向内核内插入代码。下面给出一个很简单的程序，它会迭代任务列表并会提供有关每个任务的少量信息（name、pid和 parent 名）。注意，在这里，此模块使用 printk 来发出结果。要查看具体的结果，可以通过 cat 实用工具（或实时的 tail -f&#x2F;var&#x2F;log&#x2F;messages）查看 &#x2F;var&#x2F;log&#x2F;messages 文件。next_task函数是 sched.h 内的一个宏，它简化了任务列表的迭代（返回下一个任务的 task_struct 引用）。 如下： 12#define next_task(p) \\\tlist_entry_rcu((p)-&gt;tasks.next, struct task_struct, tasks) 查询任务列表信息的简单内核模块： 12345678910111213141516171819202122&lt;pre name=&quot;code&quot; class=&quot;cpp&quot;&gt;#include &lt;linux/kernel.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/sched.h&gt; int init_module(void)&#123;\t/* Set up the anchor point */\tstruct task_struct *task=&amp;init_task;\t/* Walk through the task list, until we hit the init_task again */\tdo &#123; printk(KERN_INFO &quot;=== %s [%d] parent %s &quot;, task-&gt;comm,task-&gt;pid,task-&gt;parent-&gt;comm);\t&#125; while((task=next_task(task))!=&amp;init_task); printk(KERN_INFO &quot;Current task is %s [%d] &quot;, current-&gt;comm,current-&gt;pid);\treturn 0;&#125; void cleanup_module(void)&#123;\treturn;&#125; 编译此模块的Makefile文件如下： 1234567obj-m += procsview.o KDIR := /lib/modules/$(shell uname -r)/buildPWD := $(shell pwd) default:\t$(MAKE) -C $(KDIR) SUBDIRS=$(PWD) modules 在编译后，可以用insmod procsview.ko 插入模块对象，也可以用 rmmod procsview 删除它。插入后，&#x2F;var&#x2F;log&#x2F;messages可显示输出，如下所示。从中可以看到，这里有一个空闲任务（称为 swapper）和init 任务（pid 1）。 Dec 28 23:18:16 ubuntu kernel: [12128.910863]&#x3D;&#x3D;&#x3D; swapper [0] parent swapperDec 28 23:18:16 ubuntu kernel: [12128.910934]&#x3D;&#x3D;&#x3D; init [1] parent swapperDec 28 23:18:16 ubuntu kernel: [12128.910945]&#x3D;&#x3D;&#x3D; kthreadd [2] parent swapperDec 28 23:18:16 ubuntu kernel: [12128.910953]&#x3D;&#x3D;&#x3D; migration&#x2F;0 [3] parent kthreadd……Dec 28 23:24:12 ubuntu kernel: [12485.295015]Current task is insmod [6051] Linux 维护一个称为current的宏，标识当前正在运行的进程（类型是 task_struct）。模块尾部的那行prink用于输出当前进程的运行命令及进程号。注意到当前的任务是 insmod，这是因为 init_module 函数是在insmod 命令执行的上下文运行的。current 符号实际指的是一个函数（get_current），可在一个与 arch 有关的头部中找到它。比如 .&#x2F;linux&#x2F;arch&#x2F;x86&#x2F;include&#x2F;asm&#x2F;current.h，如下： 123456789101112131415161718#include &lt;linux/compiler.h&gt;#include &lt;asm/percpu.h&gt; #ifndef __ASSEMBLY__struct task_struct; DECLARE_PER_CPU(struct task_struct *, current_task); static __always_inline struct task_struct *get_current(void)&#123;\treturn percpu_read_stable(current_task);&#125; #define current get_current() #endif /* __ASSEMBLY__ */ #endif /* _ASM_X86_CURRENT_H */ 3，进程创建用户空间内可以通过执行一个程序、或者在程序内调用fork（或exec）系统调用来创建进程，fork调用会导致创建一个子进程，而exec调用则会用新程序代替当前进程上下文。一个新进程的诞生还可以分别通过vfork()和clone()。fork、vfork和clone三个用户态函数均由libc库提供，它们分别会调用Linux内核提供的同名系统调用fork,vfork和clone。下面以fork系统调用为例来介绍。 传统的创建一个新进程的方式是子进程拷贝父进程所有资源，这无疑使得进程的创建效率低，因为子进程需要拷贝父进程的整个地址空间。更糟糕的是，如果子进程创建后又立马去执行exec族函数，那么刚刚才从父进程那里拷贝的地址空间又要被清除以便装入新的进程映像。为了解决这个问题，内核中提供了上述三种不同的系统调用。 内核采用写时复制技术对传统的fork函数进行了下面的优化。即子进程创建后，父子以只读的方式共享父进程的资源（并不包括父进程的页表项）。当子进程需要修改进程地址空间的某一页时，才为子进程复制该页。采用这样的技术可以避免对父进程中某些数据不必要的复制。 使用vfork函数创建的子进程会完全共享父进程的地址空间，甚至是父进程的页表项。父子进程任意一方对任何数据的修改使得另一方都可以感知到。为了使得双方不受这种影响，vfork函数创建了子进程后，父进程便被阻塞直至子进程调用了exec()或exit()。由于现在fork函数引入了写时复制技术，在不考虑复制父进程页表项的情况下，vfork函数几乎不会被使用。 clone函数创建子进程时灵活度比较大，因为它可以通过传递不同的clone标志参数来选择性的复制父进程的资源。 大部分系统调用对应的例程都被命名为 sys_* 并提供某些初始功能以实现调用（例如错误检查或用户空间的行为），实际的工作常常会委派给另外一个名为 do_* 的函数。 在.&#x2F;linux&#x2F;include&#x2F;asm-generic&#x2F;unistd.h中记录了所有的系统调用号及名称。注意fork实现与体系结构相关，对32位的x86系统会使用.&#x2F;linux&#x2F;arch&#x2F;x86&#x2F;include&#x2F;asm&#x2F;unistd_32.h中的定义，fork系统调用编号为2。 fork系统调用在unistd.h中的宏关联如下： 123456#define __NR_fork 1079#ifdef CONFIG_MMU__SYSCALL(__NR_fork, sys_fork)#else__SYSCALL(__NR_fork, sys_ni_syscall)#endif 在unistd_32.h中的调用号关联为： #define __NR_fork 2 在很多情况下，用户空间任务和内核任务的底层机制是一致的。系统调用fork、vfork和clone在内核中对应的服务例程分别为sys_fork()，sys_vfork()和sys_clone()。它们最终都会依赖于一个名为do_fork 的函数来创建新进程。例如在创建内核线程时，内核会调用一个名为 kernel_thread 的函数（对32位系统）参见.&#x2F;linux&#x2F;arch&#x2F;x86&#x2F;kernel&#x2F;process_32.c，注意process.c是包含32&#x2F;64bit都适用的代码，process_32.c是特定于32位架构，process_64.c是特定于64位架构），此函数执行某些初始化后会调用 do_fork。创建用户空间进程的情况与此类似。在用户空间，一个程序会调用fork，通过int $0x80之类的软中断会导致对名为sys_fork的内核函数的系统调用（参见 .&#x2F;linux&#x2F;arch&#x2F;x86&#x2F;kernel&#x2F;process_32.c），如下： 1234int sys_fork(struct pt_regs *regs)&#123;\treturn do_fork(SIGCHLD, regs-&gt;sp, regs, 0, NULL, NULL);&#125; 最终都是直接调用do_fork。进程创建的函数层次结构如下图： 进程创建的函数层次结构 从图中可以看到 do_fork 是进程创建的基础。可以在 .&#x2F;linux&#x2F;kernel&#x2F;fork.c 内找到 do_fork 函数（以及合作函数 copy_process）。 当用户态的进程调用一个系统调用时，CPU切换到内核态并开始执行一个内核函数。在X86体系中，可以通过两种不同的方式进入系统调用：执行int $0×80汇编命令和执行sysenter汇编命令。后者是Intel在Pentium II中引入的指令，内核从2.6版本开始支持这条命令。这里将集中讨论以int $0×80方式进入系统调用的过程。 通过int $0×80方式调用系统调用实际上是用户进程产生一个中断向量号为0×80的软中断。当用户态fork()调用发生时，用户态进程会保存调用号以及参数，然后发出int $0×80指令，陷入0x80中断。CPU将从用户态切换到内核态并开始执行system_call()。这个函数是通过汇编命令来实现的，它是0×80号软中断对应的中断处理程序。对于所有系统调用来说，它们都必须先进入system_call()，也就是所谓的系统调用处理程序。再通过系统调用号跳转到具体的系统调用服务例程处。32位x86系统的系统调用处理程序在.&#x2F;linux&#x2F;arch&#x2F;x86&#x2F;kernel&#x2F;entry_32.S中，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081.macro SAVE_ALL\tcld\tPUSH_GS\tpushl %fs\tCFI_ADJUST_CFA_OFFSET 4\t/*CFI_REL_OFFSET fs, 0;*/\tpushl %es\tCFI_ADJUST_CFA_OFFSET 4\t/*CFI_REL_OFFSET es, 0;*/\tpushl %ds\tCFI_ADJUST_CFA_OFFSET 4\t/*CFI_REL_OFFSET ds, 0;*/\tpushl %eax\tCFI_ADJUST_CFA_OFFSET 4\tCFI_REL_OFFSET eax, 0\tpushl %ebp\tCFI_ADJUST_CFA_OFFSET 4\tCFI_REL_OFFSET ebp, 0\tpushl %edi\tCFI_ADJUST_CFA_OFFSET 4\tCFI_REL_OFFSET edi, 0\tpushl %esi\tCFI_ADJUST_CFA_OFFSET 4\tCFI_REL_OFFSET esi, 0\tpushl %edx\tCFI_ADJUST_CFA_OFFSET 4\tCFI_REL_OFFSET edx, 0\tpushl %ecx\tCFI_ADJUST_CFA_OFFSET 4\tCFI_REL_OFFSET ecx, 0\tpushl %ebx\tCFI_ADJUST_CFA_OFFSET 4\tCFI_REL_OFFSET ebx, 0\tmovl $(__USER_DS), %edx\tmovl %edx, %ds\tmovl %edx, %es\tmovl $(__KERNEL_PERCPU), %edx\tmovl %edx, %fs\tSET_KERNEL_GS %edx.endm/* ... */ENTRY(system_call)\tRING0_INT_FRAME # 无论如何不能进入用户空间\tpushl %eax # 将保存的系统调用编号压入栈中\tCFI_ADJUST_CFA_OFFSET 4\tSAVE_ALL\tGET_THREAD_INFO(%ebp) # 检测进程是否被跟踪\ttestl $_TIF_WORK_SYSCALL_ENTRY,TI_flags(%ebp)\tjnz syscall_trace_entry\tcmpl $(nr_syscalls), %eax\tjae syscall_badsyssyscall_call:\tcall *sys_call_table(,%eax,4)\t# 跳入对应服务例程\tmovl %eax,PT_EAX(%esp) # 保存进程的返回值syscall_exit:\tLOCKDEP_SYS_EXIT\tDISABLE_INTERRUPTS(CLBR_ANY)\t# 不要忘了在中断返回前关闭中断\tTRACE_IRQS_OFF\tmovl TI_flags(%ebp), %ecx\ttestl $_TIF_ALLWORK_MASK, %ecx\t# current-&gt;work\tjne syscall_exit_workrestore_all:\tTRACE_IRQS_IRETrestore_all_notrace:\tmovl PT_EFLAGS(%esp), %eax\t# mix EFLAGS, SS and CS\t# Warning: PT_OLDSS(%esp) contains the wrong/random values if we\t# are returning to the kernel.\t# See comments in process.c:copy_thread() for details.\tmovb PT_OLDSS(%esp), %ah\tmovb PT_CS(%esp), %al\tandl $(X86_EFLAGS_VM | (SEGMENT_TI_MASK &lt;&lt; 8) | SEGMENT_RPL_MASK), %eax\tcmpl $((SEGMENT_LDT &lt;&lt; 8) | USER_RPL), %eax\tCFI_REMEMBER_STATE\tje ldt_ss # returning to user-space with LDT SSrestore_nocheck:\tRESTORE_REGS 4 # skip orig_eax/error_code\tCFI_ADJUST_CFA_OFFSET -4irq_return:\tINTERRUPT_RETURN.section .fixup,&quot;ax&quot; 分析： （1）在system_call函数执行之前，CPU控制单元已经将eflags、cs、eip、ss和esp寄存器的值自动保存到该进程对应的内核栈中。随之，在 system_call内部首先将存储在eax寄存器中的系统调用号压入栈中。接着执行SAVE_ALL宏。该宏在栈中保存接下来的系统调用可能要用到的所有CPU寄存器。 （2）通过GET_THREAD_INFO宏获得当前进程的thread_inof结构的地址；再检测当前进程是否被其他进程所跟踪(例如调试一个程序时，被调试的程序就处于被跟踪状态)，也就是thread_info结构中flag字段的_TIF_ALLWORK_MASK被置1。如果发生被跟踪的情况则转向syscall_trace_entry标记的处理命令处。 （3）对用户态进程传递过来的系统调用号的合法性进行检查。如果不合法则跳入到syscall_badsys标记的命令处。 （4）如果系统调用好合法，则根据系统调用号查找.&#x2F;linux&#x2F;arch&#x2F;x86&#x2F;kernel&#x2F;syscall_table_32.S中的系统调用表sys_call_table，找到相应的函数入口点，跳入sys_fork这个服务例程当中。由于 sys_call_table表的表项占4字节，因此获得服务例程指针的具体方法是将由eax保存的系统调用号乘以4再与sys_call_table表的基址相加。 syscall_table_32.S中的代码如下： 123456789ENTRY(sys_call_table)\t.long sys_restart_syscall\t/* 0 - old &quot;setup()&quot; system call, used for restarting */\t.long sys_exit\t.long ptregs_fork\t.long sys_read\t.long sys_write\t.long sys_open /* 5 */\t.long sys_close\t/* ... */ sys_call_table是系统调用多路分解表，使用 eax 中提供的索引来确定要调用该表中的哪个系统调用。 （5）当系统调用服务例程结束时，从eax寄存器中获得当前进程的的返回值，并把这个返回值存放在曾保存用户态eax寄存器值的那个栈单元的位置上。这样，用户态进程就可以在eax寄存器中找到系统调用的返回码。 经过的调用链为fork()—&gt;int$0×80软中断—&gt;ENTRY(system_call)—&gt;ENTRY(sys_call_table)—&gt;sys_fork()—&gt;do_fork()。实际上fork、vfork和clone三个系统调最终都是调用do_fork()。只不过在调用时所传递的参数有所不同，而参数的不同正好导致了子进程与父进程之间对资源的共享程度不同。因此，分析do_fork()成为我们的首要任务。在进入do_fork函数进行分析之前，很有必要了解一下它的参数。 clone_flags：该标志位的4个字节分为两部分。最低的一个字节为子进程结束时发送给父进程的信号代码，通常为SIGCHLD；剩余的三个字节则是各种clone标志的组合（本文所涉及的标志含义详见下表），也就是若干个标志之间的或运算。通过 clone标志可以有选择的对父进程的资源进行复制。 本文所涉及到的clone标志详见下表： stack_start：子进程用户态堆栈的地址。 regs：指向pt_regs结构体的指针。当系统发生系统调用，即用户进程从用户态切换到内核态时，该结构体保存通用寄存器中的值，并被存放于内核态的堆栈中。 stack_size：未被使用，通常被赋值为0。 parent_tidptr：父进程在用户态下pid的地址，该参数在CLONE_PARENT_SETTID标志被设定时有意义。 child_tidptr：子进程在用户态下pid的地址，该参数在CLONE_CHILD_SETTID标志被设定时有意义。 do_fork函数在.&#x2F;linux&#x2F;kernel&#x2F;fork.c中，主要工作就是复制原来的进程成为另一个新的进程，它完成了整个进程创建中的大部分工作。 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101long do_fork(unsigned long clone_flags, unsigned long stack_start, struct pt_regs *regs, unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr)&#123;\tstruct task_struct *p;\tint trace = 0;\tlong nr; /* * 做一些预先的参数和权限检查 */\tif (clone_flags &amp; CLONE_NEWUSER) &#123; if (clone_flags &amp; CLONE_THREAD) return -EINVAL; /* 希望当用户名称被支持时，这里的检查可去掉 */ if (!capable(CAP_SYS_ADMIN) || !capable(CAP_SETUID) || !capable(CAP_SETGID)) return -EPERM;\t&#125; /* * 希望在2.6.26之后这些标志能实现循环 */\tif (unlikely(clone_flags &amp; CLONE_STOPPED)) &#123; static int __read_mostly count = 100; if (count &gt; 0 &amp;&amp; printk_ratelimit()) &#123; char comm[TASK_COMM_LEN]; count--; printk(KERN_INFO &quot;fork(): process `%s&#x27; used deprecated &quot; &quot;clone flags 0x%lx &quot;, get_task_comm(comm, current), clone_flags &amp; CLONE_STOPPED); &#125;\t&#125; /* * 当从kernel_thread调用本do_fork时，不使用跟踪 */\tif (likely(user_mode(regs)))\t/* 如果从用户态进入本调用，则使用跟踪 */ trace = tracehook_prepare_clone(clone_flags); p = copy_process(clone_flags, stack_start, regs, stack_size, child_tidptr, NULL, trace);\t/* * 在唤醒新线程之前做下面的工作，因为新线程唤醒后本线程指针会变成无效（如果退出很快的话） */\tif (!IS_ERR(p)) &#123; struct completion vfork; trace_sched_process_fork(current, p); nr = task_pid_vnr(p); if (clone_flags &amp; CLONE_PARENT_SETTID) put_user(nr, parent_tidptr); if (clone_flags &amp; CLONE_VFORK) &#123; p-&gt;vfork_done = &amp;vfork; init_completion(&amp;vfork); &#125; audit_finish_fork(p); tracehook_report_clone(regs, clone_flags, nr, p); /* * 我们在创建时设置PF_STARTING，以防止跟踪进程想使用这个标志来区分一个完全活着的进程 * 和一个还没有获得trackhook_report_clone()的进程。现在我们清除它并且设置子进程运行 */ p-&gt;flags &amp;= ~PF_STARTING; if (unlikely(clone_flags &amp; CLONE_STOPPED)) &#123; /* * 我们将立刻启动一个即时的SIGSTOP */ sigaddset(&amp;p-&gt;pending.signal, SIGSTOP); set_tsk_thread_flag(p, TIF_SIGPENDING); __set_task_state(p, TASK_STOPPED); &#125; else &#123; wake_up_new_task(p, clone_flags); &#125; tracehook_report_clone_complete(trace, regs, clone_flags, nr, p); if (clone_flags &amp; CLONE_VFORK) &#123; freezer_do_not_count(); wait_for_completion(&amp;vfork); freezer_count(); tracehook_report_vfork_done(p, nr); &#125;\t&#125; else &#123; nr = PTR_ERR(p);\t&#125;\treturn nr;&#125; （1）在一开始，该函数定义了一个task_struct类型的指针p，用来接收即将为新进程（子进程）所分配的进程描述符。trace表示跟踪状态，nr表示新进程的pid。接着做一些预先的参数和权限检查。 （2）接下来检查clone_flags是否设置了CLONE_STOPPED标志。如果设置了，则做相应处理，打印消息说明进程已过时。通常这样的情况很少发生，因此在判断时使用了unlikely修饰符。使用该修饰符的判断语句执行结果与普通判断语句相同，只不过在执行效率上有所不同。正如该单词的含义所表示的那样，当前进程很少为停止状态。因此，编译器尽量不会把if内的语句与当前语句之前的代码编译在一起，以增加cache的命中率。与此相反，likely修饰符则表示所修饰的代码很可能发生。tracehook_prepare_clone用于设置子进程是否被跟踪。所谓跟踪，最常见的例子就是处于调试状态下的进程被debugger进程所跟踪。进程的ptrace字段非0说明debugger程序正在跟踪它。如果调用是从用户态进来的（而不从kernel_thread进来的），且当前进程（父进程）被另外一个进程所跟踪，那么子进程也要设置为被跟踪，并且将跟踪标志CLONE_PTRACE加入标志变量clone_flags中。如果父进程不被跟踪，则子进程也不会被跟踪，设置好后返回trace。 （3）接下来的这条语句要做的是整个创建过程中最核心的工作：通过copy_process()创建子进程的描述符，分配pid，并创建子进程执行时所需的其他数据结构，最终则会返回这个创建好的进程描述符p。该函数中的参数意义与do_fork函数相同。注意原来内核中为子进程分配pid的工作是在do_fork中完成，现在新的内核已经移到copy_process中了。 （4）如果copy_process函数执行成功，那么将继续执行if(!IS_ERR(p))部分。首先定义了一个完成量vfork，用task_pid_vnr(p)从p中获取新进程的pid。如果clone_flags包含CLONE_VFORK标志，那么将进程描述符中的vfork_done字段指向这个完成量，之后再对vfork完成量进行初始化。完成量的作用是，直到任务A发出信号通知任务B发生了某个特定事件时，任务B才会开始执行，否则任务B一直等待。我们知道，如果使用vfork系统调用来创建子进程，那么必然是子进程先执行。究其原因就是此处vfork完成量所起到的作用。当子进程调用exec函数或退出时就向父进程发出信号，此时父进程才会被唤醒，否则一直等待。此处的代码只是对完成量进行初始化，具体的阻塞语句则在后面的代码中有所体现。 （5）如果子进程被跟踪或者设置了CLONE_STOPPED标志，那么通过sigaddset函数为子进程增加挂起信号，并将子进程的状态设置为TASK_STOPPED。signal对应一个unsignedlong类型的变量，该变量的每个位分别对应一种信号。具体的操作是将SIGSTOP信号所对应的那一位置1。如果子进程并未设置CLONE_STOPPED标志，那么通过wake_up_new_task将进程放到运行队列上，从而让调度器进行调度运行。wake_up_new_task()在.&#x2F;linux&#x2F;kernel&#x2F;sched.c中，用于唤醒第一次新创建的进程，它将为新进程做一些初始的必须的调度器统计操作，然后把进程放到运行队列中。一旦当然正在运行的进程时间片用完（通过时钟tick中断来控制），就会调用schedule()，从而进行进程调度。 代码如下： 12345678910111213141516171819202122232425262728293031323334353637void wake_up_new_task(struct task_struct *p, unsigned long clone_flags)&#123;\tunsigned long flags;\tstruct rq *rq;\tint cpu = get_cpu(); #ifdef CONFIG_SMP\trq = task_rq_lock(p, &amp;flags);\tp-&gt;state = TASK_WAKING; /* * Fork balancing, do it here and not earlier because: * - cpus_allowed can change in the fork path * - any previously selected cpu might disappear through hotplug * * We set TASK_WAKING so that select_task_rq() can drop rq-&gt;lock * without people poking at -&gt;cpus_allowed. */\tcpu = select_task_rq(rq, p, SD_BALANCE_FORK, 0);\tset_task_cpu(p, cpu); p-&gt;state = TASK_RUNNING;\ttask_rq_unlock(rq, &amp;flags);#endif rq = task_rq_lock(p, &amp;flags);\tupdate_rq_clock(rq);\tactivate_task(rq, p, 0);\ttrace_sched_wakeup_new(rq, p, 1);\tcheck_preempt_curr(rq, p, WF_FORK);#ifdef CONFIG_SMP\tif (p-&gt;sched_class-&gt;task_woken) p-&gt;sched_class-&gt;task_woken(rq, p);#endif\ttask_rq_unlock(rq, &amp;flags);\tput_cpu();&#125; 这里先用get_cpu()获取CPU，如果是对称多处理系统（SMP），先设置我为TASK_WAKING状态，由于有多个CPU（每个CPU上都有一个运行队列），需要进行负载均衡，选择一个最佳CPU并设置我使用这个CPU，然后设置我为TASK_RUNNING状态。这段操作是互斥的，因此需要加锁。注意TASK_RUNNING并不表示进程一定正在运行，无论进程是否正在占用CPU，只要具备运行条件，都处于该状态。 Linux把处于该状态的所有PCB组织成一个可运行队列run_queue，调度程序从这个队列中选择进程运行。事实上，Linux是将就绪态和运行态合并为了一种状态。然后用.&#x2F;linux&#x2F;kernel&#x2F;sched.c:activate_task()把当前进程插入到对应CPU的runqueue上，最终完成入队的函数是active_task()—&gt;enqueue_task()，其中核心代码行为：p-&gt;sched_class-&gt;enqueue_task(rq, p,wakeup, head);sched_class在.&#x2F;linux&#x2F;include&#x2F;linux&#x2F;sched.h中，是调度器一系列操作的面向对象抽象，这个类包括进程入队、出队、进程运行、进程切换等接口，用于完成对进程的调度运行。 （6）tracehook_report_clone_complete函数用于在进程复制快要完成时报告跟踪情况。如果父进程被跟踪，则将子进程的pid赋值给父进程的进程描述符的pstrace_message字段，并向父进程的父进程发送SIGCHLD信号。 （7）如果CLONE_VFORK标志被设置，则通过wait操作将父进程阻塞，直至子进程调用exec函数或者退出。 （8）如果copy_process()在执行的时候发生错误，则先释放已分配的pid，再根据PTR_ERR()的返回值得到错误代码，保存于nr中。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/425886322","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核进程管理并发同步与原子操作","path":"/2023/01/15/linux-docs/进程管理/Linux内核进程管理并发同步与原子操作/","content":"并发同步并发 是指在某一时间段内能够处理多个任务的能力，而 并行 是指同一时间能够处理多个任务的能力。并发和并行看起来很像，但实际上是有区别的，如下图（图片来源于网络）： 上图的意思是，有两条在排队买咖啡的队列，并且只有一架咖啡机在处理，而并行就有两架的咖啡机在处理。咖啡机的数量越多，并行能力就越强。可以把上面的两条队列看成两个进程，并发就是指只有单个CPU在处理，而并行就有两个CPU在处理。为了让两个进程在单核CPU中也能得到执行，一般的做法就是让每个进程交替执行一段时间，比如让每个进程固定执行100毫秒 ，执行时间使用完后切换到其他进程执行。而并行就没有这种问题，因为有两个CPU，所以两个进程可以同时执行。如下图： 上面介绍过，并发有可能会打断当前执行的进程，然后替切换成其他进程执行。如果有两个进程同时对一个共享变量 count 进行加一操作，由于C语言的 count++ 操作会被翻译成如下指令： 123mov eax, [count]inc eaxmov [count], eax 那么在并发的情况下，有可能出现如下问题： 假设count变量初始值为0： 进程1执行完 mov eax, [count] 后，寄存器eax内保存了count的值0。 进程2被调度执行。进程2执行 count++ 的所有指令，将累加后的count值1写回到内存。 进程1再次被调度执行，计算count的累加值仍为1，写回到内存。 虽然进程1和进程2执行了两次 count++ 操作，但是count最后的值为1，而不是2。要解决这个问题就需要使用 原子操作 ，原子操作是指不能被打断的操作，在单核CPU中，一条指令就是原子操作。比如上面的问题可以把 count++ 语句翻译成指令 inc [count] 即可。Linux也提供了这样的原子操作，如对整数加一操作的 atomic_inc() ： 1234567static __inline__ void atomic_inc(atomic_t *v)&#123;__asm__ __volatile__(LOCK &quot;incl %0&quot;:&quot;=m&quot; (v-&gt;counter):&quot;m&quot; (v-&gt;counter));&#125; 在多核CPU中，一条指令也不一定是原子操作，比如 inc [count] 指令在多核CPU中需要进行如下过程： \\1. 从内存将count的数据读取到cpu。 \\2. 累加读取的值。 \\3. 将修改的值写回count内存。 Intel x86 CPU 提供了 lock 前缀来锁住总线，可以让指令保证不被其他CPU中断，如下： 12lockinc [count] 锁原子操作 能够保证操作不被其他人干扰，但有时候一个复杂的操作需要由多条指令来实现，那么就不能使用原子操作了，这时候可以使用 锁 来实现。计算机科学中的 锁 与日常生活的 锁 有点类似，举个例子：比如要上公厕，首先找到一个没有人的厕所，然后把厕所门锁上。其他人要使用的话，必须等待当前这人使用完毕，并且把门锁打开才能使用。在计算机中，要对某个公共资源进行操作时，必须对公共资源进行上锁，然后才能使用。如果不上锁，那么就可能导致数据混乱的情况发生。 在Linux内核中，比较常用的锁有： 自旋锁 、 信号量 、 读写锁 等，下面介绍一下自旋锁和信号量的实现。 自旋锁自旋锁 只能在多核CPU系统中，其核心原理是 原子操作 ，原理如下图： 使用自旋锁时，必须先对自旋锁进行初始化（设置为1），上锁过程如下： \\1. 对自旋锁 lock 进行减一操作，判断结果是否等于0，如果是表示上锁成功并返回。 \\2. 如果不等于0，表示其他进程已经上锁，此时必须不断比较自旋锁 lock 的值是否等于1（表示已经解锁）。 \\3. 如果自旋锁 lock 等于1，跳转到第一步继续进行上锁操作。 由于Linux的自旋锁使用汇编实现，所以比较苦涩难懂，这里使用C语言来模拟一下： 12345678910111213void spin_lock(amtoic_t *lock)&#123;again:result = --(*lock);if (result == 0) &#123;return;&#125;while (true) &#123;if (*lock == 1) &#123;goto again;&#125;&#125;&#125; 上面代码将 result &#x3D; –(*lock); 当成原子操作，解锁过程只需要把 lock 设置为1即可。由于自旋锁会不断尝试上锁操作，并不会对进程进行调度，所以在单核CPU中可能会导致 100% 的CPU占用率。另外，自选锁只适合粒度比较小的操作，如果操作粒度比较大，就需要使用信号量这种可调度进程的锁。 信号量与 自旋锁 不一样，当前进程对 信号量 进行上锁时，如果其他进程已经对其进行上锁，那么当前进程会进入睡眠状态，等待其他人对信号量进行解锁。过程如下图： 在Linux内核中，信号量使用 struct semaphore 表示，定义如下： 12345struct semaphore &#123;raw_spinlock_t lock;unsigned int count;struct list_head wait_list;&#125;; 各个字段的作用如下： lock ：自旋锁，用于对多核CPU平台进行同步。 count ：信号量的计数器，上锁时对其进行减一操作(count–)，如果得到的结果为大于等于0，表示成功上锁，如果小于0表示已经被其他进程上锁。 wait_list ：正在等待信号量解锁的进程队列。 信号量 上锁通过 down() 函数实现，代码如下： 12345678910void down(struct semaphore *sem)&#123;unsigned long flags;spin_lock_irqsave(&amp;sem-&gt;lock, flags);if (likely(sem-&gt;count &gt; 0))sem-&gt;count--;else__down(sem);spin_unlock_irqrestore(&amp;sem-&gt;lock, flags);&#125; 上面代码可以看出， down() 函数首先对信号量进行自旋锁操作（为了避免多核CPU竞争），然后比较计数器是否大于0，如果是对计数器进行减一操作，并且返回，否则调用 __down() 函数进行下一步操作。 __down() 函数实现如下： 123456789101112131415161718192021222324static noinline void __sched __down(struct semaphore *sem)&#123;__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);&#125;static inline int __down_common(struct semaphore *sem,long state, long timeout)&#123;struct task_struct *task = current;struct semaphore_waiter waiter;// 把当前进程添加到等待队列中list_add_tail(&amp;waiter.list, &amp;sem-&gt;wait_list);waiter.task = task;waiter.up = 0;for (;;) &#123;...__set_task_state(task, state);spin_unlock_irq(&amp;sem-&gt;lock);timeout = schedule_timeout(timeout);spin_lock_irq(&amp;sem-&gt;lock);if (waiter.up) // 当前进程是否获得信号量锁?return 0;&#125;...&#125; __down() 函数最终调用 __down_common() 函数，而 __down_common() 函数的操作过程如下： 把当前进程添加到信号量的等待队列中。 切换到其他进程运行，直到被其他进程唤醒。 如果当前进程获得信号量锁（由解锁进程传递），那么函数返回。 接下来看看解锁过程，解锁过程主要通过 up() 函数实现，代码如下： 12345678910111213141516171819void up(struct semaphore *sem)&#123;unsigned long flags;raw_spin_lock_irqsave(&amp;sem-&gt;lock, flags);if (likely(list_empty(&amp;sem-&gt;wait_list))) // 如果没有等待的进程, 直接对计数器加一操作sem-&gt;count++;else__up(sem); // 如果有等待进程, 那么调用 __up() 函数进行唤醒raw_spin_unlock_irqrestore(&amp;sem-&gt;lock, flags);&#125;static noinline void __sched __up(struct semaphore *sem)&#123;// 获取到等待队列的第一个进程struct semaphore_waiter *waiter = list_first_entry(&amp;sem-&gt;wait_list, struct semaphore_waiter, list);list_del(&amp;waiter-&gt;list); // 把进程从等待队列中删除waiter-&gt;up = 1; // 告诉进程已经获得信号量锁wake_up_process(waiter-&gt;task); // 唤醒进程&#125; 解锁过程如下： 判断当前信号量是否有等待的进程，如果没有等待的进程, 直接对计数器加以操作 如果有等待的进程，那么获取到等待队列的第一个进程。 把进程从等待队列中删除。 告诉进程已经获得信号量锁 唤醒进程 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/441558619","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核六大进程通信机制原理","path":"/2023/01/15/linux-docs/进程管理/Linux内核六大进程通信机制原理/","content":"初学操作系统的时候，我就一直懵逼，为啥进程同步与互斥机制里有信号量机制，进程通信里又有信号量机制，然后你再看网络上的各种面试题汇总或者博客，你会发现很多都是千篇一律的进程通信机制有哪些？进程同步与互斥机制鲜有人问津。看多了我都想把 CSDN 屏了…..，最后知道真相的我只想说为啥不能一篇博客把东西写清楚，没头没尾真的浪费时间。 希望这篇文章能够拯救某段时间和我一样被绕晕的小伙伴。上篇文章我已经讲过进程间的同步与互斥机制，各位小伙伴看完这个再来看进程通信比较好。 全文脉络思维导图如下： 1、什么是进程通信顾名思义，进程通信（ InterProcess Communication，IPC）就是指进程之间的信息交换。实际上，进程的同步与互斥本质上也是一种进程通信（这也就是待会我们会在进程通信机制中看见信号量和 PV 操作的原因了），只不过它传输的仅仅是信号量，通过修改信号量，使得进程之间建立联系，相互协调和协同工作，但是它缺乏传递数据的能力。 虽然存在某些情况，进程之间交换的信息量很少，比如仅仅交换某个状态信息，这样进程的同步与互斥机制完全可以胜任这项工作。但是大多数情况下，进程之间需要交换大批数据，比如传送一批信息或整个文件，这就需要通过一种新的通信机制来完成，也就是所谓的进程通信。 再来从操作系统层面直观的看一些进程通信：我们知道，为了保证安全，每个进程的用户地址空间都是独立的，一般而言一个进程不能直接访问另一个进程的地址空间，不过内核空间是每个进程都共享的，所以进程之间想要进行信息交换就必须通过内核。 下面就来我们来列举一下 Linux 内核提供的常见的进程通信机制： 管道（也称作共享文件） 消息队列（也称作消息传递） 共享内存（也称作共享存储） 信号量和 PV 操作 信号 套接字（Socket） 2、管道匿名管道 各位如果学过 Linux 命令，那对管道肯定不陌生，Linux 管道使用竖线 | 连接多个命令，这被称为管道符。 1$ command1 | command2 以上这行代码就组成了一个管道，它的功能是将前一个命令（command1）的输出，作为后一个命令（command2）的输入，从这个功能描述中，我们可以看出管道中的数据只能单向流动，也就是半双工通信，如果想实现相互通信（全双工通信），我们需要创建两个管道才行。 另外，通过管道符 | 创建的管道是匿名管道，用完了就会被自动销毁。并且，匿名管道只能在具有亲缘关系（父子进程）的进程间使用，。也就是说，匿名管道只能用于父子进程之间的通信。 在 Linux 的实际编码中，是通过 pipe 函数来创建匿名管道的，若创建成功则返回 0，创建失败就返回 -1： 1int pipe (int fd[2]); 该函数拥有一个存储空间为 2 的文件描述符数组： fd[0] 指向管道的读端，fd[1] 指向管道的写端 fd[1] 的输出是 fd[0] 的输入 粗略的解释一下通过匿名管道实现进程间通信的步骤： 1）父进程创建两个匿名管道，管道 1（fd1[0]和 fd1[1]）和管道 2（fd2[0] 和 fd2[1]）； 因为管道的数据是单向流动的，所以要想实现数据双向通信，就需要两个管道，每个方向一个。 2）父进程 fork 出子进程，于是对于这两个匿名管道，子进程也分别有两个文件描述符指向匿名管道的读写两端； 3）父进程关闭管道 1 的读端 fd1[0] 和 管道 2 的写端 fd2[1]，子进程关闭管道 1 的写端 fd1[1] 和 管道 2 的读端 fd2[0]，这样，管道 1 只能用于父进程写、子进程读；管道 2 只能用于父进程读、子进程写。管道是用环形队列实现的，数据从写端流入从读端流出，这就实现了父子进程之间的双向通信。 看完上面这些讲述，我们来理解下管道的本质是什么：对于管道两端的进程而言，管道就是一个文件（这也就是为啥管道也被称为共享文件机制的原因了），但它不是普通的文件，它不属于某种文件系统，而是自立门户，单独构成一种文件系统，并且只存在于内存中。 简单来说，管道的本质就是内核在内存中开辟了一个缓冲区，这个缓冲区与管道文件相关联，对管道文件的操作，被内核转换成对这块缓冲区的操作。 有名管道 匿名管道由于没有名字，只能用于父子进程间的通信。为了克服这个缺点，提出了有名管道，也称做 FIFO，因为数据是先进先出的传输方式。 所谓有名管道也就是提供一个路径名与之关联，这样，即使与创建有名管道的进程不存在亲缘关系的进程，只要可以访问该路径，就能够通过这个有名管道进行相互通信。 使用 Linux 命令 mkfifo 来创建有名管道： 1$ mkfifo myPipe myPipe 就是这个管道的名称，接下来，我们往 myPipe 这个有名管道中写入数据： 1$ echo &quot;hello&quot; &gt; myPipe 执行这行命令后，你会发现它就停在这了，这是因为管道里的内容没有被读取，只有当管道里的数据被读完后，命令才可以正常退出。于是，我们执行另外一个命令来读取这个有名管道里的数据： 12$ cat &lt; myPipehello 3、消息队列可以看出，管道这种进程通信方式虽然使用简单，但是效率比较低，不适合进程间频繁地交换数据，并且管道只能传输无格式的字节流。为此，消息传递机制（Linux 中称消息队列）应用而生。比如，A 进程要给 B 进程发送消息，A 进程把数据放在对应的消息队列后就可以正常返回了，B 进程在需要的时候自行去消息队列中读取数据就可以了。同样的，B 进程要给 A 进程发送消息也是如此。 消息队列的本质就是存放在内存中的消息的链表，而消息本质上是用户自定义的数据结构。如果进程从消息队列中读取了某个消息，这个消息就会被从消息队列中删除。对比一下管道机制： 消息队列允许一个或多个进程向它写入或读取消息。 消息队列可以实现消息的随机查询，不一定非要以先进先出的次序读取消息，也可以按消息的类型读取。比有名管道的先进先出原则更有优势。 对于消息队列来说，在某个进程往一个队列写入消息之前，并不需要另一个进程在该消息队列上等待消息的到达。而对于管道来说，除非读进程已存在，否则先有写进程进行写入操作是没有意义的。 消息队列的生命周期随内核，如果没有释放消息队列或者没有关闭操作系统，消息队列就会一直存在。而匿名管道随进程的创建而建立，随进程的结束而销毁。 需要注意的是，消息队列对于交换较少数量的数据很有用，因为无需避免冲突。但是，由于用户进程写入数据到内存中的消息队列时，会发生从用户态拷贝数据到内核态的过程；同样的，另一个用户进程读取内存中的消息数据时，会发生从内核态拷贝数据到用户态的过程。因此，如果数据量较大，使用消息队列就会造成频繁的系统调用，也就是需要消耗更多的时间以便内核介入。 4、共享内存为了避免像消息队列那样频繁的拷贝消息、进行系统调用，共享内存机制出现了。 顾名思义，共享内存就是允许不相干的进程将同一段物理内存连接到它们各自的地址空间中，使得这些进程可以访问同一个物理内存，这个物理内存就成为共享内存。如果某个进程向共享内存写入数据，所做的改动将立即影响到可以访问同一段共享内存的任何其他进程。 集合内存管理的内容，我们来深入理解下共享内存的原理。首先，每个进程都有属于自己的进程控制块（PCB）和逻辑地址空间（Addr Space），并且都有一个与之对应的页表，负责将进程的逻辑地址（虚拟地址）与物理地址进行映射，通过内存管理单元（MMU）进行管理。两个不同进程的逻辑地址通过页表映射到物理空间的同一区域，它们所共同指向的这块区域就是共享内存。 不同于消息队列频繁的系统调用，对于共享内存机制来说，仅在建立共享内存区域时需要系统调用，一旦建立共享内存，所有的访问都可作为常规内存访问，无需借助内核。这样，数据就不需要在进程之间来回拷贝，所以这是最快的一种进程通信方式。 5、信号量和 PV 操作实际上，对具有多 CPU 系统的最新研究表明，在这类系统上，消息传递的性能其实是要优于共享内存的，因为消息队列无需避免冲突，而共享内存机制可能会发生冲突。也就是说如果多个进程同时修改同一个共享内存，先来的那个进程写的内容就会被后来的覆盖。 并且，在多道批处理系统中，多个进程是可以并发执行的，但由于系统的资源有限，进程的执行不是一贯到底的， 而是走走停停，以不可预知的速度向前推进（异步性）。但有时候我们又希望多个进程能密切合作，按照某个特定的顺序依次执行，以实现一个共同的任务。 举个例子，如果有 A、B 两个进程分别负责读和写数据的操作，这两个线程是相互合作、相互依赖的。那么写数据应该发生在读数据之前。而实际上，由于异步性的存在，可能会发生先读后写的情况，而此时由于缓冲区还没有被写入数据，读进程 A 没有数据可读，因此读进程 A 被阻塞。 因此，为了解决上述这两个问题，保证共享内存在任何时刻只有一个进程在访问（互斥），并且使得进程们能够按照某个特定顺序访问共享内存（同步），我们就可以使用进程的同步与互斥机制，常见的比如信号量与 PV 操作。 进程的同步与互斥其实是一种对进程通信的保护机制，并不是用来传输进程之间真正通信的内容的，但是由于它们会传输信号量，所以也被纳入进程通信的范畴，称为低级通信。 下面的内容和上篇文章【看完了进程同步与互斥机制，我终于彻底理解了 PV 操作】中所讲的差不多，看过的小伙伴可直接跳到下一标题。 信号量其实就是一个变量 ，我们可以用一个信号量来表示系统中某种资源的数量，比如：系统中只有一台打印机，就可以设置一个初值为 1 的信号量。 用户进程可以通过使用操作系统提供的一对原语来对信号量进行操作，从而很方便的实现进程互斥或同步。这一对原语就是 PV 操作： 1）P 操作：将信号量值减 1，表示申请占用一个资源。如果结果小于 0，表示已经没有可用资源，则执行 P 操作的进程被阻塞。如果结果大于等于 0，表示现有的资源足够你使用，则执行 P 操作的进程继续执行。 可以这么理解，当信号量的值为 2 的时候，表示有 2 个资源可以使用，当信号量的值为 -2 的时候，表示有两个进程正在等待使用这个资源。不看这句话真的无法理解 V 操作，看完顿时如梦初醒。 2）V 操作：将信号量值加 1，表示释放一个资源，即使用完资源后归还资源。若加完后信号量的值小于等于 0，表示有某些进程正在等待该资源，由于我们已经释放出一个资源了，因此需要唤醒一个等待使用该资源（就绪态）的进程，使之运行下去。 我觉得已经讲的足够通俗了，不过对于 V 操作大家可能仍然有困惑，下面再来看两个关于 V 操作的问答： 问：信号量的值 大于 0 表示有共享资源可供使用，这个时候为什么不需要唤醒进程？ 答：所谓唤醒进程是从就绪队列（阻塞队列）中唤醒进程，而信号量的值大于 0 表示有共享资源可供使用，也就是说这个时候没有进程被阻塞在这个资源上，所以不需要唤醒，正常运行即可。 问：信号量的值 等于 0 的时候表示没有共享资源可供使用，为什么还要唤醒进程？ 答：V 操作是先执行信号量值加 1 的，也就是说，把信号量的值加 1 后才变成了 0，在此之前，信号量的值是 -1，即有一个进程正在等待这个共享资源，我们需要唤醒它。 信号量和 PV 操作具体的定义如下： 互斥访问共享内存 两步走即可实现不同进程对共享内存的互斥访问： 定义一个互斥信号量，并初始化为 1 把对共享内存的访问置于 P 操作和 V 操作之间 P 操作和 V 操作必须成对出现。缺少 P 操作就不能保证对共享内存的互斥访问，缺少 V 操作就会导致共享内存永远得不到释放、处于等待态的进程永远得不到唤醒。 实现进程同步 回顾一下进程同步，就是要各并发进程按要求有序地运行。 举个例子，以下两个进程 P1、P2 并发执行，由于存在异步性，因此二者交替推进的次序是不确定的。假设 P2 的 “代码4” 要基于 P1 的 “代码1” 和 “代码2” 的运行结果才能执行，那么我们就必须保证 “代码4” 一定是在 “代码2” 之后才会执行。 如果 P2 的 “代码4” 要基于 P1 的 “代码1” 和 “代码2” 的运行结果才能执行，那么我们就必须保证 “代码4” 一定是在 “代码2” 之后才会执行。 使用信号量和 PV 操作实现进程的同步也非常方便，三步走： 定义一个同步信号量，并初始化为当前可用资源的数量 在优先级较高的操作的后面执行 V 操作，释放资源 在优先级较低的操作的前面执行 P 操作，申请占用资源 配合下面这张图直观理解下： 6、 信号注意！信号和信号量是完全不同的两个概念！ 信号是进程通信机制中唯一的异步通信机制，它可以在任何时候发送信号给某个进程。通过发送指定信号来通知进程某个异步事件的发送，以迫使进程执行信号处理程序。信号处理完毕后，被中断进程将恢复执行。用户、内核和进程都能生成和发送信号。 信号事件的来源主要有硬件来源和软件来源。所谓硬件来源就是说我们可以通过键盘输入某些组合键给进程发送信号，比如常见的组合键 Ctrl+C 产生 SIGINT 信号，表示终止该进程；而软件来源就是通过 kill 系列的命令给进程发送信号，比如 kill -9 1111 ，表示给 PID 为 1111 的进程发送 SIGKILL 信号，让其立即结束。 我们来查看一下 Linux 中有哪些信号： 7、Socket至此，上面介绍的 5 种方法都是用于同一台主机上的进程之间进行通信的，如果想要跨网络与不同主机上的进程进行通信，那该怎么做呢？这就是 Socket 通信做的事情了（当然，Socket 也能完成同主机上的进程通信）。 Socket 起源于 Unix，原意是插座，在计算机通信领域，Socket 被翻译为套接字，它是计算机之间进行通信的一种约定或一种方式。通过 Socket 这种约定，一台计算机可以接收其他计算机的数据，也可以向其他计算机发送数据。 从计算机网络层面来说，Socket 套接字是网络通信的基石，是支持 TCP&#x2F;IP 协议的网络通信的基本操作单元。它是网络通信过程中端点的抽象表示，包含进行网络通信必须的五种信息：连接使用的协议，本地主机的 IP 地址，本地进程的协议端口，远地主机的 IP 地址，远地进程的协议端口。 Socket 的本质其实是一个编程接口（API），是应用层与 TCP&#x2F;IP 协议族通信的中间软件抽象层，它对 TCP&#x2F;IP 进行了封装。它把复杂的 TCP&#x2F;IP 协议族隐藏在 Socket 接口后面。对用户来说，只要通过一组简单的 API 就可以实现网络的连接。 8、总结简单总结一下上面六种 Linux 内核提供的进程通信机制： 1）首先，最简单的方式就是管道，管道的本质是存放在内存中的特殊的文件。也就是说，内核在内存中开辟了一个缓冲区，这个缓冲区与管道文件相关联，对管道文件的操作，被内核转换成对这块缓冲区的操作。管道分为匿名管道和有名管道，匿名管道只能在父子进程之间进行通信，而有名管道没有限制。 2）虽然管道使用简单，但是效率比较低，不适合进程间频繁地交换数据，并且管道只能传输无格式的字节流。为此消息队列应用而生。消息队列的本质就是存放在内存中的消息的链表，而消息本质上是用户自定义的数据结构。如果进程从消息队列中读取了某个消息，这个消息就会被从消息队列中删除。 3）消息队列的速度比较慢，因为每次数据的写入和读取都需要经过用户态与内核态之间数据的拷贝过程，共享内存可以解决这个问题。所谓共享内存就是：两个不同进程的逻辑地址通过页表映射到物理空间的同一区域，它们所共同指向的这块区域就是共享内存。如果某个进程向共享内存写入数据，所做的改动将立即影响到可以访问同一段共享内存的任何其他进程。 对于共享内存机制来说，仅在建立共享内存区域时需要系统调用，一旦建立共享内存，所有的访问都可作为常规内存访问，无需借助内核。这样，数据就不需要在进程之间来回拷贝，所以这是最快的一种进程通信方式。 4）共享内存速度虽然非常快，但是存在冲突问题，为此，我们可以使用信号量和 PV 操作来实现对共享内存的互斥访问，并且还可以实现进程同步。 5）信号和信号量是完全不同的两个概念！信号是进程通信机制中唯一的异步通信机制，它可以在任何时候发送信号给某个进程。通过发送指定信号来通知进程某个异步事件的发送，以迫使进程执行信号处理程序。信号处理完毕后，被中断进程将恢复执行。用户、内核和进程都能生成和发送信号。 6）上面介绍的 5 种方法都是用于同一台主机上的进程之间进行通信的，如果想要跨网络与不同主机上的进程进行通信，就需要使用 Socket 通信。另外，Socket 也能完成同主机上的进程通信。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/465574868","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核进程管理进程优先级","path":"/2023/01/15/linux-docs/进程管理/Linux内核进程管理进程优先级/","content":"前言：进程优先级实际上是系统对进程重要性的一个客观评价。根据这个评价的结果来为进程分配不同的系统资源，这个资源包括内存资源和CPU资源。为了保证“公平公正”的评价每个进程，Google工程师为此设计了一套评价系统。 为什么要有进程优先级？这似乎不用过多的解释，毕竟自从多任务操作系统诞生以来，进程执行占用cpu的能力就是一个必须要可以人为控制的事情。因为有的进程相对重要，而有的进程则没那么重要。进程优先级起作用的方式从发明以来基本没有什么变化，无论是只有一个cpu的时代，还是多核cpu时代，都是通过控制进程占用cpu时间的长短来实现的。就是说在同一个调度周期中，优先级高的进程占用的时间长些，而优先级低的进程占用的短些。从这个角度看，进程优先级其实也跟cgroup的cpu限制一样，都是一种针对cpu占用的QOS机制。我曾经一直很困惑一点，为什么已经有了优先级，还要再设计一个针对cpu的cgroup？得到的答案大概是因为，优先级这个值不能很直观的反馈出资源分配的比例吧？不过这不重要，实际上从内核目前的进程调度器cfs的角度说，同时实现cpushare方式的cgroup和优先级这两个机制完全是相同的概念，并不会因为增加一个机制而提高什么实现成本。既然如此，而cgroup又显得那么酷，那么何乐而不为呢？ NICE值nice值应该是熟悉Linux&#x2F;UNIX的人很了解的概念了，我们都知它是一个反映一个进程“优先级”状态的值，其取值范围是-20至19，一共40个级别。这个值越小，表示进程”优先级”越高，而值越大“优先级”越低。我们可以通过nice命令来对一个将要执行的命令进行nice值设置，方法是： 1[root@zorrozou-pc0 zorro]# nice -n 10 bash 这样我就又打开了一个bash，并且其nice值设置为10，而默认情况下，进程的优先级应该是从父进程继承来的，这个值一般是0。我们可以通过nice命令直接查看到当前shell的nice值 1[root@zorrozou-pc0 zorro]# nice 对比一下正常情况： 1[root@zorrozou-pc0 zorro]# exit 退出当前nice值为10的bash，打开一个正常的bash： 12[root@zorrozou-pc0 zorro]# bash[root@zorrozou-pc0 zorro]# nice 另外，使用renice命令可以对一个正在运行的进程进行nice值的调整，我们也可以使用比如top、ps等命令查看进程的nice值，具体方法我就不多说了，大家可以参阅相关manpage。 需要大家注意的是，我在这里都在使用nice值这一称谓，而非优先级（priority）这个说法。当然，nice和renice的man手册中，也说的是priority这个概念，但是要强调一下，请大家真的不要混淆了系统中的这两个概念，一个是nice值，一个是priority值，他们有着千丝万缕的关系，但对于当前的Linux系统来说，它们并不是同一个概念。 我们看这个命令： 12345[root@zorrozou-pc0 zorro]# ps -lF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 6924 5776 0 80 0 - 17952 poll_s pts/5 00:00:00 sudo4 S 0 6925 6924 0 80 0 - 4435 wait pts/5 00:00:00 bash0 R 0 12971 6925 0 80 0 - 8514 - pts/5 00:00:00 ps 大家是否真的明白其中PRI列和NI列的具体含义有什么区别？同样的，如果是top命令： 12345678Tasks: 1587 total, 7 running, 1570 sleeping, 0 stopped, 10 zombieCpu(s): 13.0%us, 6.9%sy, 0.0%ni, 78.6%id, 0.0%wa, 0.0%hi, 1.5%si, 0.0%stMem: 132256952k total, 107483920k used, 24773032k free, 2264772k buffersSwap: 2101192k total, 508k used, 2100684k free, 88594404k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 3001 root 20 0 232m 21m 4500 S 12.9 0.0 0:15.09 python 11541 root 20 0 17456 2400 888 R 7.4 0.0 0:00.06 top 大家是否搞清楚了这其中PR值和NI值的差别？如果没有，那么我们可以首先搞清楚什么是nice值。 nice值虽然不是priority，但是它确实可以影响进程的优先级。在英语中，如果我们形容一个人nice，那一般说明这个人的人缘比较好。什么样的人人缘好？往往是谦让、有礼貌的人。比如，你跟一个nice的人一起去吃午饭，点了两个一样的饭，先上了一份后，nice的那位一般都会说：“你先吃你先吃！”，这就是人缘好，这人nice！但是如果另一份上的很晚，那么这位nice的人就要饿着了。这说明什么？越nice的人抢占资源的能力就越差，而越不nice的人抢占能力就越强。这就是nice值大小的含义，nice值越低，说明进程越不nice，抢占cpu的能力就越强，优先级就越高。在原来使用O1调度的Linux上，我们还会把nice值叫做静态优先级，这也基本符合nice值的特点，就是当nice值设定好了之后，除非我们用renice去改它，否则它是不变的。而priority的值在之前内核的O1调度器上表现是会变化的，所以也叫做动态优先级。 优先级和实时进程简单了解nice值的概念之后，我们再来看看什么是priority值，就是ps命令中看到的PRI值或者top命令中看到的PR值。本文为了区分这些概念，以后统一用nice值表示NI值，或者叫做静态优先级，也就是用nice和renice命令来调整的优先级；而实用priority值表示PRI和PR值，或者叫动态优先级。我们也统一将“优先级”这个词的概念规定为表示priority值的意思。 在内核中，进程优先级的取值范围是通过一个宏定义的，这个宏的名称是MAX_PRIO，它的值为140。而这个值又是由另外两个值相加组成的，一个是代表nice值取值范围的NICE_WIDTH宏，另一个是代表实时进程（realtime）优先级范围的MAX_RT_PRIO宏。说白了就是，Linux实际上实现了140个优先级范围，取值范围是从0-139，这个值越小，优先级越高。nice值的-20到19，映射到实际的优先级范围是100-139。新产生进程的默认优先级被定义为： 1#define DEFAULT_PRIO (MAX_RT_PRIO + NICE_WIDTH / 2) 实际上对应的就是nice值的0。正常情况下，任何一个进程的优先级都是这个值，即使我们通过nice和renice命令调整了进程的优先级，它的取值范围也不会超出100-139的范围，除非这个进程是一个实时进程，那么它的优先级取值才会变成0-99这个范围中的一个。这里隐含了一个信息，就是说当前的Linux是一种已经支持实时进程的操作系统。 什么是实时操作系统，我们就不再这里详细解释其含义以及在工业领域的应用了，有兴趣的可以参考一下实时操作系统的维基百科。简单来说，实时操作系统需要保证相关的实时进程在较短的时间内响应，不会有较长的延时，并且要求最小的中断延时和进程切换延时。对于这样的需求，一般的进程调度算法，无论是O1还是CFS都是无法满足的，所以内核在设计的时候，将实时进程单独映射了100个优先级，这些优先级都要高与正常进程的优先级（nice值），而实时进程的调度算法也不同，它们采用更简单的调度算法来减少调度开销。 总的来说，Linux系统中运行的进程可以分成两类： 实时进程 非实时进程 它们的主要区别就是通过优先级来区分的。所有优先级值在0-99范围内的，都是实时进程，所以这个优先级范围也可以叫做实时进程优先级，而100-139范围内的是非实时进程。在系统中可以使用chrt命令来查看、设置一个进程的实时优先级状态。 我们可以先来看一下chrt命令的使用： 123456789101112131415161718192021222324252627282930[root@zorrozou-pc0 zorro]# chrtShow or change the real-time scheduling attributes of a process.Set policy: chrt [options] &lt;priority&gt; &lt;command&gt; [&lt;arg&gt;...] chrt [options] -p &lt;priority&gt; &lt;pid&gt;Get policy: chrt [options] -p &lt;pid&gt;Policy options: -b, --batch set policy to SCHED_OTHER -f, --fifo set policy to SCHED_FIFO -i, --idle set policy to SCHED_IDLE -o, --other set policy to SCHED_OTHER -r, --rr set policy to SCHED_RR (default)Scheduling flag: -R, --reset-on-fork set SCHED_RESET_ON_FORK for FIFO or RROther options: -a, --all-tasks operate on all the tasks (threads) for a given pid -m, --max show min and max valid priorities -p, --pid operate on existing given pid -v, --verbose display status information -h, --help display this help and exit -V, --version output version information and exitFor more details see chrt(1). 我们先来关注显示出的Policy options部分，会发现系统给个种进程提供了5种调度策略。但是这里并没有说明的是，这五种调度策略是分别给两种进程用的，对于实时进程可以用的调度策略是：SCHED_FIFO、SCHED_RR，而对于非实时进程则是：SCHED_OTHER、SCHED_OTHER、SCHED_IDLE。 系统的整体优先级策略是：如果系统中存在需要执行的实时进程，则优先执行实时进程。直到实时进程退出或者主动让出CPU时，才会调度执行非实时进程。实时进程可以指定的优先级范围为1-99，将一个要执行的程序以实时方式执行的方法为： 1234[root@zorrozou-pc0 zorro]# chrt 10 bash[root@zorrozou-pc0 zorro]# chrt -p $$pid 14840&#x27;s current scheduling policy: SCHED_RRpid 14840&#x27;s current scheduling priority: 10 可以看到，新打开的bash已经是实时进程，默认调度策略为SCHED_RR，优先级为10。如果想修改调度策略，就加个参数： 1234[root@zorrozou-pc0 zorro]# chrt -f 10 bash[root@zorrozou-pc0 zorro]# chrt -p $$pid 14843&#x27;s current scheduling policy: SCHED_FIFOpid 14843&#x27;s current scheduling priority: 10 刚才说过，SCHED_RR和SCHED_FIFO都是实时调度策略，只能给实时进程设置。对于所有实时进程来说，优先级高的（就是priority数字小的）进程一定会保证先于优先级低的进程执行。SCHED_RR和SCHED_FIFO的调度策略只有当两个实时进程的优先级一样的时候才会发生作用，其区别也是顾名思义： **SCHED_FIFO:**以先进先出的队列方式进行调度，在优先级一样的情况下，谁先执行的就先调度谁，除非它退出或者主动释放CPU。 **SCHED_RR:**以时间片轮转的方式对相同优先级的多个进程进行处理。时间片长度为100ms。 这就是Linux对于实时进程的优先级和相关调度算法的描述。整体很简单，也很实用。而相对更麻烦的是非实时进程，它们才是Linux上进程的主要分类。对于非实时进程优先级的处理，我们首先还是要来介绍一下它们相关的调度算法：O1和CFS。 O1调度O1调度算法是在Linux 2.6开始引入的，到Linux 2.6.23之后内核将调度算法替换成了CFS。虽然O1算法已经不是当前内核所默认使用的调度算法了，但是由于大量线上的服务器可能使用的Linux版本还是老版本，所以我相信很多服务器还是在使用着O1调度器，那么费一点口舌简单交代一下这个调度器也是有意义的。这个调度器的名字之所以叫做O1，主要是因为其算法的时间复杂度是O1。 O1调度器仍然是根据经典的时间片分配的思路来进行整体设计的。简单来说，时间片的思路就是将CPU的执行时间分成一小段一小段的，假如是5ms一段。于是多个进程如果要“同时”执行，实际上就是每个进程轮流占用5ms的cpu时间，而从1s的时间尺度上看，这些进程就是在“同时”执行的。当然，对于多核系统来说，就是把每个核心都这样做就行了。而在这种情况下，如何支持优先级呢？实际上就是将时间片分配成大小不等的若干种，优先级高的进程使用大的时间片，优先级小的进程使用小的时间片。这样在一个周期结速后，优先级大的进程就会占用更多的时间而因此得到特殊待遇。O1算法还有一个比较特殊的地方是，即使是相同的nice值的进程，也会再根据其CPU的占用情况将其分成两种类型：CPU消耗型和IO消耗性。典型的CPU消耗型的进程的特点是，它总是要一直占用CPU进行运算，分给它的时间片总是会被耗尽之后，程序才可能发生调度。比如常见的各种算数运算程序。而IO消耗型的特点是，它经常时间片没有耗尽就自己主动先释放CPU了，比如vi，emacs这样的编辑器就是典型的IO消耗型进程。 为什么要这样区分呢？因为IO消耗型的进程经常是跟人交互的进程，比如shell、编辑器等。当系统中既有这种进程，又有CPU消耗型进程存在，并且其nice值一样时，假设给它们分的时间片长度是一样的，都是500ms，那么人的操作可能会因为CPU消耗型的进程一直占用CPU而变的卡顿。可以想象，当bash在等待人输入的时候，是不占CPU的，此时CPU消耗的程序会一直运算，假设每次都分到500ms的时间片，此时人在bash上敲入一个字符的时候，那么bash很可能要等个几百ms才能给出响应，因为在人敲入字符的时候，别的进程的时间片很可能并没有耗尽，所以系统不会调度bash程度进行处理。为了提高IO消耗型进程的响应速度，系统将区分这两类进程，并动态调整CPU消耗的进程将其优先级降低，而IO消耗型的将其优先级变高，以降低CPU消耗进程的时间片的实际长度。已知nice值的范围是-20-19，其对应priority值的范围是100－139，对于一个默认nice值为0的进程来说，其初始priority值应该是120，随着其不断执行，内核会观察进程的CPU消耗状态，并动态调整priority值，可调整的范围是+-5。就是说，最高其优先级可以呗自动调整到115，最低到125。这也是为什么nice值叫做静态优先级而priority值叫做动态优先级的原因。不过这个动态调整的功能在调度器换成CFS之后就不需要了，因为CFS换了另外一种CPU时间分配方式，这个我们后面再说。 再简单了解了O1算法按时间片分配CPU的思路之后，我们再来结合进程的状态简单看看其算法描述。我们都知道进程有5种状态： S（Interruptible sleep）：可中断休眠状态。 D（Uninterruptible sleep）：不可中断休眠状态。 R（Running or runnable）：执行或者在可执行队列中。 Z（Zombie process）：僵尸。 T（Stopped）：暂停。 在CPU调度时，主要只关心R状态进程，因为其他状态进程并不会被放倒调度队列中进行调度。调度队列中的进程一般主要有两种情况，一种是进程已经被调度到CPU上执行，另一种是进程正在等待被调度。出现这两种状态的原因应该好理解，因为需要执行的进程数可能多于硬件的CPU核心数，比如需要执行的进程有8个而CPU核心只有4个，此时cpu满载的时候，一定会有4个进程处在“等待”状态，因为此时有另外四个进程正在占用CPU执行。 根据以上情况我们可以理解，系统当下需要同时进行调度处理的进程数（R状态进程数）和系统CPU的比值，可以一定程度的反应系统的“繁忙”程度。需要调度的进程越多，核心越少，则意味着系统越繁忙。除了进程执行本身需要占用CPU以外，多个进程的调度切换也会让系统繁忙程度增加的更多。所以，我们往往会发现，R状态进程数量在增长的情况下，系统的性能表现会下降。系统中可以使用uptime命令查看系统平均负载指数（load average）： 12[zorro@zorrozou-pc0 ~]$ uptime 16:40:56 up 2:12, 1 user, load average: 0.05, 0.11, 0.16 其中load average中分别显示的是1分钟，5分钟，15分钟之内的平均负载指数（可以简单认为是相映时间范围内的R状态进程个数）。但是这个命令显示的数字是绝对个数，并没有表示出不同CPU核心数的实际情况。比如，如果我们的1分钟load average为16，而CPU核心数为32的话，那么这个系统的其实并不繁忙。但是如果CPU个数是8的话，那可能就意味着比较忙了。但是实际情况往往可能比这更加复杂，比如进程消耗类型也会对这个数字的解读有影响。总之，这个值的绝对高低并不能直观的反馈出来当前系统的繁忙程度，还需要根据系统的其它指标综合考虑。 O1调度器在处理流程上大概是这样进行调度的： 首先，进程产生（fork）的时候会给一个进程分配一个时间片长度。这个新进程的时间片一般是父进程的一半，而父进程也会因此减少它的时间片长度为原来的一半。就是说，如果一个进程产生了子进程，那么它们将会平分当前时间片长度。比如，如果父进程时间片还剩100ms，那么一个fork产生一个子进程之后，子进程的时间片是50ms，父进程剩余的时间片是也是50ms。这样设计的目的是，为了防止进程通过fork的方式让自己所处理的任务一直有时间片。不过这样做也会带来少许的不公平，因为先产生的子进程获得的时间片将会比后产生的长，第一个子进程分到父进程的一半，那么第二个子进程就只能分到1&#x2F;4。对于一个长期工作的进程组来说，这种影响可以忽略，因为第一轮时间片在耗尽后，系统会在给它们分配长度相当的时间片。 针对所有R状态进程，O1算法使用两个队列组织进程，其中一个叫做活动队列，另一个叫做过期队列。活动队列中放的都是时间片未被耗尽的进程，而过期队列中放时间片被耗尽的进程。 如1所述，新产生的进程都会先获得一个时间片，进入活动队列等待调度到CPU执行。而内核会在每个tick间隔期间对正在CPU上执行的进程进行检查。一般的tick间隔时间就是cpu时钟中断间隔，每秒钟会有1000个，即频率为1000HZ。每个tick间隔周期主要检查两个内容：1、当前正在占用CPU的进程是不是时间片已经耗尽了？2、是不是有更高优先级的进程在活动队列中等待调度？如果任何一种情况成立，就把则当前进程的执行状态终止，放到等待队列中，换当前在等待队列中优先级最高的那个进程执行。 以上就是O1调度的基本调度思路，当然实际情况是，还要加上SMP（对称多处理）的逻辑，以满足多核CPU的需求。目前在我的archlinux上可以用以下命令查看内核HZ的配置： 1234567[zorro@zorrozou-pc0 ~]$ zgrep CONFIG_HZ /proc/config.gz # CONFIG_HZ_PERIODIC is not set# CONFIG_HZ_100 is not set# CONFIG_HZ_250 is not setCONFIG_HZ_300=y# CONFIG_HZ_1000 is not setCONFIG_HZ=300 我们发现我当前系统的HZ配置为300，而不是一般情况下的1000。大家也可以思考一下，配置成不同的数字（100、250、300、1000），对系统的性能到底会有什么影响？ CFS完全公平调度O1已经是上一代调度器了，由于其对多核、多CPU系统的支持性能并不好，并且内核功能上要加入cgroup等因素，Linux在2.6.23之后开始启用CFS作为对一般优先级(SCHED_OTHER)进程调度方法。在这个重新设计的调度器中，时间片，动态、静态优先级以及IO消耗，CPU消耗的概念都不再重要。CFS采用了一种全新的方式，对上述功能进行了比较完善的支持。 其设计的基本思路是，我们想要实现一个对所有进程完全公平的调度器。又是那个老问题：如何做到完全公平？答案跟上一篇IO调度中CFQ的思路类似：如果当前有n个进程需要调度执行，那么调度器应该在一个比较小的时间范围内，把这n个进程全都调度执行一遍，并且它们平分cpu时间，这样就可以做到所有进程的公平调度。那么这个比较小的时间就是任意一个R状态进程被调度的最大延时时间，即：任意一个R状态进程，都一定会在这个时间范围内被调度相应。这个时间也可以叫做调度周期，其英文名字叫做：sched_latency_ns。进程越多，每个进程在周期内被执行的时间就会被平分的越小。调度器只需要对所有进程维护一个累积占用CPU时间数，就可以衡量出每个进程目前占用的CPU时间总量是不是过大或者过小，这个数字记录在每个进程的vruntime中。所有待执行进程都以vruntime为key放到一个由红黑树组成的队列中，每次被调度执行的进程，都是这个红黑树的最左子树上的那个进程，即vruntime时间最少的进程，这样就保证了所有进程的相对公平。 在基本驱动机制上CFS跟O1一样，每次时钟中断来临的时候，都会进行队列调度检查，判断是否要进程调度。当然还有别的时机需要调度检查，发生调度的时机可以总结为这样几个： 当前进程的状态转换时。主要是指当前进程终止退出或者进程休眠的时候。 当前进程主动放弃CPU时。状态变为sleep也可以理解为主动放弃CPU，但是当前内核给了一个方法，可以使用sched_yield()在不发生状态切换的情况下主动让出CPU。 当前进程的vruntime时间大于每个进程的理想占用时间时（delta_exec &gt;ideal_runtime）。这里的ideal_runtime实际上就是上文说的sched_latency_ns／进程数n。当然这个值并不是一定这样得出，下文会有更详细解释。 当进程从中断、异常或系统调用返回时，会发生调度检查。比如时钟中断。 CFS的优先级当然，CFS中还需要支持优先级。在新的体系中，优先级是以时间消耗（vruntime增长）的快慢来决定的。就是说，对于CFS来说，衡量的时间累积的绝对值都是一样纪录在vruntime中的，但是不同优先级的进程时间增长的比率是不同的，高优先级进程时间增长的慢，低优先级时间增长的快。比如，优先级为19的进程，实际占用cpu为1秒，那么在vruntime中就记录1s。但是如果是-20优先级的进程，那么它很可能实际占CPU用10s，在vruntime中才会纪录1s。CFS真实实现的不同nice值的cpu消耗时间比例在内核中是按照“每差一级cpu占用时间差10%左右”这个原则来设定的。这里的大概意思是说，如果有两个nice值为0的进程同时占用cpu，那么它们应该每人占50%的cpu，如果将其中一个进程的nice值调整为1的话，那么此时应保证优先级高的进程比低的多占用10%的cpu，就是nice值为0的占55%，nice值为1的占45%。那么它们占用cpu时间的比例为55:45。这个值的比例约为1.25。就是说，相邻的两个nice值之间的cpu占用时间比例的差别应该大约为1.25。根据这个原则，内核对40个nice值做了时间计算比例的对应关系，它在内核中以一个数组存在： 12345678910static const int prio_to_weight[40] = &#123; /* -20 */ 88761, 71755, 56483, 46273, 36291, /* -15 */ 29154, 23254, 18705, 14949, 11916, /* -10 */ 9548, 7620, 6100, 4904, 3906, /* -5 */ 3121, 2501, 1991, 1586, 1277, /* 0 */ 1024, 820, 655, 526, 423, /* 5 */ 335, 272, 215, 172, 137, /* 10 */ 110, 87, 70, 56, 45, /* 15 */ 36, 29, 23, 18, 15,&#125;; 我们看到，实际上nice值的最高优先级和最低优先级的时间比例差距还是很大的，绝不仅仅是例子中的十倍。由此我们也可以推导出每一个nice值级别计算vruntime的公式为： 1delta vruntime ＝ delta Time * 1024 / load 这个公式的意思是说，在nice值为0的时候（对应的比例值为1024），计算这个进程vruntime的实际增长时间值（delta vruntime）为：CPU占用时间（delta Time）* 1024 &#x2F; load。在这个公式中load代表当前sched_entity的值，其实就可以理解为需要调度的进程（R状态进程）个数。load越大，那么每个进程所能分到的时间就越少。CPU调度是内核中会频繁进行处理的一个时间，于是上面的delta vruntime的运算会被频繁计算。除法运算会占用更多的cpu时间，所以内核编程中的一个原则就是，尽可能的不用除法。内核中要用除法的地方，基本都用乘法和位移运算来代替，所以上面这个公式就会变成： 123delta vruntime ＝ delta time * 1024 * (2^32 / (load * 2^32)) = (delta time * 1024 * Inverse（load）) &gt;&gt; 32 内核中为了方便不同nice值的Inverse(load)的相关计算，对做好了一个跟prio_to_weight数组一一对应的数组，在计算中可以直接拿来使用，减少计算时的CPU消耗： 12345678910static const u32 prio_to_wmult[40] = &#123; /* -20 */ 48388, 59856, 76040, 92818, 118348, /* -15 */ 147320, 184698, 229616, 287308, 360437, /* -10 */ 449829, 563644, 704093, 875809, 1099582, /* -5 */ 1376151, 1717300, 2157191, 2708050, 3363326, /* 0 */ 4194304, 5237765, 6557202, 8165337, 10153587, /* 5 */ 12820798, 15790321, 19976592, 24970740, 31350126, /* 10 */ 39045157, 49367440, 61356676, 76695844, 95443717, /* 15 */ 119304647, 148102320, 186737708, 238609294, 286331153,&#125;; 具体计算细节不在这里细解释了，有兴趣的可以自行阅读代码：kernel&#x2F;shced&#x2F;fair.c（Linux 4.4）中的__calc_delta（）函数实现。 根据CFS的特性，我们知道调度器总是选择vruntime最小的进程进行调度。那么如果有两个进程的初始化vruntime时间一样时，一个进程被选择进行调度处理，那么只要一进行处理，它的vruntime时间就会大于另一个进程，CFS难道要马上换另一个进程处理么？出于减少频繁切换进程所带来的成本考虑，显然并不应该这样。CFS设计了一个sched_min_granularity_ns参数，用来设定进程被调度执行之后的最小CPU占用时间。 12[zorro@zorrozou-pc0 ~]$ cat /proc/sys/kernel/sched_min_granularity_ns 2250000 一个进程被调度执行后至少要被执行这么长时间才会发生调度切换。我们知道无论到少个进程要执行，它们都有一个预期延迟时间，即：sched_latency_ns，系统中可以通过如下命令来查看这个时间： 12[zorro@zorrozou-pc0 ~]$ cat /proc/sys/kernel/sched_latency_ns 18000000 在这种情况下，如果需要调度的进程个数为n，那么平均每个进程占用的CPU时间为sched_latency_ns／n。显然，每个进程实际占用的CPU时间会因为n的增大而减小。但是实现上不可能让它无限的变小，所以sched_min_granularity_ns的值也限定了每个进程可以获得的执行时间周期的最小值。当进程很多，导致使用了sched_min_granularity_ns作为最小调度周期时，对应的调度延时也就不在遵循sched_latency_ns的限制，而是以实际的需要调度的进程个数n * sched_min_granularity_ns进行计算。当然，我们也可以把这理解为CFS的”时间片”，不过我们还是要强调，CFS是没有跟O1类似的“时间片“的概念的，具体区别大家可以自己琢磨一下。 新进程的VRUNTIME值CFS是通过vruntime最小值来选择需要调度的进程的，那么可以想象，在一个已经有多个进程执行了相对较长的系统中，这个队列中的vruntime时间纪录的数值都会比较长。如果新产生的进程直接将自己的vruntime值设置为0的话，那么它将在执行开始的时间内抢占很多的CPU时间，直到自己的vruntime追赶上其他进程后才可能调度其他进程，这种情况显然是不公平的。所以CFS对每个CPU的执行队列都维护一个min_vruntime值，这个值纪录了这个CPU执行队列中vruntime的最小值，当队列中出现一个新建的进程时，它的初始化vruntime将不会被设置为0，而是根据min_vruntime的值为基础来设置。这样就保证了新建进程的vruntime与老进程的差距在一定范围内，不会因为vruntime设置为0而在进程开始的时候占用过多的CPU。 新建进程获得的实际vruntime值跟一些设置有关，比如： 1[zorro@zorrozou-pc0 ~]$ cat /proc/sys/kernel/sched_child_runs_first 这个文件是fork之后是否让子进程优先于父进程执行的开关。0为关闭，1为打开。如果这个开关打开，就意味着子进程创建后，保证子进程在父进程之前被调度。另外，在源代码目录下的kernel&#x2F;sched&#x2F;features.h文件中，还规定了一系列调度器属性开关。而其中： 12345/* * Place new tasks ahead so that they do not starve already running * tasks */SCHED_FEAT(START_DEBIT, true) 这个参数规定了新进程启动之后第一次运行会有延时。这意味着新进程的vruntime设置要比默认值大一些，这样做的目的是防止应用通过不停的fork来尽可能多的获得执行时间。子进程在创建的时候，vruntime的定义的步骤如下，首先vruntime被设置为min_vruntime。然后判断START_DEBIT位是否被值为true，如果是则会在min_vruntime的基础上增大一些，增大的时间实际上就是一个进程的调度延时时间，即上面描述过的calc_delta_fair()函数得到的结果。这个时间设置完毕之后，就检查sched_child_runs_first开关是否打开，如果打开（值被设置为1），就比较新进程的vruntime和父进程的vruntime哪个更小，并将新进程的vruntime设置为更小的那个值，而父进程的vruntime设置为更大的那个值，以此保证子进程一定在父进程之前被调度。 IO消耗型进程的处理根据前文，我们知道除了可能会一直占用CPU时间的CPU消耗型进程以外，还有一类叫做IO消耗类型的进程，它们的特点是基本不占用CPU，主要行为是在S状态等待响应。这类进程典型的是vim，bash等跟人交互的进程，以及一些压力不大的，使用了多进程（线程）的或select、poll、epoll的网络代理程序。如果CFS采用默认的策略处理这些程序的话，相比CPU消耗程序来说，这些应用由于绝大多数时间都处在sleep状态，它们的vruntime时间基本是不变的，一旦它们进入了调度队列，将会很快被选择调度执行。对比O1调度算法，这种行为相当于自然的提高了这些IO消耗型进程的优先级，于是就不需要特殊对它们的优先级进行“动态调整”了。 但这样的默认策略也是有问题的，有时CPU消耗型和IO消耗型进程的区分不是那么明显，有些进程可能会等一会，然后调度之后也会长时间占用CPU。这种情况下，如果休眠的时候进程的vruntime保持不变，那么等到休眠被唤醒之后，这个进程的vruntime时间就可能会比别人小很多，从而导致不公平。所以对于这样的进程，CFS也会对其进行时间补偿。补偿方式为，如果进程是从sleep状态被唤醒的，而且GENTLE_FAIR_SLEEPERS属性的值为true，则vruntime被设置为sched_latency_ns的一半和当前进程的vruntime值中比较大的那个。sched_latency_ns的值可以在这个文件中进行设置： 12[zorro@zorrozou-pc0 ~]$ cat /proc/sys/kernel/sched_latency_ns 18000000 因为系统中这种调度补偿的存在，IO消耗型的进程总是可以更快的获得响应速度。这是CFS处理与人交互的进程时的策略，即：通过提高响应速度让人的操作感受更好。但是有时候也会因为这样的策略导致整体性能受损。在很多使用了多进程（线程）或select、poll、epoll的网络代理程序，一般是由多个进程组成的进程组进行工作，典型的如apche、nginx和php-fpm这样的处理程序。它们往往都是由一个或者多个进程使用nanosleep()进行周期性的检查是否有新任务，如果有责唤醒一个子进程进行处理，子进程的处理可能会消耗CPU，而父进程则主要是sleep等待唤醒。这个时候，由于系统对sleep进程的补偿策略的存在，新唤醒的进程就可能会打断正在处理的子进程的过程，抢占CPU进行处理。当这种打断很多很频繁的时候，CPU处理的过程就会因为频繁的进程上下文切换而变的很低效，从而使系统整体吞吐量下降。此时我们可以使用开关禁止唤醒抢占的特性。 12[root@zorrozou-pc0 zorro]# cat /sys/kernel/debug/sched_featuresGENTLE_FAIR_SLEEPERS START_DEBIT NO_NEXT_BUDDY LAST_BUDDY CACHE_HOT_BUDDY WAKEUP_PREEMPTION NO_HRTICK NO_DOUBLE_TICK LB_BIAS NONTASK_CAPACITY TTWU_QUEUE RT_PUSH_IPI NO_FORCE_SD_OVERLAP RT_RUNTIME_SHARE NO_LB_MIN ATTACH_AGE_LOAD 上面显示的这个文件的内容就是系统中用来控制kernel&#x2F;sched&#x2F;features.h这个文件所列内容的开关文件，其中WAKEUP_PREEMPTION表示：目前的系统状态是打开sleep唤醒进程的抢占属性的。可以使用如下命令关闭这个属性： 123[root@zorrozou-pc0 zorro]# echo NO_WAKEUP_PREEMPTION &gt; /sys/kernel/debug/sched_features[root@zorrozou-pc0 zorro]# cat /sys/kernel/debug/sched_featuresGENTLE_FAIR_SLEEPERS START_DEBIT NO_NEXT_BUDDY LAST_BUDDY CACHE_HOT_BUDDY NO_WAKEUP_PREEMPTION NO_HRTICK NO_DOUBLE_TICK LB_BIAS NONTASK_CAPACITY TTWU_QUEUE RT_PUSH_IPI NO_FORCE_SD_OVERLAP RT_RUNTIME_SHARE NO_LB_MIN ATTACH_AGE_LOAD 其他相关参数的调整也是类似这样的方式。其他我没讲到的属性的含义，大家可以看kernel&#x2F;sched&#x2F;features.h文件中的注释。 系统中还提供了一个sched_wakeup_granularity_ns配置文件，这个文件的值决定了唤醒进程是否可以抢占的一个时间粒度条件。默认CFS的调度策略是，如果唤醒的进程vruntime小于当前正在执行的进程，那么就会发生唤醒进程抢占的情况。而sched_wakeup_granularity_ns这个参数是说，只有在当前进程的vruntime时间减唤醒进程的vruntime时间所得的差大于sched_wakeup_granularity_ns时，才回发生抢占。就是说sched_wakeup_granularity_ns的值越大，越不容易发生抢占。 CFS和其他调度策略SCHED_BATCH 在上文中我们说过，CFS调度策略主要是针对chrt命令显示的SCHED_OTHER范围的进程，实际上就是一般的非实时进程。我们也已经知道，这样的一般进程还包括另外两种：SCHED_BATCH和SCHED_IDLE。在CFS的实现中，集成了对SCHED_BATCH策略的支持，并且其功能和SCHED_OTHER策略几乎是一致的。唯一的区别在于，如果一个进程被用chrt命令标记成SCHED_OTHER策略的话，CFS将永远认为这个进程是CPU消耗型的进程，不会对其进行IO消耗进程的时间补偿。这样做的唯一目的是，可以在确认进程是CPU消耗型的进程的前提下，对其尽可能的进行批处理方式调度（batch），以减少进程切换带来的损耗，提高吞度量。实际上这个策略的作用并不大，内核中真正的处理区别只是在标记为SCHED_BATCH时进程在sched_yield主动让出cpu的行为发生是不去更新cfs的队列时间，这样就让这些进程在主动让出CPU的时候（执行sched_yield）不会纪录其vruntime的更新，从而可以继续优先被调度到。对于其他行为，并无不同。 SCHED_IDLE 如果一个进程被标记成了SCHED_IDLE策略，调度器将认为这个优先级是很低很低的，比nice值为19的优先级还要低。系统将只在CPU空闲的时候才会对这样的进程进行调度执行。若果存在多个这样的进程，它们之间的调度方式跟正常的CFS相同。 SCHED_DEADLINE 最新的Linux内核还实现了一个最新的调度方式叫做SCHED_DEADLINE。跟IO调度类似，这个算法也是要实现一个可以在最终期限到达前让进程可以调度执行的方法，保证进程不会饿死。目前大多数系统上的chrt还没给配置接口，暂且不做深入分析。 另外要注意的是，SCHED_BATCH和SCHED_IDLE一样，只能对静态优先级（即nice值）为0的进程设置。操作命令如下： 123456789[zorro@zorrozou-pc0 ~]$ chrt -i 0 bash[zorro@zorrozou-pc0 ~]$ chrt -p $$pid 5478&#x27;s current scheduling policy: SCHED_IDLEpid 5478&#x27;s current scheduling priority: 0[zorro@zorrozou-pc0 ~]$ chrt -b 0 bash[zorro@zorrozou-pc0 ~]$ chrt -p $$pid 5502&#x27;s current scheduling policy: SCHED_BATCHpid 5502&#x27;s current scheduling priority: 0 多CPU的CFS调度在上面的叙述中，我们可以认为系统中只有一个CPU，那么相关的调度队列只有一个。实际情况是系统是有多核甚至多个CPU的，CFS从一开始就考虑了这种情况，它对每个CPU核心都维护一个调度队列，这样每个CPU都对自己的队列进程调度即可。这也是CFS比O1调度算法更高效的根本原因：每个CPU一个队列，就可以避免对全局队列使用大内核锁，从而提高了并行效率。当然，这样最直接的影响就是CPU之间的负载可能不均，为了维持CPU之间的负载均衡，CFS要定期对所有CPU进行load balance操作，于是就有可能发生进程在不同CPU的调度队列上切换的行为。这种操作的过程也需要对相关的CPU队列进行锁操作，从而降低了多个运行队列带来的并行性。不过总的来说，CFS的并行队列方式还是要比O1的全局队列方式要高效。尤其是在CPU核心越来越多的情况下，全局锁的效率下降显著增加。 CFS对多个CPU进行负载均衡的行为是idle_balance()函数实现的，这个函数会在CPU空闲的时候由schedule()进行调用，让空闲的CPU从其他繁忙的CPU队列中取进程来执行。我们可以通过查看&#x2F;proc&#x2F;sched_debug的信息来查看所有CPU的调度队列状态信息以及系统中所有进程的调度信息。内容较多，我就不在这里一一列出了，有兴趣的同学可以自己根据相关参考资料（最好的资料就是内核源码）了解其中显示的相关内容分别是什么意思。 在CFS对不同CPU的调度队列做均衡的时候，可能会将某个进程切换到另一个CPU上执行。此时，CFS会在将这个进程出队的时候将vruntime减去当前队列的min_vruntime，其差值作为结果会在入队另一个队列的时候再加上所入队列的min_vruntime，以此来保持队列切换后CPU队列的相对公平。 最后本文的目的是从Linux系统进程的优先级为出发点，通过了解相关的知识点，希望大家对系统的进程调度有个整体的了解。其中我们也对CFS调度算法进行了比较深入的分析。在我的经验来看，这些知识对我们在观察系统的状态和相关优化的时候都是非常有用的。比如在使用top命令的时候，NI和PR值到底是什么意思？类似的地方还有ps命令中的NI和PRI值、ulimit命令-e和-r参数的区别等等。当然，希望看完本文后，能让大家对这些命令显示的了解更加深入。除此之外，我们还会发现，虽然top命令中的PR值和ps -l命令中的PRI值的含义是一样的，但是在优先级相同的情况下，它们显示的值确不一样。那么你知道为什么它们显示会有区别吗？这个问题的答案留给大家自己去寻找吧。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/426399519","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核进程述符和进程状态","path":"/2023/01/15/linux-docs/进程管理/Linux内核进程述符和进程状态/","content":"一，进程程序是指储存在外部存储(如硬盘)的一个可执行文件, 而进程是指处于执行期间的程序, 进程包括 代码段(textsection) 和 数据段(data section) , 除了代码段和数据段外, 进程一般还包含打开的文件, 要处理的信号和CPU上下文等等。 二，进程描述符Linux进程使用 struct task_struct根据描述(include&#x2F;linux&#x2F;sched.h), 如下: 1234567891011121314struct task_struct &#123;/** offsets of these are hardcoded elsewhere - touch with care */ volatile long state; /* -1 unrunnable, 0 runnable, &gt;0 stopped */ unsigned long flags; /* per process flags, defined below */ int sigpending; mm_segment_t addr_limit; /* thread address space: 0-0xBFFFFFFF for user-thead 0-0xFFFFFFFF for kernel-thread */ struct exec_domain *exec_domain; volatile long need_resched; unsigned long ptrace; int lock_depth; /* Lock depth */ /** offset 32 begins here on 32-bit platforms. We keep * all fields in a single cacheline that are needed for * the goodness() loop in schedule(). */ long counter; long nice; unsigned long policy; struct mm_struct *mm; int processor; ... &#125; Linux把所有的进程使用双向链表连接起来, 如下图(来源&lt;Linux设计与实现&gt;): Linux内核为了加快获取当前进程的的task_struct结构, 使用了一个技巧, 就是把task_struct放置在内核栈的栈底, 这样就可以通过esp寄存器快速获取到当前运行进程的task_struct结构. 如下图: 获取当前运行进程的task_struct代码如下: 123456static inline struct task_struct * get_current(void) &#123; struct task_struct *current; __asm__( &quot;andl %%esp,%0; &quot;:&quot;=r&quot; (current) : &quot;0&quot; (~8191UL)); return current;&#125; 三，进程状态进程描述符的state字段用于保存进程的当前状态, 进程的状态有以下几种: TASK_RUNNING (运行) – 进程处于可执行状态, 在这个状态下的进程要么正在被CPU执行, 要么在等待执行(CPU被其他进程占用的情况下) TASK_INTERRUPTIBLE (可中断等待) – 进程处于等待状态, 其在等待某些条件成立或者接收到某些信号, 进程会被唤醒变为运行状态 TASK_UNINTERRUPTIBLE (不可中断等待) – 进程处于等待状态, 其在等待某些条件成立, 进程会被唤醒变为运行状态, 但不能被信号唤醒. TASK_TRACED (被追踪) – 进程处于被追踪状态, 例如通过ptrace命令对进程进行调试. TASK_STOPPED (停止) – 进程处于停止状态, 进程不能被执行. 一般接收到SIGSTOP, SIGTSTP,SIGTTIN, SIGTTOU信号进程会变成TASK_STOPPED状态. 各种状态间的转换如下图: 四，进程的创建在Linux系统中，进程的创建使用fork()系统调用，fork()调用会创建一个与父进程一样的子进程，唯一不同就是fork()的返回值，父进程返回的是子进程的进程ID，而子进程返回的是0。Linux创建子进程时使用了 写时复制（Copy On Write） ，也就是创建子进程时使用的是父进程的内存空间，当子进程或者父进程修改数据时才会复制相应的内存页。当调用fork()系统调用时会陷入内核空间并且调用sys_fork()函数，sys_fork()函数会调用do_fork()函数，代码如下(arch&#x2F;i386&#x2F;kernel&#x2F;process.c)： 123asmlinkage int sys_fork(struct pt_regs regs) &#123; return do_fork(SIGCHLD, regs.esp, &amp;regs, 0); &#125; do_fork()主要的工作是申请一个进程描述符, 然后初始化进程描述符的各个字段, 包括调用 copy_files() 函数复制打开的文件, 调用 copy_sighand() 函数复制信号处理函数, 调用 copy_mm() 函数复制进程虚拟内存空间, 调用 copy_namespace() 函数复制命名空间. 代码如下: 123456789101112int do_fork( unsigned long clone_flags, unsigned long stack_start, struct pt_regs *regs, unsigned long stack_size) &#123; ... p = alloc_task_struct(); // 申请进程描述符 ... if (copy_files(clone_flags, p)) goto bad_fork_cleanup; if (copy_fs(clone_flags, p)) goto bad_fork_cleanup_files; if (copy_sighand(clone_flags, p)) goto bad_fork_cleanup_fs; if (copy_mm(clone_flags, p)) goto bad_fork_cleanup_sighand; if (copy_namespace(clone_flags, p)) goto bad_fork_cleanup_mm; retval = copy_thread(0, clone_flags, stack_start, stack_size, p, regs); ... wake_up_process(p); ... &#125; 值得注意的是do_fork() 还调用了 copy_thread() 这个函数, copy_thread()这个函数主要用于设置进程的CPU执行上下文 struct thread_struct 结构代码如下: 1234567891011121314151617181920int copy_thread( int nr, unsigned long clone_flags, unsigned long esp, unsigned long unused, struct task_struct * p, struct pt_regs * regs) &#123; struct pt_regs * childregs; // 指向栈顶(见图2) childregs = ((struct pt_regs *) (THREAD_SIZE + (unsigned long) p)) - 1; struct_cpy(childregs, regs); // 复制父进程的栈信息 childregs-&gt;eax = 0; // 这个是子进程调用fork()之后的返回值, 也就是0 childregs-&gt;esp = esp; p-&gt;thread.esp = (unsigned long) childregs; // 子进程当前的栈地址, 调用 switch_to()的时候esp设置为这个地址 p-&gt;thread.esp0 = (unsigned long) (childregs+1); // 子进程内核空间栈地址 p-&gt;thread.eip = (unsigned long) ret_from_fork; // 子进程将要执行的代码地址 savesegment(fs,p-&gt;thread.fs ); savesegment(gs,p-&gt;thread.gs); unlazy_fpu(current); struct_cpy(&amp;p-&gt;thread.i387, &amp;current-&gt;thread.i387); return 0;&#125; do_fork() 函数最后调用 wake_up_process() 函数唤醒子进程, 让子进程进入运行状态 五，内核线程Linux内核有很多任务需要去做, 例如定时把缓冲器中的数据刷到硬盘上, 当内存不足的时候进行内存的回收等, 这些所有工作都需要通过内核线程来完成. 内核线程与普通进程的主要区别就是: 内核线程没有自己的 虚拟空间结构(struct mm) , 每次内核线程执行的时候都是借助当前运行进程的虚拟内存空间结构来运行, 因为内核线程只会运行在内核状态, 而每个进程的内核态空间都是一样的, 所以借助其他进程的虚拟内存空间结构来运行是完成可行的内核线程使用 kernel_thread() 函数来创建, 代码如下: 123456789101112131415161718int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags) &#123; long retval, d0; __asm__ __volatile__( &quot;movl %%esp,%%esi \\t&quot; &quot;int $0x80 \\t&quot; /* Linux/i386 system call */ &quot;cmpl %%esp,%%esi \\t&quot; /* child or parent? */ &quot;je 1f \\t&quot; /* parent - jump */ /* Load the argument into eax, and push it. That way, it does * not matter whether the called function is compiled with * -mregparm or not. */ &quot;movl %4,%%eax \\t&quot; &quot;pushl %%eax \\t&quot; &quot;call *%5 \\t&quot; /* call fn */ &quot;movl %3,%0 \\t&quot; /* exit */ &quot;int $0x80 &quot; &quot;1:\\t&quot; :&quot;=&amp;a&quot; (retval), &quot;=&amp;S&quot; (d0) :&quot;0&quot; (__NR_clone), &quot;i&quot; (__NR_exit), &quot;r&quot; (arg), &quot;r&quot; (fn), &quot;b&quot; (flags | CLONE_VM) : &quot;memory&quot;);return retval; &#125; 因为这个函数是使用嵌入汇编来实现的, 所以有点难懂, 不过主要过程就是通过调用 _clone() 函数来创建一个新的进程, 而创建进程是通过传入CLONE_VM 标志来指定进程借用其他进程的虚拟内存空间结构。 特别说明一下：d0局部变量的作用是为了在创建内核线程时保证 struct pt_regs 结构的完整，这是因为创建内核线程是在内核态进行的，所以在内核态调用系统调用是不会压入 ss 和esp寄存器的，这样就会导致系统调用的 struct pt_regs参数信息不完整，所以 kernel_thread() 函数定义了一个 d0 局部变量是为了补充没压栈的ss和esp的。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/441346358","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核进程调度O(1)调度算法","path":"/2023/01/15/linux-docs/进程管理/Linux内核进程调度O(1)调度算法/","content":"Linux是一个支持多任务的操作系统，而多个任务之间的切换是通过 调度器 来完成，调度器 使用不同的调度算法会有不同的效果。Linux2.4版本使用的调度算法的时间复杂度为O(n)，其主要原理是通过轮询所有可运行任务列表，然后挑选一个最合适的任务运行，所以其时间复杂度与可运行任务队列的长度成正比。而Linux2.6开始替换成名为 O(1)调度算法，顾名思义，其时间复杂度为O(1)。虽然在后面的版本开始使用 CFS调度算法（完全公平调度算法），但了解 O(1)调度算法 对学习Linux调度器还是有很大帮助的，所以本文主要介绍 O(1)调度算法 的原理与实现。由于在 Linux 内核中，任务和进程是相同的概念，所以在本文混用了任务和进程这两个名词。 1、O(1)调度算法原理1.1prio_array 结构O(1)调度算法 通过优先级来对任务进行分组，可分为140个优先级（0 ~ 139，数值越小优先级越高），每个优先级的任务由一个队列来维护。 prio_array 结构就是用来维护这些任务队列，如下代码： 1234567891011#define MAX_USER_RT_PRIO 100#define MAX_RT_PRIO MAX_USER_RT_PRIO#define MAX_PRIO (MAX_RT_PRIO + 40)#define BITMAP_SIZE ((((MAX_PRIO+1+7)/8)+sizeof(long)-1)/sizeof(long))struct prio_array &#123; int nr_active; unsigned long bitmap[BITMAP_SIZE]; struct list_head queue[MAX_PRIO];&#125;; 下面介绍 prio_array 结构各个字段的作用： nr_active: 所有优先级队列中的总任务数。 bitmap: 位图，每个位对应一个优先级的任务队列，用于记录哪个任务队列不为空，能通过 bitmap 够快速找到不为空的任务队列。 queue: 优先级队列数组，每个元素维护一个优先级队列，比如索引为0的元素维护着优先级为0的任务队列。 下图更直观地展示了 prio_array 结构各个字段的关系： 如上图所述，bitmap 的第2位和第6位为1（红色代表为1，白色代表为0），表示优先级为2和6的任务队列不为空，也就是说 queue 数组的第2个元素和第6个元素的队列不为空。 1.2runqueue 结构另外，为了减少多核CPU之间的竞争，所以每个CPU都需要维护一份本地的优先队列。因为如果使用全局的优先队列，那么多核CPU就需要对全局优先队列进行上锁，从而导致性能下降。 每个CPU都需要维护一个 runqueue 结构，runqueue 结构主要维护任务调度相关的信息，比如优先队列、调度次数、CPU负载信息等。其定义如下： 1234567891011121314struct runqueue &#123; spinlock_t lock; unsigned long nr_running, nr_switches, expired_timestamp, nr_uninterruptible; task_t *curr, *idle; struct mm_struct *prev_mm; prio_array_t *active, *expired, arrays[2]; int prev_cpu_load[NR_CPUS]; task_t *migration_thread; struct list_head migration_queue; atomic_t nr_iowait;&#125;; runqueue 结构有两个重要的字段：active 和 expired，这两个字段在 O(1)调度算法 中起着至关重要的作用。我们先来了解一下 O(1)调度算法 的大概原理。 我们注意到 active 和 expired 字段的类型为 prio_array，指向任务优先队列。active 代表可以调度的任务队列，而 expired 字段代表时间片已经用完的任务队列。active 和 expired 会进行以下两个过程： 当 active 中的任务时间片用完，那么就会被移动到 expired 中。 当 active 中已经没有任务可以运行，就把 expired 与 active 交换，从而 expired 中的任务可以重新被调度。 如下图所示： O(1)调度算法 把140个优先级的前100个（0 ~ 99）作为 实时进程优先级，而后40个（100 ~ 139）作为 普通进程优先级。实时进程被放置到实时进程优先级的队列中，而普通进程放置到普通进程优先级的队列中。 2、实时进程调度实时进程分为 FIFO（先进先出） 和 RR（时间轮询） 两种，其调度算法比较简单，如下： 先进先出的实时进程调度：如果调度器在执行某个先进先出的实时进程，那么调度器会一直运行这个进程，直至其主动放弃运行权（退出进程或者sleep等）。 时间轮询的实时进程调度：如果调度器在执行某个时间轮询的实时进程，那么调度器会判断当前进程的时间片是否用完，如果用完的话，那么重新分配时间片给它，并且重新放置回 active 队列中，然后调度到其他同优先级或者优先级更高的实时进程进行运行。 2.1普通进程调度每个进程都要一个动态优先级和静态优先级，静态优先级不会变化（进程创建时被设置），而动态优先级会随着进程的睡眠时间而发生变化。动态优先级可以通过以下公式进行计算： 1动态优先级 = max(100, min(静态优先级 – bonus + 5), 139)) 上面公式的 bonus（奖励或惩罚） 是通过进程的睡眠时间计算出来，进程的睡眠时间越大，bonus 的值就越大，那么动态优先级就越高（前面说过优先级的值越小，优先级越高）。 另外要说明一下，实时进程的动态优先级与静态优先级相同。 当一个普通进程被添加到运行队列时，会先计算其动态优先级，然后按照动态优先级的值来添加到对应优先级的队列中。而调度器调度进程时，会先选择优先级最高的任务队列中的进程进行调度运行。 2.2运行时间片计算当进程的时间用完后，就需要重新进行计算。进程的运行时间片与静态优先级有关，可以通过以下公式进行计算： 12静态优先级 &lt; 120，运行时间片 = max((140-静态优先级)*20, MIN_TIMESLICE)静态优先级 &gt;= 120，运行时间片 = max((140-静态优先级)*5, MIN_TIMESLICE) 3、O(1)调度算法实现接下来我们分析一下 O(1)调度算法 在内核中的实现。 3.1时钟中断时钟中断是由硬件触发的，可以通过编程来设置其频率，Linux内核一般设置为每秒产生100 ~ 1000次。时钟中断会触发调用 scheduler_tick() 内核函数，其主要工作是：减少进程的可运行时间片，如果时间片用完，那么把进程从 active 队列移动到 expired 队列中。代码如下： 1234567891011121314151617181920212223242526void scheduler_tick(int user_ticks, int sys_ticks)&#123; runqueue_t *rq = this_rq(); task_t *p = current; ... // 处理普通进程 if (!--p-&gt;time_slice) &#123; // 减少时间片, 如果时间片用完 dequeue_task(p, rq-&gt;active); // 把进程从运行队列中删除 set_tsk_need_resched(p); // 设置要重新调度标志 p-&gt;prio = effective_prio(p); // 重新计算动态优先级 p-&gt;time_slice = task_timeslice(p); // 重新计算时间片 p-&gt;first_time_slice = 0; if (!rq-&gt;expired_timestamp) rq-&gt;expired_timestamp = jiffies; // 如果不是交互进程或者没有出来饥饿状态 if (!TASK_INTERACTIVE(p) || EXPIRED_STARVING(rq)) &#123; enqueue_task(p, rq-&gt;expired); // 移动到expired队列 &#125; else enqueue_task(p, rq-&gt;active); // 重新放置到active队列 &#125; ...&#125; 上面代码主要完成以下几个工作： 减少进程的时间片，并且判断时间片是否已经使用完。 如果时间片使用完，那么把进程从 active 队列中删除。 调用 set_tsk_need_resched() 函数设 TIF_NEED_RESCHED 标志，表示当前进程需要重新调度。 调用 effective_prio() 函数重新计算进程的动态优先级。 调用 task_timeslice() 函数重新计算进程的可运行时间片。 如果当前进程是交互进程或者出来饥饿状态，那么重新加入到 active 队列。 否则把今天移动到 expired 队列。 3.2任务调度如果进程设置了 TIF_NEED_RESCHED 标志，那么当从时钟中断返回到用户空间时，会调用 schedule() 函数进行任务调度。 schedule() 函数代码如下： 12345678910111213141516171819202122232425262728293031void schedule(void)&#123; ... prev = current; // 当前需要被调度的进程 rq = this_rq(); // 获取当前CPU的runqueue array = rq-&gt;active; // active队列 // 如果active队列中没有进程, 那么替换成expired队列 if (unlikely(!array-&gt;nr_active)) &#123; rq-&gt;active = rq-&gt;expired; rq-&gt;expired = array; array = rq-&gt;active; rq-&gt;expired_timestamp = 0; &#125; idx = sched_find_first_bit(array-&gt;bitmap); // 找到最高优先级的任务队列 queue = array-&gt;queue + idx; next = list_entry(queue-&gt;next, task_t, run_list); // 获取到下一个将要运行的进程 ... prev-&gt;sleep_avg -= run_time; // 减少当前进程的睡眠时间 ... if (likely(prev != next)) &#123; ... prev = context_switch(rq, prev, next); // 切换到next进程进行运行 ... &#125; ...&#125; 上面代码主要完成以下几个步骤： 如果当前 runqueue 的 active 队列为空，那么把 active 队列与 expired 队列进行交换。 调用 sched_find_first_bit() 函数在 bitmap 中找到优先级最高并且不为空的任务队列索引。 减少当前进程的睡眠时间。 调用 context_switch() 函数切换到next进程进行运行。 版权声明：本文为知乎博主「玩转Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://zhuanlan.zhihu.com/p/464176766","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux内核进程间通信-管道","path":"/2023/01/15/linux-docs/进程管理/Linux内核进程间通信-管道/","content":"1、管道的定义 管道是第一个广泛应用的进程间通信手段。日常在终端执行shell命令时，会大量用到管道。但管道的缺陷在于只能在有亲缘关系（有共同的祖先）的进程之间使用。为了突破这个限制，后来引入了命名管道。 2、管道的用途 管道是最早出现的进程间通信的手段。在shell中执行命令，经常会将上一个命令的输出作为下一个命令的输入，由多个命令配合完成一件事情。而这就是通过管道来实现的。在图9-3中，进程who的标准输出，通过管道传递给下游的wc进程作为标准输入，从而通过相互配合完成了一件任务。 3、管道的操作 管道的作用是在具有亲缘关系的进程之间传递消息，所谓有亲缘关系，是指有同一个祖先。所以管道并不是只可以用于父子进程通信，也可以在兄弟进程之间还可以用在祖孙之间等，反正只要共同的祖先调用了pipe函数，打开的管道文件就会在fork之后，被各个后代所共享。 不过由于管道是字节流通信，没有消息边界，多个进程同时发送的字节流混在一起，则无法分辨消息，所有管道一般用于2个进程之间通信，另外管道的内容读完后不会保存，管道是单向的，一边要么读，一边要么写，不可以又读又写，想要一边读一边写，那就创建2个管道，如下图 管道是一种文件，可以调用read、write和close等操作文件的接口来操作管道。另一方面管道又不是一种普通的文件，它属于一种独特的文件系统：pipefs。管道的本质是内核维护了一块缓冲区与管道文件相关联，对管道文件的操作，被内核转换成对这块缓冲区内存的操作。下面我们来看一下如何使用管道。 12#include&lt;unistd.h&gt;int pipe(int fd[2]) 如果成功，则返回值是0，如果失败，则返回值是-1，并且设置errno。成功调用pipe函数之后，会返回两个打开的文件描述符，一个是管道的读取端描述符pipefd[0]，另一个是管道的写入端描述符pipefd[1]。管道没有文件名与之关联，因此程序没有选择，只能通过文件描述符来访问管道，只有那些能看到这两个文件描述符的进程才能够使用管道。那么谁能看到进程打开的文件描述符呢？只有该进程及该进程的子孙进程才能看到。这就限制了管道的使用范围。 成功调用pipe函数之后，可以对写入端描述符pipefd[1]调用write，向管道里面写入数据，代码如下所示： 1write(pipefd[1],wbuf,count); 一旦向管道的写入端写入数据后，就可以对读取端描述符pipefd[0]调用read，读出管道里面的内容。如下所示，管道上的read调用返回的字节数等于请求字节数和管道中当前存在的字节数的最小值。如果当前管道为空，那么read调用会阻塞（如果没有设置O_NONBLOCK标志位的话）。 4、管道非法read与write内核实现解析调用pipe函数返回的两个文件描述符中，读取端pipefd[0]支持的文件操作定义在read_pipefifo_fops，写入端pipefd[1]支持的文件操作定义在write_pipefifo_fops，其定义如下： 12345678910111213141516171819202122const struct file_operations read_pipefifo_fops = &#123; //读端相关操作 .llseek = no_llseek, .read = do_sync_read, .aio_read = pipe_read, .write = bad_pipe_w, //一旦写，将调用bad_pipe_w .poll = pipe_poll, .unlocked_ioctl = pipe_ioctl, .open = pipe_read_open, .release = pipe_read_release, .fasync = pipe_read_fasync,&#125;;const struct file_operations write_pipefifo_fops = &#123;//写端相关操作 .llseek = no_llseek, .read = bad_pipe_r, //一旦读，将调用bad_pipe_r .write = do_sync_write, .aio_write = pipe_write, .poll = pipe_poll, .unlocked_ioctl = pipe_ioctl, .open = pipe_write_open, .release = pipe_write_release, .fasync = pipe_write_fasync,&#125;; 我们可以看到，对读取端描述符执行write操作，内核就会执行bad_pipe_w函数；对写入端描述符执行read操作，内核就会执行bad_pipe_r函数。这两个函数比较简单，都是直接返回-EBADF。因此对应的read和write调用都会失败，返回-1，并置errno为EBADF。 12345678910static ssize_t bad_pipe_r(struct file filp, char __user buf, size_t count, loff_t ppos) &#123; return -EBADF; //返回错误 &#125; static ssize_t bad_pipe_w(struct file filp, const char __user buf, size_t count,loff_t ppos) &#123; return -EBADF; &#125; 5、管道通信原理及其亲戚通信解析5.1父子进程通信解析我们只介绍了pipe函数接口，至今尚看不出来该如何使用pipe函数进行进程间通信。调用pipe之后，进程发生了什么呢？请看图9-5。 可以看到，调用pipe函数之后，系统给进程分配了两个文件描述符，即pipe函数返回的两个描述符。该进程既可以往写入端描述符写入信息，也可以从读取端描述符读出信息。可是一个进程管道，起不到任何通信的作用。这不是通信，而是自言自语。如果调用pipe函数的进程随后调用fork函数，创建了子进程，情况就不一样了。fork以后，子进程复制了父进程打开的文件描述符（如图9-6所示），两条通信的通道就建立起来了。此时，可以是父进程往管道里写，子进程从管道里面读；也可以是子进程往管道里写，父进程从管道里面读。这两条通路都是可选的，但是不能都选。原因前面介绍过，管道里面是字节流，父子进程都写、都读，就会导致内容混在一起，对于读管道的一方，解析起来就比较困难。常规的使用方法是父子进程一方只能写入，另一方只能读出，管道变成一个单向的通道，以方便使用。如图9-7所示，父进程放弃读，子进程放弃写，变成父进程写入，子进程读出，成为一个通信的通道… 父进程如何放弃读，子进程又如何放弃写？其实很简单，父进程把读端口pipefd[0]这个文件描述符关闭掉，子进程把写端口pipefd[1]这个文件描述符关闭掉就可以了，示例代码如下： 123456789101112131415int pipefd[2]; pipe(pipefd); switch(fork()) &#123; case -1: /fork failed, error handler here/ case 0: /子进程/ close(pipefd[1]) ; /关闭掉写入端对应的文件描述符/ /子进程可以对pipefd[0]调用read/ break； default: /父进程/ close(pipefd[0]); /父进程关闭掉读取端对应的文件描述符/ /父进程可以对pipefd[1]调用write, 写入想告知子进程的内容/ break &#125; 5.2亲缘关系的进程管道通信解析 图9-8也讲述了如何在兄弟进程之间通过管道通信。如图9-8所示，父进程再次创建一个子进程B，子进程B就持有管道写入端，这时候两个子进程之间就可以通过管道通信了。父进程为了不干扰两个子进程通信，很自觉地关闭了自己的写入端。从此管道成为了两个子进程之间的单向的通信通道。在shell中执行管道命令就是这种情景，只是略有特殊之处，其特殊的地方是管道描述符占用了标准输入和标准输出两个文件描述符 6、管道的注意事项及其性质6.1管道有以下三条性质 只有当所有的写入端描述符都已经关闭了，而且管道中的数据都被读出，对读取描述符调用read函数才返回0（及读到EOF标志）。 如果所有的读取端描述符都已经关闭了，此时进程再次往管道里面写入数据，写操作将会失败，并且内核会像进程发送一个SIGPIPE信号(默认杀死进程)。 当所有的读端与写端都已经关闭时，管道才会关闭. 就因为有这些特性，我们要即使关闭没用的管道文件描述符 6.2shell管道的实现 shell编程会大量使用管道，我们经常看到前一个命令的标准输出作为后一个命令的标准输入，来协作完成任务，如图9-9所示。管道是如何做到的呢？兄弟进程可以通过管道来传递消息，这并不稀奇，前面已经图示了做法。关键是如何使得一个程序的标准输出被重定向到管道中，而另一个程序的标准输入从管道中读取呢？ 答案就是复制文件描述符。对于第一个子进程，执行dup2之后，标准输出对应的文件描述符1，也成为了管道的写入端。这时候，管道就有了两个写入端，按照前面的建议，需要关闭不相干的写入端，使读取端可以顺利地读到EOF，所以应将刚开始分配的管道写入端的文件描述符pipefd[1]关闭掉。 12345if(pipefd[1] != STDOUT_FILENO)&#123;dup2(pipefd[1],STDOUT_FILENO);close(pipefd[1]);&#125; 同样的道理,对于第二个子进程,如法炮制: 12345if(pipefd[0] != STDIN_FILENO)&#123;dup2(pipefd[0],STDIN_FILENO);close(pipefd[0]);&#125; 简单来说，就是第一个子进程的标准输出被绑定到了管道的写入端，于是第一个命令的输出，写入了管道，而第二个子进程管道将其标准输入绑定到管道的读取端，只要管道里面有了内容，这些内容就成了标准输入。 两个示例代码，为什么要判断管道的文件描述符是否等于标准输入和标准输出呢？原因是，在调用pipe时，进程很可能已经关闭了标准输入和标准输出，调用pipe函数时，内核会分配最小的文件描述符，所以pipe的文件描述符可能等于0或1。在这种情况下，如果没有if判断加以保护，代码就变成了： 12dup2(1,1);close(1); 这样的话，第一行代码什么也没做，第二行代码就把管道的写入端给关闭了，于是便无法传递信息了 6.3与shell命令进行通信道的一个重要作用是和外部命令进行通信。在日常编程中，经常会需要调用一个外部命令，并且要获取命令的输出。而有些时候，需要给外部命令提供一些内容，让外部命令处理这些输入。Linux提供了popen接口来帮助程序员做这些事情。就像system函数，即使没有system函数，我们通过fork、exec及wait家族函数一样也可以实现system的功能。但终归是不方便，system函数为我们提供了一些便利。同样的道理，只用pipe函数及dup2等函数，也能完成popen要完成的工作，但popen接口给我们提供了便利。popen接口定义如下： 123#include &lt;stdio.h&gt;FILE *popen(const char *command, const char *type);int pclose(FILE *stream); popen函数会创建一个管道，并且创建一个子进程来执行shell，shell会创建一个子进程来执行command。根据type值的不同，分成以下两种情况。如果type是r：command执行的标准输出，就会写入管道，从而被调用popen的进程读到。通过对popen返回的FILE类型指针执行read或fgets等操作，就可以读取到command的标准输出，如图9-10所示。 如果type是w：调用popen的进程，可以通过对FILE类型的指针fp执行write、fputs等操作，负责往管道里面写入，写入的内容经过管道传给执行command的进程，作为命令的输入，如图9-11所示 popen函数成功时，会返回stdio库封装的FILE类型的指针，失败时会返回NULL，并且设置errno。常见的失败有fork失败，pipe失败，或者分配内存失败。 I&#x2F;O结束了以后，可以调用pclose函数来关闭管道，并且等待子进程的退出。尽管popen函数返回的是FILE类型的指针，也不应调用fclose函数来关闭popen函数打开的文件流指针，因为fclose不会等待子进程的退出。pclose函数成功时会返回子进程中shell的终止状态。popen函数和system函数类似，如果command对应的命令无法执行，就如同执行了exit（127）一样。如果发生其他错误，pclose函数则返回-1。可以从errno中获取到失败的原因。 下面给出一个简单的例子，来示范下popen的用法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;unistd.h&gt;#include&lt;string.h&gt;#include&lt;errno.h&gt;#include&lt;sys/wait.h&gt;#include&lt;signal.h&gt;#define MAX_LINE_SIZE 8192void print_wait_exit(int status)&#123; printf(&quot;status = %d &quot;,status); if(WIFEXITED(status)) &#123; printf(&quot;normal termination,exit status = %d &quot;,WEXITSTATUS(status)); &#125; else if(WIFSIGNALED(status)) &#123; printf(&quot;abnormal termination,signal number =%d%s &quot;, WTERMSIG(status),#ifdef WCOREDUMP WCOREDUMP(status)?&quot;core file generated&quot; : &quot;&quot;);#else &quot;&quot;);#endif &#125;&#125;int main(int argc ,char* argv[])&#123; FILE *fp = NULL ; char command[MAX_LINE_SIZE],buffer[MAX_LINE_SIZE]; if(argc != 2 ) &#123; fprintf(stderr,&quot;Usage: %s filename &quot;,argv[0]); exit(1); &#125; snprintf(command,sizeof(command),&quot;cat %s&quot;,argv[1]); fp = popen(command,&quot;r&quot;); if(fp == NULL) &#123; fprintf(stderr,&quot;popen failed (%s)&quot;,strerror(errno)); exit(2); &#125; while(fgets(buffer,MAX_LINE_SIZE,fp) != NULL) &#123; fprintf(stdout,&quot;%s&quot;,buffer); &#125; int ret = pclose(fp); if(ret == 127 ) &#123; fprintf(stderr,&quot;bad command : %s &quot;,command); exit(3); &#125; else if(ret == -1) &#123; fprintf(stderr,&quot;failed to get child status (%s) &quot;,strerror(errno)); exit(4); &#125; else &#123; print_wait_exit(ret); &#125; exit(0);&#125; 将文件名作为参数传递给程序，执行cat filename的命令。popen创建子进程来负责执行cat filename的命令，子进程的标准输出通过管道传给父进程，父进程可以通过fgets来读取command的标准输出。 6.4system函数与popen函数区别 popen函数和system有很多相似的地方，但是也有显著的不同。调用system函数时，shell命令的执行被封装在了函数内部，所以若system函数不返回，调用system的进程就不再继续执行。但是popen函数不同，一旦调用popen函数，调用进程和执行command的进程便处于并行状态。然后pclose函数才会关闭管道，等待执行command的进程退出。换句话说，在popen之后，pclose之前，调用popen的进程和执行command的进程是并行的，这种差异带来了两种显著的不同： 在并行期间，调用popen的进程可能会创建其他子进程，所以标准规定popen不能阻塞SIGCHLD信号.这也意味着，popen创建的子进程可能被提前执行的等待操作所捕获。若发生这种情况，调用pclose函数时，已经无法等待command子进程的退出，这种情况下，将返回-1，并且errno为ECHILD。 调用进程和command子进程是并行的，所以标准要求popen不能忽略SIGINT和SIGQUIT信号。如果是从键盘产生的上述信号，那么，调用进程和command子进程都会收到信号。 版权声明：本文为知乎博主「极致Linux内核」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://zhuanlan.zhihu.com/p/548003903","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux进程、线程、调度(一)","path":"/2023/01/15/linux-docs/进程管理/Linux进程、线程、调度(一)/","content":"希望可以通过本小结 彻底地搞清楚进程生命周期，进程生命周期创建、退出、停止，以及僵尸进程的本质； 进程 是处于执行期的程序以及相关的资源的总称，是操作系统资源分配的单位。 123456进程的资源到底包括什么？1.打开的文件2.挂起的信号3.内核的内部数据4.处理器的状态5.内存映射的内存地址空间 等等 Linux系统 对线程和进程并不特别区分。线程仅仅被视为一个与其他线程共享某些资源的进程。每个线程都拥有唯一自己的task_struct。 内核调度的对象是根据task_struct结构体。可以说是线程，而不是进程。 不仅仅要有资源，还需要有进程的描述，例如：pid 进程描述符及task_structlinux通过task_struct结构体描述一个进程。 mm 成员：描述内存资源fs 成员：描述文件系统资源files 成员：进程运行时打开了多少文件，fd的数组signal 成员：进程接收的信号资源 Linux通过slab分配器分配task_struct结构，只需在栈底创建新的结构，struct thread_info。每个任务的thread_info结构在它的内核栈的尾端分配。 pid的数量是有限的 $ cat &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;pid_max32768 task_struct被管理形成链表 –&gt; 形成树 –&gt;形成哈希： pid –&gt; task_struct 根据哈希来进行pid检索 Linux进程生命周期（就绪、运行、睡眠、停止、僵死） 进程、线程、协程在linux系统里，进程和线程都是通过task_struct结构体来描述。进程之间不共享地址空间，而线程与创建它的进程是共享地址空间的。 线程又分为：内核线程、用户级线程和 协程。 对于I&#x2F;O密集型场景，就算开多个线程来处理，也未必能提升CPU的利用率，反而会增加线程切换的开销。 此外，多线程之间如果存在临界区或者共享数据，那么同步的开销也不容忽视。 而协程就是用来解决这个问题的，一个用户线程上可以跑多个协程，以此提升单核的利用率。 tips: Linux中对进程和线程创建的几个系统调用发现， 创建时最终都会调用do_fork()函数，不同之处是传入的参数不同(clone_flags)，最终结果就是进程有独立的地址空间和栈 ，而用户线程 可以自己制定用户栈，地址空间和父进程共享，内核线程则 只有和内核共享的一个栈，同一个地址空间。 不管是进程还是线程，do_fork最终会创建一个task_struct结构。 什么是僵尸进程？僵尸是子进程死了，资源已经释放。所以不可能有内存泄漏等。但是父进程还没有来得及去wait回收它。task_struct 还在，父进程可以查到子进程的死因。 kill -9 僵尸进程，无效。 [a.out]&lt; defunct&gt; Z+ 僵尸进程是一个特别短暂的状态。 停止状态与作业控制，cpulimitLinux在早期使用cpulimit 进行 cpu利用率控制。cpulimit 限制进程 CPU利用率的原理如上，利用进程的停止态。但是不是精确的。 12cpulimit -l 10 -p 12296 把进程12296CPU使用率控制在 10%以内cpulimit -l 40 -p 12296 ctrl+z ，fg&#x2F;bg 进程的睡眠 深睡眠 和 浅睡眠，都是自发的。停止态是被动的。 深：必须等到资源才能wake_up.浅：除了被资源wake_up，还可以被信号唤醒。 睡眠是主动的，暂停是人为的信号控制，属于作业控制。深度睡眠，只能在内核中进入。 睡眠态等到资源后，为什么不能直接进入运行态？进程醒来后，优先级不一定是最高的。醒来后，先就绪。 执行应用程序代码段发生page fault，代码段还没有进内存。接下来，要从硬盘中读到内存，此时，会把进程设置到深度睡眠。为什么？发生两次pagefault，非常难控制。 进程的睡眠实现，依赖内核数据结构wait queue。类似设计模式的，发布者和订阅者。 进程P1,P2,P3,P4 把自己放在等待队列，资源来了只需要唤醒等待队列。等待队列类似订阅消息的中间媒介 初见fork12345678main()&#123;\tfork(); printf(&quot;hello &quot;); fork(); printf(&quot;hello &quot;); while(1);&#125; 内存泄漏的真实含义内存泄漏，不是（进程死了，内存没释放），而是，进程活着，运行越久，耗费内存越多。 如何观察 内存泄漏？ 连续多点观察法。","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux进程、线程、调度(三)","path":"/2023/01/15/linux-docs/进程管理/Linux进程、线程、调度(三)/","content":"本节主要介绍进程的调度器，设计的目标：吞吐和响应，轮流让其他进程获取CPU资源。 进程调度机制的架构操作系统通过中断机制，来周期性地触发调度算法进行进程的切换。 rq: 可运行队列，每个CPU对应一个，包含自旋锁，进程数量，用于公平调度的CFS结构体，当前正在运行的进程描述符。 cfs_rq: cfs调度的运行队列信息，包含红黑色的根节点，正在运行的进程指针，用于负载均衡的叶子队列等。 sched_entity: 调度实体，包含负载权重值，对应红黑树节点，虚拟运行时vruntime等。 sched_class: 调度算法抽象成的调度类，包含一组通用的调度操作接口，将接口和实现分离。 schedule函数的流程包括： 1、关闭内核抢占，标识cpu状态。通知RCU更新状态，关闭本地终端，获取所要保护的运行队列的自旋锁，为查找可运行进程做准备。2、检查prev状态，决定是否将进程插入到运行队列，或者从运行队列中删除。3、task_on_rq_queued(prev) : 将pre进程插入到运行队列的队尾。4、pick_next_task : 选取下一个将要执行的进程。5、context_switch(rq, prev, next) 进行进程上下文切换。 CPU&#x2F;IO消耗型进程吞吐 vs. 响应 响应：最小化某个任务的响应时间，哪怕牺牲其他的任务为代价。 吞吐：全局视野，整个系统的workload被最大化处理。 任何操作系统的调度器设计只追求2个目标：吞吐率大和延迟低。这2个目标有点类似零和游戏，因为吞吐率要大，势必要把更多的时间放在做真实的有用功，而不是把时间浪费在频繁的进程上下文切换；而延迟要低，势必要求优先级高的进程可以随时抢占进来，打断别人，强行插队。但是，抢占会引起上下文切换，上下文切换的时间本身对吞吐率来讲，是一个消耗，这个消耗可以低到2us或者更低（这看起来没什么？），但是上下文切换更大的消耗不是切换本身，而是切换会引起大量的cache miss。你明明weibo跑的很爽，现在切过去微信，那么CPU的cache是不太容易命中微信的。 操作系统中估算”上下文切换”对吞吐能力影响时，不是计算上下文切换本身，而是在CPU 高速cache中的miss。一旦从一个进程切到另一个进程，会造成比较多的cache miss，从而影响吞吐能力。 在内核编译的时候，Kernel Features —&gt; Preemption Model选项实际上可以让我们编译内核的时候，是倾向于支持吞吐，还是支持响应 preemption model：选择内核的抢占模型，影响调度算法1、No Forced Preemption （Server）： 不强制抢占，更在意吞吐，支撑比较大的连接等。2、Voluntary kernel preemption (Desktop)： 内核不能抢占。3、Preemtible Kernel（low-latency desktop）： 内核都可以抢占，更在意响应，滑动触摸屏等操作需要立刻响应。 I&#x2F;O消耗型 vs. CPU消耗型 IO bound： CPU利用率低，进程的运行效率主要受限于I&#x2F;O速度； tips: IO 消耗型对拿到CPU(延迟)比较敏感，应该被优先调度。一般需要CPU的响应速度快，即优先级要求比较高。 CPU bound： 多数时间花在CPU上面（做运算）； 调度算法： 策略 + 优先级早期2.6调度器：优先级数组 和 Bitmaps 0～ 139： 在内核空间， 把整个Linux优先级划分为0～139，数字越小，优先级越高。用户空间设置时，是反过来的。 某个优先级有TASK_RUNNING进程，响应bit设置1。 调度第一个bitmap设置为1的进程 SCHED_FIFO、SCHED_RR实时（RT）进程调度策略： 0～99采用的RT，100～139是非RT的。 SCHED_FIFO： 不同优先级按照优先级高的先跑到睡眠，优先级低的再跑；同等优先级先进先出。 SCHED_RR：不同优先级按照优先级高的先跑到睡眠，优先级低的再跑；同等优先级轮转。 当所有的SCHED_FIFO和SCHED_RR都运行至睡眠态，就开始运行 100～139之间的 普通task_struct。这些进程讲究 nice， SCHED_NORMAL非实时进程的调度和动态优先级： 早期内核2.6的调度器，100对应nice值为 -20，139对应nice值为19。对于普通进程，优先级高不会形成对优先级低的绝对优势，并不会阻塞优先级低的进程拿到时间片。普通进程在不同优先级之间进行轮转，nice值越高，优先级越低。此时优先级的具体作用是： 1、时间片。优先级高的进程可以得到更多时间片。2、抢占。从睡眠状态到醒来，可以优先去抢占优先级低的进程。Linux根据睡眠情况，动态奖励和惩罚。 越睡，优先级越高。想让CPU消耗型进程和IO消耗型进程竞争时，IO消耗型的进程可以竞争过CPU消耗型。 rt的门限Linux内核在period的时间里RT最多只能跑runtime的时间。在参数 &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;sched_rt_period_us 和 &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;sched_rt_runtime_us 中设置。单位：微秒。 CFS 完全公平调度后期，Linux对普通进程调度，提供了 完全公平调度算法，每次都会调vruntime最小的进程调度。 红黑树，左边节点小于右边节点的值，运行到目前为止 （vruntime最小）的进程，同时考虑了CPU&#x2F;IO和nice。 vruntime: virtual runtime，＝ pruntime&#x2F;weight 权重* 系数。 随着时间运行，分子pruntime变大，vruntime也就变大，优先级变低。喜欢睡眠、IO消耗型的进程，分子小。nice值低的，分母大。但是RT的进程，优先级高于所有普通的进程。 红黑树实现的CFS，用分子pruntime来照顾 睡眠情况，用分母来照顾nice值。 当进程里fork了多个线程，每个线程的 调度策略都可以不同，优先级可以不同。原因显然。 工具 chrt 和 renice12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;#include &lt;sys/types.h&gt;void *thread_fun(void *param)&#123; printf(&quot;thread pid:%d, tid:%lu &quot;, getpid(), pthread_self()); while (1) ; return NULL;&#125;int main(void)&#123; pthread_t tid1, tid2; int ret; printf(&quot;main pid:%d, tid:%lu &quot;, getpid(), pthread_self()); ret = pthread_create(&amp;tid1, NULL, thread_fun, NULL); if (ret == -1) &#123; perror(&quot;cannot create new thread&quot;); return 1; &#125; ret = pthread_create(&amp;tid2, NULL, thread_fun, NULL); if (ret == -1) &#123; perror(&quot;cannot create new thread&quot;); return 1; &#125; if (pthread_join(tid1, NULL) != 0) &#123; perror(&quot;call pthread_join function fail&quot;); return 1; &#125; if (pthread_join(tid2, NULL) != 0) &#123; perror(&quot;call pthread_join function fail&quot;); return 1; &#125; return 0;&#125;1.编译two-loops.c, gcc two-loops.c -pthread，运行两份root@whale:~/develop$ gcc two-loops.c -pthreadroot@whale:~/develop$ ./a.out &amp;[1] 13682root@whale:~/develop$ main pid:13682, tid:3075434240thread pid:13682, tid:3067038528thread pid:13682, tid:3075431232root@whale:~/develop$ ./a.out &amp;[2] 13685root@whale:~/develop$main pid:13685, tid:3075925760thread pid:13685, tid:3067530048thread pid:13685, tid:3075922752 ### top命令观察CPU利用率： 13682 root 20 0 18684 616 552 S 98.4 0.0 1:12.09 a.out 13685 root 20 0 18684 644 580 S 98.1 0.0 1:07.32 a.out ### renice其中之一，再观察CPU利用率 sudo renice -n -5 -g 13682 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 13682 root 15 -5 18684 616 552 S 147.4 0.0 4:52.73 a.out 13685 root 20 0 18684 644 580 S 48.6 0.0 4:12.77 a.out killall a.out2.编译two-loops.c, gcc two-loops.c -pthread，运行一份top发现其CPU利用率接近200%-f：把它的所有线程设置为SCHED_FIFO-a : 所有线程 chrt -f -a -p 50 进程PID 再观察它的CPU利用率答：CPU利用率会下降，rt的限制，1s中只能占0.95。此时虽然CPU使用率下降，但是服务器的响应更慢了。因为鼠标等应用的优先级没有a.out 这个进程的优先级高。","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux进程、线程、调度(二)","path":"/2023/01/15/linux-docs/进程管理/Linux进程、线程、调度(二)/","content":"fork 、vfork、clone Linux 内核的调度算法，是根据task_struct结构体来进行调度的。 写时拷贝技术当p1把p2创建出来时，会把task_struct里描述的资源结构体对拷给p2。区分进程的标志，就是 p2的资源不是p1的资源。两个task_struct 的资源都相同，那就不叫两个进程了。 执行一个copy，但是任何修改都造成分裂，如：chroot,open，写memory，最难copy的是 mm 这个部分，因为要做写时拷贝。 Linux通过MMU进行虚拟地址到物理地址的转换，当进程执行fork()后，会把页表中的权限设置为RD-ONLY，当P1,P2去写该页时，CPU会收到page fault，申请新的内存。Linux再将页表中的virt1指向新的物理地址。 1234567891011121314151617181920212223242526#include &lt;sched.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int data = 10;int child_process()&#123; printf(&quot;Child process %d, data %d &quot;,getpid(),data); data = 20; printf(&quot;Child process %d, data %d &quot;,getpid(),data); _exit(0);&#125;int main(int argc, char* argv[])&#123; int pid; pid = fork(); if(pid==0) &#123; child_process(); &#125; else&#123; sleep(1); printf(&quot;Parent process %d, data %d &quot;,getpid(), data); exit(0); &#125;&#125; CoW，严重依赖CPU的MMU。Mmu-less Linux 无copy-on-write, 没有fork，而使用vfork。 使用vfork：父进程p1 vfork出子进程p2之后阻塞，直到子进程发生以下两种情况。1） exit2） exec 进程执行vfork时，P2的task_struct中的*mm 与 P1共享，P1的内存资源就是P2的内存资源。 pthread_create -&gt; clone Linux创建线程的API，本质上去调 clone。要求把P2的所有资源的指针，都指向P1。线程，也被称为 Light weight process。 而Linux在clone线程时也十分灵活，可以选择共享&#x2F;不共享部分资源。 POSIX标准要求，进程里面如果有多个线程，在用户空间 getpid() 看到的都是同一个id，这个id其实是TGID。 一个进程里面创建了多个线程，在&#x2F;proc 下 的是 tgid，&#x2F;proc&#x2F;tgid&#x2F;task&#x2F;{pidx,y,z} pthread_self() 看到的是用户空间pthread线程库里获得的id 。 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;#include &lt;stdio.h&gt;#include &lt;linux/unistd.h&gt;#include &lt;sys/syscall.h&gt;static pid_t gettid( void )&#123; return syscall(__NR_gettid);&#125;static void *thread_fun(void *param)&#123; printf(&quot;thread pid:%d, tid:%d pthread_self:%lu &quot;, getpid(), gettid(),pthread_self()); while(1); return NULL;&#125;int main(void)&#123; pthread_t tid1, tid2; int ret; printf(&quot;thread pid:%d, tid:%d pthread_self:%lu &quot;, getpid(), gettid(),pthread_self()); ret = pthread_create(&amp;tid1, NULL, thread_fun, NULL); if (ret == -1) &#123; perror(&quot;cannot create new thread&quot;); return -1; &#125; ret = pthread_create(&amp;tid2, NULL, thread_fun, NULL); if (ret == -1) &#123; perror(&quot;cannot create new thread&quot;); return -1; &#125; if (pthread_join(tid1, NULL) != 0) &#123; perror(&quot;call pthread_join function fail&quot;); return -1; &#125; if (pthread_join(tid2, NULL) != 0) &#123; perror(&quot;call pthread_join function fail&quot;); return -1; &#125; return 0;&#125; 1gcc thread.c -pthread 总结 fork , vfork, clone由于执行fork()引入了 写时拷贝并且明确了子进程先执行，所以 vfork()的好处就仅限于不拷贝父进程的页表项mm_struct。vfork()系统调用的实现是通过向clone()系统调用传递一个特殊标志来进行。 vfork场景下父进程会先休眠，等唤醒子进程后，再唤醒父进程。 这么做的好处是：由于子进程被创建出来，与父进程共享地址空间，且只读。只有在执行exec的创建新的内存映射时才会拷贝父进程的数据，来创建新的地址空间。如果此时，父进程还在执行，就有可能产生脏数据，或发生死锁。 进程0和进程1init进程是被Linux 0进程创建，0进程把init进程fork出来后，就退化成IDLE进程。这个进程，是特殊调度类，所有进程都停止或睡眠后，就会调度进程0运行，此时处于CPU低功耗状态。 孤儿进程与托孤，subreaper 当父进程退出后，子进程会寻找subreaper 或 init进程。","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux进程、线程、调度(四)","path":"/2023/01/15/linux-docs/进程管理/Linux进程、线程、调度(四)/","content":"延续（三）中，调度器的其他内容：关于多核、分群、硬实时 多核下的负载均衡Linux 每个CPU可能有多个操作线程，每个核均运行的调度算法是 SCHED_FIFO, SCHED_RR，SCHED_NORMAL(CFS)等，每个核都“以劳动为乐”。 tips: 旧的调度算法是通过+&#x2F;- 5 nice值，来照顾IO型，惩罚CPU型。新的进程调度算法CFS，会根据ptime&#x2F;nice值进行红黑树匹配。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;#include &lt;sys/types.h&gt;void *thread_fun(void *param)&#123; printf(&quot;thread pid:%d, tid:%lu &quot;, getpid(), pthread_self()); while (1) ; return NULL;&#125;int main(void)&#123; pthread_t tid1, tid2; int ret; printf(&quot;main pid:%d, tid:%lu &quot;, getpid(), pthread_self()); ret = pthread_create(&amp;tid1, NULL, thread_fun, NULL); if (ret == -1) &#123; perror(&quot;cannot create new thread&quot;); return 1; &#125; ret = pthread_create(&amp;tid2, NULL, thread_fun, NULL); if (ret == -1) &#123; perror(&quot;cannot create new thread&quot;); return 1; &#125; if (pthread_join(tid1, NULL) != 0) &#123; perror(&quot;call pthread_join function fail&quot;); return 1; &#125; if (pthread_join(tid2, NULL) != 0) &#123; perror(&quot;call pthread_join function fail&quot;); return 1; &#125; return 0;&#125;1.编译two-loops.c, gcc two-loops.c -pthread，运行:$ time ./a.out main pid:14958, tid:3075917568thread pid:14958, tid:3067521856thread pid:14958, tid:3075914560^Creal\t1m10.050suser\t2m20.016ssys\t0m0.004s * 我们得到时间分布比例，理解2个死循环被均分到2个core。* (user+sys)/2=real ，原因：两个线程被Linux自动分配到两个核上，但是两个线程可能随机分配到任意CPU上运行。 RT进程（task_struct）： 保证N个优先级最高的RT分布到N个核pull_rt_task()push_rt_task() RT的进程，更多强调的是实时性，因为优先级大于Normal 进程。例如，4核CPU，有8个RT的进程，会优先找其中4个优先级最高的让他们运行到4个核上。 普通进程：周期性负载均衡： 当操作系统的时钟节拍来临，会查这个核是否空闲，旁边一个核是否忙，当旁边的核忙到一定程度，会自动从旁边较忙的CPU核上，pull task过来。IDLE时负载均衡： 当CPU IDLE为0，会从旁边的CPU核上pull task。fork和exec时负载均衡：fork时会创建一个新的task_struct，会把这个task_struct放在最空闲的核上运行。总结：每个核通过push&#x2F;pull task来实现任务的负载均衡。所以，Linux上运行的多线程，可能会“动态”的出现在各个不同的CPU核上。 设置 CPU task affinity程序员通过设置 affinity，即设置某个线程跟哪个CPU更亲和。内核API提供了两个系统调用，让用户可以修改位掩码或查看当前的位掩码。而该位掩码 , 正是对应 进程task_struct数据结构中的cpus_allowed属性，与cpu的每个逻辑核心一一对应。 上图 np代表 none posix，比如电脑有7个核，但线程只想在第1、2个核上运行。就把cpu_set_t设置为 0x6，代表 110。如果线程只想在第2个核上，就设置为 0x4。 还可以通过 taskset工具 设置进程的线程在哪个CPU上跑。 1234567891011121314152. 编译two-loops.c, gcc two-loops.c -pthread，运行一份top发现其CPU利用率接近200%把它的所有线程affinity设置为01, 02, 03后分辨来看看CPU利用率 taskset -a -p 02 进程PID taskset -a -p 01 进程PID taskset -a -p 03 进程PID 前两次设置后，a.out CPU利用率应该接近100%，最后一次接近200%03代表，第1个或第2个CPU核上运行。-a 代表 进程下的所有线程。运行结果：前两次，a.out程序CPU的使用率均为100%，第3次，a.out程序CPU的使用率为200%。 中断负载均衡、RPS软中断负载均衡IRQ affinity 中断也可以负载均衡，Linux每个中断号下面smp_affinity。 分配IRQ到某个CPU[root@boss ~]# echo 01 &gt; &#x2F;proc&#x2F;irq&#x2F;145&#x2F;smp_affinity[root@boss ~]# cat &#x2F;proc&#x2F;irq&#x2F;145&#x2F;smp_affinity00000001 比如上图，有个网卡有4个收发队列，分别是 74，75，76，77。把4个网卡收发队列的中断分别设置为01，02，04，08，那么这个网卡4个队列的中断就被均分到4个核上。 但是有些中断不能被负载均衡。比如，一张网卡只有1个队列，8个核。队列上的中断就发到1个核上。网卡中断里，CPU0收到一个中断irq之后，如果在这个irq中调用了soft irq，那么这个soft irq也会运行在CPU0。因为CPU0上中断中调度的软中断，也是会运行在CPU0上。那么这个核上 ，中断的负载和 软中断的负载 均很重，因为TCP&#x2F;IP协议栈的处理，都丢到软中断中，此时网卡的吞吐率肯定上不来。 此时，出现了RPS补丁，实现 多核间的softIRQ 负载均衡 RPS将包处理负载均衡到多个CPU [root@whale]# echo fffe &gt; &#x2F;sys&#x2F;class&#x2F;net&#x2F;eth1&#x2F;queues&#x2F;rx-0&#x2F;rps_cpusfffe[root@whale]# watch -d “cat &#x2F;proc&#x2F;softirqs |grep NET_RX” 每个网卡的队列下面，均有一个文件 rps_cpus， 如果echo fffe到这个文件，就会让某个核上的软中断，负载均衡到0～15个核上。一般来说，CPU0上收到的中断，软中断都会在CPU0。但是CPU0会把收到中断的软中断派发到其他核上，这样其他核也可以处理TCP&#x2F;IP收到的包处理的工作。 cgroups和CPU资源分群分配比如，有两个用户在OS上执行编译程序，用户A创建1000个线程，用户B创建32个线程。如果这1010个线程nice值均为0，那么按照CFS的调度算法，用户A可以拿到1000&#x2F;1032的cpu时间片，用户B只能拿到 32&#x2F;1032的cpu时间片。 此时，Linux通过分层调度，把某些task_struct 加到cgroup A，另外的task_struct加到cgroup B , cgroup A和B 先按照某个权重进行CPU的分配，再到不同cgroup里面，按照调度算法进行调度。 定义不同cgroup CPU分享的share –&gt; cpu.shares 定义某个cgroup在某个周期里面最多跑多久 –&gt; cpu.cfs_quota_us 和 cpu.cfs_period_us cpu.cfs_period_us：默认100000 us&#x3D; 100mscpu.cfs_quota_us： demo 123456789101112131415161718192021222324252627282930313233343536373839403.编译two-loops.c, gcc two-loops.c -pthread，运行三份用top观察CPU利用率，大概各自66%。创建A,B两个cgroup root@whale:/sys/fs/cgroup/cpu$ sudo mkdir A root@whale:/sys/fs/cgroup/cpu$ sudo mkdir B把3个a.out中的2个加到A，1个加到B。此时，发现两个cgroup下的cpu.shares 相同，均为1024.然后把两个进程下的所有线程都加入到cgroupA，如果只想加某个线程，则echo pid到tasks文件。 root@whale:/sys/fs/cgroup/cpu/A$ sudo sh -c &#x27;echo 14995 &gt; cgroup.procs&#x27; root@whale:/sys/fs/cgroup/cpu/A$ sudo sh -c &#x27;echo 14998 &gt; cgroup.procs&#x27; root@whale:/sys/fs/cgroup/cpu/A$ cd .. root@whale:/sys/fs/cgroup/cpu$ cd B/ root@whale:/sys/fs/cgroup/cpu/B$ sudo sh -c &#x27;echo 15001 &gt; cgroup.procs&#x27;这次发现3个a.out的CPU利用率大概是50%, 50%, 100%。杀掉第2个和第3个a.out，然后调整cgroup A的quota，观察14995的CPU利用率变化 root@whale:/sys/fs/cgroup/cpu/B$ kill 14998 root@whale:/sys/fs/cgroup/cpu/B$ kill 15001设置A group的quota为20ms： root@whale:/sys/fs/cgroup/cpu/A$ sudo sh -c &#x27;echo 20000 &gt; cpu.cfs_quota_us&#x27; 设置A group的quota为40ms： root@whale:/sys/fs/cgroup/cpu/A$ sudo sh -c &#x27;echo 40000 &gt; cpu.cfs_quota_us&#x27; 以上各自情况，用top观察a.out CPU利用率。 当设置为 20000 us = 20ms , CPU利用率立即变为20%； 当设置为 40000 us = 40ms ,CPU利用率立即变为40% 当设置为 120000 us 时， CPU利用率立即变为120%。quota可以大于period，因为此时 CPU为多核，100ms里面可以运行200ms，该值最大为CPU核数* period。 Android和Docker对cgroup的采用 apps, bg_non_interactive安卓把应用分为app group ，和 bg_non_interactive 背景非交互的group，并且bg_non_interactive的group权重非常低，这样做的好处是，让桌面运行的程序可以更大程度的抢到CPU。 123456789Shares：apps: cpu.shares = 1024bg_non_interactive: cpu.shares = 52Quota:apps:cpu.rt_period_us: 1000000 cpu.rt_runtime_us: 800000bg_non_interactive: cpu.rt_period_us: 1000000 cpu.rt_runtime_us: 700000 docker run时也可以指定 –cpu-quota 、–cpu-period、 –cpu-shares参数 Linux 通过Cgroup控制多个容器在运行时，如何分享CPU。这些会在之后的Cgroups详解中详细阐述。 比如说A容器配置的–cpu-period&#x3D;100000 –cpu-quota&#x3D;50000，那么A容器就可以最多使用50%个CPU资源，如果配置的–cpu-quota&#x3D;200000，那就可以使用200%个CPU资源。所有对采集到的CPU used的绝对值没有意义，还需要参考上限。还是这个例子–cpu-period&#x3D;100000 –cpu-quota&#x3D;50000，如果容器试图在0.1秒内使用超过0.05秒，则throttled就会触发，所有throttled的count和time是衡量CPU是否达到瓶颈的最直观指标。 Linux为什么不是硬实时的如何理解硬实时？并不代表越快越好，硬实时最主要的意思是：可预期。 如上图，在一个硬实时操作系统中，当唤醒一个高优先级的RT任务，从“你唤醒它”到“它可以被调度”的这段时间，是不会超过截止期限的。 12344.cyclictest -p 99 -t1 -n观察min, max, act, avg时间，分析hard realtime问题加到系统负载，运行一些硬盘访问，狂收发包的程序，观察cyclictest的max变化延迟具有不确定性，最大值可能随着load改变而改变。 Kernel 越发支持抢占 Linux为什么不是硬实时？ Linux运行时，CPU时间主要花在“四类区间”上，包括中断、软中断、进程上下文（spin_lock），进程上下文（可调度）。其中 进程上下文陷入内核拿到spin_lock，此时拿到spin_lock的CPU核上的调度器就会被关掉，该核就无法进行调度。 程序运行在只有 进程上下文（可调度）这个区间可以调度，其他区间都不能被调度。 中断是指 进程收到硬件的中断信号，就算在中断里唤醒一个高优先级的RT，也无法调度。软中断和中断的唯一区别是，软中断中可以再中断，但中断中不能再中断。Linux 2.6.32之后完全不允许中断嵌套。软中断里唤醒一个高优先级的RT，也无法调度。 如下图，一个绿色的普通进程在T1时刻持有spin_lock进入一个critical section（该核调度被关），绿色进程T2时刻被中断打断，而后T3时刻IRQ1里面唤醒了红色的RT进程（如果是硬实时RTOS，这个时候RT进程应该能抢入），之后IRQ1后又执行了IRQ2，到T4时刻IRQ1和IRQ2都结束了，红色RT进程仍然不能执行（因为绿色进程还在spin_lock里面），直到T5时刻，普通进程释放spin_lock后，红色RT进程才抢入。从T3到T5要多久，鬼都不知道，这样就无法满足硬实时系统的“可预期”延迟性，因此Linux不是硬实时操作系统。 preempt-rt对Linux实时性的改造 Linux的preempt-rt补丁试图把中断、软中断线程化，变成可以被抢占的区间，而把会关本核调度器的spin_lock替换为可以调度的mutex，它实现了在T3时刻唤醒RT进程的时刻，RT进程可以立即抢占调度进入的目标，避免了T3-T5之间延迟的非确定性。","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux进程状态总结","path":"/2023/01/15/linux-docs/进程管理/Linux进程状态总结/","content":"进程生命周期在Linux内核里，无论是进程还是线程，统一使用 task_struct{} 结构体来表示，也就是统一抽象为任务（task）。task_struct{} 定义在 include&#x2F;linux&#x2F;sched.h 文件中，十分复杂，这里简单了解下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// include/linux/sched.h// ... 省略struct task_struct &#123;#ifdef CONFIG_THREAD_INFO_IN_TASK\t/* * For reasons of header soup (see current_thread_info()), this * must be the first element of task_struct. */\tstruct thread_info thread_info;#endif\t/* -1 unrunnable, 0 runnable, &gt;0 stopped: */\tvolatile long state;\tint exit_state;\tint exit_code;\tint exit_signal;\t/* * This begins the randomizable portion of task_struct. Only * scheduling-critical items should be added above here. */\trandomized_struct_fields_start\tvoid *stack;\trefcount_t usage;\t/* Per task flags (PF_*), defined further below: */\tunsigned int flags;\tunsigned int ptrace;#ifdef CONFIG_SMP\tint on_cpu;\tstruct __call_single_node\twake_entry;#ifdef CONFIG_THREAD_INFO_IN_TASK\t/* Current CPU: */\tunsigned int cpu;#endif\tunsigned int wakee_flips;\tunsigned long wakee_flip_decay_ts;\tstruct task_struct *last_wakee; // ...省略 struct sched_info sched_info;\tstruct list_head tasks; // 链表，将所有task_struct串起来\tpid_t pid; // process id，指的是线程id\tpid_t tgid; // thread group ID，指的是进程的主线程id struct task_struct *group_leader; // 指向的是进程的主线程\t/* Signal handlers: */\tstruct signal_struct *signal;\tstruct sighand_struct __rcu *sighand;\tsigset_t blocked;\tsigset_t real_blocked;\t/* Restored if set_restore_sigmask() was used: */\tsigset_t saved_sigmask;\tstruct sigpending pending;\tunsigned long sas_ss_sp;\tsize_t sas_ss_size;\tunsigned int sas_ss_flags; // ... 省略&#125; 查阅相关资料后，对Linux中进程的生命周期总结如下： 从图上可以看出，进程的睡眠状态是最多的，那进程一般在何时进入睡眠状态呢？答案是I&#x2F;O操作时，因为I&#x2F;O操作的速度与CPU运行速度相比，相差太大，所以此时进程会释放CPU，进入睡眠状态。 进程状态相关的定义同样在 include&#x2F;linux&#x2F;sched.h 文件的开头部分，以 #define TASK_KILLABLE (TASK_WAKEKILL | TASK_UNINTERRUPTIBLE) 为例，TASK_WAKEKILL表示用于在接收到致命信号时唤醒进程，将它与 TASK_UNINTERRUPTIBLE 按位或，就得到了 TASK_KILLABLE。代码注释中提到了 fs&#x2F;proc&#x2F;array.c，所以也将其代码贴出，作为补充。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// include/linux/sched.h// ... 省略/* * Task state bitmask. NOTE! These bits are also * encoded in fs/proc/array.c: get_task_state(). * * We have two separate sets of flags: task-&gt;state * is about runnability, while task-&gt;exit_state are * about the task exiting. Confusing, but this way * modifying one set can&#x27;t modify the other one by * mistake. *//* Used in tsk-&gt;state: */#define TASK_RUNNING 0x0000#define TASK_INTERRUPTIBLE 0x0001#define TASK_UNINTERRUPTIBLE 0x0002#define __TASK_STOPPED 0x0004#define __TASK_TRACED 0x0008/* Used in tsk-&gt;exit_state: */#define EXIT_DEAD 0x0010#define EXIT_ZOMBIE 0x0020#define EXIT_TRACE (EXIT_ZOMBIE | EXIT_DEAD)/* Used in tsk-&gt;state again: */#define TASK_PARKED 0x0040#define TASK_DEAD 0x0080#define TASK_WAKEKILL 0x0100#define TASK_WAKING 0x0200#define TASK_NOLOAD 0x0400#define TASK_NEW 0x0800#define TASK_STATE_MAX 0x1000/* Convenience macros for the sake of set_current_state: */#define TASK_KILLABLE (TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)#define TASK_STOPPED (TASK_WAKEKILL | __TASK_STOPPED)#define TASK_TRACED (TASK_WAKEKILL | __TASK_TRACED)#define TASK_IDLE (TASK_UNINTERRUPTIBLE | TASK_NOLOAD)/* Convenience macros for the sake of wake_up(): */#define TASK_NORMAL (TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)/* get_task_state(): */#define TASK_REPORT (TASK_RUNNING | TASK_INTERRUPTIBLE | \\ TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \\ __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \\ TASK_PARKED)#define task_is_traced(task) ((task-&gt;state &amp; __TASK_TRACED) != 0)#define task_is_stopped(task) ((task-&gt;state &amp; __TASK_STOPPED) != 0)#define task_is_stopped_or_traced(task)\t((task-&gt;state &amp; (__TASK_STOPPED | __TASK_TRACED)) != 0)// ... 省略 1234567891011121314151617181920212223242526272829303132// fs/proc/array.c// ... 省略/* * The task state array is a strange &quot;bitmap&quot; of * reasons to sleep. Thus &quot;running&quot; is zero, and * you can test for combinations of others with * simple bit tests. */static const char * const task_state_array[] = &#123;\t/* states in TASK_REPORT: */\t&quot;R (running)&quot;, /* 0x00 */\t&quot;S (sleeping)&quot;, /* 0x01 */\t&quot;D (disk sleep)&quot;,\t/* 0x02 */\t&quot;T (stopped)&quot;, /* 0x04 */\t&quot;t (tracing stop)&quot;,\t/* 0x08 */\t&quot;X (dead)&quot;, /* 0x10 */\t&quot;Z (zombie)&quot;, /* 0x20 */\t&quot;P (parked)&quot;, /* 0x40 */\t/* states beyond TASK_REPORT: */\t&quot;I (idle)&quot;, /* 0x80 */&#125;;static inline const char *get_task_state(struct task_struct *tsk)&#123;\tBUILD_BUG_ON(1 + ilog2(TASK_REPORT_MAX) != ARRAY_SIZE(task_state_array));\treturn task_state_array[task_state_index(tsk)];&#125;// ... 省略 在单核的CPU上，同一时刻只有一个task会被调度，所以即使看到了 R 状态，也不代表进程就被分配到了CPU时间片。但了解了进程状态之后，我们再通过 top, ps aux 等命令查看进程，分析问题起来效率就更高了: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647top - 17:24:07 up 10:20, 1 user, load average: 0.15, 0.08, 0.02Tasks: 216 total, 1 running, 215 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.4 us, 0.3 sy, 0.0 ni, 99.2 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 5945.2 total, 2655.4 free, 1580.8 used, 1709.1 buff/cacheMiB Swap: 2048.0 total, 2048.0 free, 0.0 used. 4084.6 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1914 demonlee 20 0 5252100 395940 132816 S 1.0 6.5 1:33.09 gnome-shell 824 root 20 0 2495652 89512 48936 S 0.7 1.5 0:05.59 dockerd 1687 demonlee 20 0 1153156 81436 50128 S 0.3 1.3 0:15.90 Xorg 1957 demonlee 20 0 206556 28348 18504 S 0.3 0.5 0:00.26 ibus-x11 2897 demonlee 20 0 874684 61296 44532 S 0.3 1.0 0:06.22 gnome-terminal- 19984 demonlee 20 0 20632 4036 3376 R 0.3 0.1 0:00.02 top 1 root 20 0 169076 12952 8288 S 0.0 0.2 0:04.61 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd 3 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 rcu_gp 4 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 rcu_par_gp 6 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/0:0H-kblockd 9 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 mm_percpu_wq 10 root 20 0 0 0 0 S 0.0 0.0 0:00.08 ksoftirqd/0 11 root 20 0 0 0 0 I 0.0 0.0 0:03.60 rcu_sched 12 root rt 0 0 0 0 S 0.0 0.0 0:00.51 migration/0 13 root -51 0 0 0 0 S 0.0 0.0 0:00.00 idle_inject/0 14 root 20 0 0 0 0 S 0.0 0.0 0:00.00 cpuhp/0 15 root 20 0 0 0 0 S 0.0 0.0 0:00.00 cpuhp/1 16 root -51 0 0 0 0 S 0.0 0.0 0:00.00 idle_inject/1 17 root rt 0 0 0 0 S 0.0 0.0 0:00.89 migration/1 18 root 20 0 0 0 0 S 0.0 0.0 0:00.09 ksoftirqd/1 20 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/1:0H 21 root 20 0 0 0 0 S 0.0 0.0 0:00.00 cpuhp/2 22 root -51 0 0 0 0 S 0.0 0.0 0:00.00 idle_inject/2 23 root rt 0 0 0 0 S 0.0 0.0 0:00.83 migration/2 24 root 20 0 0 0 0 S 0.0 0.0 0:00.07 ksoftirqd/2 26 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/2:0H-kblockd 27 root 20 0 0 0 0 S 0.0 0.0 0:00.00 cpuhp/3 28 root -51 0 0 0 0 S 0.0 0.0 0:00.00 idle_inject/3 29 root rt 0 0 0 0 S 0.0 0.0 0:00.77 migration/3 30 root 20 0 0 0 0 S 0.0 0.0 0:00.18 ksoftirqd/3 32 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/3:0H-kblockd 33 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kdevtmpfs 34 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 netns 35 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_tasks_kthre 36 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_tasks_rude_ 37 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_tasks_trace 38 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kauditd 39 root 20 0 0 0 0 S 0.0 0.0 0:00.03 khungtaskd demonlee@demonlee-ubuntu:~$ 最后，再补充一个知识点：使用ps命令查看进程时，会发现状态上面有其他符号，比如 S+、Z+ 等，如下所示， 123456789demonlee@demonlee-ubuntu:~$ demonlee 1704 0.0 0.6 557904 37256 ? Sl 05:01 0:00 /usr/libexec/goa-daemondemonlee 1707 0.0 0.1 172652 6936 tty2 Ssl+ 05:01 0:00 /usr/lib/gdm3/gdm-x-session --run-script env GNOME_SHELL_SESSION_MODE=ubuntu /usr/bin/gdemonlee 1714 0.0 0.1 323388 9068 ? Sl 05:01 0:00 /usr/libexec/goa-identity-servicedemonlee 1720 0.1 1.3 1151880 80576 tty2 Sl+ 05:01 1:05 /usr/lib/xorg/Xorg vt2 -displayfd 3 -auth /run/user/1000/gdm/Xauthority -background nondemonlee 1723 0.0 0.1 325356 9016 ? Ssl 05:01 0:02 /usr/libexec/gvfs-afc-volume-monitordemonlee 1728 0.0 0.1 244336 6532 ? Ssl 05:01 0:00 /usr/libexec/gvfs-mtp-volume-monitordemonlee 1759 0.0 0.2 197052 14276 tty2 Sl+ 05:01 0:00 /usr/libexec/gnome-session-binary --systemd --systemd --session=ubuntu... 这个 + 是啥意思呢，其实 man ps 中就有描述，只是我们从来都没认真看说明文档： 12345678910111213141516171819202122PROCESS STATE CODES Here are the different values that the s, stat and state output specifiers (header &quot;STAT&quot; or &quot;S&quot;) will display to describe the state of a process: D uninterruptible sleep (usually IO) I Idle kernel thread R running or runnable (on run queue) S interruptible sleep (waiting for an event to complete) T stopped by job control signal t stopped by debugger during the tracing W paging (not valid since the 2.6.xx kernel) X dead (should never be seen) Z defunct (&quot;zombie&quot;) process, terminated but not reaped by its parent For BSD formats and when the stat keyword is used, additional characters may be displayed: &lt; high-priority (not nice to other users) N low-priority (nice to other users) L has pages locked into memory (for real-time and custom IO) s is a session leader l is multi-threaded (using CLONE_THREAD, like NPTL pthreads do) + is in the foreground process group","tags":["Linux开发"],"categories":["linux"]},{"title":"一文带你，彻底了解，零拷贝Zero-Copy技术","path":"/2023/01/15/linux-docs/进程管理/一文带你，彻底了解，零拷贝Zero-Copy技术/","content":"1、数据拷贝基础过程在Linux系统内部缓存和内存容量都是有限的，更多的数据都是存储在磁盘中。对于Web服务器来说，经常需要从磁盘中读取数据到内存，然后再通过网卡传输给用户： 1.1 仅CPU方式 当应用程序需要读取磁盘数据时，调用read()从用户态陷入内核态，read()这个系统调用最终由CPU来完成； CPU向磁盘发起I&#x2F;O请求，磁盘收到之后开始准备数据； 磁盘将数据放到磁盘缓冲区之后，向CPU发起I&#x2F;O中断，报告CPU数据已经Ready了； CPU收到磁盘控制器的I&#x2F;O中断之后，开始拷贝数据，完成之后read()返回，再从内核态切换到用户态； 1.2 CPU&amp;DMA方式CPU的时间宝贵，让它做杂活就是浪费资源。 直接内存访问（Direct Memory Access），是一种硬件设备绕开CPU独立直接访问内存的机制。所以DMA在一定程度上解放了CPU，把之前CPU的杂活让硬件直接自己做了，提高了CPU效率。 目前支持DMA的硬件包括：网卡、声卡、显卡、磁盘控制器等。 有了DMA的参与之后的流程发生了一些变化： 最主要的变化是，CPU不再和磁盘直接交互，而是DMA和磁盘交互并且将数据从磁盘缓冲区拷贝到内核缓冲区，之后的过程类似。 无论从仅CPU方式和DMA&amp;CPU方式，都存在多次冗余数据拷贝和内核态&amp;用户态的切换。 继续思考Web服务器读取本地磁盘文件数据再通过网络传输给用户的详细过程。 2、普通模式数据交互一次完成的数据交互包括几个部分：系统调用syscall、CPU、DMA、网卡、磁盘等。 系统调用syscall是应用程序和内核交互的桥梁，每次进行调用&#x2F;返回就会产生两次切换： 调用syscall 从用户态切换到内核态 syscall返回 从内核态切换到用户态 来看下完整的数据拷贝过程简图： 读数据过程： 应用程序要读取磁盘数据，调用read()函数从而实现用户态切换内核态，这是第1次状态切换； DMA控制器将数据从磁盘拷贝到内核缓冲区，这是第1次DMA拷贝； CPU将数据从内核缓冲区复制到用户缓冲区，这是第1次CPU拷贝； CPU完成拷贝之后，read()函数返回实现用户态切换用户态，这是第2次状态切换； 写数据过程： 应用程序要向网卡写数据，调用write()函数实现用户态切换内核态，这是第1次切换； CPU将用户缓冲区数据拷贝到内核缓冲区，这是第1次CPU拷贝； DMA控制器将数据从内核缓冲区复制到socket缓冲区，这是第1次DMA拷贝； 完成拷贝之后，write()函数返回实现内核态切换用户态，这是第2次切换； 综上所述： 读过程涉及2次空间切换、1次DMA拷贝、1次CPU拷贝； 写过程涉及2次空间切换、1次DMA拷贝、1次CPU拷贝； 可见传统模式下，涉及多次空间切换和数据冗余拷贝，效率并不高，接下来就该零拷贝技术出场了。 3、零拷贝技术3.1 出现原因可以看到，如果应用程序不对数据做修改，从内核缓冲区到用户缓冲区，再从用户缓冲区到内核缓冲区。两次数据拷贝都需要CPU的参与，并且涉及用户态与内核态的多次切换，加重了CPU负担。需要降低冗余数据拷贝、解放CPU，这也就是零拷贝Zero-Copy技术。 3.2 解决思路目前来看，零拷贝技术的几个实现手段包括：mmap+write、sendfile、sendfile+DMA收集、splice等。 3.2.1 mmap方式mmap是Linux提供的一种内存映射文件的机制，它实现了将内核中读缓冲区地址与用户空间缓冲区地址进行映射，从而实现内核缓冲区与用户缓冲区的共享。 这样就减少了一次用户态和内核态的CPU拷贝，但是在内核空间内仍然有一次CPU拷贝。 mmap对大文件传输有一定优势，但是小文件可能出现碎片，并且在多个进程同时操作文件时可能产生引发coredump的signal。 3.2.2 sendfile方式mmap+write方式有一定改进，但是由系统调用引起的状态切换并没有减少。 sendfile系统调用是在 Linux 内核2.1版本中被引入，它建立了两个文件之间的传输通道。 sendfile方式只使用一个函数就可以完成之前的read+write 和 mmap+write的功能，这样就少了2次状态切换，由于数据不经过用户缓冲区，因此该数据无法被修改。 从图中可以看到，应用程序只需要调用sendfile函数即可完成，只有2次状态切换、1次CPU拷贝、2次DMA拷贝。 但是sendfile在内核缓冲区和socket缓冲区仍然存在一次CPU拷贝，或许这个还可以优化。 3.2.3 sendfile+DMA收集Linux 2.4 内核对 sendfile 系统调用进行优化，但是需要硬件DMA控制器的配合。 升级后的sendfile将内核空间缓冲区中对应的数据描述信息（文件描述符、地址偏移量等信息）记录到socket缓冲区中。 DMA控制器根据socket缓冲区中的地址和偏移量将数据从内核缓冲区拷贝到网卡中，从而省去了内核空间中仅剩1次CPU拷贝。 这种方式有2次状态切换、0次CPU拷贝、2次DMA拷贝，但是仍然无法对数据进行修改，并且需要硬件层面DMA的支持，并且sendfile只能将文件数据拷贝到socket描述符上，有一定的局限性。 3.2.4 splice方式splice系统调用是Linux 在 2.6 版本引入的，其不需要硬件支持，并且不再限定于socket上，实现两个普通文件之间的数据零拷贝。 splice 系统调用可以在内核缓冲区和socket缓冲区之间建立管道来传输数据，避免了两者之间的 CPU 拷贝操作。 splice也有一些局限，它的两个文件描述符参数中有一个必须是管道设备。","tags":["Linux开发"],"categories":["linux"]},{"title":"任务调度","path":"/2023/01/15/linux-docs/进程管理/任务调度/","content":"一. 前言 在前文中，我们分析了内核中进程和线程的统一结构体task_struct，并分析进程、线程的创建和派生的过程。在本文中，我们会对任务间调度进行详细剖析，了解其原理和整个执行过程。由此，进程、线程部分的大体框架就算是介绍完了。本节主要分为三个部分：Linux内核中常见的调度策略，调度的基本结构体以及调度发生的整个流程。下面将详细展开说明。 二. 调度策略 Linux 作为一个多任务操作系统，将每个 CPU 的时间划分为很短的时间片，再通过调度器轮流分配给各个任务使用，因此造成多任务同时运行的错觉。为了维护 CPU 时间，Linux 通过事先定义的节拍率（内核中表示为 HZ），触发时间中断，并使用全局变量 Jiffies 记录了开机以来的节拍数。每发生一次时间中断，Jiffies 的值就加 1。节拍率 HZ 是内核的可配选项，可以设置为 100、250、1000 等。不同的系统可能设置不同数值，以通过查询 &#x2F;boot&#x2F;config 内核选项来查看它的配置值。 Linux的调度策略主要分为实时任务和普通任务。实时任务需求尽快返回结果，而普通任务则没有较高的要求。在前文中我们提到了task_struct中调度策略相应的变量为policy，调度优先级有prio, static_prio, normal_prio, rt_priority几个。优先级其实就是一个数值，对于实时进程，优先级的范围是 0～99；对于普通进程，优先级的范围是 100～139。数值越小，优先级越高。 2.1 实时调度策略 实时调度策略主要包括以下几种 SCHED_FIFO：先来先出型策略，顾名思义相同优先级的情况下先到先得 SCHED_RR：轮询策略，注重公平性，相同优先级的任务会使用相同的时间片轮流执行 SCHED_DEADLINE：根据任务结束时间来进行调度，即将结束的拥有较高的优先级 2.2 普通调度策略 普通调度策略主要包括以下几种 SCHED_NORMAL：普通任务 SCHED_BATCH：后台任务，优先级较低 SCHED_IDLE：空闲时间才会跑的任务 CFS：完全公平调度策略，较为特殊的一种策略。CFS 会为每一个任务安排一个虚拟运行时间 vruntime。如果一个任务在运行，随着一个个 CPU时钟tick 的到来，任务的 vruntime 将不断增大，而没有得到执行的任务的 vruntime 不变。由此，当调度的时候，vruntime较小的就拥有较高的优先级。 vruntime的实际计算方式和权重相关，由此保证了优先级高的按比例拥有更多的执行时间，从而达到完全公平。 三. 调度相关的结构体 首先，我们需要一个结构体去执行调度策略，即sched_class。该类有几种实现 stop_sched_class 优先级最高的任务会使用这种策略，会中断所有其他线程，且不会被其他任务打断； dl_sched_class 就对应上面的 deadline 调度策略； rt_sched_class 就对应 RR 算法或者 FIFO 算法的调度策略，具体调度策略由进程的 task_struct-&gt;policy 指定； fair_sched_class 就是普通进程的调度策略； idle_sched_class 就是空闲进程的调度策略。 其次，我们需要一个调度结构体来集合调度信息，用于调度，即sched_entity，主要有 struct sched_entity se：普通任务调度实体 struct sched_rt_entity rt：实时调度实体 struct sched_dl_entity dl：DEADLINE调度实体 普通任务调度实体源码如下，这里面包含了 vruntime 和权重 load_weight，以及对于运行时间的统计。 12345678910111213141516171819202122232425262728293031struct sched_entity &#123; /* For load-balancing: */ struct load_weight load; unsigned long runnable_weight; struct rb_node run_node; struct list_head group_node; unsigned int on_rq; u64 exec_start; u64 sum_exec_runtime; u64 vruntime; u64 prev_sum_exec_runtime; u64 nr_migrations; struct sched_statistics statistics;#ifdef CONFIG_FAIR_GROUP_SCHED int depth; struct sched_entity *parent; /* rq on which this entity is (to be) queued: */ struct cfs_rq *cfs_rq; /* rq &quot;owned&quot; by this entity/group: */ struct cfs_rq *my_q;#endif#ifdef CONFIG_SMP /* * Per entity load average tracking. * * Put into separate cache line so it does not * collide with read-mostly values above. */ struct sched_avg avg;#endif&#125;; 在调度时，多个任务调度实体会首先区分是实时任务还是普通任务，然后通过以时间为顺序的红黑树结构组合起来，vruntime 最小的在树的左侧，vruntime最多的在树的右侧。以CFS策略为例，则会选择红黑树最左边的叶子节点作为下一个将获得 CPU 的任务。而这颗红黑树，我们称之为运行时队列（run queue），即struct rq。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/* * This is the main, per-CPU runqueue data structure. * * Locking rule: those places that want to lock multiple runqueues * (such as the load balancing or the thread migration code), lock * acquire operations must be ordered by ascending &amp;runqueue. */struct rq &#123; /* runqueue lock: */ raw_spinlock_t lock; /* * nr_running and cpu_load should be in the same cacheline because * remote CPUs use both these fields when doing load calculation. */ unsigned int nr_running;...... #define CPU_LOAD_IDX_MAX 5 unsigned long cpu_load[CPU_LOAD_IDX_MAX];...... /* capture load from *all* tasks on this CPU: */ struct load_weight\tload; unsigned long nr_load_updates; u64 nr_switches; struct cfs_rq cfs; struct rt_rq rt; struct dl_rq dl;...... /* * This is part of a global counter where only the total sum * over all CPUs matters. A task can increase this counter on * one CPU and if it got migrated afterwards it may decrease * it on another CPU. Always updated under the runqueue lock: */ unsigned long nr_uninterruptible; struct task_struct\t*curr; struct task_struct\t*idle; struct task_struct\t*stop; unsigned long next_balance; struct mm_struct\t*prev_mm; unsigned int clock_update_flags; u64 clock; /* Ensure that all clocks are in the same cache line */ u64 clock_task ____cacheline_aligned; u64 clock_pelt; unsigned long lost_idle_time; atomic_t nr_iowait;...... /* calc_load related fields */ unsigned long calc_load_update; long calc_load_active;......&#125;; 其中包含结构体cfs_rq，其定义如下，主要是CFS调度相关的结构体，主要有权值相关变量、vruntime相关变量以及红黑树指针，其中结构体rb_root_cached即为红黑树的节点 12345678910111213141516171819202122/* CFS-related fields in a runqueue */struct cfs_rq &#123; struct load_weight\tload; unsigned long runnable_weight; unsigned int nr_running; unsigned int h_nr_running; u64 exec_clock; u64 min_vruntime;#ifndef CONFIG_64BIT u64 min_vruntime_copy;#endif struct rb_root_cached\ttasks_timeline; /* * &#x27;curr&#x27; points to currently running entity on this cfs_rq. * It is set to NULL otherwise (i.e when none are currently running). */ struct sched_entity\t*curr; struct sched_entity\t*next; struct sched_entity\t*last; struct sched_entity\t*skip;......&#125;; 对结构体dl_rq有类似的定义，运行队列由红黑树结构体构成，并按照deadline策略进行管理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/* Deadline class&#x27; related fields in a runqueue */struct dl_rq &#123; /* runqueue is an rbtree, ordered by deadline */ struct rb_root_cached\troot; unsigned long dl_nr_running;#ifdef CONFIG_SMP /* * Deadline values of the currently executing and the * earliest ready task on this rq. Caching these facilitates * the decision whether or not a ready but not running task * should migrate somewhere else. */ struct &#123; u64 curr; u64 next; &#125; earliest_dl; unsigned long dl_nr_migratory; int overloaded; /* * Tasks on this rq that can be pushed away. They are kept in * an rb-tree, ordered by tasks&#x27; deadlines, with caching * of the leftmost (earliest deadline) element. */ struct rb_root_cached\tpushable_dl_tasks_root;#else struct dl_bw dl_bw;#endif /* * &quot;Active utilization&quot; for this runqueue: increased when a * task wakes up (becomes TASK_RUNNING) and decreased when a * task blocks */ u64 running_bw; /* * Utilization of the tasks &quot;assigned&quot; to this runqueue (including * the tasks that are in runqueue and the tasks that executed on this * CPU and blocked). Increased when a task moves to this runqueue, and * decreased when the task moves away (migrates, changes scheduling * policy, or terminates). * This is needed to compute the &quot;inactive utilization&quot; for the * runqueue (inactive utilization = this_bw - running_bw). */ u64 this_bw; u64 extra_bw; /* * Inverse of the fraction of CPU utilization that can be reclaimed * by the GRUB algorithm. */ u64 bw_ratio;&#125;; 对于实施队列相应的rt_rq则有所不同，并没有用红黑树实现。 12345678910111213141516171819202122232425262728293031/* Real-Time classes&#x27; related field in a runqueue: */struct rt_rq &#123; struct rt_prio_array\tactive; unsigned int rt_nr_running; unsigned int rr_nr_running;#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED struct &#123; int curr; /* highest queued rt task prio */#ifdef CONFIG_SMP int next; /* next highest */#endif &#125; highest_prio;#endif#ifdef CONFIG_SMP unsigned long rt_nr_migratory; unsigned long rt_nr_total; int overloaded; struct plist_head\tpushable_tasks;#endif /* CONFIG_SMP */ int rt_queued; int rt_throttled; u64 rt_time; u64 rt_runtime; /* Nests inside the rq lock: */ raw_spinlock_t rt_runtime_lock;#ifdef CONFIG_RT_GROUP_SCHED unsigned long rt_nr_boosted; struct rq *rq; struct task_group\t*tg;#endif&#125;; 下面再看看调度类sched_class，该类以函数指针的形式定义了诸多队列操作，如 enqueue_task 向就绪队列中添加一个任务，当某个任务进入可运行状态时，调用这个函数； dequeue_task 将一个任务从就绪队列中删除； yield_task将主动放弃CPU； yield_to_task主动放弃CPU并执行指定的task_struct； check_preempt_curr检查当前任务是否可被强占； pick_next_task 选择接下来要运行的任务； put_prev_task 用另一个进程代替当前运行的任务； set_curr_task 用于修改调度策略； task_tick 每次周期性时钟到的时候，这个函数被调用，可能触发调度。 task_dead:进程结束时调用 switched_from、switched_to:进程改变调度器时使用 prio_changed:改变进程优先级 12345678910111213141516171819202122232425262728293031323334353637383940struct sched_class &#123; const struct sched_class *next; void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags); void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags); void (*yield_task) (struct rq *rq); bool (*yield_to_task)(struct rq *rq, struct task_struct *p, bool preempt); void (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags); /* * It is the responsibility of the pick_next_task() method that will * return the next task to call put_prev_task() on the @prev task or * something equivalent. * * May return RETRY_TASK when it finds a higher prio class has runnable * tasks. */ struct task_struct * (*pick_next_task)(struct rq *rq, struct task_struct *prev, * struct rq_flags *rf); void (*put_prev_task)(struct rq *rq, struct task_struct *p);...... void (*set_curr_task)(struct rq *rq); void (*task_tick)(struct rq *rq, struct task_struct *p, int queued); void (*task_fork)(struct task_struct *p); void (*task_dead)(struct task_struct *p); /* * The switched_from() call is allowed to drop rq-&gt;lock, therefore we * cannot assume the switched_from/switched_to pair is serliazed by * rq-&gt;lock. They are however serialized by p-&gt;pi_lock. */ void (*switched_from)(struct rq *this_rq, struct task_struct *task); void (*switched_to) (struct rq *this_rq, struct task_struct *task); void (*prio_changed) (struct rq *this_rq, struct task_struct *task, int oldprio); unsigned int (*get_rr_interval)(struct rq *rq, struct task_struct *task); void (*update_curr)(struct rq *rq);#define TASK_SET_GROUP 0#define TASK_MOVE_GROUP 1......&#125;; 调度类分为下面几种： 12345extern const struct sched_class stop_sched_class;extern const struct sched_class dl_sched_class;extern const struct sched_class rt_sched_class;extern const struct sched_class fair_sched_class;extern const struct sched_class idle_sched_class; 队列操作中函数指针指向不同策略队列的实际执行函数函数，在linux&#x2F;kernel&#x2F;sched&#x2F;目录下，fair.c、idle.c、rt.c等文件对不同类型的策略实现了不同的函数，如fair.c中定义了 1234567891011121314151617181920212223/* * All the scheduling class methods: */const struct sched_class fair_sched_class = &#123; .next = &amp;idle_sched_class, .enqueue_task = enqueue_task_fair, .dequeue_task = dequeue_task_fair, .yield_task = yield_task_fair, .yield_to_task = yield_to_task_fair, .check_preempt_curr = check_preempt_wakeup, .pick_next_task = pick_next_task_fair, .put_prev_task = put_prev_task_fair,...... .set_curr_task = set_curr_task_fair, .task_tick = task_tick_fair, .task_fork = task_fork_fair, .prio_changed = prio_changed_fair, .switched_from = switched_from_fair, .switched_to = switched_to_fair, .get_rr_interval\t= get_rr_interval_fair, .update_curr = update_curr_fair,......&#125;; 以选择下一个任务为例，CFS对应的是pick_next_task_fair，而rt_rq对应的则是pick_next_task_rt，等等。 由此，我们来总结一下： 每个CPU都有一个struct rq结构体，里面会有着cfs_rq, rt_rq等一系列队列 每个队列由一个红黑树组织，红黑树里每一个节点为一个任务实体sched_entity 每一个任务实体sched_entity对应于一个任务task_struct 在task_struct中对应的sched_class会根据不同策略申明不同的对应处理函数，处理实际的调度工作 四. 调度流程 有了上述的基本策略和基本调度结构体，我们可以形成大致的骨架，下面就是需要核心的调度流程将其拼凑成一个整体，实现调度系统。调度分为两种，主动调度和抢占式调度。 主动调度即任务执行一定时间以后主动让出CPU，通过调度策略选择合适的下一个任务执行。 抢占式调度即任务执行中收到了其他任务的中断，由此停止执行并切换至下一个任务。 4.1 主动调度 说到调用，逃不过核心函数schedule()。其中sched_submit_work()函数完成当前任务的收尾工作，以避免出现如死锁或者IO中断等情况。之后首先禁止抢占式调度的发生，然后调用__schedule()函数完成调度，之后重新打开抢占式调度，如果需要重新调度则会一直重复该过程，否则结束函数。 1234567891011asmlinkage __visible void __sched schedule(void)&#123; struct task_struct *tsk = current; sched_submit_work(tsk); do &#123; preempt_disable(); __schedule(false); sched_preempt_enable_no_resched(); &#125; while (need_resched());&#125;EXPORT_SYMBOL(schedule); 而__schedule()函数则是实际的核心调度函数，该函数主要操作包括选取下一进程和进行上下文切换，而上下文切换又包括用户态空间切换和内核态的切换。具体的解释可以参照英文源码注释以及中文对各个步骤的注释。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126/* * __schedule() is the main scheduler function. * The main means of driving the scheduler and thus entering this function are: * 1. Explicit blocking: mutex, semaphore, waitqueue, etc. * 2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return * paths. For example, see arch/x86/entry_64.S. * To drive preemption between tasks, the scheduler sets the flag in timer * interrupt handler scheduler_tick(). * 3. Wakeups don&#x27;t really cause entry into schedule(). They add a * task to the run-queue and that&#x27;s it. * Now, if the new task added to the run-queue preempts the current * task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets * called on the nearest possible occasion: * - If the kernel is preemptible (CONFIG_PREEMPT=y): * - in syscall or exception context, at the next outmost * preempt_enable(). (this might be as soon as the wake_up()&#x27;s * spin_unlock()!) * - in IRQ context, return from interrupt-handler to * preemptible context * - If the kernel is not preemptible (CONFIG_PREEMPT is not set) * then at the next: * - cond_resched() call * - explicit schedule() call * - return from syscall or exception to user-space * - return from interrupt-handler to user-space * WARNING: must be called with preemption disabled! */static void __sched notrace __schedule(bool preempt)&#123; struct task_struct *prev, *next; unsigned long *switch_count; struct rq_flags rf; struct rq *rq; int cpu; //从当前的CPU中取出任务队列rq，prev赋值为当前任务 cpu = smp_processor_id(); rq = cpu_rq(cpu); prev = rq-&gt;curr; //检测当前任务是否可以调度 schedule_debug(prev); if (sched_feat(HRTICK)) hrtick_clear(rq); //禁止中断，RCU抢占关闭，队列加锁，SMP加锁 local_irq_disable(); rcu_note_context_switch(preempt); /* * Make sure that signal_pending_state()-&gt;signal_pending() below * can&#x27;t be reordered with __set_current_state(TASK_INTERRUPTIBLE) * done by the caller to avoid the race with signal_wake_up(). * * The membarrier system call requires a full memory barrier * after coming from user-space, before storing to rq-&gt;curr. */ rq_lock(rq, &amp;rf); smp_mb__after_spinlock(); /* Promote REQ to ACT */ rq-&gt;clock_update_flags &lt;&lt;= 1; update_rq_clock(rq); switch_count = &amp;prev-&gt;nivcsw; if (!preempt &amp;&amp; prev-&gt;state) &#123; //不可中断的任务则继续执行 if (signal_pending_state(prev-&gt;state, prev)) &#123; prev-&gt;state = TASK_RUNNING; &#125; else &#123; //当前任务从队列rq中出队，on_rq设置为0，如果存在I/O未完成则延时完成 deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK); prev-&gt;on_rq = 0; if (prev-&gt;in_iowait) &#123; atomic_inc(&amp;rq-&gt;nr_iowait); delayacct_blkio_start(); &#125; /* 唤醒睡眠进程 * If a worker went to sleep, notify and ask workqueue * whether it wants to wake up a task to maintain * concurrency. */ if (prev-&gt;flags &amp; PF_WQ_WORKER) &#123; struct task_struct *to_wakeup; to_wakeup = wq_worker_sleeping(prev); if (to_wakeup) try_to_wake_up_local(to_wakeup, &amp;rf); &#125; &#125; switch_count = &amp;prev-&gt;nvcsw; &#125; // 调用pick_next_task获取下一个任务，赋值给next next = pick_next_task(rq, prev, &amp;rf); clear_tsk_need_resched(prev); clear_preempt_need_resched(); // 如果产生了任务切换，则需要切换上下文 if (likely(prev != next)) &#123; rq-&gt;nr_switches++; rq-&gt;curr = next; /* * The membarrier system call requires each architecture * to have a full memory barrier after updating * rq-&gt;curr, before returning to user-space. * * Here are the schemes providing that barrier on the * various architectures: * - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC. * switch_mm() rely on membarrier_arch_switch_mm() on PowerPC. * - finish_lock_switch() for weakly-ordered * architectures where spin_unlock is a full barrier, * - switch_to() for arm64 (weakly-ordered, spin_unlock * is a RELEASE barrier), */ ++*switch_count; trace_sched_switch(preempt, prev, next); /* Also unlocks the rq: */ rq = context_switch(rq, prev, next, &amp;rf); &#125; else &#123; // 清除标记位，重开中断 rq-&gt;clock_update_flags &amp;= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP); rq_unlock_irq(rq, &amp;rf); &#125; //队列自平衡：红黑树平衡操作 balance_callback(rq);&#125; 其中核心函数是获取下一个任务的pick_next_task()以及上下文切换的context_switch()，下面详细展开剖析。首先看看pick_next_task()，该函数会根据调度策略分类，调用该类对应的调度函数选择下一个任务实体。根据前文分析我们知道，最终是在不同的红黑树上选择最左节点作为下一个任务实体并返回。 1234567891011121314151617181920212223242526272829303132333435363738/* * Pick up the highest-prio task: */static inline struct task_struct *pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)&#123; const struct sched_class *class; struct task_struct *p; /* 这里做了一个优化：如果是普通调度策略则直接调用fair_sched_class中的pick_next_task * Optimization: we know that if all tasks are in the fair class we can * call that function directly, but only if the @prev task wasn&#x27;t of a * higher scheduling class, because otherwise those loose the * opportunity to pull in more work from other CPUs. */ if (likely((prev-&gt;sched_class == &amp;idle_sched_class || prev-&gt;sched_class == &amp;fair_sched_class) &amp;&amp; rq-&gt;nr_running == rq-&gt;cfs.h_nr_running)) &#123; p = fair_sched_class.pick_next_task(rq, prev, rf); if (unlikely(p == RETRY_TASK)) goto again; /* Assumes fair_sched_class-&gt;next == idle_sched_class */ if (unlikely(!p)) p = idle_sched_class.pick_next_task(rq, prev, rf); return p; &#125;again: //依次调用类中的选择函数，如果正确选择到下一个任务则返回 for_each_class(class) &#123; p = class-&gt;pick_next_task(rq, prev, rf); if (p) &#123; if (unlikely(p == RETRY_TASK)) goto again; return p; &#125; &#125; /* The idle class should always have a runnable task: */ BUG();&#125; 下面来看看上下文切换。上下文切换主要干两件事情，一是切换任务空间，也即虚拟内存；二是切换寄存器和 CPU 上下文。关于任务空间的切换放在内存部分的文章中详细介绍，这里先按下不表，通过任务空间切换实际完成了用户态的上下文切换工作。下面我们重点看一下内核态切换，即寄存器和CPU上下文的切换。 123456789101112131415161718192021222324252627282930313233343536373839404142/* * context_switch - switch to the new MM and the new thread&#x27;s register state. */static __always_inline struct rq *context_switch(struct rq *rq, struct task_struct *prev, struct task_struct *next, struct rq_flags *rf)&#123; struct mm_struct *mm, *oldmm; prepare_task_switch(rq, prev, next); mm = next-&gt;mm; oldmm = prev-&gt;active_mm; /* * For paravirt, this is coupled with an exit in switch_to to * combine the page table reload and the switch backend into * one hypercall. */ arch_start_context_switch(prev); /* * If mm is non-NULL, we pass through switch_mm(). If mm is * NULL, we will pass through mmdrop() in finish_task_switch(). * Both of these contain the full memory barrier required by * membarrier after storing to rq-&gt;curr, before returning to * user-space. */ if (!mm) &#123; next-&gt;active_mm = oldmm; mmgrab(oldmm); enter_lazy_tlb(oldmm, next); &#125; else switch_mm_irqs_off(oldmm, mm, next); if (!prev-&gt;mm) &#123; prev-&gt;active_mm = NULL; rq-&gt;prev_mm = oldmm; &#125; rq-&gt;clock_update_flags &amp;= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP); prepare_lock_switch(rq, next, rf); /* Here we just switch the register state and the stack. */ switch_to(prev, next, prev); //barrier 语句是一个编译器指令，用于保证 switch_to 和 finish_task_switch 的执行顺序不会因为编译阶段优化而改变 barrier(); return finish_task_switch(prev);&#125; switch_to()就是寄存器和栈的切换，它调用到了 __switch_to_asm。这是一段汇编代码，主要用于栈的切换， 其中32位使用esp作为栈顶指针，64位使用rsp，其他部分代码一致。通过该段汇编代码我们完成了栈顶指针的切换，并调用__switch_to完成最终TSS的切换。注意switch_to中其实是有三个变量，分别是prev, next, last，而实际在使用时，我们会对last也赋值为prev。这里的设计意图需要结合一个例子来说明。假设有ABC三个任务，从A调度到B，B到C，最后C回到A，我们假设仅保存prev和next，则流程如下 A保存内核栈和寄存器，切换至B，此时prev &#x3D; A, next &#x3D; B，该状态会保存在栈里，等下次调用A的时候再恢复。然后调用B的finish_task_switch()继续执行下去，返回B的队列rq， B保存内核栈和寄存器，切换至C C保存内核栈和寄存器，切换至A。A从barrier()开始运行，而A从步骤1中保存的prev &#x3D; A, next &#x3D; B则完美的避开了C，丢失了C的信息。因此last指针的重要性就出现了。在执行完__switch_to_asm后，A的内核栈和寄存器重新覆盖了prev和next，但是我们通过返回值提供了C的内存地址，保存在last中，在finish_task_switch中完成清理工作。 12345678910111213141516171819#define switch_to(prev, next, last) \\do &#123; \\ prepare_switch_to(next); \\ \\ ((last) = __switch_to_asm((prev), (next))); \\&#125; while (0)/* * %eax: prev task * %edx: next task */ENTRY(__switch_to_asm)...... /* switch stack */ movl %esp, TASK_threadsp(%eax) movl TASK_threadsp(%edx), %esp...... jmp __switch_toEND(__switch_to_asm) 最终调用__switch_to()函数。该函数中涉及到一个结构体TSS(Task State Segment)，该结构体存放了所有的寄存器。另外还有一个特殊的寄存器TR（Task Register）会指向TSS，我们通过更改TR的值，会触发硬件保存CPU所有寄存器在当前TSS，并从新的TSS读取寄存器的值加载入CPU，从而完成一次硬中断带来的上下文切换工作。系统初始化的时候，会调用 cpu_init()给每一个 CPU 关联一个 TSS，然后将 TR 指向这个 TSS，然后在操作系统的运行过程中，TR 就不切换了，永远指向这个 TSS。当修改TR的值得时候，则为任务调度。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/* *\tswitch_to(x,y) should switch tasks from x to y. * * We fsave/fwait so that an exception goes off at the right time * (as a call from the fsave or fwait in effect) rather than to * the wrong process. Lazy FP saving no longer makes any sense * with modern CPU&#x27;s, and this simplifies a lot of things (SMP * and UP become the same). * * NOTE! We used to use the x86 hardware context switching. The * reason for not using it any more becomes apparent when you * try to recover gracefully from saved state that is no longer * valid (stale segment register values in particular). With the * hardware task-switch, there is no way to fix up bad state in * a reasonable manner. * * The fact that Intel documents the hardware task-switching to * be slow is a fairly red herring - this code is not noticeably * faster. However, there _is_ some room for improvement here, * so the performance issues may eventually be a valid point. * More important, however, is the fact that this allows us much * more flexibility. * * The return value (in %ax) will be the &quot;prev&quot; task after * the task-switch, and shows up in ret_from_fork in entry.S, * for example. */__visible __notrace_funcgraph struct task_struct *__switch_to(struct task_struct *prev_p, struct task_struct *next_p)&#123; struct thread_struct *prev = &amp;prev_p-&gt;thread, *next = &amp;next_p-&gt;thread; struct fpu *prev_fpu = &amp;prev-&gt;fpu; struct fpu *next_fpu = &amp;next-&gt;fpu; int cpu = smp_processor_id(); /* never put a printk in __switch_to... printk() calls wake_up*() indirectly */ switch_fpu_prepare(prev_fpu, cpu); /* * Save away %gs. No need to save %fs, as it was saved on the * stack on entry. No need to save %es and %ds, as those are * always kernel segments while inside the kernel. Doing this * before setting the new TLS descriptors avoids the situation * where we temporarily have non-reloadable segments in %fs * and %gs. This could be an issue if the NMI handler ever * used %fs or %gs (it does not today), or if the kernel is * running inside of a hypervisor layer. */ lazy_save_gs(prev-&gt;gs); /* * Load the per-thread Thread-Local Storage descriptor. */ load_TLS(next, cpu); /* * Restore IOPL if needed. In normal use, the flags restore * in the switch assembly will handle this. But if the kernel * is running virtualized at a non-zero CPL, the popf will * not restore flags, so it must be done in a separate step. */ if (get_kernel_rpl() &amp;&amp; unlikely(prev-&gt;iopl != next-&gt;iopl)) set_iopl_mask(next-&gt;iopl); switch_to_extra(prev_p, next_p); /* * Leave lazy mode, flushing any hypercalls made here. * This must be done before restoring TLS segments so * the GDT and LDT are properly updated, and must be * done before fpu__restore(), so the TS bit is up * to date. */ arch_end_context_switch(next_p); /* * Reload esp0 and cpu_current_top_of_stack. This changes * current_thread_info(). Refresh the SYSENTER configuration in * case prev or next is vm86. */ update_task_stack(next_p); refresh_sysenter_cs(next); this_cpu_write(cpu_current_top_of_stack, (unsigned long)task_stack_page(next_p) + THREAD_SIZE); /* * Restore %gs if needed (which is common) */ if (prev-&gt;gs | next-&gt;gs) lazy_load_gs(next-&gt;gs); switch_fpu_finish(next_fpu, cpu); this_cpu_write(current_task, next_p); /* Load the Intel cache allocation PQR MSR. */ resctrl_sched_in(); return prev_p;&#125; 在完成了switch_to()的内核态切换后，还有一个重要的函数finish_task_switch()负责善后清理工作。在前面介绍switch_to三个参数的时候我们已经说明了使用last的重要性。而这里为何让prev和last均赋值为prev，是因为prev在后面没有需要用到，所以节省了一个指针空间来存储last。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091/** * finish_task_switch - clean up after a task-switch * @prev: the thread we just switched away from. * * finish_task_switch must be called after the context switch, paired * with a prepare_task_switch call before the context switch. * finish_task_switch will reconcile locking set up by prepare_task_switch, * and do any other architecture-specific cleanup actions. * * Note that we may have delayed dropping an mm in context_switch(). If * so, we finish that here outside of the runqueue lock. (Doing it * with the lock held can cause deadlocks; see schedule() for * details.) * * The context switch have flipped the stack from under us and restored the * local variables which were saved when this task called schedule() in the * past. prev == current is still correct but we need to recalculate this_rq * because prev may have moved to another CPU. */static struct rq *finish_task_switch(struct task_struct *prev) __releases(rq-&gt;lock)&#123; struct rq *rq = this_rq(); struct mm_struct *mm = rq-&gt;prev_mm; long prev_state; /* * The previous task will have left us with a preempt_count of 2 * because it left us after: * *\tschedule() * preempt_disable(); // 1 * __schedule() * raw_spin_lock_irq(&amp;rq-&gt;lock)\t// 2 * * Also, see FORK_PREEMPT_COUNT. */ if (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET, &quot;corrupted preempt_count: %s/%d/0x%x &quot;, current-&gt;comm, current-&gt;pid, preempt_count())) preempt_count_set(FORK_PREEMPT_COUNT); rq-&gt;prev_mm = NULL; /* * A task struct has one reference for the use as &quot;current&quot;. * If a task dies, then it sets TASK_DEAD in tsk-&gt;state and calls * schedule one last time. The schedule call will never return, and * the scheduled task must drop that reference. * * We must observe prev-&gt;state before clearing prev-&gt;on_cpu (in * finish_task), otherwise a concurrent wakeup can get prev * running on another CPU and we could rave with its RUNNING -&gt; DEAD * transition, resulting in a double drop. */ prev_state = prev-&gt;state; vtime_task_switch(prev); perf_event_task_sched_in(prev, current); finish_task(prev); finish_lock_switch(rq); finish_arch_post_lock_switch(); kcov_finish_switch(current); fire_sched_in_preempt_notifiers(current); /* * When switching through a kernel thread, the loop in * membarrier_&#123;private,global&#125;_expedited() may have observed that * kernel thread and not issued an IPI. It is therefore possible to * schedule between user-&gt;kernel-&gt;user threads without passing though * switch_mm(). Membarrier requires a barrier after storing to * rq-&gt;curr, before returning to userspace, so provide them here: * * - a full memory barrier for &#123;PRIVATE,GLOBAL&#125;_EXPEDITED, implicitly * provided by mmdrop(), * - a sync_core for SYNC_CORE. */ if (mm) &#123; membarrier_mm_sync_core_before_usermode(mm); mmdrop(mm); &#125; if (unlikely(prev_state == TASK_DEAD)) &#123; if (prev-&gt;sched_class-&gt;task_dead) prev-&gt;sched_class-&gt;task_dead(prev); /* * Remove function-return probe instances associated with this * task and put them back on the free list. */ kprobe_flush_task(prev); /* Task is done with its stack. */ put_task_stack(prev); put_task_struct(prev); &#125; tick_nohz_task_switch(); return rq;&#125; 至此，我们完成了内核态的切换工作，也完成了整个主动调度的过程。 4.2 抢占式调度 抢占式调度通常发生在两种情况下。一种是某任务执行时间过长，另一种是当某任务被唤醒的时候。首先看看任务执行时间过长的情况。 4.2.1 任务运行时间检测 该情况需要衡量一个任务的执行时间长短，执行时间过长则发起抢占。在计算机里面有一个时钟，会过一段时间触发一次时钟中断，通知操作系统时间又过去一个时钟周期，通过这种方式可以查看是否是需要抢占的时间点。 时钟中断处理函数会调用scheduler_tick()。该函数首先取出当前CPU，并由此获取对应的运行队列rq和当前任务curr。接着调用该任务的调度类sched_class对应的task_tick()函数进行时间事件处理。 123456789101112131415161718192021/* * This function gets called by the timer code, with HZ frequency. * We call it with interrupts disabled. */void scheduler_tick(void)&#123; int cpu = smp_processor_id(); struct rq *rq = cpu_rq(cpu); struct task_struct *curr = rq-&gt;curr; struct rq_flags rf; sched_clock_tick(); rq_lock(rq, &amp;rf); update_rq_clock(rq); curr-&gt;sched_class-&gt;task_tick(rq, curr, 0); cpu_load_update_active(rq); calc_global_load_tick(rq); psi_task_tick(rq); rq_unlock(rq, &amp;rf); perf_event_task_tick();......&#125; 以普通任务队列为例，对应的调度类为fair_sched_class，对应的时钟处理函数为task_tick_fair()，该函数会获取当前的调度实体和运行队列，并调用entity_tick()函数更新时间。 1234567891011121314151617181920/* * scheduler tick hitting a task of our scheduling class. * NOTE: This function can be called remotely by the tick offload that * goes along full dynticks. Therefore no local assumption can be made * and everything must be accessed through the @rq and @curr passed in * parameters. */static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)&#123; struct cfs_rq *cfs_rq; struct sched_entity *se = &amp;curr-&gt;se; for_each_sched_entity(se) &#123; cfs_rq = cfs_rq_of(se); entity_tick(cfs_rq, se, queued); &#125; if (static_branch_unlikely(&amp;sched_numa_balancing)) task_tick_numa(rq, curr); update_misfit_status(curr, rq); update_overutilized_status(task_rq(curr));&#125; 在entity_tick()中，首先会调用update_curr()更新当前任务的vruntime，然后调用check_preempt_tick()检测现在是否可以发起抢占。 12345678910111213141516static voidentity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)&#123; /* * Update run-time statistics of the &#x27;current&#x27;. */ update_curr(cfs_rq); /* * Ensure that runnable average is periodically updated. */ update_load_avg(cfs_rq, curr, UPDATE_TG); update_cfs_group(curr);...... if (cfs_rq-&gt;nr_running &gt; 1) check_preempt_tick(cfs_rq, curr);&#125; check_preempt_tick() 先是调用 sched_slice() 函数计算出一个调度周期中该任务运行的实际时间 ideal_runtime。sum_exec_runtime 指任务总共执行的实际时间，prev_sum_exec_runtime 指上次该进程被调度时已经占用的实际时间，所以 sum_exec_runtime - prev_sum_exec_runtime 就是这次调度占用实际时间。如果这个时间大于 ideal_runtime，则应该被抢占了。除了这个条件之外，还会通过 __pick_first_entity 取出红黑树中最小的进程。如果当前进程的 vruntime 大于红黑树中最小的进程的 vruntime，且差值大于 ideal_runtime，也应该被抢占了。 12345678910111213141516171819202122232425262728293031323334/* * Preempt the current task with a newly woken task if needed: */static voidcheck_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)&#123; unsigned long ideal_runtime, delta_exec; struct sched_entity *se; s64 delta; ideal_runtime = sched_slice(cfs_rq, curr); delta_exec = curr-&gt;sum_exec_runtime - curr-&gt;prev_sum_exec_runtime; if (delta_exec &gt; ideal_runtime) &#123; resched_curr(rq_of(cfs_rq)); /* * The current task ran long enough, ensure it doesn&#x27;t get * re-elected due to buddy favours. */ clear_buddies(cfs_rq, curr); return; &#125; /* * Ensure that a task that missed wakeup preemption by a * narrow margin doesn&#x27;t have to wait for a full slice. * This also mitigates buddy induced latencies under load. */ if (delta_exec &lt; sysctl_sched_min_granularity) return; se = __pick_first_entity(cfs_rq); delta = curr-&gt;vruntime - se-&gt;vruntime; if (delta &lt; 0) return; if (delta &gt; ideal_runtime) resched_curr(rq_of(cfs_rq));&#125; 如果确认需要被抢占，则会调用resched_curr()函数，该函数会调用set_tsk_need_resched()标记该任务为_TIF_NEED_RESCHED，即该任务应该被抢占。 1234567891011121314151617181920212223/* * resched_curr - mark rq&#x27;s current task &#x27;to be rescheduled now&#x27;. * * On UP this means the setting of the need_resched flag, on SMP it * might also involve a cross-CPU call to trigger the scheduler on * the target CPU. */void resched_curr(struct rq *rq)&#123; struct task_struct *curr = rq-&gt;curr; int cpu;....... cpu = cpu_of(rq); if (cpu == smp_processor_id()) &#123; set_tsk_need_resched(curr); set_preempt_need_resched(); return; &#125; if (set_nr_and_not_polling(curr)) smp_send_reschedule(cpu); else trace_sched_wake_idle_without_ipi(cpu);&#125; 4.2.2 任务唤醒情况 某些任务会因为中断而唤醒，如当 I&#x2F;O 到来的时候，I&#x2F;O进程往往会被唤醒。在这种时候，如果被唤醒的任务优先级高于 CPU 上的当前任务，就会触发抢占。try_to_wake_up() 调用 ttwu_queue() 将这个唤醒的任务添加到队列当中。ttwu_queue() 再调用 ttwu_do_activate() 激活这个任务。ttwu_do_activate() 调用 ttwu_do_wakeup()。这里面调用了 check_preempt_curr() 检查是否应该发生抢占。如果应该发生抢占，也不是直接踢走当前进程，而是将当前进程标记为应该被抢占。 123456static void ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags, struct rq_flags *rf)&#123; check_preempt_curr(rq, p, wake_flags); p-&gt;state = TASK_RUNNING; trace_sched_wakeup(p); 4.2.3 抢占的发生由前面的分析，我们知道了不论是是当前任务执行时间过长还是新任务唤醒，我们均会对现在的任务标记位_TIF_NEED_RESCUED，下面分析实际抢占的发生。真正的抢占还需要一个特定的时机让正在运行中的进程有机会调用一下 __schedule()函数，发起真正的调度。 实际上会调用__schedule()函数共有以下几个时机 从系统调用返回用户态：以64位为例，系统调用的链路为do_syscall_64-&gt;syscall_return_slowpath-&gt;prepare_exit_to_usermode-&gt;exit_to_usermode_loop。在exit_to_usermode_loop中，会检测是否为_TIF_NEED_RESCHED，如果是则调用__schedule() 12345678910static void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags)&#123; while (true) &#123; /* We have work to do. */ local_irq_enable(); if (cached_flags &amp; _TIF_NEED_RESCHED) schedule();...... &#125; 内核态启动：内核态的执行中，被抢占的时机一般发生在 preempt_enable() 中。在内核态的执行中，有的操作是不能被中断的，所以在进行这些操作之前，总是先调用 preempt_disable() 关闭抢占，当再次打开的时候，就是一次内核态代码被抢占的机会。preempt_enable() 会调用 preempt_count_dec_and_test()，判断 preempt_count 和 TIF_NEED_RESCHED 是否可以被抢占。如果可以，就调用 preempt_schedule-&gt;preempt_schedule_common-&gt;__schedule 进行调度。 123456789101112131415161718192021222324#define preempt_enable() \\do &#123; \\ if (unlikely(preempt_count_dec_and_test())) \\ __preempt_schedule(); \\&#125; while (0)#define preempt_count_dec_and_test() \\(&#123; preempt_count_sub(1); should_resched(0); &#125;)static __always_inline bool should_resched(int preempt_offset)&#123; return unlikely(preempt_count() == preempt_offset &amp;&amp; tif_need_resched());&#125;#define tif_need_resched() test_thread_flag(TIF_NEED_RESCHED)static void __sched notrace preempt_schedule_common(void)&#123; do &#123;...... __schedule(true);...... &#125; while (need_resched()) 从中断返回内核态&#x2F;用户态：中断处理调用的是 do_IRQ 函数，中断完毕后分为两种情况，一个是返回用户态，一个是返回内核态。 返回用户态会调用 prepare_exit_to_usermode()，最终调用 exit_to_usermode_loop() 返回内核态会调用preempt_schedule_irq()，最终调用__schedule() 123456789101112131415161718192021222324common_interrupt: ASM_CLAC addq $-0x80, (%rsp) interrupt do_IRQret_from_intr: popq %rsp testb $3, CS(%rsp) jz retint_kernel/* Interrupt came from user space */GLOBAL(retint_user) mov %rsp,%rdi call prepare_exit_to_usermode TRACE_IRQS_IRETQ SWAPGS jmp restore_regs_and_iret/* Returning to kernel space */retint_kernel:#ifdef CONFIG_PREEMPT bt $9, EFLAGS(%rsp) jnc 1f0: cmpl $0, PER_CPU_VAR(__preempt_count) jnz 1f call preempt_schedule_irq jmp 0b 123456789101112asmlinkage __visible void __sched preempt_schedule_irq(void)&#123;...... do &#123; preempt_disable(); local_irq_enable(); __schedule(true); local_irq_disable(); sched_preempt_enable_no_resched(); &#125; while (need_resched());......&#125; 五. 总结 本文分析了任务调度的策略、结构体以及整个调度流程，其中关于内存上下文切换的部分尚未详细叙述，留待内存部分展开剖析。 源码资料1、调度相关结构体及函数实现 2、schedule核心函数","tags":["Linux开发"],"categories":["linux"]},{"title":"进程、线程的创建和派生","path":"/2023/01/15/linux-docs/进程管理/进程、线程的创建和派生/","content":"一. 前言 在前文中，我们分析了内核中进程和线程的统一结构体task_struct，本文将继续分析进程、线程的创建和派生的过程。首先介绍如何将一个程序编辑为执行文件最后成为进程执行，然后会介绍线程的执行，最后会分析如何通过已有的进程、线程实现多进程、多线程。因为进程和线程有诸多相似之处，也有一些不同之处，因此本文会对比进程和线程来加深理解和记忆。 二. 进程的创建以C语言为例，我们在Linux下编写C语言代码，然后通过gcc编译和链接生成可执行文件后直接执行即可完成一个进程的创建和工作。下面将详细展开介绍这个创建进程的过程。在 Linux 下面，二进制的程序也要有严格的格式，这个格式我们称为 ELF（Executable and Linkable Format，可执行与可链接格式）。这个格式可以根据编译的结果不同，分为不同的格式。主要包括 1、可重定位的对象文件(Relocatable file) 由汇编器汇编生成的 .o 文件 2、可执行的对象文件(Executable file) 可执行应用程序 3、可被共享的对象文件(Shared object file) 动态库文件，也即 .so 文件 下面在进程创建过程中会详细说明三种文件。 2. 1 编译写完C程序后第一步就是程序编译（其实还有IDE的预编译，那些属于编辑器操作这里不表）。编译指令如下所示 gcc -c -fPIC xxxx.c -c表示编译、汇编指定的源文件，不进行链接。-fPIC表示生成与位置无关（Position-Independent Code）代码，即采用相对地址而非绝对地址，从而满足共享库加载需求。在编译的时候，先做预处理工作，例如将头文件嵌入到正文中，将定义的宏展开，然后就是真正的编译过程，最终编译成为.o 文件，这就是 ELF 的第一种类型，可重定位文件（Relocatable File）。之所以叫做可重定位文件，是因为对于编译好的代码和变量，将来加载到内存里面的时候，都是要加载到一定位置的。比如说，调用一个函数，其实就是跳到这个函数所在的代码位置执行；再比如修改一个全局变量，也是要到变量的位置那里去修改。但是现在这个时候，还是.o 文件，不是一个可以直接运行的程序，这里面只是部分代码片段。因此.o 里面的位置是不确定的，但是必须是可重新定位的以适应需求。 ELF 文件的头是用于描述整个文件的。这个文件格式在内核中有定义，分别为 struct elf32_hdr 和struct elf64_hdr。其他各个section作用如下所示 .text：放编译好的二进制可执行代码 .rodata：只读数据，例如字符串常量、const 的变量 .data：已经初始化好的全局变量 .bss：未初始化全局变量，运行时会置 0 .symtab：符号表，记录的则是函数和变量 .rel.text： .text部分的重定位表 .rel.data：.data部分的重定位表 .strtab：字符串表、字符串常量和变量名 这些节的元数据信息也需要有一个地方保存，就是最后的节头部表（Section Header Table）。在这个表里面，每一个 section 都有一项，在代码里面也有定义 struct elf32_shdr和struct elf64_shdr。在 ELF 的头里面，有描述这个文件的节头部表的位置，有多少个表项等等信息。 2.2 链接链接分为静态链接和动态链接。静态链接库会和目标文件通过链接生成一个可执行文件，而动态链接则会通过链接形成动态连接器，在可执行文件执行的时候动态的选择并加载其中的部分或全部函数。二者的各自优缺点如下所示 静态链接库的优点 (1) 代码装载速度快，执行速度略比动态链接库快； (2) 只需保证在开发者的计算机中有正确的.LIB文件，在以二进制形式发布程序时不需考虑在用户的计算机上.LIB文件是否存在及版本问题，可避免DLL地狱等问题。 静态链接库的缺点 使用静态链接生成的可执行文件体积较大，包含相同的公共代码，造成浪费 动态链接库的优点 (1) 更加节省内存并减少页面交换； (2) DLL文件与EXE文件独立，只要输出接口不变（即名称、参数、返回值类型和调用约定不变），更换DLL文件不会对EXE文件造成任何影响，因而极大地提高了可维护性和可扩展性； (3) 不同编程语言编写的程序只要按照函数调用约定就可以调用同一个DLL函数； (4)适用于大规模的软件开发，使开发过程独立、耦合度小，便于不同开发者和开发组织之间进行开发和测试。 动态链接库的缺点 使用动态链接库的应用程序不是自完备的，它依赖的DLL模块也要存在，如果使用载入时动态链接，程序启动时发现DLL不存在，系统将终止程序并给出错误信息。而使用运行时动态链接，系统不会终止，但由于DLL中的导出函数不可用，程序会加载失败；速度比静态链接慢。当某个模块更新后，如果新模块与旧的模块不兼容，那么那些需要该模块才能运行的软件均无法执行。这在早期Windows中很常见。 下面分别介绍静态链接和动态链接 2.2.1 静态链接静态链接库.a文件（Archives）的执行指令如下 ar cr libXXX.a XXX.o XXXX.o 当需要使用该静态库的时候，会将.o文件从.a文件中依次抽取并链接到程序中，指令如下 gcc -o XXXX XXX.O -L. -lsXXX -L表示在当前目录下找.a 文件，-lsXXXX会自动补全文件名，比如加前缀 lib，后缀.a，变成libXXX.a，找到这个.a文件后，将里面的 XXXX.o 取出来，和 XXX.o 做一个链接，形成二进制执行文件XXXX。在这里，重定位会从.o中抽取函数并和.a中的文件抽取的函数进行合并，找到实际的调用位置，形成最终的可执行文件(Executable file)，即ELF的第二种格式文件。 对比ELF第一种格式可重定位文件，这里可执行文件略去了重定位表相关段落。此处将ELF文件分为了代码段、数据段和不加载到内存中的部分，并加上了段头表（Segment Header Table）用以记录管理，在代码中定义为struct elf32_phdr和 struct elf64_phdr，这里面除了有对于段的描述之外，最重要的是 p_vaddr，这个是这个段加载到内存的虚拟地址。这部分会在内存篇章详细介绍。 2.2.2 动态链接动态链接库（Shared Libraries)的作用主要是为了解决静态链接大量使用会造成空间浪费的问题，因此这里设计成了可以被多个程序共享的形式，其执行命令如下 gcc -shared -fPIC -o libXXX.so XXX.o 当一个动态链接库被链接到一个程序文件中的时候，最后的程序文件并不包括动态链接库中的代码，而仅仅包括对动态链接库的引用，并且不保存动态链接库的全路径，仅仅保存动态链接库的名称。 gcc -o XXX XXX.O -L. -lXXX 当运行这个程序的时候，首先寻找动态链接库，然后加载它。默认情况下，系统在 &#x2F;lib 和&#x2F;usr&#x2F;lib 文件夹下寻找动态链接库。如果找不到就会报错，我们可以设定 LD_LIBRARY_PATH环境变量，程序运行时会在此环境变量指定的文件夹下寻找动态链接库。动态链接库，就是 ELF 的第三种类型，共享对象文件（Shared Object）。 动态链接的ELF相对于静态链接主要多了以下部分 .interp段，里面是ld-linux.so，负责运行时的链接动作 .plt（Procedure Linkage Table），过程链接表 .got.plt（Global Offset Table），全局偏移量表 当程序编译时，会对每个函数在PLT中建立新的项，如PLT[n]，而动态库中则存有该函数的实际地址，记为GOT[m]。整体寻址过程如下所示 PLT[n]向GOT[m]寻求地址 GOT[m]初始并无地址，需要采取以下方式获取地址 回调PLT[0] PLT[0]调用GOT[2]，即ld-linux.so ld-linux.so查找所需函数实际地址并存放在GOT[m]中 由此，我们建立了PLT[n]到GOT[m]的对应关系，从而实现了动态链接。 2.3 加载运行完成了上述的编译、汇编、链接，我们最终形成了可执行文件，并加载运行。在内核中，有这样一个数据结构，用来定义加载二进制文件的方法。 12345678struct linux_binfmt &#123; struct list_head lh; struct module *module; int (*load_binary)(struct linux_binprm *); int (*load_shlib)(struct file *); int (*core_dump)(struct coredump_params *cprm); unsigned long min_coredump; /* minimal dump size */&#125; __randomize_layout; 对于ELF文件格式，其对应实现为 1234567static struct linux_binfmt elf_format = &#123; .module = THIS_MODULE, .load_binary = load_elf_binary, .load_shlib = load_elf_library, .core_dump = elf_core_dump, .min_coredump = ELF_EXEC_PAGESIZE,&#125;; 其中加载的函数指针指向的函数和内核镜像加载是同一份函数，实际上通过exec函数完成调用。exec 比较特殊，它是一组函数： 包含 p 的函数（execvp, execlp）会在 PATH 路径下面寻找程序；不包含 p 的函数需要输入程序的全路径； 包含 v 的函数（execv, execvp, execve）以数组的形式接收参数； 包含 l 的函数（execl, execlp, execle）以列表的形式接收参数； 包含 e 的函数（execve, execle）以数组的形式接收环境变量。 当我们通过shell运行可执行文件或者通过fork派生子类，均是通过该类函数实现加载。 三. 线程的创建之用户态线程的创建对应的函数是pthread_create()，线程不是一个完全由内核实现的机制，它是由内核态和用户态合作完成的。pthread_create()不是一个系统调用，是 Glibc 库的一个函数，所以我们还要从 Glibc 说起。但是在开始之前，我们先要提一下，线程的创建到了内核态和进程的派生会使用同一个函数：__do_fork()，这也很容易理解，因为对内核态来说，线程和进程是同样的task_struct结构体。本节介绍线程在用户态的创建，而内核态的创建则会和进程的派生放在一起说明。 在Glibc的ntpl&#x2F;pthread_create.c中定义了__pthread_create_2_1()函数，该函数主要进行了以下操作 处理线程的属性参数。例如前面写程序的时候，我们设置的线程栈大小。如果没有传入线程属性，就取默认值。 123456789const struct pthread_attr *iattr = (struct pthread_attr *) attr;struct pthread_attr default_attr;//c11 thrd_createbool c11 = (attr == ATTR_C11_THREAD);if (iattr == NULL || c11)&#123; ...... iattr = &amp;default_attr;&#125; 就像在内核里每一个进程或者线程都有一个 task_struct 结构，在用户态也有一个用于维护线程的结构，就是这个 pthread 结构。 1struct pthread *pd = NULL; 凡是涉及函数的调用，都要使用到栈。每个线程也有自己的栈，接下来就是创建线程栈了。 1int err = ALLOCATE_STACK (iattr, &amp;pd); ALLOCATE_STACK 是一个宏，对应的函数allocate_stack()主要做了以下这些事情： 如果在线程属性里面设置过栈的大小，则取出属性值； 为了防止栈的访问越界在栈的末尾添加一块空间 guardsize，一旦访问到这里就会报错； 线程栈是在进程的堆里面创建的。如果一个进程不断地创建和删除线程，我们不可能不断地去申请和清除线程栈使用的内存块，这样就需要有一个缓存。get_cached_stack 就是根据计算出来的 size 大小，看一看已经有的缓存中，有没有已经能够满足条件的。如果缓存里面没有，就需要调用__mmap创建一块新的缓存，系统调用那一节我们讲过，如果要在堆里面 malloc 一块内存，比较大的话，用__mmap； 线程栈也是自顶向下生长的，每个线程要有一个pthread 结构，这个结构也是放在栈的空间里面的。在栈底的位置，其实是地址最高位； 计算出guard内存的位置，调用 setup_stack_prot 设置这块内存的是受保护的； 填充pthread 这个结构里面的成员变量 stackblock、stackblock_size、guardsize、specific。这里的 specific 是用于存放Thread Specific Data 的，也即属于线程的全局变量； 将这个线程栈放到 stack_used 链表中，其实管理线程栈总共有两个链表，一个是 stack_used，也就是这个栈正被使用；另一个是stack_cache，就是上面说的，一旦线程结束，先缓存起来，不释放，等有其他的线程创建的时候，给其他的线程用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# define ALLOCATE_STACK(attr, pd) allocate_stack (attr, pd, &amp;stackaddr)static intallocate_stack (const struct pthread_attr *attr, struct pthread **pdp, ALLOCATE_STACK_PARMS)&#123; struct pthread *pd; size_t size; size_t pagesize_m1 = __getpagesize () - 1;...... /* Get the stack size from the attribute if it is set. Otherwise we use the default we determined at start time. */ if (attr-&gt;stacksize != 0) size = attr-&gt;stacksize; else &#123; lll_lock (__default_pthread_attr_lock, LLL_PRIVATE); size = __default_pthread_attr.stacksize; lll_unlock (__default_pthread_attr_lock, LLL_PRIVATE); &#125;...... /* Allocate some anonymous memory. If possible use the cache. */ size_t guardsize; void *mem; const int prot = (PROT_READ | PROT_WRITE | ((GL(dl_stack_flags) &amp; PF_X) ? PROT_EXEC : 0)); /* Adjust the stack size for alignment. */ size &amp;= ~__static_tls_align_m1; /* Make sure the size of the stack is enough for the guard and eventually the thread descriptor. */ guardsize = (attr-&gt;guardsize + pagesize_m1) &amp; ~pagesize_m1; size += guardsize;...... /* Try to get a stack from the cache. */ pd = get_cached_stack (&amp;size, &amp;mem); if (pd == NULL) &#123; /* If a guard page is required, avoid committing memory by first allocate with PROT_NONE and then reserve with required permission excluding the guard page. */ mem = __mmap (NULL, size, (guardsize == 0) ? prot : PROT_NONE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_STACK, -1, 0); /* Place the thread descriptor at the end of the stack. */#if TLS_TCB_AT_TP pd = (struct pthread *) ((char *) mem + size) - 1;#elif TLS_DTV_AT_TP pd = (struct pthread *) ((((uintptr_t) mem + size - __static_tls_size) &amp; ~__static_tls_align_m1) - TLS_PRE_TCB_SIZE);#endif /* Now mprotect the required region excluding the guard area. */ char *guard = guard_position (mem, size, guardsize, pd, pagesize_m1); setup_stack_prot (mem, size, guard, guardsize, prot); pd-&gt;stackblock = mem; pd-&gt;stackblock_size = size; pd-&gt;guardsize = guardsize; pd-&gt;specific[0] = pd-&gt;specific_1stblock; /* And add to the list of stacks in use. */ stack_list_add (&amp;pd-&gt;list, &amp;stack_used); &#125; *pdp = pd; void *stacktop;# if TLS_TCB_AT_TP /* The stack begins before the TCB and the static TLS block. */ stacktop = ((char *) (pd + 1) - __static_tls_size);# elif TLS_DTV_AT_TP stacktop = (char *) (pd - 1);# endif *stack = stacktop;...... &#125; 四. 线程的内核态创建及进程的派生多进程是一种常见的程序实现方式，采用的系统调用为fork()函数。前文中已经详细叙述了系统调用的整个过程，对于fork()来说，最终会在系统调用表中查找到对应的系统调用sys_fork完成子进程的生成，而sys_fork 会调用 _do_fork()。 12345SYSCALL_DEFINE0(fork)&#123;...... return _do_fork(SIGCHLD, 0, 0, NULL, NULL, 0);&#125; 关于__do_fork()先按下不表，再接着看看线程。我们接着pthread_create ()看。其实有了用户态的栈，接着需要解决的就是用户态的程序从哪里开始运行的问题。start_routine() 就是给线程的函数，start_routine()， 参数 arg，以及调度策略都要赋值给 pthread。接下来 __nptl_nthreads 加一，说明又多了一个线程。 12345678pd-&gt;start_routine = start_routine;pd-&gt;arg = arg;pd-&gt;schedpolicy = self-&gt;schedpolicy;pd-&gt;schedparam = self-&gt;schedparam;/* Pass the descriptor to the caller. */*newthread = (pthread_t) pd;atomic_increment (&amp;__nptl_nthreads);retval = create_thread (pd, iattr, &amp;stopped_start, STACK_VARIABLES_ARGS, &amp;thread_ran); 真正创建线程的是调用 create_thread() 函数，这个函数定义如下。同时，这里还规定了当完成了内核态线程创建后回调的位置：start_thread()。 12345678910static intcreate_thread (struct pthread *pd, const struct pthread_attr *attr,bool *stopped_start, STACK_VARIABLES_PARMS, bool *thread_ran)&#123; const int clone_flags = (CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SYSVSEM | CLONE_SIGHAND | CLONE_THREAD | CLONE_SETTLS | CLONE_PARENT_SETTID | CLONE_CHILD_CLEARTID | 0); ARCH_CLONE (&amp;start_thread, STACK_VARIABLES_ARGS, clone_flags, pd, &amp;pd-&gt;tid, tp, &amp;pd-&gt;tid)； /* It&#x27;s started now, so if we fail below, we&#x27;ll have to cancel it and let it clean itself up. */ *thread_ran = true;&#125; 在 start_thread() 入口函数中，才真正的调用用户提供的函数，在用户的函数执行完毕之后，会释放这个线程相关的数据。例如，线程本地数据 thread_local variables，线程数目也减一。如果这是最后一个线程了，就直接退出进程，另外 __free_tcb() 用于释放 pthread。 1234567891011121314151617#define START_THREAD_DEFN \\ static int __attribute__ ((noreturn)) start_thread (void *arg)START_THREAD_DEFN&#123; struct pthread *pd = START_THREAD_SELF; /* Run the code the user provided. */ THREAD_SETMEM (pd, result, pd-&gt;start_routine (pd-&gt;arg)); /* Call destructors for the thread_local TLS variables. */ /* Run the destructor for the thread-local data. */ __nptl_deallocate_tsd (); if (__glibc_unlikely (atomic_decrement_and_test (&amp;__nptl_nthreads))) /* This was the last thread. */ exit (0); __free_tcb (pd); __exit_thread ();&#125; __free_tcb ()会调用 __deallocate_stack()来释放整个线程栈，这个线程栈要从当前使用线程栈的列表 stack_used 中拿下来，放到缓存的线程栈列表 stack_cache中，从而结束了线程的生命周期。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869voidinternal_function__free_tcb (struct pthread *pd)&#123; ...... __deallocate_stack (pd);&#125;voidinternal_function__deallocate_stack (struct pthread *pd)&#123; /* Remove the thread from the list of threads with user defined stacks. */ stack_list_del (&amp;pd-&gt;list); /* Not much to do. Just free the mmap()ed memory. Note that we do not reset the &#x27;used&#x27; flag in the &#x27;tid&#x27; field. This is done by the kernel. If no thread has been created yet this field is still zero. */ if (__glibc_likely (! pd-&gt;user_stack)) (void) queue_stack (pd);&#125; ARCH_CLONE其实调用的是 __clone()。# define ARCH_CLONE __clone/* The userland implementation is: int clone (int (*fn)(void *arg), void *child_stack, int flags, void *arg), the kernel entry is: int clone (long flags, void *child_stack). The parameters are passed in register and on the stack from userland: rdi: fn rsi: child_stack rdx: flags rcx: arg r8d: TID field in parent r9d: thread pointer%esp+8: TID field in child The kernel expects: rax: system call number rdi: flags rsi: child_stack rdx: TID field in parent r10: TID field in child r8: thread pointer */ .textENTRY (__clone) movq $-EINVAL,%rax...... /* Insert the argument onto the new stack. */ subq $16,%rsi movq %rcx,8(%rsi) /* Save the function pointer. It will be popped off in the child in the ebx frobbing below. */ movq %rdi,0(%rsi) /* Do the system call. */ movq %rdx, %rdi movq %r8, %rdx movq %r9, %r8 mov 8(%rsp), %R10_LP movl $SYS_ify(clone),%eax...... syscall......PSEUDO_END (__clone) 内核中的clone()定义如下。如果在进程的主线程里面调用其他系统调用，当前用户态的栈是指向整个进程的栈，栈顶指针也是指向进程的栈，指令指针也是指向进程的主线程的代码。此时此刻执行到这里，调用 clone的时候，用户态的栈、栈顶指针、指令指针和其他系统调用一样，都是指向主线程的。但是对于线程来说，这些都要变。因为我们希望当 clone 这个系统调用成功的时候，除了内核里面有这个线程对应的 task_struct，当系统调用返回到用户态的时候，用户态的栈应该是线程的栈，栈顶指针应该指向线程的栈，指令指针应该指向线程将要执行的那个函数。所以这些都需要我们自己做，将线程要执行的函数的参数和指令的位置都压到栈里面，当从内核返回，从栈里弹出来的时候，就从这个函数开始，带着这些参数执行下去。 1234567SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp, int __user *, parent_tidptr, int __user *, child_tidptr, unsigned long, tls)&#123; return _do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr, tls);&#125; 线程和进程到了这里殊途同归，进入了同一个函数__do_fork()工作。其源码如下所示，主要工作包括复制结构copy_process()和唤醒新进程wak_up_new()两部分。其中线程会根据create_thread()函数中的clone_flags完成上文所述的栈顶指针和指令指针的切换，以及一些线程和进程的微妙区别。 123456789101112131415161718192021222324252627282930313233long _do_fork(unsigned long clone_flags, unsigned long stack_start, unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr, unsigned long tls)&#123; struct task_struct *p; int trace = 0; long nr;...... p = copy_process(clone_flags, stack_start, stack_size, child_tidptr, NULL, trace, tls, NUMA_NO_NODE);...... if (IS_ERR(p)) return PTR_ERR(p); struct pid *pid; pid = get_task_pid(p, PIDTYPE_PID); nr = pid_vnr(pid); if (clone_flags &amp; CLONE_PARENT_SETTID) put_user(nr, parent_tidptr); if (clone_flags &amp; CLONE_VFORK) &#123; p-&gt;vfork_done = &amp;vfork; init_completion(&amp;vfork); get_task_struct(p); &#125; wake_up_new_task(p);...... put_pid(pid); return nr;&#125;; 4.1 任务结构体复制 如下所示为copy_process()函数源码精简版，task_struct结构复杂也注定了复制过程的复杂性，因此此处省略了很多，仅保留了各个部分的主要调用函数 1234567891011121314151617181920212223242526272829303132333435363738394041static __latent_entropy struct task_struct *copy_process( unsigned long clone_flags, unsigned long stack_start, unsigned long stack_size, int __user *child_tidptr, struct pid *pid, int trace, unsigned long tls, int node)&#123; int retval; struct task_struct *p;...... //分配task_struct结构 p = dup_task_struct(current, node); ...... //权限处理 retval = copy_creds(p, clone_flags);...... //设置调度相关变量 retval = sched_fork(clone_flags, p); ...... //初始化文件和文件系统相关变量 retval = copy_files(clone_flags, p); retval = copy_fs(clone_flags, p); ...... //初始化信号相关变量 init_sigpending(&amp;p-&gt;pending); retval = copy_sighand(clone_flags, p); retval = copy_signal(clone_flags, p); ...... //拷贝进程内存空间 retval = copy_mm(clone_flags, p);...... //初始化亲缘关系变量 INIT_LIST_HEAD(&amp;p-&gt;children); INIT_LIST_HEAD(&amp;p-&gt;sibling);...... //建立亲缘关系 //源码放在后面说明 &#125;; 1、copy_process()首先调用了dup_task_struct()分配task_struct结构，dup_task_struct() 主要做了下面几件事情： 调用 alloc_task_struct_node 分配一个 task_struct结构； 调用 alloc_thread_stack_node 来创建内核栈，这里面调用 __vmalloc_node_range 分配一个连续的 THREAD_SIZE 的内存空间，赋值给 task_struct 的 void *stack成员变量； 调用 arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)，将 task_struct 进行复制，其实就是调用 memcpy； 调用setup_thread_stack设置 thread_info。 12345678910111213141516171819202122static struct task_struct *dup_task_struct(struct task_struct *orig, int node)&#123; struct task_struct *tsk; unsigned long *stack;...... tsk = alloc_task_struct_node(node); if (!tsk) return NULL; stack = alloc_thread_stack_node(tsk, node); if (!stack) goto free_tsk; if (memcg_charge_kernel_stack(tsk)) goto free_stack; stack_vm_area = task_stack_vm_area(tsk); err = arch_dup_task_struct(tsk, orig);...... setup_thread_stack(tsk, orig);...... &#125;; 2、接着，调用copy_creds处理权限相关内容 调用prepare_creds，准备一个新的 struct cred *new。如何准备呢？其实还是从内存中分配一个新的 struct cred结构，然后调用 memcpy 复制一份父进程的 cred； 接着 p-&gt;cred &#x3D; p-&gt;real_cred &#x3D; get_cred(new)，将新进程的“我能操作谁”和“谁能操作我”两个权限都指向新的 cred。 123456789101112131415161718192021222324/* * Copy credentials for the new process created by fork() * * We share if we can, but under some circumstances we have to generate a new * set. * * The new process gets the current process&#x27;s subjective credentials as its * objective and subjective credentials */int copy_creds(struct task_struct *p, unsigned long clone_flags)&#123; struct cred *new; int ret;...... new = prepare_creds(); if (!new) return -ENOMEM;...... atomic_inc(&amp;new-&gt;user-&gt;processes); p-&gt;cred = p-&gt;real_cred = get_cred(new); alter_cred_subscribers(new, 2); validate_creds(new); return 0;&#125; 3、设置调度相关的变量。该部分源码先不展示，会在进程调度中详细介绍。sched_fork主要做了下面几件事情： 调用__sched_fork，在这里面将on_rq设为 0，初始化sched_entity，将里面的 exec_start、sum_exec_runtime、prev_sum_exec_runtime、vruntime 都设为 0。这几个变量涉及进程的实际运行时间和虚拟运行时间。是否到时间应该被调度了，就靠它们几个； 设置进程的状态 p-&gt;state &#x3D; TASK_NEW； 初始化优先级 prio、normal_prio、static_prio； 设置调度类，如果是普通进程，就设置为 p-&gt;sched_class &#x3D; &amp;fair_sched_class； 调用调度类的 task_fork 函数，对于 CFS 来讲，就是调用 task_fork_fair。在这个函数里，先调用 update_curr，对于当前的进程进行统计量更新，然后把子进程和父进程的 vruntime 设成一样，最后调用 place_entity，初始化 sched_entity。这里有一个变量 sysctl_sched_child_runs_first，可以设置父进程和子进程谁先运行。如果设置了子进程先运行，即便两个进程的 vruntime 一样，也要把子进程的 sched_entity 放在前面，然后调用 resched_curr，标记当前运行的进程 TIF_NEED_RESCHED，也就是说，把父进程设置为应该被调度，这样下次调度的时候，父进程会被子进程抢占。 4、初始化文件和文件系统相关变量 copy_files 主要用于复制一个任务打开的文件信息。 对于进程来说，这些信息用一个结构 files_struct 来维护，每个打开的文件都有一个文件描述符。在 copy_files 函数里面调用 dup_fd，在这里面会创建一个新的 files_struct，然后将所有的文件描述符数组 fdtable 拷贝一份。 对于线程来说，由于设置了CLONE_FILES 标识位变成将原来的files_struct 引用计数加一，并不会拷贝文件。 123456789101112131415161718192021222324static int copy_files(unsigned long clone_flags, struct task_struct *tsk)&#123; struct files_struct *oldf, *newf; int error = 0; /* * A background process may not have any files ... */ oldf = current-&gt;files; if (!oldf) goto out; if (clone_flags &amp; CLONE_FILES) &#123; atomic_inc(&amp;oldf-&gt;count); goto out; &#125; newf = dup_fd(oldf, &amp;error); if (!newf) goto out; tsk-&gt;files = newf; error = 0;out: return error;&#125; copy_fs 主要用于复制一个任务的目录信息。 对于进程来说，这些信息用一个结构 fs_struct 来维护。一个进程有自己的根目录和根文件系统 root，也有当前目录 pwd 和当前目录的文件系统，都在 fs_struct 里面维护。copy_fs 函数里面调用 copy_fs_struct，创建一个新的 fs_struct，并复制原来进程的 fs_struct。 对于线程来说，由于设置了CLONE_FS 标识位变成将原来的fs_struct 的用户数加一，并不会拷贝文件系统结构。 12345678910111213141516171819static int copy_fs(unsigned long clone_flags, struct task_struct *tsk)&#123; struct fs_struct *fs = current-&gt;fs; if (clone_flags &amp; CLONE_FS) &#123; /* tsk-&gt;fs is already what we want */ spin_lock(&amp;fs-&gt;lock); if (fs-&gt;in_exec) &#123; spin_unlock(&amp;fs-&gt;lock); return -EAGAIN; &#125; fs-&gt;users++; spin_unlock(&amp;fs-&gt;lock); return 0; &#125; tsk-&gt;fs = copy_fs_struct(fs); if (!tsk-&gt;fs) return -ENOMEM; return 0;&#125; 5、初始化信号相关变量 整个进程里的所有线程共享一个shared_pending，这也是一个信号列表，是发给整个进程的，哪个线程处理都一样。由此我们可以做到发给进程的信号虽然可以被一个线程处理，但是影响范围应该是整个进程的。例如，kill 一个进程，则所有线程都要被干掉。如果一个信号是发给一个线程的 pthread_kill，则应该只有线程能够收到。 copy_sighand 对于进程来说，会分配一个新的 sighand_struct。这里最主要的是维护信号处理函数，在 copy_sighand 里面会调用 memcpy，将信号处理函数 sighand-&gt;action 从父进程复制到子进程。 对于线程来说，由于设计了CLONE_SIGHAND标记位，会对引用计数加一并退出，没有分配新的信号变量。 1234567891011121314151617static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)&#123; struct sighand_struct *sig; if (clone_flags &amp; CLONE_SIGHAND) &#123; refcount_inc(&amp;current-&gt;sighand-&gt;count); return 0; &#125; sig = kmem_cache_alloc(sighand_cachep, GFP_KERNEL); rcu_assign_pointer(tsk-&gt;sighand, sig); if (!sig) return -ENOMEM; refcount_set(&amp;sig-&gt;count, 1); spin_lock_irq(&amp;current-&gt;sighand-&gt;siglock); memcpy(sig-&gt;action, current-&gt;sighand-&gt;action, sizeof(sig-&gt;action)); spin_unlock_irq(&amp;current-&gt;sighand-&gt;siglock); return 0;&#125; init_sigpending 和 copy_signal 用于初始化信号结构体，并且复制用于维护发给这个进程的信号的数据结构。copy_signal 函数会分配一个新的 signal_struct，并进行初始化。对于线程来说也是直接退出并未复制。 123456789101112131415161718static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)&#123; struct signal_struct *sig; if (clone_flags &amp; CLONE_THREAD) return 0; sig = kmem_cache_zalloc(signal_cachep, GFP_KERNEL);...... /* list_add(thread_node, thread_head) without INIT_LIST_HEAD() */ sig-&gt;thread_head = (struct list_head)LIST_HEAD_INIT(tsk-&gt;thread_node); tsk-&gt;thread_node = (struct list_head)LIST_HEAD_INIT(sig-&gt;thread_head); init_waitqueue_head(&amp;sig-&gt;wait_chldexit); sig-&gt;curr_target = tsk; init_sigpending(&amp;sig-&gt;shared_pending); INIT_HLIST_HEAD(&amp;sig-&gt;multiprocess); seqlock_init(&amp;sig-&gt;stats_lock); prev_cputime_init(&amp;sig-&gt;prev_cputime);......&#125;; 6、复制进程内存空间 进程都有自己的内存空间，用 mm_struct 结构来表示。copy_mm() 函数中调用 dup_mm()，分配一个新的 mm_struct 结构，调用 memcpy 复制这个结构。dup_mmap() 用于复制内存空间中内存映射的部分。前面讲系统调用的时候，我们说过，mmap 可以分配大块的内存，其实 mmap 也可以将一个文件映射到内存中，方便可以像读写内存一样读写文件，这个在内存管理那节我们讲。 线程不会复制内存空间，因此因为CLONE_VM标识位而直接指向了原来的mm_struct。 123456789101112131415161718192021222324252627282930static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)&#123; struct mm_struct *mm, *oldmm; int retval;...... /* * Are we cloning a kernel thread? * We need to steal a active VM for that.. */ oldmm = current-&gt;mm; if (!oldmm) return 0; /* initialize the new vmacache entries */ vmacache_flush(tsk); if (clone_flags &amp; CLONE_VM) &#123; mmget(oldmm); mm = oldmm; goto good_mm; &#125; retval = -ENOMEM; mm = dup_mm(tsk); if (!mm) goto fail_nomem;good_mm: tsk-&gt;mm = mm; tsk-&gt;active_mm = mm; return 0;fail_nomem: return retval;&#125; 7、分配 pid，设置 tid，group_leader，并且建立任务之间的亲缘关系。 group_leader：进程的话 group_leader就是它自己，和旧进程分开。线程的话则设置为当前进程的group_leader。 tgid: 对进程来说是自己的pid，对线程来说是当前进程的pid real_parent : 对进程来说即当前进程，对线程来说则是当前进程的real_parent 12345678910111213141516171819202122232425static __latent_entropy struct task_struct *copy_process(......) &#123;...... p-&gt;pid = pid_nr(pid); if (clone_flags &amp; CLONE_THREAD) &#123; p-&gt;exit_signal = -1; p-&gt;group_leader = current-&gt;group_leader; p-&gt;tgid = current-&gt;tgid; &#125; else &#123; if (clone_flags &amp; CLONE_PARENT) p-&gt;exit_signal = current-&gt;group_leader-&gt;exit_signal; else p-&gt;exit_signal = (clone_flags &amp; CSIGNAL); p-&gt;group_leader = p; p-&gt;tgid = p-&gt;pid; &#125;...... if (clone_flags &amp; (CLONE_PARENT|CLONE_THREAD)) &#123; p-&gt;real_parent = current-&gt;real_parent; p-&gt;parent_exec_id = current-&gt;parent_exec_id; &#125; else &#123; p-&gt;real_parent = current; p-&gt;parent_exec_id = current-&gt;self_exec_id; &#125; ...... &#125;; 4.2 新进程的唤醒1234567891011121314_do_fork 做的第二件大事是通过调用 wake_up_new_task()唤醒进程。void wake_up_new_task(struct task_struct *p)&#123; struct rq_flags rf; struct rq *rq;...... p-&gt;state = TASK_RUNNING;...... activate_task(rq, p, ENQUEUE_NOCLOCK); trace_sched_wakeup_new(p); check_preempt_curr(rq, p, WF_FORK);......&#125; 首先，我们需要将进程的状态设置为 TASK_RUNNING。activate_task() 函数中会调用 enqueue_task()。 123456789101112131415void activate_task(struct rq *rq, struct task_struct *p, int flags)&#123; if (task_contributes_to_load(p)) rq-&gt;nr_uninterruptible--; enqueue_task(rq, p, flags); p-&gt;on_rq = TASK_ON_RQ_QUEUED;&#125;static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags)&#123;..... p-&gt;sched_class-&gt;enqueue_task(rq, p, flags);&#125; 如果是 CFS 的调度类，则执行相应的 enqueue_task_fair()。在 enqueue_task_fair() 中取出的队列就是 cfs_rq，然后调用 enqueue_entity()。在 enqueue_entity() 函数里面，会调用 update_curr()，更新运行的统计量，然后调用 __enqueue_entity，将 sched_entity 加入到红黑树里面，然后将 se-&gt;on_rq &#x3D; 1 设置在队列上。回到 enqueue_task_fair 后，将这个队列上运行的进程数目加一。然后，wake_up_new_task 会调用 check_preempt_curr，看是否能够抢占当前进程。 1234567891011121314151617181920212223static voidenqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)&#123; struct cfs_rq *cfs_rq; struct sched_entity *se = &amp;p-&gt;se;...... for_each_sched_entity(se) &#123; if (se-&gt;on_rq) break; cfs_rq = cfs_rq_of(se); enqueue_entity(cfs_rq, se, flags); cfs_rq-&gt;h_nr_running++; cfs_rq-&gt;idle_h_nr_running += idle_h_nr_running; /* end evaluation on encountering a throttled cfs_rq */ if (cfs_rq_throttled(cfs_rq)) goto enqueue_throttle; flags = ENQUEUE_WAKEUP; &#125;......&#125; 在 check_preempt_curr 中，会调用相应的调度类的 rq-&gt;curr-&gt;sched_class-&gt;check_preempt_curr(rq, p, flags)。对于CFS调度类来讲，调用的是 check_preempt_wakeup。在 check_preempt_wakeup函数中，前面调用 task_fork_fair的时候，设置 sysctl_sched_child_runs_first 了，已经将当前父进程的 TIF_NEED_RESCHED 设置了，则直接返回。否则，check_preempt_wakeup 还是会调用 update_curr 更新一次统计量，然后 wakeup_preempt_entity 将父进程和子进程 PK 一次，看是不是要抢占，如果要则调用 resched_curr 标记父进程为 TIF_NEED_RESCHED。如果新创建的进程应该抢占父进程，在什么时间抢占呢？别忘了 fork 是一个系统调用，从系统调用返回的时候，是抢占的一个好时机，如果父进程判断自己已经被设置为 TIF_NEED_RESCHED，就让子进程先跑，抢占自己。 12345678910111213141516171819static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)&#123; struct task_struct *curr = rq-&gt;curr; struct sched_entity *se = &amp;curr-&gt;se, *pse = &amp;p-&gt;se; struct cfs_rq *cfs_rq = task_cfs_rq(curr);...... if (test_tsk_need_resched(curr)) return;...... find_matching_se(&amp;se, &amp;pse); update_curr(cfs_rq_of(se)); if (wakeup_preempt_entity(se, pse) == 1) &#123; goto preempt; &#125; return;preempt: resched_curr(rq);......&#125; 至此，我们就完成了任务的整个创建过程，并根据情况唤醒任务开始执行。 五. 总结本文十分之长，因为内容极多，源码复杂，本来想拆分为两篇文章，但是又因为过于紧密的联系因此合在了一起。本文介绍了进程的创建和线程的创建，而多进程的派生因为使用和线程内核态创建一样的函数因此放在了一起变对比边说明。由此，进程、线程的结构体以及创建过程就全部分析完了，下文将继续分析进程、线程的调度。","tags":["Linux开发"],"categories":["linux"]},{"title":"进程的核心——task_truct","path":"/2023/01/15/linux-docs/进程管理/进程的核心——task_truct/","content":"一. 前言在前文中，我们分析了内核启动的整个过程以及系统调用的过程，从本文开始我们会介绍Linux系统各个重要的组成部分。这一切就从进程和线程开始，在 Linux 里面，无论是进程，还是线程，到了内核里面，我们统一都叫任务（Task），由一个统一的结构 task_struct 进行管理。这个结构非常复杂，本文将细细分析task_struct结构。主要分析顺序会按照该架构体中的成员变量和函数的作用进行分类，主要包括： 任务ID 亲缘关系 任务状态 任务权限 运行统计 进程调度 信号处理 内存管理 文件与文件系统 内核栈 二. 详细介绍2.1 任务ID任务ID是任务的唯一标识，在tast_struct中，主要涉及以下几个ID 123pid_t pid;pid_t tgid;struct task_struct *group_leader; 之所以有pid(process id)，tgid(thread group ID)以及group_leader，是因为线程和进程在内核中是统一管理，视为相同的任务（task）。 任何一个进程，如果只有主线程，那 pid 和tgid相同，group_leader 指向自己。但是，如果一个进程创建了其他线程，那就会有所变化了。线程有自己的pid，tgid 就是进程的主线程的 pid，group_leader 指向的进程的主线程。因此根据pid和tgid是否相等我们可以判断该任务是进程还是线程。 2.2 亲缘关系除了0号进程以外，其他进程都是有父进程的。全部进程其实就是一颗进程树，相关成员变量如下所示 1234struct task_struct __rcu *real_parent; /* real parent process */struct task_struct __rcu *parent; /* recipient of SIGCHLD, wait4() reports */struct list_head children; /* list of my children */struct list_head sibling; /* linkage in my parent&#x27;s children list */ parent 指向其父进程。当它终止时，必须向它的父进程发送信号。 children 指向子进程链表的头部。链表中的所有元素都是它的子进程。 sibling 用于把当前进程插入到兄弟链表中。 通常情况下，real_parent 和 parent 是一样的，但是也会有另外的情况存在。例如，bash 创建一个进程，那进程的 parent 和 real_parent 就都是 bash。如果在 bash 上使用 GDB 来 debug 一个进程，这个时候 GDB 是 parent，bash 是这个进程的 real_parent。 2.3 任务状态任务状态部分主要涉及以下变量 123volatile long state; /* -1 unrunnable, 0 runnable, &gt;0 stopped */int exit_state;unsigned int flags; 其中状态state通过设置比特位的方式来赋值，具体值在include&#x2F;linux&#x2F;sched.h中定义 1234567891011121314151617181920/* Used in tsk-&gt;state: */#define TASK_RUNNING 0#define TASK_INTERRUPTIBLE 1#define TASK_UNINTERRUPTIBLE 2#define __TASK_STOPPED 4#define __TASK_TRACED 8/* Used in tsk-&gt;exit_state: */#define EXIT_DEAD 16#define EXIT_ZOMBIE 32#define EXIT_TRACE (EXIT_ZOMBIE | EXIT_DEAD)/* Used in tsk-&gt;state again: */#define TASK_DEAD 64#define TASK_WAKEKILL 128#define TASK_WAKING 256#define TASK_PARKED 512#define TASK_NOLOAD 1024#define TASK_NEW 2048#define TASK_STATE_MAX 4096#define TASK_KILLABLE (TASK_WAKEKILL | TASK_UNINTERRUPTIBLE) TASK_RUNNING并不是说进程正在运行，而是表示进程在时刻准备运行的状态。当处于这个状态的进程获得时间片的时候，就是在运行中；如果没有获得时间片，就说明它被其他进程抢占了，在等待再次分配时间片。在运行中的进程，一旦要进行一些 I&#x2F;O 操作，需要等待 I&#x2F;O 完毕，这个时候会释放 CPU，进入睡眠状态。 在 Linux 中，有两种睡眠状态。 一种是 TASK_INTERRUPTIBLE，可中断的睡眠状态。这是一种浅睡眠的状态，也就是说，虽然在睡眠，等待 I&#x2F;O 完成，但是这个时候一个信号来的时候，进程还是要被唤醒。只不过唤醒后，不是继续刚才的操作，而是进行信号处理。当然程序员可以根据自己的意愿，来写信号处理函数，例如收到某些信号，就放弃等待这个 I&#x2F;O 操作完成，直接退出；或者收到某些信息，继续等待。 另一种睡眠是 TASK_UNINTERRUPTIBLE，不可中断的睡眠状态。这是一种深度睡眠状态，不可被信号唤醒，只能死等 I&#x2F;O 操作完成。一旦 I&#x2F;O 操作因为特殊原因不能完成，这个时候，谁也叫不醒这个进程了。你可能会说，我 kill 它呢？别忘了，kill 本身也是一个信号，既然这个状态不可被信号唤醒，kill 信号也被忽略了。除非重启电脑，没有其他办法。因此，这其实是一个比较危险的事情，除非程序员极其有把握，不然还是不要设置成 TASK_UNINTERRUPTIBLE。 于是，我们就有了一种新的进程睡眠状态，TASK_KILLABLE，可以终止的新睡眠状态。进程处于这种状态中，它的运行原理类似 TASK_UNINTERRUPTIBLE，只不过可以响应致命信号。由于TASK_WAKEKILL 用于在接收到致命信号时唤醒进程，因此TASK_KILLABLE即在TASK_UNINTERUPTIBLE的基础上增加一个TASK_WAKEKILL标记位即可。 TASK_STOPPED是在进程接收到 SIGSTOP、SIGTTIN、SIGTSTP或者 SIGTTOU 信号之后进入该状态。 TASK_TRACED 表示进程被 debugger 等进程监视，进程执行被调试程序所停止。当一个进程被另外的进程所监视，每一个信号都会让进程进入该状态。 一旦一个进程要结束，先进入的是 EXIT_ZOMBIE 状态，但是这个时候它的父进程还没有使用wait() 等系统调用来获知它的终止信息，此时进程就成了僵尸进程。EXIT_DEAD 是进程的最终状态。EXIT_ZOMBIE 和 EXIT_DEAD 也可以用于 exit_state。 上面的进程状态和进程的运行、调度有关系，还有其他的一些状态，我们称为标志。放在 flags字段中，这些字段都被定义成为宏，以 PF 开头。 123#define PF_EXITING 0x00000004#define PF_VCPU 0x00000010#define PF_FORKNOEXEC 0x00000040 PF_EXITING 表示正在退出。当有这个 flag 的时候，在函数 find_alive_thread() 中，找活着的线程，遇到有这个 flag 的，就直接跳过。 PF_VCPU 表示进程运行在虚拟 CPU 上。在函数 account_system_time中，统计进程的系统运行时间，如果有这个 flag，就调用 account_guest_time，按照客户机的时间进行统计。 PF_FORKNOEXEC 表示 fork 完了，还没有 exec。在 _do_fork ()函数里面调用 copy_process()，这个时候把 flag 设置为 PF_FORKNOEXEC()。当 exec 中调用了 load_elf_binary() 的时候，又把这个 flag 去掉。 2.4 任务权限任务权限主要包括以下两个变量，real_cred是指可以操作本任务的对象，而red是指本任务可以操作的对象。 1234/* Objective and real subjective task credentials (COW): */const struct cred __rcu *real_cred;/* Effective (overridable) subjective task credentials (COW): */const struct cred __rcu *cred; cred定义如下所示 123456789101112131415161718struct cred &#123;...... kuid_t uid; /* real UID of the task */ kgid_t gid; /* real GID of the task */ kuid_t suid; /* saved UID of the task */ kgid_t sgid; /* saved GID of the task */ kuid_t euid; /* effective UID of the task */ kgid_t egid; /* effective GID of the task */ kuid_t fsuid; /* UID for VFS ops */ kgid_t fsgid; /* GID for VFS ops */...... kernel_cap_t cap_inheritable; /* caps our children can inherit */ kernel_cap_t cap_permitted; /* caps we&#x27;re permitted */ kernel_cap_t cap_effective; /* caps we can actually use */ kernel_cap_t cap_bset; /* capability bounding set */ kernel_cap_t cap_ambient; /* Ambient capability set */......&#125; __randomize_layout; 从这里的定义可以看出，大部分是关于用户和用户所属的用户组信息。 uid和 gid，注释是 real user&#x2F;group id。一般情况下，谁启动的进程，就是谁的 ID。但是权限审核的时候，往往不比较这两个，也就是说不大起作用。 euid 和 egid，注释是 effective user&#x2F;group id。一看这个名字，就知道这个是起“作用”的。当这个进程要操作消息队列、共享内存、信号量等对象的时候，其实就是在比较这个用户和组是否有权限。 fsuid 和fsgid，也就是 filesystem user&#x2F;group id。这个是对文件操作会审核的权限。 在Linux中，我们可以通过chmod u+s program命令更改更改euid和fsuid来获取权限。 除了以用户和用户组控制权限，Linux 还有另一个机制就是 capabilities。 原来控制进程的权限，要么是高权限的 root 用户，要么是一般权限的普通用户，这时候的问题是，root 用户权限太大，而普通用户权限太小。有时候一个普通用户想做一点高权限的事情，必须给他整个 root 的权限。这个太不安全了。于是，我们引入新的机制 capabilities，用位图表示权限，在capability.h可以找到定义的权限。我这里列举几个。 12345678910#define CAP_CHOWN 0#define CAP_KILL 5#define CAP_NET_BIND_SERVICE 10#define CAP_NET_RAW 13#define CAP_SYS_MODULE 16#define CAP_SYS_RAWIO 17#define CAP_SYS_BOOT 22#define CAP_SYS_TIME 25#define CAP_AUDIT_READ 37#define CAP_LAST_CAP CAP_AUDIT_READ 对于普通用户运行的进程，当有这个权限的时候，就能做这些操作；没有的时候，就不能做，这样粒度要小很多。 2.5 运行统计运行统计从宏观来说也是一种状态变量，但是和任务状态不同，其存储的主要是运行时间相关的成员变量，具体如下所示 123456u64 utime;//用户态消耗的CPU时间u64 stime;//内核态消耗的CPU时间unsigned long nvcsw;//自愿(voluntary)上下文切换计数unsigned long nivcsw;//非自愿(involuntary)上下文切换计数u64 start_time;//进程启动时间，不包含睡眠时间u64 real_start_time;//进程启动时间，包含睡眠时间 2.6 进程调度进程调度部分较为复杂，会单独拆分讲解，这里先简单罗列成员变量。 12345678910111213141516171819//是否在运行队列上int on_rq;//优先级int prio;int static_prio;int normal_prio;unsigned int rt_priority;//调度器类const struct sched_class *sched_class;//调度实体struct sched_entity se;struct sched_rt_entity rt;struct sched_dl_entity dl;//调度策略unsigned int policy;//可以使用哪些CPUint nr_cpus_allowed;cpumask_t cpus_allowed;struct sched_info sched_info; 2.7 信号处理信号处理相关的数据结构如下所示 12345678910/* Signal handlers: */struct signal_struct *signal;struct sighand_struct *sighand;sigset_t blocked;sigset_t real_blocked;sigset_t saved_sigmask;struct sigpending pending;unsigned long sas_ss_sp;size_t sas_ss_size;unsigned int sas_ss_flags; 这里将信号分为三类 阻塞暂不处理的信号（blocked） 等待处理的信号（pending） 正在通过信号处理函数处理的信号（sighand） 信号处理函数默认使用用户态的函数栈，当然也可以开辟新的栈专门用于信号处理，这就是 sas_ss_xxx 这三个变量的作用。 2.8 内存管理内存管理部分成员变量如下所示 12struct mm_struct *mm;struct mm_struct *active_mm; 由于内存部分较为复杂，会放在后面单独介绍，这里了先不做详细说明。 2.9 文件与文件系统文件系统部分也会在后面详细说明，这里先简单列举成员变量 1234/* Filesystem information: */struct fs_struct *fs;/* Open file information: */struct files_struct *files; 2.10 内核栈内核栈相关的成员变量如下所示。为了介绍清楚其作用，我们需要从为什么需要内核栈开始逐步讨论。 12struct thread_info thread_info;void *stack; 当进程产生系统调用时，会利用中断陷入内核态。而内核态中也存在着各种函数的调用，因此我们需要有内核态函数栈。Linux 给每个 task 都分配了内核栈。在 32 位系统上 arch&#x2F;x86&#x2F;include&#x2F;asm&#x2F;page_32_types.h，是这样定义的：一个 PAGE_SIZE是 4K，左移一位就是乘以 2，也就是 8K。 12#define THREAD_SIZE_ORDER 1#define THREAD_SIZE (PAGE_SIZE &lt;&lt; THREAD_SIZE_ORDER) 内核栈在 64 位系统上 arch&#x2F;x86&#x2F;include&#x2F;asm&#x2F;page_64_types.h，是这样定义的：在 PAGE_SIZE 的基础上左移两位，也即 16K，并且要求起始地址必须是 8192 的整数倍。 12345678#ifdef CONFIG_KASAN#define KASAN_STACK_ORDER 1#else#define KASAN_STACK_ORDER 0#endif#define THREAD_SIZE_ORDER (2 + KASAN_STACK_ORDER)#define THREAD_SIZE (PAGE_SIZE &lt;&lt; THREAD_SIZE_ORDER) 内核栈的结构如下所示，首先是预留的8个字节，然后是存储寄存器，最后存储thread_info结构体。 这个结构是对 task_struct 结构的补充。因为 task_struct 结构庞大但是通用，不同的体系结构就需要保存不同的东西，所以往往与体系结构有关的，都放在 thread_info 里面。在内核代码里面采用一个 union将thread_info和stack 放在一起，在 include&#x2F;linux&#x2F;sched.h 中定义用以表示内核栈。由代码可见，这里根据架构不同可能采用旧版的task_struct直接放在内核栈，而新版的均采用thread_info，以节约空间。 123456789union thread_union &#123;#ifndef CONFIG_ARCH_TASK_STRUCT_ON_STACK struct task_struct task;#endif#ifndef CONFIG_THREAD_INFO_IN_TASK struct thread_info thread_info;#endif unsigned long stack[THREAD_SIZE/sizeof(long)];&#125;; 另一个结构 pt_regs，定义如下。其中，32 位和 64 位的定义不一样。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#ifdef __i386__struct pt_regs &#123; unsigned long bx; unsigned long cx; unsigned long dx; unsigned long si; unsigned long di; unsigned long bp; unsigned long ax; unsigned long ds; unsigned long es; unsigned long fs; unsigned long gs; unsigned long orig_ax; unsigned long ip; unsigned long cs; unsigned long flags; unsigned long sp; unsigned long ss;&#125;;#else struct pt_regs &#123; unsigned long r15; unsigned long r14; unsigned long r13; unsigned long r12; unsigned long bp; unsigned long bx; unsigned long r11; unsigned long r10; unsigned long r9; unsigned long r8; unsigned long ax; unsigned long cx; unsigned long dx; unsigned long si; unsigned long di; unsigned long orig_ax; unsigned long ip; unsigned long cs; unsigned long flags; unsigned long sp; unsigned long ss;/* top of stack page */&#125;;#endif 内核栈和task_struct是可以互相查找的，而这里就需要用到task_struct中的两个内核栈相关成员变量了。 2.10.1 通过task_struct查找内核栈如果有一个 task_struct 的 stack 指针在手，即可通过下面的函数找到这个线程内核栈： 1234static inline void *task_stack_page(const struct task_struct *task)&#123; return task-&gt;stack;&#125; 从 task_struct 如何得到相应的 pt_regs 呢？我们可以通过下面的函数，先从 task_struct找到内核栈的开始位置。然后这个位置加上 THREAD_SIZE 就到了最后的位置，然后转换为 struct pt_regs，再减一，就相当于减少了一个 pt_regs 的位置，就到了这个结构的首地址。 1234567891011121314151617/* * TOP_OF_KERNEL_STACK_PADDING reserves 8 bytes on top of the ring0 stack. * This is necessary to guarantee that the entire &quot;struct pt_regs&quot; * is accessible even if the CPU haven&#x27;t stored the SS/ESP registers * on the stack (interrupt gate does not save these registers * when switching to the same priv ring). * Therefore beware: accessing the ss/esp fields of the * &quot;struct pt_regs&quot; is possible, but they may contain the * completely wrong values. */#define task_pt_regs(task) \\(&#123; \\ unsigned long __ptr = (unsigned long)task_stack_page(task); \\ __ptr += THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING; \\ ((struct pt_regs *)__ptr) - 1; \\&#125;) 这里面有一个TOP_OF_KERNEL_STACK_PADDING，这个的定义如下： 123456789#ifdef CONFIG_X86_32# ifdef CONFIG_VM86# define TOP_OF_KERNEL_STACK_PADDING 16# else# define TOP_OF_KERNEL_STACK_PADDING 8# endif#else# define TOP_OF_KERNEL_STACK_PADDING 0#endif 也就是说，32 位机器上是 8，其他是 0。这是为什么呢？因为压栈 pt_regs 有两种情况。我们知道，CPU 用 ring 来区分权限，从而 Linux 可以区分内核态和用户态。因此，第一种情况，我们拿涉及从用户态到内核态的变化的系统调用来说。因为涉及权限的改变，会压栈保存 SS、ESP 寄存器的，这两个寄存器共占用 8 个 byte。另一种情况是，不涉及权限的变化，就不会压栈这 8 个 byte。这样就会使得两种情况不兼容。如果没有压栈还访问，就会报错，所以还不如预留在这里，保证安全。在 64 位上，修改了这个问题，变成了定长的。 2.10.2 通过内核栈找task_struct首先来看看thread_info的定义吧。下面所示为早期版本的thread_info和新版本thread_info的源码 12345678910111213struct thread_info &#123; struct task_struct *task; /* main task structure */ __u32 flags; /* low level flags */ __u32 status; /* thread synchronous flags */ __u32 cpu; /* current CPU */ mm_segment_t addr_limit; unsigned int sig_on_uaccess_error:1; unsigned int uaccess_err:1; /* uaccess failed */&#125;;struct thread_info &#123; unsigned long flags; /* low level flags */ unsigned long status; /* thread synchronous flags */ &#125;; 老版中采取current_thread_info()-&gt;task 来获取task_struct。thread_info 的位置就是内核栈的最高位置，减去 THREAD_SIZE，就到了 thread_info 的起始地址。 123456789static inline struct thread_info *current_thread_info(void)&#123; return (struct thread_info *)(current_top_of_stack() - THREAD_SIZE);&#125; 而新版本则采用了另一种current_thread_info#include &lt;asm/current.h&gt;#define current_thread_info() ((struct thread_info *)current)#endif 那 current 又是什么呢？在 arch&#x2F;x86&#x2F;include&#x2F;asm&#x2F;current.h 中定义了。 12345678910struct task_struct;DECLARE_PER_CPU(struct task_struct *, current_task);static __always_inline struct task_struct *get_current(void)&#123; return this_cpu_read_stable(current_task);&#125;#define current get_current 新的机制里面，每个 CPU 运行的 task_struct 不通过thread_info 获取了，而是直接放在 Per CPU 变量里面了。多核情况下，CPU 是同时运行的，但是它们共同使用其他的硬件资源的时候，我们需要解决多个 CPU 之间的同步问题。Per CPU 变量是内核中一种重要的同步机制。顾名思义，Per CPU 变量就是为每个 CPU 构造一个变量的副本，这样多个 CPU 各自操作自己的副本，互不干涉。比如，当前进程的变量 current_task 就被声明为 Per CPU 变量。要使用 Per CPU 变量，首先要声明这个变量，在 arch&#x2F;x86&#x2F;include&#x2F;asm&#x2F;current.h 中有： 1DECLARE_PER_CPU(struct task_struct *, current_task); 然后是定义这个变量，在 arch&#x2F;x86&#x2F;kernel&#x2F;cpu&#x2F;common.c 中有： 1DEFINE_PER_CPU(struct task_struct *, current_task) = &amp;init_task; 也就是说，系统刚刚初始化的时候，current_task 都指向init_task。当某个 CPU 上的进程进行切换的时候，current_task 被修改为将要切换到的目标进程。例如，进程切换函数__switch_to 就会改变 current_task。 12345678__visible __notrace_funcgraph struct task_struct *__switch_to(struct task_struct *prev_p, struct task_struct *next_p)&#123;...... this_cpu_write(current_task, next_p);...... return prev_p;&#125; 当要获取当前的运行中的 task_struct 的时候，就需要调用 this_cpu_read_stable 进行读取。 #define this_cpu_read_stable(var) percpu_stable_op(&quot;mov&quot;, var) 通过这种方式，即可轻松的获得task_struct的地址。 三. 总结本文大体介绍了task_struct的整体结构，对于很多涉及到复杂模块的部分并未展开讲解，在后文中会一一叙述。","tags":["Linux开发"],"categories":["linux"]},{"title":"进程间通信之共享内存和信号量","path":"/2023/01/15/linux-docs/进程管理/进程间通信之共享内存和信号量/","content":"一. 前言本文为进程间通信的最后一篇，介绍共享内存和信号量。之所以将二者一起叙述，是因为二者有着密不可分的关系。共享内存会利用虚拟内存和物理内存的映射关系，让不同进程开辟一块虚拟空间映射到相同的物理内存上，从而实现了两个进程对相同区域的读写，即进程间通信。而信号量则实现了互斥锁，可以为共享内存提供数据一致性的保证，因此二者常结合使用。 二. 基础知识共享内存的使用包括 调用shmget()创建共享内存 调用shmat()映射共享内存至进程虚拟空间 调用shmdt()接触映射关系 信号量有着类似的操作 调用semget()创建信号量集合。 调用semctl()，信号量往往代表某种资源的数量，如果用信号量做互斥，那往往将信号量设置为 1。 调用semop()修改信号量数目，即加锁和解锁之用 整体通信过程可用如下生产者消费者的模式图来理解。 三. 统一封装的接口消息队列、共享内存和信号量有着统一的封装和管理机制，为此我们提供了对应的名字空间和ipc_ids结构体。根据代码中的定义，第 0 项用于信号量，第 1 项用于消息队列，第 2 项用于共享内存，分别可以通过 sem_ids、msg_ids、shm_ids 来访问。ipc_ids中in_use 表示当前有多少个 ipc，seq 和 next_id 用于一起生成 ipc 唯一的 id，ipcs_idr 是一棵基数树，一旦涉及从一个整数查找一个对象它都是最好的选择。 12345678910111213141516171819202122232425262728struct ipc_namespace &#123;...... struct ipc_ids\tids[3];......&#125; __randomize_layout;#define IPC_SEM_IDS 0#define IPC_MSG_IDS 1#define IPC_SHM_IDS 2#define sem_ids(ns) ((ns)-&gt;ids[IPC_SEM_IDS])#define msg_ids(ns) ((ns)-&gt;ids[IPC_MSG_IDS])#define shm_ids(ns) ((ns)-&gt;ids[IPC_SHM_IDS])struct ipc_ids &#123; int in_use; unsigned short seq; struct rw_semaphore rwsem; struct idr ipcs_idr; int max_idx; struct rhashtable key_ht;&#125;;struct idr &#123; struct radix_tree_root\tidr_rt; unsigned int idr_base; unsigned int idr_next;&#125;; 信号量、消息队列、共享内存的通过基数树来管理各自的对象，三种ipc对应的结构体中第一项均为struct kern_ipc_perm，该结构体对应的id会存储在基数树之中，可以通过ipc_obtain_object_idr()获取。 123456789101112131415161718192021222324252627282930313233343536373839404142struct sem_array &#123; struct kern_ipc_perm sem_perm; /* permissions .. see ipc.h */......&#125; __randomize_layout;struct msg_queue &#123; struct kern_ipc_perm q_perm;......&#125; __randomize_layout;struct shmid_kernel /* private to the kernel */&#123; struct kern_ipc_perm shm_perm;......&#125; __randomize_layout;struct kern_ipc_perm *ipc_obtain_object_idr(struct ipc_ids *ids, int id)&#123; struct kern_ipc_perm *out; int lid = ipcid_to_idx(id); out = idr_find(&amp;ids-&gt;ipcs_idr, lid); return out;&#125; 对于这三种不同的通信方式，会对ipc_obtain_object_idr()进行封装static inline struct sem_array *sem_obtain_object(struct ipc_namespace *ns, int id)&#123; struct kern_ipc_perm *ipcp = ipc_obtain_object_idr(&amp;sem_ids(ns), id); return container_of(ipcp, struct sem_array, sem_perm);&#125;static inline struct msg_queue *msq_obtain_object(struct ipc_namespace *ns, int id)&#123; struct kern_ipc_perm *ipcp = ipc_obtain_object_idr(&amp;msg_ids(ns), id); return container_of(ipcp, struct msg_queue, q_perm);&#125;static inline struct shmid_kernel *shm_obtain_object(struct ipc_namespace *ns, int id)&#123; struct kern_ipc_perm *ipcp = ipc_obtain_object_idr(&amp;shm_ids(ns), id); return container_of(ipcp, struct shmid_kernel, shm_perm);&#125; 由此，我们实现了对这三种进程间通信方式统一的封装抽象。首先用名字空间存储三种ipc，然后对应的ipc_ids会描述该通信方式的特点，并包含一个基数树存储id从而找到其实际运行的多个通信的结构体。 四. 共享内存的创建和映射4.1 创建共享内存共享内存的创建通过shmget()实现。该函数创建对应的ipc_namespaace指针并指向该进程的ipc_ns，初始化共享内存对应的操作shm_ops，并将传参key, size, shmflg封装为传参shm_params，最终调用ipcget()。 12345678910111213141516171819202122232425SYSCALL_DEFINE3(shmget, key_t, key, size_t, size, int, shmflg)&#123; struct ipc_namespace *ns; static const struct ipc_ops shm_ops = &#123; .getnew = newseg, .associate = shm_security, .more_checks = shm_more_checks, &#125;; struct ipc_params shm_params; ns = current-&gt;nsproxy-&gt;ipc_ns; shm_params.key = key; shm_params.flg = shmflg; shm_params.u.size = size; return ipcget(ns, &amp;shm_ids(ns), &amp;shm_ops, &amp;shm_params);&#125; ipcget()会根据传参key的类型是否是IPC_PRIVATE选择调用ipcget_new()创建或者调用ipcget_public()打开对应的共享内存。int ipcget(struct ipc_namespace *ns, struct ipc_ids *ids, const struct ipc_ops *ops, struct ipc_params *params)&#123; if (params-&gt;key == IPC_PRIVATE) return ipcget_new(ns, ids, ops, params); else return ipcget_public(ns, ids, ops, params);&#125; ipcget_new()会根据定义的ops-&gt;getnew()创建新的ipc对象，即上面定义的newseg()。ipcget_public()会按照 key查找 struct kern_ipc_perm。如果没有找到，那就看是否设置了 IPC_CREAT，如果设置了，就调用ops-&gt;getnew()创建一个新的，否则返回错误ENOENT。如果找到了，就将对应的 id 返回。 123456789101112131415161718192021222324252627282930313233343536373839404142static int ipcget_new(struct ipc_namespace *ns, struct ipc_ids *ids, const struct ipc_ops *ops, struct ipc_params *params)&#123; int err; down_write(&amp;ids-&gt;rwsem); err = ops-&gt;getnew(ns, params); up_write(&amp;ids-&gt;rwsem); return err;&#125;static int ipcget_public(struct ipc_namespace *ns, struct ipc_ids *ids, const struct ipc_ops *ops, struct ipc_params *params)&#123; struct kern_ipc_perm *ipcp; int flg = params-&gt;flg; int err; /* * Take the lock as a writer since we are potentially going to add * a new entry + read locks are not &quot;upgradable&quot; */ down_write(&amp;ids-&gt;rwsem); ipcp = ipc_findkey(ids, params-&gt;key); if (ipcp == NULL) &#123; /* key not used */ if (!(flg &amp; IPC_CREAT)) err = -ENOENT; else err = ops-&gt;getnew(ns, params); &#125; else &#123;...... if (!err) /* * ipc_check_perms returns the IPC id on * success */ err = ipc_check_perms(ns, ipcp, ops, params); &#125; ipc_unlock(ipcp); &#125; up_write(&amp;ids-&gt;rwsem); return err;&#125; 所以新的创建最后都会走到注册的newseg()函数。该函数主要逻辑为 通过 kvmalloc() 在直接映射区分配一个 struct shmid_kernel 结构体，该结构体用于描述共享内存。 调用hugetlb_file_setup()或shmem_kernel_file_setup()关联文件。虚拟地址空间可以和物理内存关联，但是页表的申请条件中会避开已分配的映射，即物理内存是某个进程独享的。所以如何实现物理内存向多个进程的虚拟内存映射呢？这里就要靠文件来实现了：虚拟地址空间也可以映射到一个文件，文件是可以跨进程共享的。这里我们并不是映射到硬盘上存储的文件，而是映射到内存文件系统上的文件。这里定要注意区分 shmem 和 shm ，前者是一个文件系统，后者是进程通信机制。 通过 ipc_addid() 将新创建的 struct shmid_kernel 结构挂到 shm_ids 里面的基数树上，返回相应的 id，并且将 struct shmid_kernel 挂到当前进程的 sysvshm 队列中。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * newseg - Create a new shared memory segment * @ns: namespace * @params: ptr to the structure that contains key, size and shmflg * * Called with shm_ids.rwsem held as a writer. */static int newseg(struct ipc_namespace *ns, struct ipc_params *params)&#123; key_t key = params-&gt;key; int shmflg = params-&gt;flg; size_t size = params-&gt;u.size; int error; struct shmid_kernel *shp; size_t numpages = (size + PAGE_SIZE - 1) &gt;&gt; PAGE_SHIFT; struct file *file; char name[13];...... shp = kvmalloc(sizeof(*shp), GFP_KERNEL);...... shp-&gt;shm_perm.key = key; shp-&gt;shm_perm.mode = (shmflg &amp; S_IRWXUGO); shp-&gt;mlock_user = NULL; shp-&gt;shm_perm.security = NULL;...... if (shmflg &amp; SHM_HUGETLB) &#123;...... file = hugetlb_file_setup(name, hugesize, acctflag, &amp;shp-&gt;mlock_user, HUGETLB_SHMFS_INODE, (shmflg &gt;&gt; SHM_HUGE_SHIFT) &amp; SHM_HUGE_MASK); &#125; else &#123;...... file = shmem_kernel_file_setup(name, size, acctflag); &#125;...... shp-&gt;shm_cprid = get_pid(task_tgid(current)); shp-&gt;shm_lprid = NULL; shp-&gt;shm_atim = shp-&gt;shm_dtim = 0; shp-&gt;shm_ctim = ktime_get_real_seconds(); shp-&gt;shm_segsz = size; shp-&gt;shm_nattch = 0; shp-&gt;shm_file = file; shp-&gt;shm_creator = current; /* ipc_addid() locks shp upon success. */ error = ipc_addid(&amp;shm_ids(ns), &amp;shp-&gt;shm_perm, ns-&gt;shm_ctlmni);...... list_add(&amp;shp-&gt;shm_clist, &amp;current-&gt;sysvshm.shm_clist); /* * shmid gets reported as &quot;inode#&quot; in /proc/pid/maps. * proc-ps tools use this. Changing this will break them. */ file_inode(file)-&gt;i_ino = shp-&gt;shm_perm.id; ns-&gt;shm_tot += numpages; error = shp-&gt;shm_perm.id;......&#125; 实际上shmem_kernel_file_setup()会在shmem文件系统里面创建一个文件：__shmem_file_setup() 会创建新的 shmem 文件对应的 dentry 和 inode，并将它们两个关联起来，然后分配一个 struct file 结构来表示新的 shmem 文件，并且指向独特的 shmem_file_operations。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156/** * shmem_kernel_file_setup - get an unlinked file living in tmpfs which must be kernel internal. * @name: name for dentry (to be seen in /proc/&lt;pid&gt;/maps * @size: size to be set for the file * @flags: VM_NORESERVE suppresses pre-accounting of the entire object size */struct file *shmem_kernel_file_setup(const char *name, loff_t size, unsigned long flags)&#123; return __shmem_file_setup(name, size, flags, S_PRIVATE);&#125;static struct file *__shmem_file_setup(const char *name, loff_t size, unsigned long flags, unsigned int i_flags)&#123; struct file *res; struct inode *inode; struct path path; struct super_block *sb; struct qstr this;...... this.name = name; this.len = strlen(name); this.hash = 0; /* will go */ sb = shm_mnt-&gt;mnt_sb; path.mnt = mntget(shm_mnt); path.dentry = d_alloc_pseudo(sb, &amp;this); d_set_d_op(path.dentry, &amp;anon_ops);...... inode = shmem_get_inode(sb, NULL, S_IFREG | S_IRWXUGO, 0, flags); inode-&gt;i_flags |= i_flags; d_instantiate(path.dentry, inode); inode-&gt;i_size = size;...... res = alloc_file(&amp;path, FMODE_WRITE | FMODE_READ, &amp;shmem_file_operations); return res;&#125;``` ### 4.2 共享内存的映射 从上面的代码解析中我们知道，共享内存的数据结构 struct shmid_kernel通过它的成员 struct file *shm_file来管理内存文件系统 shmem 上的内存文件。无论这个共享内存是否被映射，shm_file 都是存在的。对于用户来说，共享内存的映射通过调用shmat()完成。该函数主要逻辑为：* 调用shm_obtain_object_check()通过共享内存的 id，在基数树中找到对应的 struct shmid_kernel 结构，通过它找到 shmem 上的内存文件base。* 分配结构体struct shm_file_data sfd表示该内存文件base。* 创建base的备份文件file，指向该内存文件base，并将private_data保存为sfd。在源码中注释部分已经叙述了为什么要再创建一个文件而不是直接使用base，简而言之就是base是共享内存文件系统shmem中的shm_file，用于管理内存文件，是一个中立、独立于任何一个进程的文件。新创建的 struct file 则专门用于做内存映射。* 调用do_mmap_pgoff()，分配vm_area_struct指向虚拟地址空间中未分配区域，其vm_file指向文件file，接着调用shm_file_operations中的mmap()函数，即shm_mmap()完成映射。```cSYSCALL_DEFINE3(shmat, int, shmid, char __user *, shmaddr, int, shmflg)&#123; unsigned long ret; long err; err = do_shmat(shmid, shmaddr, shmflg, &amp;ret, SHMLBA); force_successful_syscall_return(); return (long)ret;&#125;long do_shmat(int shmid, char __user *shmaddr, int shmflg, ulong *raddr, unsigned long shmlba)&#123; struct shmid_kernel *shp; unsigned long addr = (unsigned long)shmaddr; unsigned long size; struct file *file, *base; int err; unsigned long flags = MAP_SHARED; unsigned long prot; int acc_mode; struct ipc_namespace *ns; struct shm_file_data *sfd; int f_flags; unsigned long populate = 0;...... if (shmflg &amp; SHM_RDONLY) &#123; prot = PROT_READ; acc_mode = S_IRUGO; f_flags = O_RDONLY; &#125; else &#123; prot = PROT_READ | PROT_WRITE; acc_mode = S_IRUGO | S_IWUGO; f_flags = O_RDWR; &#125; if (shmflg &amp; SHM_EXEC) &#123; prot |= PROT_EXEC; acc_mode |= S_IXUGO; &#125; /* * We cannot rely on the fs check since SYSV IPC does have an * additional creator id... */ ns = current-&gt;nsproxy-&gt;ipc_ns; shp = shm_obtain_object_check(ns, shmid);...... /* * We need to take a reference to the real shm file to prevent the * pointer from becoming stale in cases where the lifetime of the outer * file extends beyond that of the shm segment. It&#x27;s not usually * possible, but it can happen during remap_file_pages() emulation as * that unmaps the memory, then does -&gt;mmap() via file reference only. * We&#x27;ll deny the -&gt;mmap() if the shm segment was since removed, but to * detect shm ID reuse we need to compare the file pointers. */ base = get_file(shp-&gt;shm_file); shp-&gt;shm_nattch++; size = i_size_read(file_inode(base)); ipc_unlock_object(&amp;shp-&gt;shm_perm); rcu_read_unlock(); err = -ENOMEM; sfd = kzalloc(sizeof(*sfd), GFP_KERNEL);...... file = alloc_file_clone(base, f_flags, is_file_hugepages(base) ? &amp;shm_file_operations_huge : &amp;shm_file_operations);...... sfd-&gt;id = shp-&gt;shm_perm.id; sfd-&gt;ns = get_ipc_ns(ns); sfd-&gt;file = base; sfd-&gt;vm_ops = NULL; file-&gt;private_data = sfd;...... addr = do_mmap_pgoff(file, addr, size, prot, flags, 0, &amp;populate, NULL); *raddr = addr; err = 0;......&#125; shm_mmap() 中调用了 shm_file_data 中的 file 的 mmap() 函数，这次调用的是 shmem_file_operations 的 mmap，也即 shmem_mmap()。static int shm_mmap(struct file *file, struct vm_area_struct *vma)&#123; struct shm_file_data *sfd = shm_file_data(file); int ret; /* * In case of remap_file_pages() emulation, the file can represent an * IPC ID that was removed, and possibly even reused by another shm * segment already. Propagate this case as an error to caller. */ ret = __shm_open(vma); if (ret) return ret; ret = call_mmap(sfd-&gt;file, vma); if (ret) &#123; shm_close(vma); return ret; &#125; sfd-&gt;vm_ops = vma-&gt;vm_ops; vma-&gt;vm_ops = &amp;shm_vm_ops; return 0;&#125;static int shmem_mmap(struct file *file, struct vm_area_struct *vma)&#123; file_accessed(file); vma-&gt;vm_ops = &amp;shmem_vm_ops; return 0;&#125; 这里vm_area_struct 的 vm_ops 指向 shmem_vm_ops。等从 call_mmap() 中返回之后，shm_file_data 的 vm_ops 指向了 shmem_vm_ops，而 vm_area_struct 的 vm_ops 改为指向 shm_vm_ops。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152static const struct vm_operations_struct shm_vm_ops = &#123; .open = shm_open, /* callback for a new vm-area open */ .close = shm_close, /* callback for when the vm-area is released */ .fault = shm_fault,&#125;;static const struct vm_operations_struct shmem_vm_ops = &#123; .fault = shmem_fault, .map_pages = filemap_map_pages,&#125;;``` 在前文内存映射中，我们提到了实际物理内存的分配不是在映射关系建立时就分配，而是当实际访问的时候通过缺页异常再进行分配。对于共享内存也是一样。当访问不到的时候，先调用 vm_area_struct 的 vm_ops，也即 shm_vm_ops 的 fault 函数 shm_fault()。然后它会转而调用 shm_file_data 的 vm_ops，也即 shmem_vm_ops 的 fault 函数 shmem_fault()。```cshmem_fault() 会调用 shmem_getpage_gfp() 在 page cache 和 swap 中找一个空闲页，如果找不到就通过 shmem_alloc_and_acct_page() 分配一个新的页，他最终会调用内存管理系统的 alloc_page_vma 在物理内存中分配一个页。static int shm_fault(struct vm_fault *vmf)&#123; struct file *file = vmf-&gt;vma-&gt;vm_file; struct shm_file_data *sfd = shm_file_data(file); return sfd-&gt;vm_ops-&gt;fault(vmf);&#125;static int shmem_fault(struct vm_fault *vmf)&#123; struct vm_area_struct *vma = vmf-&gt;vma; struct inode *inode = file_inode(vma-&gt;vm_file); gfp_t gfp = mapping_gfp_mask(inode-&gt;i_mapping);...... error = shmem_getpage_gfp(inode, vmf-&gt;pgoff, &amp;vmf-&gt;page, sgp, gfp, vma, vmf, &amp;ret);......&#125;/* * shmem_getpage_gfp - find page in cache, or get from swap, or allocate * * If we allocate a new one we do not mark it dirty. That&#x27;s up to the * vm. If we swap it in we mark it dirty since we also free the swap * entry since a page cannot live in both the swap and page cache. * * fault_mm and fault_type are only supplied by shmem_fault: * otherwise they are NULL. */static int shmem_getpage_gfp(struct inode *inode, pgoff_t index, struct page **pagep, enum sgp_type sgp, gfp_t gfp, struct vm_area_struct *vma, struct vm_fault *vmf, int *fault_type)&#123;...... page = shmem_alloc_and_acct_page(gfp, info, sbinfo, index, false);......&#125; 至此，共享内存才真的映射到了虚拟地址空间中，进程可以像访问本地内存一样访问共享内存。 五. 信号量的创建和使用5.1 信号量的创建信号量的创建和共享内存类似，实际调用semget()，操作也大同小异：创建对应的ipc_namespaace指针并指向该进程的ipc_ns，初始化共享内存对应的操作sem_ops，并将传参key, size, semflg封装为传参sem_params，最终调用ipcget()。 123456789101112131415SYSCALL_DEFINE3(semget, key_t, key, int, nsems, int, semflg)&#123; struct ipc_namespace *ns; static const struct ipc_ops sem_ops = &#123; .getnew = newary, .associate = sem_security, .more_checks = sem_more_checks, &#125;; struct ipc_params sem_params; ns = current-&gt;nsproxy-&gt;ipc_ns; sem_params.key = key; sem_params.flg = semflg; sem_params.u.nsems = nsems; return ipcget(ns, &amp;sem_ids(ns), &amp;sem_ops, &amp;sem_params);&#125; 共享内存最终走到newseg()函数，而信号量则调用newary()，该函数也有着类似的逻辑： 通过kvmalloc()在直接映射区分配struct sem_array结构体描述该信号量。在该结构体中会有多个信号量保存在struct sem sems[]中，通过semval表示当前信号量。 初始化sem_array和sems中的各个链表 通过ipc_addid()将创建的sem_array挂载到基数树上，并返回对应id 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899static int newary(struct ipc_namespace *ns, struct ipc_params *params)&#123; int retval; struct sem_array *sma; key_t key = params-&gt;key; int nsems = params-&gt;u.nsems; int semflg = params-&gt;flg; int i;...... sma = sem_alloc(nsems);...... sma-&gt;sem_perm.mode = (semflg &amp; S_IRWXUGO); sma-&gt;sem_perm.key = key; sma-&gt;sem_perm.security = NULL;...... for (i = 0; i &lt; nsems; i++) &#123; INIT_LIST_HEAD(&amp;sma-&gt;sems[i].pending_alter); INIT_LIST_HEAD(&amp;sma-&gt;sems[i].pending_const); spin_lock_init(&amp;sma-&gt;sems[i].lock); &#125; sma-&gt;complex_count = 0; sma-&gt;use_global_lock = USE_GLOBAL_LOCK_HYSTERESIS; INIT_LIST_HEAD(&amp;sma-&gt;pending_alter); INIT_LIST_HEAD(&amp;sma-&gt;pending_const); INIT_LIST_HEAD(&amp;sma-&gt;list_id); sma-&gt;sem_nsems = nsems; sma-&gt;sem_ctime = ktime_get_real_seconds(); /* ipc_addid() locks sma upon success. */ retval = ipc_addid(&amp;sem_ids(ns), &amp;sma-&gt;sem_perm, ns-&gt;sc_semmni);...... ns-&gt;used_sems += nsems; sem_unlock(sma, -1); rcu_read_unlock(); return sma-&gt;sem_perm.id;&#125;struct sem_array &#123; struct kern_ipc_perm\tsem_perm;\t/* permissions .. see ipc.h */ time64_t sem_ctime;\t/* create/last semctl() time */ struct list_head\tpending_alter;\t/* pending operations */ /* that alter the array */ struct list_head\tpending_const;\t/* pending complex operations */ /* that do not alter semvals */ struct list_head\tlist_id;\t/* undo requests on this array */ int sem_nsems;\t/* no. of semaphores in array */ int complex_count;\t/* pending complex operations */ unsigned int use_global_lock;/* &gt;0: global lock required */ struct sem sems[];&#125; __randomize_layout;struct sem &#123; int\tsemval; /* current value */ /* * PID of the process that last modified the semaphore. For * Linux, specifically these are: * - semop * - semctl, via SETVAL and SETALL. * - at task exit when performing undo adjustments (see exit_sem). */ struct pid *sempid; spinlock_t\tlock;\t/* spinlock for fine-grained semtimedop */ struct list_head pending_alter; /* pending single-sop operations */ /* that alter the semaphore */ struct list_head pending_const; /* pending single-sop operations */ /* that do not alter the semaphore*/ time64_t sem_otime;\t/* candidate for sem_otime */&#125; ____cacheline_aligned_in_smp;``` ### 5.2 信号量的初始化信号量通过semctl()实现初始化，主要使用semctl_main()和semctl_setval()函数。```cSYSCALL_DEFINE4(semctl, int, semid, int, semnum, int, cmd, unsigned long, arg)&#123; int version; struct ipc_namespace *ns; void __user *p = (void __user *)arg; ns = current-&gt;nsproxy-&gt;ipc_ns; switch (cmd) &#123; case IPC_INFO: case SEM_INFO: case IPC_STAT: case SEM_STAT: return semctl_nolock(ns, semid, cmd, version, p); case GETALL: case GETVAL: case GETPID: case GETNCNT: case GETZCNT: case SETALL: return semctl_main(ns, semid, semnum, cmd, p); case SETVAL: return semctl_setval(ns, semid, semnum, arg); case IPC_RMID: case IPC_SET: return semctl_down(ns, semid, cmd, version, p); default: return -EINVAL; &#125;&#125; SETALL操作调用semctl_main()，传参为 union semun 里面的 unsigned short *array，会设置整个信号量集合。semctl_main() 函数中，先是通过 sem_obtain_object_check()根据信号量集合的 id 在基数树里面找到 struct sem_array 对象，发现如果是 SETALL 操作，就将用户的参数中的 unsigned short *array 通过 copy_from_user() 拷贝到内核里面的 sem_io 数组，然后是一个循环，对于信号量集合里面的每一个信号量，设置 semval，以及修改这个信号量值的 pid。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364static int semctl_main(struct ipc_namespace *ns, int semid, int semnum, int cmd, void __user *p)&#123; struct sem_array *sma; struct sem *curr; int err, nsems; ushort fast_sem_io[SEMMSL_FAST]; ushort *sem_io = fast_sem_io; DEFINE_WAKE_Q(wake_q); sma = sem_obtain_object_check(ns, semid); nsems = sma-&gt;sem_nsems;...... switch (cmd) &#123;...... case SETALL: &#123; int i; struct sem_undo *un;...... if (copy_from_user(sem_io, p, nsems*sizeof(ushort))) &#123;...... &#125;...... for (i = 0; i &lt; nsems; i++) &#123; sma-&gt;sems[i].semval = sem_io[i]; sma-&gt;sems[i].sempid = task_tgid_vnr(current); &#125;...... sma-&gt;sem_ctime = get_seconds(); /* maybe some queued-up processes were waiting for this */ do_smart_update(sma, NULL, 0, 0, &amp;wake_q); err = 0; goto out_unlock; &#125; &#125;...... wake_up_q(&amp;wake_q);......&#125;``` SETVAL 操作调用semctl_setval()函数，传进来的参数 union semun 里面的 int val仅仅会设置某个信号量。在 semctl_setval() 函数中，我们先是通过 sem_obtain_object_check()根据信号量集合的 id 在基数树里面找到 struct sem_array 对象，对于 SETVAL 操作，直接根据参数中的 val 设置 semval，以及修改这个信号量值的 pid。```cstatic int semctl_setval(struct ipc_namespace *ns, int semid, int semnum, unsigned long arg)&#123; struct sem_undo *un; struct sem_array *sma; struct sem *curr; int err, val; DEFINE_WAKE_Q(wake_q);...... sma = sem_obtain_object_check(ns, semid);...... curr = &amp;sma-&gt;sems[semnum];...... curr-&gt;semval = val; curr-&gt;sempid = task_tgid_vnr(current); sma-&gt;sem_ctime = get_seconds(); /* maybe some queued-up processes were waiting for this */ do_smart_update(sma, NULL, 0, 0, &amp;wake_q);...... wake_up_q(&amp;wake_q); return 0;&#125; 5.3 信号量的操作信号量的操作通过semop()实现，实际调用sys_emtimedop()，最终调用为do_semtimedop() 1234SYSCALL_DEFINE3(semop, int, semid, struct sembuf __user *, tsops, unsigned, nsops)&#123; return sys_semtimedop(semid, tsops, nsops, NULL);&#125; do_semtimedop()是一个很较长的函数，逻辑比较复杂，主要为： 调用copy_from_user()拷贝用户参数至内核态，如对信号量的操作struct sembuf。 如果需要进入等待状态，，则需要设置超时 调用sem_obtain_object_check()根据id获取对应的信号量集合sma 创建struct sem_queue queue表示当前信号量操作。这里之所以称之为queue是因为操作的执行不可预期，因此排在队列之中等待信号量满足条件时再调用perform_atomic_semop()实施信号量操作。 如果不需要等待，则说明信号量操作已完成，也改变了信号量的值。接下来，就是一个标准流程。首先通过 DEFINE_WAKE_Q(wake_q) 声明一个 wake_q，调用 do_smart_update()看这次对于信号量的值的改变可以影响并可以激活等待队列中的哪些 struct sem_queue，然后把它们都放在 wake_q 里面，调用 wake_up_q() 唤醒这些进程。 如果需要等待，则会根据信号量操作是对单个信号量还是整个信号量集合，将queue挂载至信号量链表pending_alter或者信号量集合的链表pending_alter中 进入do-while循环等待，如果没有时间限制则调用schedule()让出CPU资源，如果有则调用schedule_timeout()让出资源并过一段时间后回来。当回来的时候，判断是否等待超时，如果没有等待超时则进入下一轮循环，再次等待，如果超时则退出循环，返回错误。在让出 CPU 的时候，设置进程的状态为 TASK_INTERRUPTIBLE，并且循环的结束会通过 signal_pending 查看是否收到过信号，这说明这个等待信号量的进程是可以被信号中断的，也即一个等待信号量的进程是可以通过 kill 杀掉的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798static long do_semtimedop(int semid, struct sembuf __user *tsops, unsigned nsops, const struct timespec64 *timeout)&#123; int error = -EINVAL; struct sem_array *sma; struct sembuf fast_sops[SEMOPM_FAST]; struct sembuf *sops = fast_sops, *sop; struct sem_undo *un; int max, locknum; bool undos = false, alter = false, dupsop = false; struct sem_queue queue; unsigned long dup = 0, jiffies_left = 0; struct ipc_namespace *ns; ns = current-&gt;nsproxy-&gt;ipc_ns;...... if (copy_from_user(sops, tsops, nsops * sizeof(*tsops))) &#123; error = -EFAULT; goto out_free; &#125; if (timeout) &#123; if (timeout-&gt;tv_sec &lt; 0 || timeout-&gt;tv_nsec &lt; 0 || timeout-&gt;tv_nsec &gt;= 1000000000L) &#123; error = -EINVAL; goto out_free; &#125; jiffies_left = timespec64_to_jiffies(timeout); &#125;...... un = find_alloc_undo(ns, semid);...... sma = sem_obtain_object_check(ns, semid);...... queue.sops = sops; queue.nsops = nsops; queue.undo = un; queue.pid = task_tgid(current); queue.alter = alter; queue.dupsop = dupsop; error = perform_atomic_semop(sma, &amp;queue); if (error == 0) &#123; /* non-blocking succesfull path */ DEFINE_WAKE_Q(wake_q); /* * If the operation was successful, then do * the required updates. */ if (alter) do_smart_update(sma, sops, nsops, 1, &amp;wake_q); else set_semotime(sma, sops);...... &#125;...... /* * We need to sleep on this operation, so we put the current * task into the pending queue and go to sleep. */ if (nsops == 1) &#123; struct sem *curr; int idx = array_index_nospec(sops-&gt;sem_num, sma-&gt;sem_nsems); curr = &amp;sma-&gt;sems[idx]; if (alter) &#123; if (sma-&gt;complex_count) &#123; list_add_tail(&amp;queue.list, &amp;sma-&gt;pending_alter); &#125; else &#123; list_add_tail(&amp;queue.list, &amp;curr-&gt;pending_alter); &#125; &#125; else &#123; list_add_tail(&amp;queue.list, &amp;curr-&gt;pending_const); &#125; &#125; else &#123; if (!sma-&gt;complex_count) merge_queues(sma); if (alter) list_add_tail(&amp;queue.list, &amp;sma-&gt;pending_alter); else list_add_tail(&amp;queue.list, &amp;sma-&gt;pending_const); sma-&gt;complex_count++; &#125; do &#123; WRITE_ONCE(queue.status, -EINTR); queue.sleeper = current; __set_current_state(TASK_INTERRUPTIBLE);...... if (timeout) jiffies_left = schedule_timeout(jiffies_left); else schedule();...... /* * If an interrupt occurred we have to clean up the queue. */ if (timeout &amp;&amp; jiffies_left == 0) error = -EAGAIN; &#125; while (error == -EINTR &amp;&amp; !signal_pending(current)); /* spurious */......&#125; do_smart_update() 会调用 update_queue()，update_queue() 会依次循环整个信号量集合的等待队列 pending_alter或者某个信号量的等待队列，试图在信号量的值变了的情况下，再次尝试 perform_atomic_semop 进行信号量操作。如果不成功，则尝试队列中的下一个；如果尝试成功，则调用 unlink_queue() 从队列上取下来，然后调用 wake_up_sem_queue_prepare()将 q-&gt;sleeper 加到 wake_q 上去。q-&gt;sleeper 是一个 task_struct，是等待在这个信号量操作上的进程。 1234567891011121314151617181920212223242526272829303132333435static int update_queue(struct sem_array *sma, int semnum, struct wake_q_head *wake_q)&#123; struct sem_queue *q, *tmp; struct list_head *pending_list; int semop_completed = 0; if (semnum == -1) pending_list = &amp;sma-&gt;pending_alter; else pending_list = &amp;sma-&gt;sems[semnum].pending_alter;again: list_for_each_entry_safe(q, tmp, pending_list, list) &#123; int error, restart;...... error = perform_atomic_semop(sma, q); /* Does q-&gt;sleeper still need to sleep? */ if (error &gt; 0) continue; unlink_queue(sma, q);...... wake_up_sem_queue_prepare(q, error, wake_q);...... &#125; return semop_completed;&#125;static inline void wake_up_sem_queue_prepare(struct sem_queue *q, int error, struct wake_q_head *wake_q)&#123; wake_q_add(wake_q, q-&gt;sleeper);......&#125; 接下来wake_up_q 就依次唤醒 wake_q 上的所有 task_struct，调用的是进程调度中分析过的 wake_up_process()方法。 12345678910111213141516void wake_up_q(struct wake_q_head *head)&#123; struct wake_q_node *node = head-&gt;first; while (node != WAKE_Q_TAIL) &#123; struct task_struct *task; task = container_of(node, struct task_struct, wake_q); node = node-&gt;next; task-&gt;wake_q.next = NULL; wake_up_process(task); put_task_struct(task); &#125;&#125; perform_atomic_semop() 函数对于所有信号量操作都进行两次循环。在第一次循环中，如果发现计算出的 result 小于 0，则说明必须等待，于是跳到 would_block 中，设置 q-&gt;blocking &#x3D; sop 表示这个 queue 是 block 在这个操作上，然后如果需要等待，则返回 1。如果第一次循环中发现无需等待，则第二个循环实施所有的信号量操作，将信号量的值设置为新的值，并且返回 0。 1234567891011121314151617181920212223242526272829303132333435363738394041424344static int perform_atomic_semop(struct sem_array *sma, struct sem_queue *q)&#123; int result, sem_op, nsops; struct sembuf *sop; struct sem *curr; struct sembuf *sops; struct sem_undo *un; sops = q-&gt;sops; nsops = q-&gt;nsops; un = q-&gt;undo; for (sop = sops; sop &lt; sops + nsops; sop++) &#123; curr = &amp;sma-&gt;sems[sop-&gt;sem_num]; sem_op = sop-&gt;sem_op; result = curr-&gt;semval;...... result += sem_op; if (result &lt; 0) goto would_block;...... if (sop-&gt;sem_flg &amp; SEM_UNDO) &#123; int undo = un-&gt;semadj[sop-&gt;sem_num] - sem_op;..... &#125; &#125; for (sop = sops; sop &lt; sops + nsops; sop++) &#123; curr = &amp;sma-&gt;sems[sop-&gt;sem_num]; sem_op = sop-&gt;sem_op; result = curr-&gt;semval; if (sop-&gt;sem_flg &amp; SEM_UNDO) &#123; int undo = un-&gt;semadj[sop-&gt;sem_num] - sem_op; un-&gt;semadj[sop-&gt;sem_num] = undo; &#125; curr-&gt;semval += sem_op; curr-&gt;sempid = q-&gt;pid; &#125; return 0;would_block: q-&gt;blocking = sop; return sop-&gt;sem_flg &amp; IPC_NOWAIT ? -EAGAIN : 1;&#125; 5.4 SEM_UNDO机制信号量是整个 Linux 可见的全局资源，而不是某个进程独占的资源，好处是可以跨进程通信，坏处就是如果一个进程通过操作拿到了一个信号量，但是不幸异常退出了，如果没有来得及归还这个信号量，可能所有其他的进程都阻塞了。为此，Linux设计了SEM_UNDO机制解决该问题。 该机制简而言之就是每一个 semop 操作都会保存一个反向 struct sem_undo 操作，当因为某个进程异常退出的时候，这个进程做的所有的操作都会回退，从而保证其他进程可以正常工作。在sem_flg标记位设置SUM_UNDO即可开启该功能。 12345struct sem_queue &#123;...... struct sem_undo *undo; /* undo structure */......&#125;; 在进程的 task_struct 里面对于信号量有一个成员 struct sysv_sem，里面是一个 struct sem_undo_list将这个进程所有的 semop 所带来的 undo 操作都串起来。 12345678910111213141516171819202122232425262728struct task_struct &#123;...... struct sysv_sem sysvsem;......&#125;struct sysv_sem &#123; struct sem_undo_list *undo_list;&#125;;struct sem_undo &#123; struct list_head list_proc; /* per-process list: * * all undos from one process * rcu protected */ struct rcu_head rcu; /* rcu struct for sem_undo */ struct sem_undo_list *ulp; /* back ptr to sem_undo_list */ struct list_head list_id; /* per semaphore array list: * all undos for one array */ int semid; /* semaphore set identifier */ short *semadj; /* array of adjustments */ /* one per semaphore */&#125;;struct sem_undo_list &#123; atomic_t refcnt; spinlock_t lock; struct list_head list_proc;&#125;; 这种设计思想较为常见，在MySQL的innodb的日志系统中也有着类似的实现。 总结共享内存和信号量是有着相似性有可以共同使用从而完成进程通信的手段。下面引用极客时间中的两幅图来总结二者的整个过程。","tags":["Linux开发"],"categories":["linux"]},{"title":"进程间通信之信号","path":"/2023/01/15/linux-docs/进程管理/进程间通信之信号/","content":"一. 前言 众所周知，System V IPC进程间通信机制体系中有着多种多样的进程间通信方式，如管道和有名管道，消息队列，信号，共享内存和信号量，套接字。从本文开始我们就逐个剖析进程间通信的机制和底层原理，就从信号开始讲起吧。 二. 信号基本知识 信号是进程处理紧急情况所用的一种方式，它没有特别复杂的数据结构，就是用一个代号一样的数字。Linux 提供了几十种信号，分别代表不同的意义。我们可以通过kill -l命令查看信号。 1234567891011121314# kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX 信号可以在任何时候发送给某一进程，进程需要为这个信号配置信号处理函数。当某个信号发生的时候，就默认执行这个函数就可以了。通过man 7 signal可以查看各个信号的具体含义和对应的处理方法 Signal Value Action Comment ────────────────────────────────────────────────────────────────────── SIGHUP 1 Term Hangup detected on controlling terminal or death of controlling process SIGINT 2 Term Interrupt from keyboard SIGQUIT 3 Core Quit from keyboard SIGILL 4 Core Illegal Instruction SIGABRT 6 Core Abort signal from abort(3) SIGFPE 8 Core Floating point exception SIGKILL 9 Term Kill signal SIGSEGV 11 Core Invalid memory reference SIGPIPE 13 Term Broken pipe: write to pipe with no readers SIGALRM 14 Term Timer signal from alarm(2) SIGTERM 15 Term Termination signal SIGUSR1 30,10,16 Term User-defined signal 1 SIGUSR2 31,12,17 Term User-defined signal 2 …… 由上表可见，信号的处理通常分为三种： 执行默认操作。Linux 对每种信号都规定了默认操作，例如上面列表中的 Term，就是终止进程的意思。Core 的意思是 Core Dump，也即终止进程后通过 Core Dump 将当前进程的运行状态保存在文件里面，方便程序员事后进行分析问题在哪里。 捕捉信号。我们可以为信号定义一个信号处理函数。当信号发生时，我们就执行相应的信号处理函数。 忽略信号。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即 SIGKILL 和 SEGSTOP，它们用于在任何时候中断或结束某一进程。 三. 信号和中断信号和中断有着诸多相似之处： 均会注册处理函数 都是用于对当前的任务进行一些处理，如调度、停止等等 但是二者实际上是有很多不同的，其不同的用途导致了运行逻辑的不同，最终在代码实现上体现出了不同的设计特点。其主要区别有： 中断和信号都可能源于硬件和软件，但是中断处理函数注册于内核之中，由内核中运行，而信号的处理函数注册于用户态，内核收到信号后会根据当前任务task_struct结构体中的信号相关数据结构找寻对应的处理函数并最终在用户态处理 中断作用于内核全局，而信号作用于当前任务（进程）。即信号影响的往往是一个进程，而中断处理如果出现问题则会导致整个Linux内核的崩溃 四. 注册信号处理函数 有些时候我们希望能够让信号运行一些特殊功能，所以有了自定义的信号处理函数。注册API主要有signal()和sigaction()两个，其中sigaction()比较推荐使用。 typedef void (*sighandler_t)(int); sighandler_t signal(int signum, sighandler_t handler); int sigaction(int signum, const struct sigaction *act, struct sigaction *oldact); 其主要区别在于sigaction()对于信号signum会绑定对应的结构体sigaction而不仅仅是一个处理函数sighandler_t。这样做的好处是可以更精细的控制信号处理，通过不同参数实现不同的效果。例如sa_flags可以设置如 SA_ONESHOT：信号处理函数仅作用一次，之后启用默认行为 SA_NOMASK：该信号处理函数执行过程中允许被其他信号或者相同信号中断，即不屏蔽 SA_INTERRUPT：该信号处理函数若执行过程中被中断，则不会再调度回该函数继续执行，而是直接返回-EINTR，将执行逻辑交还给调用方 SA_RESTART：与SA_INTERRUPT相反，会自动重启该函数 sa_restorer保存的是sa_handler 执行完毕之后，马上要执行的函数，即下一个函数地址的位置。 123456struct sigaction &#123; __sighandler_t sa_handler; unsigned long sa_flags; __sigrestore_t sa_restorer; sigset_t sa_mask; /* mask last for extensibility */&#125;; sigaction()也是glibc封装的函数，最终系统调用为rt_sigaction()。该函数首先将用户态的 struct sigaction 结构拷贝为内核态的 k_sigaction，然后调用 do_sigaction()设置对应的信号处理动作。 12345678910111213141516171819202122SYSCALL_DEFINE4(rt_sigaction, int, sig, const struct sigaction __user *, act, struct sigaction __user *, oact, size_t, sigsetsize)&#123; struct k_sigaction new_sa, old_sa; int ret = -EINVAL;...... if (act) &#123; if (copy_from_user(&amp;new_sa.sa, act, sizeof(new_sa.sa))) return -EFAULT; &#125; ret = do_sigaction(sig, act ? &amp;new_sa : NULL, oact ? &amp;old_sa : NULL); if (!ret &amp;&amp; oact) &#123; if (copy_to_user(oact, &amp;old_sa.sa, sizeof(old_sa.sa))) return -EFAULT; &#125;out: return ret;&#125; do_sigaction()会将用户层传来的信号处理函数赋值给当前任务task_struct currrent对应的sighand-&gt;action[]数组中sig信号对应的位置，以用于之后调用。 123456789101112131415161718192021int do_sigaction(int sig, struct k_sigaction *act, struct k_sigaction *oact)&#123; struct task_struct *p = current, *t; struct k_sigaction *k; sigset_t mask;...... k = &amp;p-&gt;sighand-&gt;action[sig-1]; spin_lock_irq(&amp;p-&gt;sighand-&gt;siglock); if (oact) *oact = *k; if (act) &#123; sigdelsetmask(&amp;act-&gt;sa.sa_mask, sigmask(SIGKILL) | sigmask(SIGSTOP)); *k = *act;...... &#125; spin_unlock_irq(&amp;p-&gt;sighand-&gt;siglock); return 0;&#125; 五. 发送信号信号发送来源广泛，有可能来自于用户态，有可能来自于硬件，也有可能来自于内核。 有时候，我们在终端输入某些组合键的时候会给进程发送信号，例如，Ctrl+C 产生 SIGINT 信号，Ctrl+Z 产生 SIGTSTP 信号。再比如， kill -9 pid 可以发送信号给一个进程，杀死它。 有的时候，硬件异常也会产生信号。比如，执行了除以 0 的指令，CPU 就会产生异常，然后把 SIGFPE 信号发送给进程。再如，进程访问了非法内存，内存管理模块就会产生异常，然后把信号 SIGSEGV 发送给进程。 有时候，内核在某些情况下，也会给进程发送信号。例如，向读端已关闭的管道写数据时产生 SIGPIPE 信号，当子进程退出时，我们要给父进程发送 SIG_CHLD 信号等。 不论通过kill或者sigqueue系统调用还是通过tkill或者tgkill发送指定线程的信号，其最终调用的均是do_send_sig_info()函数，其调用链如下所示 kill()-&gt;kill_something_info()-&gt;kill_pid_info()-&gt;group_send_sig_info()-&gt;do_send_sig_info() tkill()-&gt;do_tkill()-&gt;do_send_specific()-&gt;do_send_sig_info() tgkill()-&gt;do_tkill()-&gt;do_send_specific()-&gt;do_send_sig_info() rt_sigqueueinfo()-&gt;do_rt_sigqueueinfo()-&gt;kill_proc_info()-&gt;kill_pid_info()-&gt;group_send_sig_info()-&gt;do_send_sig_info() do_send_sig_info() 会调用 send_signal()，进而调用 __send_signal()。这里代码比较复杂，主要逻辑如下 根据发送信号的类型判断是共享信号还是线程独享信号，由此赋值pending。如果是 kill 发送的，也就是发送给整个进程的，就应该发送给 t-&gt;signal-&gt;shared_pending，这里面是整个进程所有线程共享的信号；如果是 tkill 发送的，也就是发给某个线程的，就应该发给 t-&gt;pending，这里面是这个线程的 task_struct 独享的。 调用 legacy_queue()判断是否为可靠信号，不可靠则直接退出 调用__sigqueue_alloc() 分配一个 struct sigqueue 对象，然后通过 list_add_tail 挂在 struct sigpending 里面的链表上。 调用 complete_signal()分配线程处理该信号 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778int do_send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p, enum pid_type type)&#123;...... ret = send_signal(sig, info, p, type);......&#125;static int send_signal(int sig, struct kernel_siginfo *info, struct task_struct *t, enum pid_type type)&#123;...... return __send_signal(sig, info, t, type, from_ancestor_ns);&#125;static int __send_signal(int sig, struct kernel_siginfo *info, struct task_struct *t, enum pid_type type, int from_ancestor_ns)&#123; struct sigpending *pending; struct sigqueue *q;...... pending = (type != PIDTYPE_PID) ? &amp;t-&gt;signal-&gt;shared_pending : &amp;t-&gt;pending;...... if (legacy_queue(pending, sig)) goto ret;...... /* * Real-time signals must be queued if sent by sigqueue, or * some other real-time mechanism. It is implementation * defined whether kill() does so. We attempt to do so, on * the principle of least surprise, but since kill is not * allowed to fail with EAGAIN when low on memory we just * make sure at least one signal gets delivered and don&#x27;t * pass on the info struct. */ if (sig &lt; SIGRTMIN) override_rlimit = (is_si_special(info) || info-&gt;si_code &gt;= 0); else override_rlimit = 0; q = __sigqueue_alloc(sig, t, GFP_ATOMIC, override_rlimit); if (q) &#123; list_add_tail(&amp;q-&gt;list, &amp;pending-&gt;list); switch ((unsigned long) info) &#123; case (unsigned long) SEND_SIG_NOINFO: clear_siginfo(&amp;q-&gt;info); q-&gt;info.si_signo = sig; q-&gt;info.si_errno = 0; q-&gt;info.si_code = SI_USER; q-&gt;info.si_pid = task_tgid_nr_ns(current, task_active_pid_ns(t)); q-&gt;info.si_uid = from_kuid_munged(current_user_ns(), current_uid()); break; case (unsigned long) SEND_SIG_PRIV: clear_siginfo(&amp;q-&gt;info); q-&gt;info.si_signo = sig; q-&gt;info.si_errno = 0; q-&gt;info.si_code = SI_KERNEL; q-&gt;info.si_pid = 0; q-&gt;info.si_uid = 0; break; default: copy_siginfo(&amp;q-&gt;info, info); if (from_ancestor_ns) q-&gt;info.si_pid = 0; break; &#125; userns_fixup_signal_uid(&amp;q-&gt;info, t); &#125;......out_set: signalfd_notify(t, sig); sigaddset(&amp;pending-&gt;signal, sig);...... complete_signal(sig, t, type);ret: trace_signal_generate(sig, info, t, type != PIDTYPE_PID, result); return ret;&#125; legacy_queue()中主要是判断是否为可靠信号，判断的依据是当信号小于 SIGRTMIN也即 32 的时候，如果我们发现这个信号已经在集合里面了，就直接退出。这里之所以前32位信号称之为不可靠信号其实是历史遗留问题，早期UNIX系统只定义了32种信号，而这些经过检验被定义为不可靠信号，主要指的是进程可能对信号做出错误的反应以及信号可能丢失：UNIX系统每次信号处理完需要重新安装信号，因此容易出现各种错误。linux也支持不可靠信号，但是对不可靠信号机制做出了改进：在调用完信号处理函数后，不必重新调用该信号的安装函数(信号安装函数是在可靠机制上是实现的)。因此，linux下的不可靠信号问题主要指的是信号可能丢失。 这里之所以会出现信号丢失，是因为这些信号可能会频繁快速出现。这样信号能够处理多少，和信号处理函数什么时候被调用，信号多大频率被发送，都有关系，而信号处理函数的调用时间也是不确定的，因此这种信号称之为不可靠信号。与之相对的，其他信号称之为可靠信号，支持排队执行。 12345678static inline int legacy_queue(struct sigpending *signals, int sig)&#123; return (sig &lt; SIGRTMIN) &amp;&amp; sigismember(&amp;signals-&gt;signal, sig);&#125;#define SIGRTMIN 32#define SIGRTMAX _NSIG#define _NSIG 64 对于可靠信号我们通过__sigqueue_alloc()分配sigqueue对象，并挂载在sigpending中的链表上，最终调用complete_signal()找一个线程处理。其主要逻辑为： 首先找是否有可唤醒的线程来执行，如果是主线程或者仅有一个线程，则直接从链表主队并分配 如果没找到可唤醒的线程，则查看当前是否有不需要唤醒的线程可以执行 如果没找到并且该信号为非常重要的信号如SIGKILL，则强行关闭当前线程 调用signal_wake_up()唤醒线程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172static void complete_signal(int sig, struct task_struct *p, enum pid_type type)&#123; struct signal_struct *signal = p-&gt;signal; struct task_struct *t; /* * Now find a thread we can wake up to take the signal off the queue. * * If the main thread wants the signal, it gets first crack. * Probably the least surprising to the average bear. */ if (wants_signal(sig, p)) t = p; else if ((type == PIDTYPE_PID) || thread_group_empty(p)) /* * There is just one thread and it does not need to be woken. * It will dequeue unblocked signals before it runs again. */ return; else &#123; /* * Otherwise try to find a suitable thread. */ t = signal-&gt;curr_target; while (!wants_signal(sig, t)) &#123; t = next_thread(t); if (t == signal-&gt;curr_target) /* * No thread needs to be woken. * Any eligible threads will see * the signal in the queue soon. */ return; &#125; signal-&gt;curr_target = t; &#125; /* * Found a killable thread. If the signal will be fatal, * then start taking the whole group down immediately. */ if (sig_fatal(p, sig) &amp;&amp; !(signal-&gt;flags &amp; SIGNAL_GROUP_EXIT) &amp;&amp; !sigismember(&amp;t-&gt;real_blocked, sig) &amp;&amp; (sig == SIGKILL || !p-&gt;ptrace)) &#123; /* * This signal will be fatal to the whole group. */ if (!sig_kernel_coredump(sig)) &#123; /* * Start a group exit and wake everybody up. * This way we don&#x27;t have other threads * running and doing things after a slower * thread has the fatal signal pending. */ signal-&gt;flags = SIGNAL_GROUP_EXIT; signal-&gt;group_exit_code = sig; signal-&gt;group_stop_count = 0; t = p; do &#123; task_clear_jobctl_pending(t, JOBCTL_PENDING_MASK); sigaddset(&amp;t-&gt;pending.signal, SIGKILL); signal_wake_up(t, 1); &#125; while_each_thread(p, t); return; &#125; &#125; /* * The signal is already in the shared-pending queue. * Tell the chosen thread to wake up and dequeue it. */ signal_wake_up(t, sig == SIGKILL); return;&#125; signal_wake_up()函数主要逻辑为 设置TIF_SIGPENDING标记位 尝试唤醒该线程&#x2F;进程 信号处理的调度和任务调度类似，均是采用标记位的方式进行。当信号来的时候，内核并不直接处理这个信号，而是设置一个标识位 TIF_SIGPENDING来表示已经有信号等待处理。同样等待系统调用结束，或者中断处理结束，从内核态返回用户态的时候再进行信号的处理。 进程&#x2F;线程的唤醒和任务调度一样最终会调用 try_to_wake_up() ，具体逻辑就不重复分析了。如果 wake_up_state 返回 0，说明进程或者线程已经是 TASK_RUNNING 状态了，如果它在另外一个 CPU 上运行，则调用 kick_process 发送一个处理器间中断，强制那个进程或者线程重新调度，重新调度完毕后，会返回用户态运行。 123456789101112131415161718static inline void signal_wake_up(struct task_struct *t, bool resume)&#123; signal_wake_up_state(t, resume ? TASK_WAKEKILL : 0);&#125;void signal_wake_up_state(struct task_struct *t, unsigned int state)&#123; set_tsk_thread_flag(t, TIF_SIGPENDING); /* * TASK_WAKEKILL also means wake it up in the stopped/traced/killable * case. We don&#x27;t check t-&gt;state here because there is a race with it * executing another processor and just now entering stopped state. * By using wake_up_state, we ensure the process will wake up and * handle its death signal. */ if (!wake_up_state(t, state | TASK_INTERRUPTIBLE)) kick_process(t);&#125; 六. 信号的处理这里我们以一个从tap 网卡中读取数据的例子来分析信号的处理逻辑。这部分涉及到了系统调用、任务调度、中断等知识，对前面的文章也算是一个回顾。从网卡读取数据会通过系统调用进入内核，之后通过函数调用表找到对应的函数执行。在读的过程中，如果没有数据处理则会调用schedule()函数主动让出CPU进入休眠状态并等待再次唤醒。 tap_do_read()主要逻辑为： 把当前进程或者线程的状态设置为 TASK_INTERRUPTIBLE，这样才能使这个系统调用可以被中断。 可以被中断的系统调用往往是比较慢的调用，并且会因为数据不就绪而通过 schedule() 让出 CPU 进入等待状态。在发送信号的时候，我们除了设置这个进程和线程的 _TIF_SIGPENDING 标识位之外，还试图唤醒这个进程或者线程，也就是将它从等待状态中设置为 TASK_RUNNING。当这个进程或者线程再次运行的时候，会从 schedule() 函数中返回，然后再次进入 while 循环。由于这个进程或者线程是由信号唤醒的而不是因为数据来了而唤醒的，因而是读不到数据的，但是在 signal_pending() 函数中，我们检测到了 _TIF_SIGPENDING 标识位，这说明系统调用没有真的做完，于是返回一个错误 ERESTARTSYS，然后带着这个错误从系统调用返回。 如果没有信号，则继续调用schedule()让出CPU 1234567891011121314151617181920212223242526static ssize_t tap_do_read(struct tap_queue *q, struct iov_iter *to, int noblock, struct sk_buff *skb)&#123;...... while (1) &#123; if (!noblock) prepare_to_wait(sk_sleep(&amp;q-&gt;sk), &amp;wait, TASK_INTERRUPTIBLE); /* Read frames from the queue */ skb = skb_array_consume(&amp;q-&gt;skb_array); if (skb) break; if (noblock) &#123; ret = -EAGAIN; break; &#125; if (signal_pending(current)) &#123; ret = -ERESTARTSYS; break; &#125; /* Nothing to read, let&#x27;s sleep */ schedule(); &#125;......&#125; schedule()会在系统调用返回或者中断返回的时刻调用exit_to_usermode_loop()，在任务调度中标记位为_TIF_NEED_RESCHED，而对于信号来说是_TIF_SIGPENDING。 123456789101112131415static void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags)&#123; while (true) &#123;...... if (cached_flags &amp; _TIF_NEED_RESCHED) schedule();...... /* deal with pending signal delivery */ if (cached_flags &amp; _TIF_SIGPENDING) do_signal(regs);...... if (!(cached_flags &amp; EXIT_TO_USERMODE_LOOP_FLAGS)) break; &#125;&#125; do_signal()函数会调用 handle_signal()，这里主要存在一个问题使得逻辑变得较为复杂：信号处理函数定义于用户态，而调度过程位于内核态。 1234567891011121314151617181920212223242526272829303132333435/* * Note that &#x27;init&#x27; is a special process: it doesn&#x27;t get signals it doesn&#x27;t * want to handle. Thus you cannot kill init even with a SIGKILL even by * mistake. */void do_signal(struct pt_regs *regs)&#123; struct ksignal ksig; if (get_signal(&amp;ksig)) &#123; /* Whee! Actually deliver the signal. */ handle_signal(&amp;ksig, regs); return; &#125; /* Did we come from a system call? */ if (syscall_get_nr(current, regs) &gt;= 0) &#123; /* Restart the system call - no handlers present */ switch (syscall_get_error(current, regs)) &#123; case -ERESTARTNOHAND: case -ERESTARTSYS: case -ERESTARTNOINTR: regs-&gt;ax = regs-&gt;orig_ax; regs-&gt;ip -= 2; break; case -ERESTART_RESTARTBLOCK: regs-&gt;ax = get_nr_restart_syscall(regs); regs-&gt;ip -= 2; break; &#125; &#125; /* * If there&#x27;s no signal to deliver, we just put the saved sigmask * back. */ restore_saved_sigmask();&#125; handle_signal()会判断当前是否从系统调用调度而来，当发现错误码为ERESTARTSYS的时候就知道这是从一个没有调用完的系统调用返回的，设置系统错误码为EINTR。由于此处不会直接返回任务调度前记录的用户态状态，而是进入注册好的信号处理函数，因此需要调用setup_rt_frame()构建新的寄存器结构体pt_regs。 1234567891011121314151617181920212223242526272829static voidhandle_signal(struct ksignal *ksig, struct pt_regs *regs)&#123; bool stepping, failed;...... /* Are we from a system call? */ if (syscall_get_nr(current, regs) &gt;= 0) &#123; /* If so, check system call restarting.. */ switch (syscall_get_error(current, regs)) &#123; case -ERESTART_RESTARTBLOCK: case -ERESTARTNOHAND: regs-&gt;ax = -EINTR; break; case -ERESTARTSYS: if (!(ksig-&gt;ka.sa.sa_flags &amp; SA_RESTART)) &#123; regs-&gt;ax = -EINTR; break; &#125; /* fallthrough */ case -ERESTARTNOINTR: regs-&gt;ax = regs-&gt;orig_ax; regs-&gt;ip -= 2; break; &#125; &#125;...... failed = (setup_rt_frame(ksig, regs) &lt; 0);...... signal setup_rt_frame()主要调用__setup_rt_frame()，主要逻辑为： 调用get_sigframe()得到regs中的sp寄存器值，即原进程用户态的栈顶指针，将sp减去sizeof(struct rt_sigframe)从而把该新建栈帧压入栈 调用put_user_ex()，将 sa_restorer 按照函数栈的规则放到了 frame-&gt;pretcode 里面。函数栈里面包含了函数执行完跳回去的地址,当 sa_handler 执行完之后，弹出的函数栈是 frame，也就应该跳到 sa_restorer 的地址 调用setup_sigcontext() 里面，将原来的 pt_regs 保存在了 frame 中的 uc_mcontext 里 填充regs，将regs-&gt;ip设置为自定义的信号处理函数sa_handler，将栈顶regs-&gt;sp设置为新栈帧frame地址 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465static intsetup_rt_frame(struct ksignal *ksig, struct pt_regs *regs)&#123;...... return __setup_rt_frame(ksig-&gt;sig, ksig, set, regs);......&#125;static int __setup_rt_frame(int sig, struct ksignal *ksig, sigset_t *set, struct pt_regs *regs)&#123; struct rt_sigframe __user *frame; void __user *fp = NULL; int err = 0; frame = get_sigframe(&amp;ksig-&gt;ka, regs, sizeof(struct rt_sigframe), &amp;fp);...... put_user_try &#123;...... /* Set up to return from userspace. If provided, use a stub already in userspace. */ /* x86-64 should always use SA_RESTORER. */ if (ksig-&gt;ka.sa.sa_flags &amp; SA_RESTORER) &#123; put_user_ex(ksig-&gt;ka.sa.sa_restorer, &amp;frame-&gt;pretcode); &#125; else &#123; /* could use a vstub here */ err |= -EFAULT; &#125; &#125; put_user_catch(err); err |= setup_sigcontext(&amp;frame-&gt;uc.uc_mcontext, fp, regs, set-&gt;sig[0]); err |= __copy_to_user(&amp;frame-&gt;uc.uc_sigmask, set, sizeof(*set)); if (err) return -EFAULT; /* Set up registers for signal handler */ regs-&gt;di = sig; /* In case the signal handler was declared without prototypes */ regs-&gt;ax = 0; /* This also works for non SA_SIGINFO handlers because they expect the next argument after the signal number on the stack. */ regs-&gt;si = (unsigned long)&amp;frame-&gt;info; regs-&gt;dx = (unsigned long)&amp;frame-&gt;uc; regs-&gt;ip = (unsigned long) ksig-&gt;ka.sa.sa_handler; regs-&gt;sp = (unsigned long)frame; /* * Set up the CS and SS registers to run signal handlers in * 64-bit mode, even if the handler happens to be interrupting * 32-bit or 16-bit code. * * SS is subtle. In 64-bit mode, we don&#x27;t need any particular * SS descriptor, but we do need SS to be valid. It&#x27;s possible * that the old SS is entirely bogus -- this can happen if the * signal we&#x27;re trying to deliver is #GP or #SS caused by a bad * SS value. We also have a compatbility issue here: DOSEMU * relies on the contents of the SS register indicating the * SS value at the time of the signal, even though that code in * DOSEMU predates sigreturn&#x27;s ability to restore SS. (DOSEMU * avoids relying on sigreturn to restore SS; instead it uses * a trampoline.) So we do our best: if the old SS was valid, * we keep it. Otherwise we replace it. */ regs-&gt;cs = __USER_CS; if (unlikely(regs-&gt;ss != __USER_DS)) force_valid_ss(regs); return 0;&#125; sa_restorer在__libc_sigaction()函数中被赋值为restore_rt，实际上调用函数调用__NR_rt_sigreturn() 12345678910111213RESTORE (restore_rt, __NR_rt_sigreturn)#define RESTORE(name, syscall) RESTORE2 (name, syscall)# define RESTORE2(name, syscall) \\asm \\ ( \\ &quot;.LSTART_&quot; #name &quot;: &quot; \\ &quot; .type __&quot; #name &quot;,@function &quot; \\ &quot;__&quot; #name &quot;: &quot; \\ &quot; movq $&quot; #syscall &quot;, %rax &quot; \\ &quot; syscall &quot; \\...... __NR_rt_sigreturn()对应的内核函数为sys_rt_sigreturn()，这里会调用restore_sigframe()将pt_regs恢复成原进程的栈帧状态，从而继续执行函数调用后续的内容。 12345678910111213141516171819202122232425asmlinkage int sys_rt_sigreturn(struct pt_regs *regs)&#123; struct rt_sigframe __user *frame; /* Always make any pending restarted system calls return -EINTR */ current-&gt;restart_block.fn = do_no_restart_syscall; /* * Since we stacked the signal on a 64-bit boundary, * then &#x27;sp&#x27; should be word aligned here. If it&#x27;s * not, then the user is trying to mess with us. */ if (regs-&gt;ARM_sp &amp; 7) goto badframe; frame = (struct rt_sigframe __user *)regs-&gt;ARM_sp; if (!access_ok(frame, sizeof (*frame))) goto badframe; if (restore_sigframe(regs, &amp;frame-&gt;sig)) goto badframe; if (restore_altstack(&amp;frame-&gt;sig.uc.uc_stack)) goto badframe; return regs-&gt;ARM_r0;badframe: force_sig(SIGSEGV, current); return 0;&#125; 总结信号的发送与处理是一个复杂的过程，这里来总结一下。 假设我们有一个进程 A会从tap 网卡中读取数据，main 函数里面调用系统调用通过中断陷入内核。 按照系统调用的原理，将用户态栈的信息保存在 pt_regs 里面，也即记住原来用户态是运行到了 line A 的地方。 在内核中执行系统调用读取数据。 当发现没有什么数据可读取的时候进入睡眠状态，并且调用 schedule() 让出 CPU。 将进程状态设置为可中断的睡眠状态 TASK_INTERRUPTIBLE，也即如果有信号来的话是可以唤醒它的。 其他的进程或者 shell 通过调用 kill()、tkill()、tgkill()、rt_sigqueueinfo()发送信号。四个发送信号的函数，在内核中最终都是调用 do_send_sig_info()。 do_send_sig_info() 调用 send_signal() 给进程 A 发送一个信号，其实就是找到进程 A 的 task_struct，不可靠信号加入信号集合，可靠信号加入信号链表。 do_send_sig_info() 调用 signal_wake_up() 唤醒进程 A。 进程 A 重新进入运行状态 TASK_RUNNING，接着 schedule() 运行。 进程 A 被唤醒后检查是否有信号到来，如果没有，重新循环到一开始，尝试再次读取数据，如果还是没有数据，再次进入 TASK_INTERRUPTIBLE，即可中断的睡眠状态。 当发现有信号到来的时候，就返回当前正在执行的系统调用，并返回一个错误表示系统调用被中断了。 系统调用返回的时候，会调用 exit_to_usermode_loop()，这是一个处理信号的时机。 调用 do_signal() 开始处理信号。 根据信号得到信号处理函数 sa_handler，然后修改 pt_regs 中的用户态栈的信息让 pt_regs 指向 sa_handler，同时修改用户态的栈，插入一个栈帧 sa_restorer，里面保存了原来的指向 line A 的 pt_regs，并且设置让 sa_handler 运行完毕后跳到 sa_restorer 运行。 返回用户态，由于 pt_regs 已经设置为 sa_handler，则返回用户态执行 sa_handler。 sa_handler 执行完毕后，信号处理函数就执行完了，接着会跳到 sa_restorer 运行。 sa_restorer 会调用系统调用 rt_sigreturn 再次进入内核。 在内核中，rt_sigreturn 恢复原来的 pt_regs，重新指向 line A。 从 rt_sigreturn 返回用户态，还是调用 exit_to_usermode_loop()。 这次因为 pt_regs 已经指向 line A 了，于是就到了进程 A 中接着系统调用之后运行，当然这个系统调用返回的是它被中断了没有执行完的错误。","tags":["Linux开发"],"categories":["linux"]},{"title":"进程间通信，管道，socket，XSI(System V)","path":"/2023/01/15/linux-docs/进程管理/进程间通信，管道，socket，XSI(System V)/","content":"进程通信(IPC)分为PIPE(管道)、Socket(套接字)和XSI(System_V)。XSI又分为msg(消息队列)、sem(信号量数组)和shm(共享内存)。这些手段都是用于进程间通信的，只有进程间通讯才需要借助第三方机制，线程之间通讯是不需要借助第三方机制，因为线程之间的地址空间是共享的。线程之间可以通过互斥量，死锁，唤醒，信号等来进行通讯。 管道(PIPE-&gt;FIFO) 内核帮你创建和维护管道的特点: 管道是半双工的，也就是同一时间数据只能从一端流向另一段。就像水一样，两端水同时流入管道，那么数据就会乱 管道的两端一端作为读端，一端是写端 管道具有自适应的特点， 默认会适应速度比较慢的一方，管道被写满或读空时速度快的一方会自动阻塞 1234pipe - create pipe#include &lt;unistd.h&gt;int pipe(int pipefd[2]);// 也就只有两端，一端读，一端写 pipe用于创建管道，pipefd是一个数组，表示管道的两端文件描述符，pipefd[0]端作为读端，pipefd[1]作为写端。 pipe产生的是匿名管道，在磁盘的任何位置上找不到这个管道文件，而且匿名管道只能用于具有亲缘关系的进程之间通信(还要分亲缘关系) 一般情况下有亲缘关系的进程之间使用管道进行通信时，会把自己不用的一端文件描述符关闭 12345678910111213141516171819202122232425262728293031323334353637#include &quot;../include/apue.h&quot;#define BUFSIZE 1024int main()&#123; int pd[2]; char buf[BUFSIZE]; pid_t pid; int len; // 创建匿名管道 if(pipe(pd)&lt;0) err_sys(&quot;pipe()&quot;); pid = fork(); if(pid == 0)&#123; // 子进程 读取管道数据 // 关闭写端 close(pd[1]); // 从管道中读取数据，如果子进程比父进程先被调度会阻塞等待数据写入 len = read(pd[0],buf,BUFSIZE); puts(buf); /*** * 管道fork之前创建 * 父子进程都有一份 * 所有退出之前要确保管道两端都关闭 * ***/ close(pd[0]); exit(0); &#125;else&#123; // 父进程 向管道写入数据 // 关闭读端 close(pd[0]); // 写端 write(pd[1],&quot;Hello,world!&quot;,100); close(pd[1]); wait(NULL); exit(0); &#125;&#125; 创建了一个匿名的管道，在pd[2]数组中凑齐了读写双方，子进程同样继承了具有读写双方的数组pd[2] 当关闭之后就是取决于我们需要对管道的数据流方向做准备。要么从子进程流向父进程，要么从父进程流向子进程。 mkfifo函数 12345678mkfifo - make a FIFO special file (a named pipe)#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;int mkfifo(const char *pathname,mode_t mode);// pathname: 管道文件的路径和文件名// mode: 创建管道文件的权限。还是老规矩，传入的mode值要与系统的umask值做运算(mode&amp;~umask)// 成功返回0，失败返回-1并设置errno mkfifo函数用于创建命名管道，作用与匿名管道相同，不过可以在不同的进程之间使用，相当于对一个普通文件进行写操作就可以了。 这个管道文件是任何有权限的进程都可以使用的，两端都像操作一个普通文件一样对它进行打开、读写、关闭动作就可以了，只要一端写入数据另一端就可以读出来。 命名管道文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &quot;../include/apue.h&quot;#include &lt;fcntl.h&gt;#define PATHNAME &quot;./mkfifof.txt&quot;int main(void)&#123; pid_t pid; int fd = -1; char buf[BUFSIZ] = &quot;&quot;; // 创建一个命名管道，通过ls -l查看这个管道的属性 if(mkfifo(PATHNAME,0664)&lt;0)&#123; err_sys(&quot;mkfifo&quot;); &#125; fflush(NULL); // 刷新缓冲区 pid = fork(); if(pid&lt;0) err_sys(&quot;fork()&quot;); if(!pid)&#123; pid = fork(); // 继续fork 三个进程了 if(pid&lt;0)err_sys(&quot;fork()2&quot;); if(!pid) exit(0); // 爸爸走了 // child2 fd = open(PATHNAME,O_RDWR); if(fd&lt;0) err_sys(&quot;open()&quot;); // 阻塞，等待条件满足 read(fd,buf,BUFSIZ); printf(&quot;%s &quot;,buf); write(fd,&quot; World!&quot;,8); close(fd); exit(0); &#125;else&#123; fd = open(PATHNAME,O_RDWR); if(fd &lt; 0) err_sys(&quot;open()&quot;); // 写 write(fd,&quot;hello&quot;,6); sleep(1); // 要是不休眠就没有给另一个进程机会写，最后自娱自乐，第二个进程也打不开文件 read(fd,buf,BUFSIZ); close(fd); puts(buf); // 这个进程最后退出，所以把管道文件删除，不然下次在创建的时候会报文件已存在的错误 remove(PATHNAME); exit(0); &#125; return 0;&#125; 看到了下面的创建，不是普通的文件，而是管道文件 1prw-r--r-- 1 transcheung staff 0 2 14 11:41 mkfifof.txt 协同进程 管道是半双工的 两进程一个只能读，一个只能写要实现双工通信，必须采用两个管道，一个进程对一个管道只读，对另一个管道只写。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &quot;../include/apue.h&quot;#define BUFSIZE 1024int main()&#123; int pd[2]; int ipd[2]; char buf[BUFSIZE]; char dbuf[BUFSIZE]; pid_t pid; int len; // 创建匿名管道 if(pipe(pd)&lt;0) err_sys(&quot;pipe()&quot;); if(pipe(ipd)&lt;0) err_sys(&quot;pipe2&quot;); pid = fork(); if(pid == 0)&#123; // 子进程 读取管道数据 // 关闭写端 // close(pd[1]); // 从管道中读取数据，如果子进程比父进程先被调度会阻塞等待数据写入 //len = read(pd[0],buf,BUFSIZE); // puts(buf); /*** * 管道fork之前创建 * 父子进程都有一份 * 所有退出之前要确保管道两端都关闭 * ***/ close(pd[0]); write(pd[1],&quot;hello,child!&quot;,15); // sleep(10)； //sleep(10); len = read(ipd[0],dbuf,BUFSIZE); puts(dbuf); close(pd[1]); close(ipd[0]); exit(0); &#125;else&#123; // 父进程 向管道写入数据 // 关闭读端 //close(pd[0]); // 写端 // write(pd[1],&quot;Hello,world!&quot;,15); close(pd[1]); len = read(pd[0],buf,BUFSIZE); puts(buf); // 关闭读端 // sleep(5); write(ipd[1],&quot;hello parent!&quot;,BUFSIZE); close(pd[0]); close(ipd[1]); wait(NULL); exit(0); &#125;&#125; 两个管道，实现你来我往，mkfifo同理，两个文件就好了。 popen与pclosepopen和pclose提供了原子操作，创建一个管道，fork一个子进程，关闭未使用的管道端，执行一个个shell运行命令，然后等待命令终止 1234#include &lt;stdio.h&gt;FILE *popen(const char *cmdstring,const char *type);// 成功返回文件指针，出错，返回NULLint pclose(FILE *fp); popen先执行fork，然后调用exec执行cmdstring，并且返回一个标准IO文件指针。如果type是”r”，则文件指针连接到cmdstring的标准输出；如果是”w”，则文件指针连接到cmfstring的标准输入 由图可以看出，stdout和stdin是较于子进程而言的。 pclose关闭标准IO流，等待命令终止，返回shell的终止状态。如果shell不能被执行，则pclose返回的终止状态与shell已执行exit一样。 cmdstring fp = popen(&quot;ls *.c&quot;,&quot;r&quot;) fp = popen(&quot;cmd 2&gt;&amp;1&quot;,&quot;r&quot;) XSI IPC System V规范的进程间通信手段，而不是POSIX标准多进程与多线程多线程使用的基本是POSIZ标准提供的接口函数，而多进程则是基于System V。在信号量这种常用的同步互斥手段方面，POSIX在无竞争条件下是不会陷入内核的，而SystemV即无论何时都会陷入内核。这就给多线程每次调用都会陷入内核，丧失了线程的清亮优势。所以多线程之间的通信不是用System V。 ipcs命令可以查看CSI IPC的使用情况 ipcrm 命令可以删除指定的XSI IPC 通过查看ipcs 12345678IPC status from &lt;running system&gt; as of Fri Feb 15 10:31:54 CST 2019T ID KEY MODE OWNER GROUPMessage Queues:T ID KEY MODE OWNER GROUPShared Memory:T ID KEY MODE OWNER GROUPSemaphores: 第一部分是消息队列，第二部分是共享内存，第三部分是信号量数组 每一列都有一列叫做”key”，使用XSI IPC通信的进程就是通过同一个key值操作同一个共享资源的。key是一个正整数，与文件描述符不同的是，生成一个新key值时，不采用当前可用的数值的最小值，而是类似生成进程ID的方式，key的值连续➕1，直到达到一个整数的最大正值，然后回转到0从头开始累加。 XSI消息队列 让通信双方传送结构体数据，提高传送数据灵活性通信，就需要在通信之前双方约定通信协议，协议就是通信双方约定的数据交换格式。 从消息队列开始一直到Socket，都会看到类似的程序架构，无论是消息队列还是Socket，都需要约定通信协议，而且都是按照一定的步骤才能实现通讯。 消息队列在约定协议的时候，需要自定义结构体里要强制添加一个long mtype成员 这个成员的作用是用于区分多种消息类型中的不同类型的数据包，当只有一种类型的包时这个成员没什么作用，但是也要一定必须带上。 既然时通讯也要区分 主动端(先发包的一方) 和 被动端(先收包的一方，先运行)，它们运行的时机不同，作用不同甚至调用函数也不同，所以我们的后面的每个例子几乎都要编译处2个不同的可执行程序来测试。 msg、sem和shm都有一系列函数遵循 xxxget() &#x2F;&#x2F; 创建 xxxop() &#x2F;&#x2F; 相关操作 xxxctl() &#x2F;&#x2F; 其他的控制或销毁 1234567msgget - get a Syatem V message queue identifier#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/msg.h&gt;int msgget(key_t key,int msgflg); msgget函数的作用是创建一个消息队列，消息读列是双工的，两边都可以读写。 key相当于通信双方的街头暗号，拥有相同key的双方才可以通信 key值必须是唯一的，系统中有个ftok函数可以用于获取key，通过文件inode和salt进行hash运算来生成唯一的key， 只要两个进程使用相同的文件和salt就可以生成一样的key值了。 msgflg: 特殊要求。无论有多少特殊要求，只要使用了IPC_CREAT，就必须按位或一个权限，权限不是想指定多大就能多大，要用它&amp;&#x3D;~umask。 同一个消息队列只需要创建一次，所以谁先运行起来谁有责任创建消息队列，后运行起来的就不需要创建了。 同理，对于后启动的进程来说，消息队列不是他创建的，就没必要销毁了。 msgrcv函数和msgsnd函数 从msgid这个消息队列中接收数据12345678910111213msgrcv,msgsnd - message operations#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/msg.h&gt;int msgsnd(int msgid, const void *msgp,size_t msgsz,int msgflg);ssize_t msgrcv(int msgid,void *msgp,size_t msgsz,long msgtyp,int msgflg);// msgp成员的定义要类似msgbuf这个结构体，第一成员必须是long类型的mtype，并且必须是&gt;0的值struct msgbuf&#123; long mtype; // 消息类型 必须&gt;0 char mtext[1]; // 消息数据字段&#125;; msgrcv函数从msgid这个消息队列中接收数据，并将接收到的数据放到msgp结构体中，这段空间有msgsz这个字节的大小，msgsz的值要减掉强制的成员mtype的大小(sizeof(long))。 msgtyp是msgp结构体中的mtype的成员，表示需要接收那种类型的消息。虽然msg是消息队列，但是 它并不完全遵循队列的形式，可以让接收者挑消息接收。 如果不挑消息可以填写0，这样就按照队列中的消息顺序返回。 msgflg是特殊要求位图，没有写0 msgsnd函数向msgid这个消息队列发送msgp结构体数据，msgp的大小是msgsz，msgflg是特殊要求没有写0。 msgctl函数 跟iocrtl、fcntl函数用法类似1234567msgctl - message control operations#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/msg.h&gt;int msgctl(int msgid,int cmd,struct msqid_ds *buf);// 通过cmd指定具体命令，然后通过buf为cmd命令设定参数，当然有些命令是需要参数的，有些命令则不需要参数 最常用的cmd就是IPC_RMID，表示删除(结束)某个IPC通信，并且这个命令不需要buf参数，直接传入NULL即可。 buf结构体里面的成员很多。 1234567891011121314151617181920212223/*** * 共同的协议proto.h * ***//*定义双方都需要使用的数据或对象*/#ifndef PROTO_H__#define PROTO_H__#define NAMESIZE 32// 通讯双方生成key值共同使用的文件#define KEYPATH &quot;./test.txt&quot;// 通讯双方生成key值共同使用的salt值#define KEYPROJ &#x27;a&#x27;// 消息类型，只要是大于0的合法整数即可#define MSGTYPE 10// 通讯双方约定的协议struct msg_st&#123; long mtype; char name[NAMESIZE]; int math; int chinese;&#125;;#endif // PROTO_H__ 接收端要先运行，先创建接收端的消息队列 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include &quot;../include/apue.h&quot;#include &lt;sys/ipc.h&gt;#include &lt;sys/msg.h&gt;#include &quot;proto.h&quot;int main(void)&#123; key_t key; int msgid; struct msg_st rbuf; // 通过/tmp/out文件和字符&#x27;a&#x27;生成唯一的key，文件必须真实存在 key = ftok(KEYPATH,KEYPROJ); // 来自proto.h if(key&lt;0) err_sys(&quot;ftok()&quot;); // 接收端先启动，所以消息队列由接收端创建 if((msgid = msgget(key,IPC_CREAT|0600))&lt;0) err_sys(&quot;msgget&quot;); // 不停的接收消息 轮询 while(1)&#123; // 没有消息就会阻塞等待 if(msgrcv(msgid,&amp;rbuf,sizeof(rbuf)-sizeof(long),0,0)&lt;0) err_sys(&quot;msgrcv&quot;); /*** * 用结构体中强制添加的成员判断消息类型 * 当然这个栗子只有一种消息类型，所以不判断也可以 * 如果包含多种消息类型就可以协议组switch ...case结构 * ***/ if(rbuf.mtype == MSGTYPE)&#123; printf(&quot;Name = %s &quot;,rbuf.name); printf(&quot;Math = %d &quot;,rbuf.math); printf(&quot;Chinese = %d &quot;,rbuf.chinese); &#125; &#125; /*** * 谁创建谁销毁 * 这个程序无法正常结束只能等信号杀死 * 使用信号杀死之后可以用ipcs命令查看，消息队列应该没有被销毁 * 使用ipcrm删掉 * ***/ msgctl(msgid,IPC_RMID,NULL); // 销毁 exit(0);&#125; 发送端 123456789101112131415161718192021222324252627282930313233#include &quot;../include/apue.h&quot;#include &lt;sys/ipc.h&gt;#include &lt;sys/msg.h&gt;#include &lt;string.h&gt;#include &lt;time.h&gt;#include &quot;proto.h&quot;int main()&#123; key_t key; int msgid; struct msg_st sbuf; // 设置随机数种子 srand(time(NULL)); // 用于接收方相同的文件和salt生成一样的key key = ftok(KEYPATH,KEYPROJ); if(key&lt;0) err_sys(&quot;ftok()&quot;); // 取得消息队列 msgid = msgget(key,0); if(msgid&lt;0) err_sys(&quot;msgget&quot;); // 要发送的结构体赋值 sbuf.mtype = MSGTYPE; strcpy(sbuf.name,&quot;Trans&quot;); sbuf.math = rand()%100; sbuf.chinese = rand()%100; if(msgsnd(msgid,&amp;sbuf,sizeof(sbuf)-sizeof(long),0)&lt;0) err_sys(&quot;msgsnd&quot;); puts(&quot;ok!&quot;); exit(0);&#125; 最后使用ipcs查看，消息队列显式 12345678910IPC status from &lt;running system&gt; as of Fri Feb 15 12:21:02 CST 2019T ID KEY MODE OWNER GROUPMessage Queues:q 65536 0x6104a0ae --rw------- transcheung staffT ID KEY MODE OWNER GROUPShared Memory:T ID KEY MODE OWNER GROUPSemaphores: KEYPROJ直接充当salt值，接收方先运行，所以接收方先创建消息队列，发送方要使用相同的文件和salt生成于接收方相同的key值，这样才能使用同一个消息队列。 发送方发送一个结构体，接收方接收结构体并解析打印，所以这个结构体保证了数据能够正常被解析，所以 这个结构体就是我们所说的”协议”。 所以协议就是要保证一样的，所以写了一个proto.h文件，让发送方共同引用，就保证是相同的结构体了。 信号量(semget) 按部就班，步骤与消息队列差不多1234567semget - get a semaphore set identifier#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/sem.h&gt;int semget(key_t key,int nsems,int semflg);// 用于创建信号量，成功返回sem ID，失败返回-1并设置errnp key: 具有亲缘关系的进程之间可以使用一个匿名的key值，key使用宏IPC_PRIVATE即可 nsems:表示你到底有多少个sem。信号量其实是一个计数器，如果设置为1可以用来模拟互斥量 semflg:IPC_CREAT表示创建sem，同时需要按位或一个权限，如果是匿名IPC则无需执行这个宏，直接给权限就好 12345678semctl - semaphore control operations#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/sem.h&gt;int semctl(int semid, int semnum, int cmd, ...);// 用来控制或销毁信号量 semnum:信号量数组下标 cmd: 可选的宏。常用的由IPC_RMID，表示从系统中删除该信号量集合，SETVAL可以为第几个成员设置值。 …: 根据不同命令设置不同的参数，后面的参数是变长的 1234567891011121314semop - semaphore operations#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/sem.h&gt;int semop(int semid, struct sembuf *sops, unsigned nsops);// 操作信号量。由于多个信号量可以组成数组。sops参数是数组的起始位置，nspos是指定数组的长度// 成功返回0，失败返回-1并设置errnostruct sembuf &#123; unsigned short sem_num; /* 对第几个资源（数组下标）操作 */ short sem_op; /* 取几个资源写负数几(不要写减等于)，归还几个资源就写正数几 */ short sem_flg; /* 特殊要求 */&#125;; 信号量实际上是一个计数器，所以每次在使用资源之前，我们需要扣减信号量，当信号量被减到0时会阻塞等待。每次使用完成资源后，归还信号量，也就是增加信号量的数值 通过操作信号量的函数实现一个通过信号量实现互斥量的例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#include &quot;../include/apue.h&quot;#include &lt;string.h&gt;#include &lt;sys/wait.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/sem.h&gt;#include &lt;errno.h&gt;#define PROCNUM 20#define FNAME &quot;./test.txt&quot;#define BUFSIZE 1024// 多个函数都要使用这个信号量ID，所以定义为全局变量static int semid;// P操作static void P(void)&#123; struct sembuf op; op.sem_num = 0; // 只有一个资源所以数组下标是0 op.sem_op = -1; // 取一个资源就减1 op.sem_flg = 0; // 没有特殊要求 while(semop(semid,&amp;op,1)&lt;0)&#123; if(errno != EINTR &amp;&amp; errno!=EAGAIN) err_sys(&quot;semop()&quot;); &#125;&#125;// V操作static void V(void)&#123; struct sembuf op; op.sem_num = 0; op.sem_op = 1; op.sem_flg = 0; while(semop(semid,&amp;op,1)&lt;0)&#123; if(errno != EINTR &amp;&amp; errno != EAGAIN) err_sys(&quot;semop()&quot;); &#125;&#125;static void func_add()&#123; FILE *fp; char buf[BUFSIZE]; fp = fopen(FNAME,&quot;r+&quot;); if(fp == NULL) err_sys(&quot;fopen()&quot;); // 先取得信号量再操作文件，取不到就阻塞等待，避免发生竞争 P(); fgets(buf,BUFSIZE,fp); // 获取一行字符串 rewind(fp); // 从头指针开始 sleep(1); // 放大竞争 fprintf(fp,&quot;%d &quot;,atoi(buf)+1); fflush(fp); // 操作结束，归还信号量，让其他进程可以取得信号量 V(); fclose(fp); return;&#125;int main()&#123; int i; pid_t pid; // 在具有亲缘关系的进程之间使用，所以设置为IPC_PRIVATE // 另外想要实现互斥量的效果，所以信号量数量设置为1个即可 semid = semget(IPC_PRIVATE,1,0600); if(semid&lt;0) err_sys(&quot;semget()&quot;); // 将union semun.val的值设置为1 if(semctl(semid,0,SETVAL,1)&lt;0) err_sys(&quot;semctl()&quot;); // 创建20个子进程 for(i = 0;i&lt;PROCNUM;i++)&#123; pid = fork(); if(pid&lt;0) err_sys(&quot;fork()&quot;); if(pid == 0)&#123; func_add(); exit(0); &#125; &#125; for(i = 0;i&lt;PROCNUM;i++) wait(NULL); // 销毁信号量 semctl(semid,0,IPC_RMID); exit(0);&#125; 共享存储 shmgetXSI的共享内存，一样按命名规则来 12345shmget - allocates a shared memory segment#include &lt;sys/ipc.h&gt;#include &lt;sys/shm.h&gt;int shmget(key_t key,size_t size,int shmflg);// 成功返回shm ID；失败，返回-1 key: 共享内存的唯一标识，具有亲缘关系的进程之间使用共享内存可以使用IPC_PRIVATE宏代替 size: 是共享内存大小 shmflg: IPC_CREAT表示创建shm，同时需要按位或一个权限，如果是&#x3D;匿名IPC就无须指定这个宏，直接给权限就好 1234567shmat - shared memory operations#include &lt;sys/types.h&gt;#include &lt;sys/shm.h&gt;void *shmat(int shmid, const void *shmaddr, int shmflg);int shmdt(const void *shmaddr); shmat使进程与共享内存关联起来。shmat函数中的shmaddr参数是共享内存的起始地址，传入NULL由内核帮我们寻找合适的地址。一般情况我们都是传入NULL值。 shmdt函数用于使进程分离共享内存，共享内存使用完毕之后需要用这个函数分离。分离不代表释放了这块空间，使用共享内存的双方依然要遵守”谁申请，谁释放“的原则。没有申请的一方是不需要释放的，但是双方都需要分离。 12345shmctl - shared memory control#include &lt;sys/ipc.h&gt;#include &lt;sys/shm.h&gt;int shmctl(int shmid,int cmd,struct shmid_ds *buf);// 控制或删除共享内存 cmd设置IPC_RMID并且buf参数设置为NULL，就可以删除共享内存 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &quot;../include/apue.h&quot;#include &lt;sys/mman.h&gt;#include &lt;fcntl.h&gt;#include &lt;string.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/shm.h&gt;#include &lt;sys/wait.h&gt;#define MEMSIZE 1024int main()&#123; char *str; pid_t pid; int shmid; // 有亲缘关系的进程key参数可以使用IPC_PRIVATE宏，并且创建共享内存 // shmflg参数不需要使用IPC_CREAT宏 shmid = shmget(IPC_PRIVATE,MEMSIZE,0600); if(shmid &lt; 0) err_sys(&quot;shmget()&quot;); if((pid = fork())&lt;0) err_sys(&quot;fork()&quot;); if(pid==0)&#123;// 子进程 // 关联共享内存 str = shmat(shmid,NULL,0); if(str == (void*)-1) err_sys(&quot;shmat()&quot;); // 向共享内存写入数据 strcpy(str,&quot;hello!&quot;); // 分离共享内存 shmdt(str); // 无需释放 exit(0); &#125;else&#123; // 等待子进程结束后再运行，需要读取子进程写入的共享内存的数据 wait(NULL); // 关联共享内存 str = shmat(shmid,NULL,0); if(str == (void *)-1) err_sys(&quot;shmat()&quot;); // 打印读出来的数据 puts(str); // 分离共享内存 shmdt(str); // 释放共享内存 回收寺院 shmctl(shmid,IPC_RMID,NULL); exit(0); &#125; exit(0);&#125;","tags":["Linux开发"],"categories":["linux"]},{"title":"Linux 常用查看系统信息命令","path":"/2023/01/15/categories/嵌入式开发/Linux/Linux-常用查看系统信息命令/","content":"常用命令12345678910111213141516171819202122232425262728293031323334353637uname -a # 查看内核/操作系统/CPU信息head -n 1 /etc/issue # 查看操作系统版本 cat /proc/cpuinfo # 查看CPU信息 hostname # 查看计算机名 lspci -tv # 列出所有PCI设备 lsusb -tv # 列出所有USB设备 lsmod # 列出加载的内核模块 env # 查看环境变量资源 free -m # 查看内存使用量和交换区使用量 df -h # 查看各分区使用情况 du -sh &lt;目录名&gt; # 查看指定目录的大小 grep MemTotal /proc/meminfo # 查看内存总量 grep MemFree /proc/meminfo # 查看空闲内存量 uptime # 查看系统运行时间、用户数、负载 cat /proc/loadavg # 查看系统负载磁盘和分区 mount | column -t # 查看挂接的分区状态 fdisk -l # 查看所有分区 swapon -s # 查看所有交换分区 hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备) dmesg | grep IDE # 查看启动时IDE设备检测状况网络 ifconfig # 查看所有网络接口的属性 iptables -L # 查看防火墙设置 route -n # 查看路由表 netstat -lntp # 查看所有监听端口 netstat -antp # 查看所有已经建立的连接 netstat -s # 查看网络统计信息进程 ps -ef # 查看所有进程 top # 实时显示进程状态用户 w # 查看活动用户 id &lt;用户名&gt; # 查看指定用户信息 last # 查看用户登录日志 cut -d: -f1 /etc/passwd # 查看系统所有用户 cut -d: -f1 /etc/group # 查看系统所有组 crontab -l # 查看当前用户的计划任务服务 chkconfig –list # 列出所有系统服务 chkconfig –list | grep on # 列出所有启动的系统服务程序 rpm -qa # 查看所有安装的软件包 查看linux系统版本信息（Oracle Linux、Centos Linux、Redhat Linux、Debian、Ubuntu）一、查看Linux系统版本的命令（3种方法） 1、此命令也适用于所有的Linux发行版： 1cat /etc/issue 2、这种方法只适合Redhat系的Linux： 1cat /etc/redhat-release 3、列出所有版本信息： 1lsb_release -a 二、查看Linux内核版本命令（两种方法）： 1、此命令也适用于所有的Linux发行版： 1cat /proc/version 2、此命令也适用于所有的Linux发行版： 1uname -a 一、linux CPU大小 1cat /proc/cpuinfo |grep &quot;model name&quot; &amp;&amp; cat /proc/cpuinfo |grep &quot;physical id&quot; 说明：Linux下可以在&#x2F;proc&#x2F;cpuinfo中看到每个cpu的详细信息。但是对于双核的cpu，在cpuinfo中会看到两个cpu。常常会让人误以为是两个单核的cpu。其实应该通过Physical Processor ID来区分单核和双核。而Physical Processor ID可以从cpuinfo或者dmesg中找到. flags 如果有 ht 说明支持超线程技术 判断物理CPU的个数可以查看physical id 的值 二、内存大小 1cat /proc/meminfo |grep MemTotal 三、硬盘大小 1fdisk -l |grep Disk 四、查看内核&#x2F;操作系统&#x2F;CPU信息的linux系统信息命令 1uname -a 五、查看操作系统版本，是数字1不是字母L 1head -n 1 /etc/issue 六、查看CPU信息的linux系统信息命令 1cat /proc/cpuinfo 七、查看计算机名的linux系统信息命令 1hostname 八、列出所有PCI设备 1lspci -tv 九、列出所有USB设备的linux系统信息命令 1lsusb -tv 十、列出加载的内核模块 1lsmod 十一、查看环境变量资源 1env 十二、查看内存使用量和交换区使用量 1free -m 十三、查看各分区使用情况 1df -h 十四、查看指定目录的大小 1du -sh 十五、查看内存总量 1grep MemTotal /proc/meminfo 十六、查看空闲内存量 1grep MemFree /proc/meminfo 十七、查看系统运行时间、用户数、负载 1uptime 十八、查看系统负载磁盘和分区 1cat /proc/loadavg 十九、查看挂接的分区状态 1mount | column -t 二十、查看所有分区 1fdisk -l 二十一、查看所有交换分区 1swapon -s 二十二、查看磁盘参数(仅适用于IDE设备) 1hdparm -i /dev/hda 二十三、查看启动时IDE设备检测状况网络 1dmesg | grep IDE 二十四、查看所有网络接口的属性 1ifconfig 二十五、查看防火墙设置 1iptables -L 二十六、查看路由表 1route -n 二十七、查看所有监听端口 1netstat -lntp 二十八、查看所有已经建立的连接 1netstat -antp 二十九、查看网络统计信息进程 1netstat -s 三十、查看所有进程 1ps -ef 三十一、实时显示进程状态用户 1top 三十二、查看活动用户 1w 三十三、查看指定用户信息 1id 三十四、查看用户登录日志 1last 三十五、查看系统所有用户 1cut -d: -f1 /etc/passwd 三十六、查看系统所有组 1cut -d: -f1 /etc/group 三十七、查看当前用户的计划任务服务 1crontab -l 三十七、列出所有系统服务 1chkconfig –list 三十八、列出所有启动的系统服务程序 1chkconfig –list | grep on 三十九、查看所有安装的软件包 1rpm -qa 四十、查看CPU相关参数的linux系统命令 1cat /proc/cpuinfo 四十一、查看linux硬盘和分区信息的系统信息命令 1cat /proc/partitions 四十二、查看linux系统内存信息的linux系统命令 1cat /proc/meminfo 四十三、查看版本，类似uname -r 1cat /proc/version 四十四、查看设备io端口 1cat /proc/ioports 四十五、查看中断 1cat /proc/interrupts 四十六、查看pci设备的信息 1cat /proc/pci 四十七、查看所有swap分区的信息 1cat /proc/swaps 系统\t发行版本\t–\t内核版本、位数RedHat 1cat /etc/issue\tcat /etc/redhat-release\tlsb_release -a CentOS 1cat /etc/issue\tcat /etc/centos-release\tcat /proc/version Debian 1cat /etc/issue\tcat /etc/debian_version\tcat /proc/version Ubuntu 1cat /etc/issue\tcat /etc/lsb_release\tcat /proc/version Oracle 1cat /etc/issue\tcat /etc/oracle-release\tlsb_release -a","tags":["Linux开发"],"categories":["嵌入式","linux","常用查看系统信息命令"]},{"title":"x86 Ubuntu上建立aarch64/arm64 Ubuntu的交叉编译链","path":"/2023/01/15/categories/嵌入式开发/Linux/x86 Ubuntu上建立aarch64&arm64 Ubuntu的交叉编译链/","content":"注意：x86 Ubuntu下构建的aarch64 Ubuntu版本需要和程序最终运行的aarch64 Ubuntu的版本保持一致（如果x86 Ubuntu版本和构建的aarch64 Ubuntu版本一致，安装aarch64的编译工具会简单一些，直接apt install。本文以Ubuntu 20.04.03举例）。准备aarch64（即arm64）的文件系统rootfs；从http://cdimage.ubuntu.com/ubuntu-base/releases/下载对应版本的base镜像，之后将基于此镜像构建编译环境； 在x86 Ubuntu上新建一个rootfs目录，并将下载的base镜像解压到该目录； 12$ mkdir /path/to/rootfs$ tar -zxf ubuntu-base-20.04.3-base-arm64.tar.gz -C /path/to/rootfs 在x86 Ubuntu上安装chroot到&#x2F;path&#x2F;to&#x2F;rootfs所必须的软件（正常情况，因为可执行程序被编译目标架构不同，在x86 Ubuntu上无法执行aarch64 Ubuntu上的命令，也就无法在x86 Ubuntu上给aarch64 Ubuntu以apt的方式安装程序。然而qemu可以在x86 Ubuntu上构建一个aarch64 Ubuntu的运行环境，帮我们解决这个问题）； 1$ sudo apt-get install qemu qemu-user-static binfmt-support debootstrap 注册aarch64的运行环境（可以通过update-binfmts –display查看当前x86 Ubuntu上的注册情况）； 1$ sudo update-binfmts --enable qemu-aarch64 为了能在x86 Ubuntu上chroot到&#x2F;path&#x2F;to&#x2F;rootfs并执行aarch64的程序，还需要将qemu-aarch64-static拷贝到&#x2F;path&#x2F;to&#x2F;rootfs&#x2F;usr&#x2F;bin&#x2F;； 1$ sudo cp -av /usr/bin/qemu-aarch64-static /path/to/rootfs/usr/bin/ 配置arrch64 Ubuntu的网关； 1$ cp /etc/resolv.conf /path/to/rootfs/etc/resolv.conf chroot到aarch64 Ubuntu的rootfs，并安装开发库。 12$ sudo chroot /path/to/rootfs# apt update &amp;&amp; apt install -y libopencv-dev 准备x86上aarch64的编译工具。 1sudo apt-get install gcc-aarch64-linux-gnu g++-aarch64-linux-gnu 之后做arm程序开发，需要使用第三方库，只需要chroot到rootfs中安装，并通过配置CMakeLists.txt告诉CLion去&#x2F;path&#x2F;to&#x2F;rootfs寻库和去&#x2F;寻aarch64的编译工具gcc、g++就OK了。这样，在x86 Ubuntu上编译的aarch64程序就能在对应版本的aarch64 Ubuntu上运行了。","tags":["Linux开发"],"categories":["嵌入式","linux","交叉编译链"]},{"title":"进程间通信之管道","path":"/2023/01/15/linux-docs/进程管理/进程间通信之管道/","content":"一. 前言上文中我们介绍了进程间通信的方法之一：信号，本文将继续介绍另一种进程间通信的方法，即管道。管道是Linux中使用shell经常用到的一个技术，本文将深入剖析管道的实现和运行逻辑。 二. 管道简介在Linux的日常使用中，我们常常会用到管道，如下所示 ps -ef | grep 关键字 | awk &#39;&#123;print $2&#125;&#39; | xargs kill -9 这里面的竖线|就是一个管道。它会将前一个命令的输出，作为后一个命令的输入。从管道的这个名称可以看出来，管道是一种单向传输数据的机制，它其实是一段缓存，里面的数据只能从一端写入，从另一端读出。如果想互相通信，我们需要创建两个管道才行。 管道分为两种类型，| 表示的管道称为匿名管道，意思就是这个类型的管道没有名字，用完了就销毁了。就像上面那个命令里面的一样，竖线代表的管道随着命令的执行自动创建、自动销毁。用户甚至都不知道自己在用管道这种技术，就已经解决了问题。另外一种类型是命名管道。这个类型的管道需要通过 mkfifo 命令显式地创建。 mkfifo hello 我们可以往管道里面写入东西。例如，写入一个字符串。 # echo &quot;hello world&quot; &gt; hello 这个时候管道里面的内容没有被读出，这个命令就会停在这里。这个时候，我们就需要重新连接一个终端。在终端中用下面的命令读取管道里面的内容： # cat &lt; hello hello world 一方面，我们能够看到，管道里面的内容被读取出来，打印到了终端上；另一方面，echo 那个命令正常退出了。这就是有名管道的执行流程。 三. 匿名管道创建实际管道的创建调用的是系统调用pipe()，该函数建了一个管道 pipe，返回了两个文件描述符，这表示管道的两端，一个是管道的读取端描述符 fd[0]，另一个是管道的写入端描述符 fd[1]。 int pipe(int fd[2]) 其内核实现如下所示，pipe2 ()调用 __do_pipe_flags() 创建一个数组 files来存放管道的两端的打开文件，另一个数组 fd 存放管道的两端的文件描述符。如果 __do_pipe_flags() 没有错误，那就调用 fd_install()将两个 fd 和两个 struct file 关联起来，这一点和打开一个文件的过程类似。 1234567891011121314151617181920212223SYSCALL_DEFINE1(pipe, int __user *, fildes)&#123; return sys_pipe2(fildes, 0);&#125;SYSCALL_DEFINE2(pipe2, int __user *, fildes, int, flags)&#123; struct file *files[2]; int fd[2]; int error; error = __do_pipe_flags(fd, files, flags); if (!error) &#123; if (unlikely(copy_to_user(fildes, fd, sizeof(fd)))) &#123;...... error = -EFAULT; &#125; else &#123; fd_install(fd[0], files[0]); fd_install(fd[1], files[1]); &#125; &#125; return error;&#125; __do_pipe_flags()调用了create_pipe_files()生成fd，然后调用get_unused_fd_flags()赋值fdr和fdw，即读文件描述符和写文件描述符。由此也可以看出管道的特性：由一端写入，由另一端读出。 12345678910111213141516171819static int __do_pipe_flags(int *fd, struct file **files, int flags)&#123; int error; int fdw, fdr;...... error = create_pipe_files(files, flags);...... error = get_unused_fd_flags(flags);...... fdr = error; error = get_unused_fd_flags(flags);...... fdw = error; audit_fd_pair(fdr, fdw); fd[0] = fdr; fd[1] = fdw; return 0;......&#125; create_pipe_files()是管道创建的关键逻辑，从这里可以看出来管道实际上也是一种抽象的文件系统pipefs，有着对应的特殊文件以及inode。这里首先通过get_pipe_inode()获取特殊inode，然后调用alloc_file_pseudo()通过inode以及对应的挂载结构体pipe_mnt，文件操作结构体pipefifo_fops创建关联的dentry并以此创建文件结构体并分配内存，通过alloc_file_clone()创建一份新的file后将两个文件分别保存在res[0]和res[1]中。 1234567891011121314151617181920212223242526int create_pipe_files(struct file **res, int flags)&#123; struct inode *inode = get_pipe_inode(); struct file *f; if (!inode) return -ENFILE; f = alloc_file_pseudo(inode, pipe_mnt, &quot;&quot;, O_WRONLY | (flags &amp; (O_NONBLOCK | O_DIRECT)), &amp;pipefifo_fops); if (IS_ERR(f)) &#123; free_pipe_info(inode-&gt;i_pipe); iput(inode); return PTR_ERR(f); &#125; f-&gt;private_data = inode-&gt;i_pipe; res[0] = alloc_file_clone(f, O_RDONLY | (flags &amp; O_NONBLOCK), &amp;pipefifo_fops); if (IS_ERR(res[0])) &#123; put_pipe_info(inode, inode-&gt;i_pipe); fput(f); return PTR_ERR(res[0]); &#125; res[0]-&gt;private_data = inode-&gt;i_pipe; res[1] = f; return 0;&#125; 其虚拟文件系统pipefs对应的结构体和操作如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849static struct file_system_type pipe_fs_type = &#123; .name = &quot;pipefs&quot;, .mount = pipefs_mount, .kill_sb = kill_anon_super,&#125;;static int __init init_pipe_fs(void)&#123; int err = register_filesystem(&amp;pipe_fs_type); if (!err) &#123; pipe_mnt = kern_mount(&amp;pipe_fs_type); &#125;......&#125;const struct file_operations pipefifo_fops = &#123; .open = fifo_open, .llseek = no_llseek, .read_iter = pipe_read, .write_iter = pipe_write, .poll = pipe_poll, .unlocked_ioctl = pipe_ioctl, .release = pipe_release, .fasync = pipe_fasync,&#125;;static struct inode * get_pipe_inode(void)&#123; struct inode *inode = new_inode_pseudo(pipe_mnt-&gt;mnt_sb); struct pipe_inode_info *pipe;...... inode-&gt;i_ino = get_next_ino(); pipe = alloc_pipe_info();...... inode-&gt;i_pipe = pipe; pipe-&gt;files = 2; pipe-&gt;readers = pipe-&gt;writers = 1; inode-&gt;i_fop = &amp;pipefifo_fops; inode-&gt;i_state = I_DIRTY; inode-&gt;i_mode = S_IFIFO | S_IRUSR | S_IWUSR; inode-&gt;i_uid = current_fsuid(); inode-&gt;i_gid = current_fsgid(); inode-&gt;i_atime = inode-&gt;i_mtime = inode-&gt;i_ctime = current_time(inode); return inode;......&#125; 至此，一个匿名管道就创建成功了。如果对于 fd[1]写入，调用的是 pipe_write()，向 pipe_buffer 里面写入数据；如果对于 fd[0]的读入，调用的是 pipe_read()，也就是从 pipe_buffer 里面读取数据。至此，我们在一个进程内创建了管道，但是尚未实现进程间通信。 四. 匿名管道通信在上文中我们提到了匿名管道通过|符号实现进程间的通信，传递输入给下一个进程作为输出，其实现原理如下： 利用fork创建子进程，复制file_struct会同样复制fd输入输出数组，但是fd指向的文件仅有一份，即两个进程间可以通过fd数组实现对同一个管道文件的跨进程读写操作 禁用父进程的读，禁用子进程的写，即从父进程写入从子进程读出，从而实现了单向管道，避免了混乱 对于A|B来说，shell首先创建子进程A，接着创建子进程B，由于二者均从shell创建，因此共用fd数组。shell关闭读写，A开写B开读，从而实现了A 和B之间的通信。 接着我们需要调用dup2()实现输入输出和管道两端的关联，该函数会将fd赋值给fd2 12345678910111213141516/* Duplicate FD to FD2, closing the old FD2 and making FD2 be open the same file as FD is. Return FD2 or -1. */int__dup2 (int fd, int fd2)&#123; if (fd &lt; 0 || fd2 &lt; 0) &#123; __set_errno (EBADF); return -1; &#125; if (fd == fd2) /* No way to check that they are valid. */ return fd2; __set_errno (ENOSYS); return -1;&#125; 在 files_struct 里面，有这样一个表，下标是 fd，内容指向一个打开的文件 struct file。在这个表里面，前三项是定下来的，其中第零项 STDIN_FILENO 表示标准输入，第一项 STDOUT_FILENO 表示标准输出，第三项 STDERR_FILENO 表示错误输出。 123struct files_struct &#123; struct file __rcu * fd_array[NR_OPEN_DEFAULT];&#125; 在 A 进程写入端通过dup2(fd[1],STDOUT_FILENO)将 STDOUT_FILENO（也即第一项）不再指向标准输出，而是指向创建的管道文件，那么以后往标准输出写入的任何东西，都会写入管道文件。 在 B 进程中读取端通过dup2(fd[0],STDIN_FILENO)将 STDIN_FILENO 也即第零项不再指向标准输入，而是指向创建的管道文件，那么以后从标准输入读取的任何东西，都来自于管道文件。 至此，我们将 A|B 的功能完成。 五. 有名管道对于有名管道，我们需要通过mkfifo创建，实际调用__xmknod()函数，最终调用mknod()，和字符设备创建一样。 1234567891011121314151617181920212223/* Create a named pipe (FIFO) named PATH with protections MODE. */intmkfifo (const char *path, mode_t mode)&#123; dev_t dev = 0; return __xmknod (_MKNOD_VER, path, mode | S_IFIFO, &amp;dev);&#125;/* Create a device file named PATH, with permission and special bits MODE and device number DEV (which can be constructed from major and minor device numbers with the `makedev&#x27; macro above). */int__xmknod (int vers, const char *path, mode_t mode, dev_t *dev)&#123; unsigned long long int k_dev; if (vers != _MKNOD_VER) return INLINE_SYSCALL_ERROR_RETURN_VALUE (EINVAL); /* We must convert the value to dev_t type used by the kernel. */ k_dev = (*dev) &amp; ((1ULL &lt;&lt; 32) - 1); if (k_dev != *dev) return INLINE_SYSCALL_ERROR_RETURN_VALUE (EINVAL); return INLINE_SYSCALL (mknod, 3, path, mode, (unsigned int) k_dev);&#125; mknod 在字符设备那一节已经解析过了，先是通过 user_path_create() 对于这个管道文件创建一个 dentry，然后因为是 S_IFIFO，所以调用 vfs_mknod()。由于这个管道文件是创建在一个普通文件系统上的，假设是在 ext4 文件上，于是 vfs_mknod 会调用 ext4_dir_inode_operations 的 mknod，也即会调用 ext4_mknod()。 在 ext4_mknod() 中，ext4_new_inode_start_handle() 会调用 __ext4_new_inode()，在 ext4 文件系统上真的创建一个文件，但是会调用 init_special_inode()，创建一个内存中特殊的 inode，这个函数我们在字符设备文件中也遇到过，只不过当时 inode 的 i_fop 指向的是 def_chr_fops，这次换成管道文件了，inode 的 i_fop 变成指向 pipefifo_fops，这一点和匿名管道是一样的。这样，管道文件就创建完毕了。 接下来，要打开这个管道文件，我们还是会调用文件系统的 open() 函数。还是沿着文件系统的调用方式，一路调用到 pipefifo_fops 的 open() 函数，也就是 fifo_open()。在 fifo_open() 里面会创建 pipe_inode_info，这一点和匿名管道也是一样的。这个结构里面有个成员是 struct pipe_buffer *bufs。我们可以知道，所谓的命名管道，其实是也是内核里面的一串缓存。接下来，对于命名管道的写入，我们还是会调用 pipefifo_fops 的 pipe_write() 函数，向 pipe_buffer 里面写入数据。对于命名管道的读入，我们还是会调用 pipefifo_fops 的 pipe_read()，也就是从 pipe_buffer 里面读取数据。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596static int fifo_open(struct inode *inode, struct file *filp)&#123; struct pipe_inode_info *pipe; bool is_pipe = inode-&gt;i_sb-&gt;s_magic == PIPEFS_MAGIC; int ret; filp-&gt;f_version = 0; spin_lock(&amp;inode-&gt;i_lock); if (inode-&gt;i_pipe) &#123; pipe = inode-&gt;i_pipe; pipe-&gt;files++; spin_unlock(&amp;inode-&gt;i_lock); &#125; else &#123; spin_unlock(&amp;inode-&gt;i_lock); pipe = alloc_pipe_info(); if (!pipe) return -ENOMEM; pipe-&gt;files = 1; spin_lock(&amp;inode-&gt;i_lock); if (unlikely(inode-&gt;i_pipe)) &#123; inode-&gt;i_pipe-&gt;files++; spin_unlock(&amp;inode-&gt;i_lock); free_pipe_info(pipe); pipe = inode-&gt;i_pipe; &#125; else &#123; inode-&gt;i_pipe = pipe; spin_unlock(&amp;inode-&gt;i_lock); &#125; &#125; filp-&gt;private_data = pipe; /* OK, we have a pipe and it&#x27;s pinned down */ __pipe_lock(pipe); /* We can only do regular read/write on fifos */ filp-&gt;f_mode &amp;= (FMODE_READ | FMODE_WRITE); switch (filp-&gt;f_mode) &#123; case FMODE_READ: /* * O_RDONLY * POSIX.1 says that O_NONBLOCK means return with the FIFO * opened, even when there is no process writing the FIFO. */ pipe-&gt;r_counter++; if (pipe-&gt;readers++ == 0) wake_up_partner(pipe); if (!is_pipe &amp;&amp; !pipe-&gt;writers) &#123; if ((filp-&gt;f_flags &amp; O_NONBLOCK)) &#123; /* suppress EPOLLHUP until we have * seen a writer */ filp-&gt;f_version = pipe-&gt;w_counter; &#125; else &#123; if (wait_for_partner(pipe, &amp;pipe-&gt;w_counter)) goto err_rd; &#125; &#125; break; case FMODE_WRITE: /* * O_WRONLY * POSIX.1 says that O_NONBLOCK means return -1 with * errno=ENXIO when there is no process reading the FIFO. */ ret = -ENXIO; if (!is_pipe &amp;&amp; (filp-&gt;f_flags &amp; O_NONBLOCK) &amp;&amp; !pipe-&gt;readers) goto err; pipe-&gt;w_counter++; if (!pipe-&gt;writers++) wake_up_partner(pipe); if (!is_pipe &amp;&amp; !pipe-&gt;readers) &#123; if (wait_for_partner(pipe, &amp;pipe-&gt;r_counter)) goto err_wr; &#125; break; case FMODE_READ | FMODE_WRITE: /* * O_RDWR * POSIX.1 leaves this case &quot;undefined&quot; when O_NONBLOCK is set. * This implementation will NEVER block on a O_RDWR open, since * the process can at least talk to itself. */ pipe-&gt;readers++; pipe-&gt;writers++; pipe-&gt;r_counter++; pipe-&gt;w_counter++; if (pipe-&gt;readers == 1 || pipe-&gt;writers == 1) wake_up_partner(pipe); break; default: ret = -EINVAL; goto err; &#125; /* Ok! */ __pipe_unlock(pipe); return 0;......&#125; 总结无论是匿名管道还是命名管道，在内核都是一个文件。只要是文件就要有一个 inode。在这种特殊的 inode 里面，file_operations 指向管道特殊的 pipefifo_fops，这个 inode 对应内存里面的缓存。当我们用文件的 open 函数打开这个管道设备文件的时候，会调用 pipefifo_fops 里面的方法创建 struct file 结构，他的 inode 指向特殊的 inode，也对应内存里面的缓存，file_operations 也指向管道特殊的 pipefifo_fops。写入一个 pipe 就是从 struct file 结构找到缓存写入，读取一个 pipe 就是从 struct file 结构找到缓存读出。匿名管道和命名管道区别就在于匿名管道会通过dup2()指定输入输出源，完成之后立即释放，而命名管道通过mkfifo创建挂载后，需要手动调用pipe_read()和pipe_write()来完成其功能，表现到用户端即为前面提到的例子。","tags":["Linux开发"],"categories":["linux"]},{"title":"About me","path":"/about/index.html","content":"1$ hexo new &quot;My New Post&quot;"},{"title":"友情链接","path":"/friends/index.html","content":"本站信息 站名： 優萌初華 站长： 霜月琉璃 地址： https://shoka.lostyu.me 标志： {height&#x3D;”100” width&#x3D;”100”} 简介： 琉璃的医学 &amp; 编程笔记 申请方法 添加本站后，在本页留言，格式如下"},{"title":"About me","path":"/links/index.html","content":"1$ hexo new &quot;My New Post&quot;"}]